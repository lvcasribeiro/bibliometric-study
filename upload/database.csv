"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Molecular Sequence Numbers","Chemicals/CAS","Tradenames","Manufacturers","Funding Details","Funding Texts","References","Correspondence Address","Editors","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Abbreviated Source Title","Document Type","Publication Stage","Open Access","Source","EID"
"Wang Y.; Yang G.; Lu H.","Wang, Yisha (58114218100); Yang, Gang (56984691100); Lu, Hao (56654564800)","58114218100; 56984691100; 56654564800","Domain adaptive tree crown detection using high-resolution remote sensing images","2022","Journal of Applied Remote Sensing","16","4","044505","","","","0","10.1117/1.JRS.16.044505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148691214&doi=10.1117%2f1.JRS.16.044505&partnerID=40&md5=91589f6f76e3da16092d55470cb62a26","Beijing Forestry University, School of Information Science and Technology, Beijing, China; Eng. Res. Ctr. for Forest.-oriented Intelligent Info. Proc. of Natl. Forest. and Grass. Admin., Beijing, China","Wang Y., Beijing Forestry University, School of Information Science and Technology, Beijing, China, Eng. Res. Ctr. for Forest.-oriented Intelligent Info. Proc. of Natl. Forest. and Grass. Admin., Beijing, China; Yang G., Beijing Forestry University, School of Information Science and Technology, Beijing, China, Eng. Res. Ctr. for Forest.-oriented Intelligent Info. Proc. of Natl. Forest. and Grass. Admin., Beijing, China; Lu H., Beijing Forestry University, School of Information Science and Technology, Beijing, China, Eng. Res. Ctr. for Forest.-oriented Intelligent Info. Proc. of Natl. Forest. and Grass. Admin., Beijing, China","Rapid and accurate tree crown detection is significant to forestry management and precision forestry. However, the variability of the data is challenging for traditional deep learning-based methods in cross-regional scenarios. To solve the problem of low accuracy of existing methods due to the distribution characteristics of tree crowns and the special background in which they are located, we propose a multi-adversarial domain adaptive (DA) crown detection model, which saves much of labeling efforts needed for covering different features in different regions. First, we embed a multilevel transferable attention mechanism to extract the multiscale transferable features of the tree crown and combat the negative transfer caused by the background. Second, a multi-adversarial instance alignment module with entropy regularization and a weighting factor is designed to match the cross-regional data distributions to reduce the misdetection and misidentification of complex backgrounds. The above-mentioned adaptations are integrated into a two-stage object detector to realize end-to-end training. We conduct comprehensive experiments with four transfer tasks. Our method improves F1-score to 95.68%, 83.12%, 78.56%, and 66.89% and performs 3.61% and 15.38% better than the state-of-the-art domain adaptation approaches without extra inference costs. The results demonstrate the great potential of our method for DA tree crown detection, which provides a reference for further research on forestry detection. © 2022 Society of Photo-Optical Instrumentation Engineers (SPIE).","domain adaptation; multi-adversarial neural networks; multilevel transferable attention; tree crown detection","Object detection; Optical remote sensing; Timber; Adaptive tree; Domain adaptation; Forestry management; High-resolution remote sensing images; Multi-adversarial neural network; Multilevel transferable attention; Multilevels; Neural-networks; Tree crown detection; Tree crowns; Deep learning","","","","","","","Daliakopoulos I. N., Et al., Tree crown detection on multispectral VHR satellite imagery, Photogramm. Eng. Remote Sens, 75, 10, pp. 1201-1211, (2009); Wulder M., Niemann K. O., Goo De Nough D. G., Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery, Remote Sens. Environ, 73, 1, pp. 103-114, (2000); Chemura A., Duren I. V., Leeuwen L., Determination of the age of oil palm from crown projection area detected from worldview-2 multispectral remote sensing data: the case of Ejisu-Juaben district, Ghana, ISPRS J. Photogramm. Remote Sens, 100, pp. 118-127, (2015); Dalponte M., Et al., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data, Remote Sens. Environ, 140, pp. 306-317, (2014); Wang Y., Zhu X., Wu B., Automatic detection of individual oil palm trees from UAV images using hog features and an SVM classifier, Int. J. Remote Sens, 40, pp. 7356-7370, (2019); Pu R., Landry S., A comparative analysis of high spatial resolution IKONOS and worldview-2 imagery for mapping urban tree species, Remote Sens. Environ, 124, pp. 516-533, (2012); Hung C., Bryson M., Sukkarieh S., Multi-class predictive template for tree crown detection, ISPRS J. Photogramm. Remote Sens, 68, pp. 170-183, (2012); Krizhevsky A., Sutskever I., Hinton G. E., ImageNet classification with deep convolutional neural networks, Adv. Neural Inf. Process. Syst. 25: 26th Annu. Conf. Neural Inf. Process. Syst. 2012. Proc. Meeting Held December 3-6, pp. 1106-1114, (2012); Hu H., Xu Z., Wu G., A novel method of missing road generation in city blocks based on big mobile navigation trajectory data, ISPRS Int. J. Geo-Inf, 8, (2019); Li W., Et al., Semantic segmentation-based building footprint extraction using very highresolution satellite images and multi-source GIS data, Remote Sens, 11, 4, (2019); Di A., Et al., Combining sentinel-1 and sentinel-2 satellite image time series for land cover mapping via a multi-source deep learning architecture, ISPRS J. Photogramm. Remote Sens, 158, pp. 11-22, (2019); Dong R., Et al., Oil palm plantation mapping from high-resolution remote sensing images using deep learning, Int. J. Remote Sens, 41, pp. 2022-2046, (2019); Weijia L., Et al., Deep learning based oil palm tree detection and counting for highresolution remote sensing images, Remote Sens, 9, 1, (2016); Mubin N. A., Et al., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method, Int. J. Remote Sens, 40, pp. 7500-7515, (2019); Neupane B., Horanont T., Hung N. D., Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV), PLoS One, 14, 10, (2019); Ren S., Et al., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, 6, pp. 1137-1149, (2017); Lin T. Y., Et al., Feature pyramid networks for object detection, IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR), (2017); Liu W., Et al., SSD: Single Shot Multibox Detector, (2016); Redmon J., Et al., You only look once: unified, real-time object detection, IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR), (2016); Zheng J., Et al., Large-scale oil palm tree detection from high-resolution remote sensing images using faster-RCNN, IGARSS 2019-2019 IEEE Int. Geosci. Remote Sens. Symp, (2019); Santos A., Et al., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVs, Sensors, 19, 16, (2019); Puttemans S., Beeck K. V., Goedeme T., Comparing boosted cascades to deep learning architectures for fast and robust coconut tree detection in aerial images, Int. Conf. Comput. Vision Theory and Appl. (VISAPP), (2018); Wang M., Deng W., Deep visual domain adaptation: a survey, Neurocomputing, 312, pp. 135-153, (2018); Csurka G., Domain adaptation for visual applications: a comprehensive survey, (2017); Ghifary M., Kleijn W. B., Zhang M., Domain Adaptive Neural Networks for Object Recognition, (2014); Tzeng E., Et al., Deep domain confusion: maximizing for domain invariance, (2014); Long M., Wang J., Learning transferable features with deep adaptation networks, Int. Conf. Mach. Learn, pp. 97-105, (2015); Zhuang F., Et al., Supervised Representation Learning: Transfer Learning with Deep Autoencoders, (2015); You K., Et al., Towards accurate model selection in deep unsupervised domain adaptation, ICML, (2019); Wang X., Et al., Transferable normalization: towards improving transferability of deep neural networks, Adv. Neural Inf. Process. Syst. 32: Annu. Conf. Neural Inf. Process. Syst. 2019, NeurIPS 2019, pp. 1951-1961, (2019); Ganin Y., Lempitsky V. S., Unsupervised domain adaptation by backpropagation, Int. Conf. Mach. Learn, pp. 1180-1189, (2015); Ganin Y., Et al., Domain-adversarial training of neural networks, J. Mach. Learn. Res, 17, 1, pp. 2096-2030, (2016); Pei Z., Et al., Multi-adversarial domain adaptation, Proc. Thirty-Second AAAI Conf. Artif. Intell., (AAAI-18), the 30th Innov. Appl. of Artif. Intell. (IAAI-18), and the 8th AAAI Symp. Educ. Adv. in Artif. Intell. (EAAI-18), pp. 3934-3941, (2018); Chen X., Et al., Transferability vs. discriminability: batch spectral penalization for adversarial domain adaptation, Proc. 36th Int. Conf. Mach. Learn., ICML 2019, 9-15 June 2019, Proc. Mach. Learn. Res, 97, pp. 1081-1090; Wang X., Et al., Transferable calibration with lower bias and variance in domain adaptation, Adv. Neural Inf. Process. Syst. 33: Annu. Conf. Neural Inf. Process. Syst. 2020, NeurIPS 2020, (2020); Li C., Et al., MMD GAN: towards deeper understanding of moment matching network, Adv. Neural Inf. Process. Syst. 30: Annu. Conf. Neural Inf. Process. Syst, pp. 2203-2213, (2017); Long M., Et al., Conditional adversarial domain adaptation, Adv. Neural Inf. Process. Syst. 31: Annu. Conf. Neural Inf. Process. Syst. 2018, NeurIPS 2018, pp. 1647-1657, (2018); Hoffman J., Et al., Simultaneous deep transfer across domains and tasks, Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition, pp. 173-187, (2017); Zhang W., Et al., Collaborative and adversarial network for unsupervised domain adaptation, IEEE/CVF Conf. Comput. Vision and Pattern Recognit. (CVPR), (2018); Ghifary M., Et al., Deep reconstruction-classification networks for unsupervised domain adaptation, Eur. Conf. Comput. Vision, pp. 597-613, (2016); Kim T., Et al., Learning to discover cross-domain relations with generative adversarial networks, Int. Conf. Mach. Learn, pp. 1857-1865, (2017); Wu W., Et al., Cross-regional oil palm tree detection, IEEE/CVF Conf. Comput. Vision and Pattern Recognit. Workshops, (2020); Chen Y., Et al., Domain adaptive faster R-CNN for object detection in the wild, IEEE Conf. Comput. Vision and Pattern Recognit, (2018); Saito K., Et al., Strong-weak distribution alignment for adaptive object detection, IEEE/ CVF Conf. Comput. Vision and Pattern Recognit. (CVPR), (2019); Xu C. D., Et al., Exploring categorical regularization for domain adaptive object detection, IEEE Conf. Comput. Vision and Pattern Recognit, (2020); Everingham M., Et al., The Pascal visual object classes (VOC) challenge, Int. J. Comput. Vision, 88, 2, pp. 303-338, (2010); Cordts M., Et al., The cityscapes dataset for semantic urban scene understanding, IEEE Conf. Comput. Vision and Pattern Recognit, (2016); Jz A., Et al., Growing status observation for oil palm trees using unmanned aerial vehicle (UAV) images, ISPRS J. Photogramm. Remote Sens, 173, pp. 95-121, (2021); Wang X., Et al., Transferable attention for domain adaptation, Proc. AAAI Conf. Artif. Intell, 33, pp. 5345-5352, (2019); Wang F., Et al., Residual attention network for image classification, IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR 2017), pp. 6450-6458, (2017); Lin T. Y., Et al., Focal loss for dense object detection, IEEE Trans. Pattern Anal. Mach. Intell, 42, pp. 318-327, (2017); Paszke A., Et al., Pytorch: an imperative style, high-performance deep learning library, Adv. Neural Inf. Process. Syst. 32: Annu. Conf. Neural Inf. Process. Syst. 2019, NeurIPS 2019, pp. 8024-8035, (2019); He K., Et al., Deep residual learning for image recognition, IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR), (2016); Deng J., Et al., Imagenet: a large-scale hierarchical image database, IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recognit. (CVPR 2009), pp. 248-255, (2009); He K., Et al., Mask R-CNN, Proc. IEEE Int. Conf. Comput. Vision, pp. 2961-2969, (2017); Weinstein B. G., Et al., Individual tree-crown detection in RGB imagery using semisupervised deep learning neural networks, Remote. Sens, 11, 11, (2019)","G. Yang; Beijing Forestry University, School of Information Science and Technology, Beijing, China; email: yanggang@bjfu.edu.cn","","SPIE","","","","","","19313195","","JARSC","","English","J. Appl. Remote Sens.","Article","Final","","Scopus","2-s2.0-85148691214"
"Lin S.; He Z.; Sun L.","Lin, Song (57203682888); He, Zhiyong (13609182600); Sun, Lining (57218130781)","57203682888; 13609182600; 57218130781","A novel micro-defect classification system based on attention enhancement","2023","Journal of Intelligent Manufacturing","","","","","","","0","10.1007/s10845-022-02064-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145822978&doi=10.1007%2fs10845-022-02064-2&partnerID=40&md5=cb34692650cec1f4f8321ed503042083","Harbin Institute of Technology, Harbin, China; Soochow University, Soochow, China","Lin S., Harbin Institute of Technology, Harbin, China; He Z., Harbin Institute of Technology, Harbin, China, Soochow University, Soochow, China; Sun L., Harbin Institute of Technology, Harbin, China","A surface micro-defect is characterized by a small size and a susceptibility to noise. Micro-defect detection and classification is very challenging. This paper proposes a Micro-defect classification system based on attention enhancement (MDCS) for solving the detection and classification of micro-defects. We combine defect detection with defect classification in MDSC. Micro-defects classification can be better realized based on the auxiliary task of defect detection. In this system, the aim of attention formation in bionic vision is to guide the system to focus on the target by zooming in and out on micro-defects. To avoid noise interference, an attention module based on trilinear feature confluence has been incorporated. Last but not least, the enhancement process based on the attention map improves the classification ability of micro-defects. As part of comparative experiment, we analyzed data including 19,200 fabric images and 4,800 bamboo images. In the micro-defect classification experiment based on MDCS(ResNet-50), the accuracy of fabric data and bamboo data is 88.2% and 89.4% respectively. Compared with ResNet-50, the classification accuracy (64.8%, 67.7%) is improved by 23.4% and 21.7% respectively. In the object detection experiment of micro-defects based on MDCS (ResNet-50), the accuracy of fabric data and bamboo data is 65.1% mAPs and 63.3% mAPs respectively. Compared with HRDNet, the detection accuracy (59.6% mAPs, 52.2% mAPs) is improved by 5.5% mAPs and 11.1% mAPs respectively. Experimental results demonstrate that the proposed system can counteract the interference caused by noise in small object detection, localize micro-defects accurately, and improve micro-defect classification accuracy significantly. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural network; Deep learning; Defect classification; Intelligent manufacturing; Vision attention","Bamboo; Convolutional neural networks; Deep learning; Defects; Object recognition; Classification accuracy; Classification system; Convolutional neural network; Deep learning; Defect classification; Defect detection; Intelligent Manufacturing; Micro-defects; Surface micro-defects; Vision attention; Object detection","","","","","National Key Research and Development Program of China, NKRDPC, (2018YFB1309200)","This work was supported by the National Key R&D Program of China under Grant Number 2018YFB1309200. ","Anvar A., Cho Y.I., Automatic metallic surface defect detection using ShuffleDefectNet, J. Korea Soc. Comput. Inf., 25, pp. 19-26, (2020); Bochkovskiy A., Wang C.Y., Liao H., Yolov4: Optimal Speed and Accuracy of Object Detection, pp. 1-17, (2020); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs, IEEE Transactions on Pattern Analysis and Machine Intelligence, 40, 4, pp. 834-848, (2018); Chen S., Tan X., Wang B., Hu X., Reverse attention for salient object detection, Proceedings of the European Conference on Computer Vision, pp. 234-250, (2018); Cheng K.C., Chen L.L., Li J.W., Li K.S., Tsai N.C., Wang S.J., Huang A.Y., Chou L., Lee C.S., Chen J.E., Liang H.C., Machine learning based detection method for wafer test induced defects[J], IEEE Transactions on Semiconductor Manufacturing, 34, 2, pp. 161-167, (2021); Cheng P., Wang H., Stojanovic V., He S., Shi K., Luan X., Liu F., Sun C., Asynchronous fault detection observer for 2-D Markov jump systems[J], IEEE Transactions on Cybernetics, (2021); Cheng P., Wang H., Stojanovic V., Dissipativity-Based Finite-Time Asynchronous Output Feedback Control for Wind Turbine System via a Hidden Markov Model, pp. 3177-3189, (2022); Cohen N., Hoshen Y., Sub-Image Anomaly Detection with Deep Pyramid Correspondences, pp. 1-17, (2020); Defard T., Setkov A., Loesch A., Audigier R., Padim: A Patch Distribution Modeling Framework for Anomaly Detection and Localization, (2020); Dike H.U., Zhou Y., Deveerasetty K.K., Wu Q., Unsupervised learning based on artificial neural network: A review, Proceedings of the 2018 IEEE International Conference on Cyborg and Bionic Systems (CBS), pp. 322-327, (2018); Fu J., Zheng H., Tao M., Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition, (2017); He D., Xu K., Zhou P., Zhou D., Surface defect classification of steels with a new semi-supervised learning method, Optics and Lasers in Engineering, 117, pp. 40-48, (2019); Hsu C.Y., Liu W.C., Kusiak A., Multiple time-series convolutional neural network for fault detection and diagnosis and empirical study in semiconductor manufacturing, Journal of Intelligent Manufacturing, 32, (2021); Hu B., Wang J., Detection of PCB surface defects with improved Faster-RCNN and feature pyramid network, IEEE Access, 8, pp. 108335-108345, (2020); Hu T., Xu J., Huang C., Weakly Supervised Bilinear Attention Network for Fine-Grained Visual Classification, pp. 1-8, (2018); Jie H., Li S., Gang S., Squeeze-and-excitation networks, (2018); Kim Y., Cho D., Lee J.H., Wafer defect pattern classification with detecting out-of-distribution, Microelectronics Reliability, 122, 4, (2021); Lantzanakis G., Mitraka Z., Chrysoulakis N., X-SVM: An extension of C-SVM Algorithm for classification of high-resolution satellite imagery, IEEE Transactions on Geoscience and Remote Sensing, 59, (2020); Lee K., Maji S., Ravichandran A., Soatto S., Meta-learning with differentiable convex optimization, In CVPR, 6, (2019); Li A., Zhang J., Lv Y., Liu B., Zhang T., Dai Y., Uncertainty aware joint salient object and camouflaged object detection, Proceeding of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10071-10081, (2021); Li L., Ma W., Li L., Lu C., Research on detection algorithm for bridge cracks based on deep learning, Acta Autom Sin, 45, pp. 1727-1742, (2019); Li W., Wang L., Huo J., Shi Y., Gao Y., Luo J., (2020); Li Z., Yang Y., Liu X., Zhou F., Wen S., Xu W., Dynamic computational time for visual attention, pp. 1199-1209, (2017); Lin S., He Z., Sun L., Defect Enhancement Generative Adversarial Net-work for Enlarging Data Set of Microcrack Defect, IEEE Access, 7, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., (2016); Liu X., Sun L., Feng S., Incomplete multi-view partial multi-label learning, Applied Intelligence, 3, (2021); Liu Z., Gao G., Sun L., (2020); Mao Y., Zhang J., Wan Z., Transformer transforms salient object detection and camouflaged object detection., (2021); Mayr M., Hoffmann M., Maier A., Christlein V., Weakly supervised segmentation of cracks on solar cells using normalized Lp norm., pp. 1885-1889, (2019); Ozgenel C.F., Sorguc A.G., Performance comparison of pretrained convolutional neural networks on crack detection in buildings, Proceedings of the International Symposium on Automation and Robotics in Construction (IAARC), pp. 693-700, (2018); Qian X., Liu F., Jiao L., Zhang X., Guo Y., Liu X., Cui Y., Ridgelet-Nets with speckle reduction regularization for SAR image scene classification[J], IEEE Transactions on Geoscience and Remote Sensing, 59, 11, pp. 9290-9306, (2021); Roth K., Pemula L., Zepeda J., Scholkopf B., Brox T., Gehler P., Towards Total Recall in Industrial Anomaly Detection, (2021); Schlosser T., Friedrich M., Beuth F., Kowerko D., Improving automated visual fault inspection for semiconductor manufacturing using a hybrid multistage system of deep neural networks[J], Journal of Intelligent Manufacturing, 33, 4, pp. 1099-1123, (2022); Shen J., Chen P., Su L., Shi T., Tang Z., Liao G., X-ray inspection of TSV defects with self-organizing map network and Otsu algorithm, Microelectronics Reliability, 67, pp. 129-134, (2016); Silven O., Niskanen M., Kauppine H., Wood inspection with non-supervised clustering, Machine Vision and Applications, 13, pp. 275-285, (2003); Silvestre-Blanes J., Albero-Albero T., Miralles I., Perez-Llorens R., Moreno J., A public fabric database for defect detection methods and results, Autex Res J, 19, pp. 363-374, (2019); Sridharan N.V., Sugumaran V., Visual fault detection in photovoltaic modules using decision tree algorithms with deep learning features, Energy Sources Part A-Recovery Utilization and Environmental Effects, pp. 1-17, (2022); Sun M., Yuan Y., Zhou F., Ding E., Multi attention multi-class constraint for fine-grained image recognition, pp. 805-821, (2018); Tao X., Zhang D., Wang Z., Liu X., Zhang H., Xu D., Detection of Power Line Insulator Defects Using Aerial Images Analyzed with Convolutional Neural Networks. In IEEE Transactions on Systems, Man, and Cybernetics Systems, pp. 1486-1498, (2020); Wang G., Han S., Ding E., Huang D., Student–teacher Feature Pyramid Matching for Unsupervised Anomaly Detection, (2021); Wang X., Li J., Yao M., He W., Qian Y., Solar cells surface defects detection based on deep learning, Pattern Recognit Artif Intell, 27, pp. 517-523, (2014); Woo S., Park J., Lee J.Y., Et al., CBAM: Convolutional block attention module, Proceedings of the European Conference on Computer Vision (ECCV), pp. 3-19, (2018); Xie S., Girshickdollar R.P., Et al., (2016); Xu K., Et al., Show, attend and tell: Neural image caption generation with visual attention, pp. 2048-2057, (2015); Xu Z., Li X., Stojanovic V., Exponential stability of nonlinear state-dependent delayed impulsive systems with applications, Nonlinear Analysis Hybrid Systems, 42, 1, (2021); Yang F., Choi W., Lin Y., Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1, 2, pp. 2129-2137, (2016); Ye W., Li X., Dong Y., Et al., Single Image Surface Appearance Modeling with Self-augmented CNNs and Inexact Supervision[C]//John Wiley and Sons, pp. 201-211, (2018); Zhang Z., Wang X., Jung C., DCSR: Dilated convolutions for single image super-resolution, IEEE Transactions on Image Processing, 28, 4, pp. 1625-1635, (2019); Zheng H., Fu J., Mei T., Luo J., Learning multi-attention convolutional neural network for fine grained image recognition, pp. 5209-5217, (2017); Zheng H., Fu J., Zha Z.J., Et al., Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-Grained Image Recognition, (2019); Zheng H., Yang Y., Sun X., Wen C., Nondestructive detection of anchorage quality of rock bolt based on DS-DBN-SVM, Proceedings of the 2018 International Conference on Machine Learning and Cybernetics (ICMLC), pp. 288-293, (2018)","Z. He; Harbin Institute of Technology, Harbin, China; email: hezhiyong@suda.edu.cn","","Springer","","","","","","09565515","","JIMNE","","English","J Intell Manuf","Article","Article in press","","Scopus","2-s2.0-85145822978"
"Himeur Y.; Al-Maadeed S.; Varlamis I.; Al-Maadeed N.; Abualsaud K.; Mohamed A.","Himeur, Yassine (55636199800); Al-Maadeed, Somaya (36467897100); Varlamis, Iraklis (6603228762); Al-Maadeed, Noor (55479959400); Abualsaud, Khalid (26025901900); Mohamed, Amr (55763482600)","55636199800; 36467897100; 6603228762; 55479959400; 26025901900; 55763482600","Face Mask Detection in Smart Cities Using Deep and Transfer Learning: Lessons Learned from the COVID-19 Pandemic","2023","Systems","11","2","107","","","","1","10.3390/systems11020107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149232389&doi=10.3390%2fsystems11020107&partnerID=40&md5=70d3076090b7bfbe2d3d25ea68d957ca","Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar; College of Engineering and IT, University of Dubai, Dubai, 4343, United Arab Emirates; Department of Informatics and Telematics, Harokopio University of Athens, Omirou 9, Tavros, Athens, 17778, Greece","Himeur Y., Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar, College of Engineering and IT, University of Dubai, Dubai, 4343, United Arab Emirates; Al-Maadeed S., Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar; Varlamis I., Department of Informatics and Telematics, Harokopio University of Athens, Omirou 9, Tavros, Athens, 17778, Greece; Al-Maadeed N., Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar; Abualsaud K., Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar; Mohamed A., Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar","After different consecutive waves, the pandemic phase of Coronavirus disease 2019 does not look to be ending soon for most countries across the world. To slow the spread of the COVID-19 virus, several measures have been adopted since the start of the outbreak, including wearing face masks and maintaining social distancing. Ensuring safety in public areas of smart cities requires modern technologies, such as deep learning and deep transfer learning, and computer vision for automatic face mask detection and accurate control of whether people wear masks correctly. This paper reviews the progress in face mask detection research, emphasizing deep learning and deep transfer learning techniques. Existing face mask detection datasets are first described and discussed before presenting recent advances to all the related processing stages using a well-defined taxonomy, the nature of object detectors and Convolutional Neural Network architectures employed and their complexity, and the different deep learning techniques that have been applied so far. Moving on, benchmarking results are summarized, and discussions regarding the limitations of datasets and methodologies are provided. Last but not least, future research directions are discussed in detail. © 2023 by the authors.","deep domain adaptation; deep learning; deep transfer learning; face mask detection; MobileNet; YOLO","","","","","","","","Hyysalo J., Dasanayake S., Hannu J., Schuss C., Rajanen M., Leppanen T., Doermann D., Sauvola J., Smart mask–Wearable IoT solution for improved protection and personal health, Internet Things, 18, (2022); Himeur Y., Al-Maadeed S., Almadeed N., Abualsaud K., Mohamed A., Khattab T., Elharrouss O., Deep visual social distancing monitoring to combat COVID-19: A comprehensive survey, Sustain. Cities Soc, 85, (2022); Fanelli D., Piazza F., Analysis and forecast of COVID-19 spreading in China, Italy and France, Chaos Solitons Fractals, 134, (2020); Galbadage T., Peterson B.M., Gunasekera R.S., Does COVID-19 spread through droplets alone?, Front. Public Health, 8, (2020); Liao M., Liu H., Wang X., Hu X., Huang Y., Liu X., Brenan K., Mecha J., Nirmalan M., Lu J.R., A technical review of face mask wearing in preventing respiratory COVID-19 transmission, Curr. Opin. Colloid Interface Sci, 52, (2021); Agarwal C., Kaur I., Yadav S., Hybrid CNN-SVM Model for Face Mask Detector to Protect from COVID-19, Artificial Intelligence on Medical Data, pp. 419-426, (2023); Elharrouss O., Al-Maadeed S., Subramanian N., Ottakath N., Almaadeed N., Himeur Y., Panoptic segmentation: A review, arXiv, (2021); Liberatori B., Mami C.A., Santacatterina G., Zullich M., Pellegrino F.A., YOLO-Based Face Mask Detection on Low-End Devices Using Pruning and Quantization, Proceedings of the 2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO), pp. 900-905; Wakchaure A., Kanawade P., Jawale M., William P., Pawar A., Face Mask Detection in Realtime Environment using Machine Learning based Google Cloud, Proceedings of the 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC), pp. 557-561; Kuhl N., Martin D., Wolff C., Volkamer M., Healthy surveillance”: Designing a concept for privacy-preserving mask recognition AI in the age of pandemics, arXiv, (2020); Kaur G., Sinha R., Tiwari P.K., Yadav S.K., Pandey P., Raj R., Vashisth A., Rakhra M., Face Mask Recognition System using CNN Model, Neurosci. Inform, 2, (2021); Mohamed M.M., Nessiem M.A., Batliner A., Bergler C., Hantke S., Schmitt M., Baird A., Mallol-Ragolta A., Karas V., Amiriparian S., Et al., Face mask recognition from audio: The MASC database and an overview on the mask challenge, Pattern Recognit, 122, (2022); Mohamed S.K., Abdel Samee B.E., Social Distancing Model Utilizing Machine Learning Techniques, Advances in Data Science and Intelligent Data Communication Technologies for COVID-19, pp. 41-53, (2022); Selvakarthi D., Sivabalaselvamani D., Ashwath S., Kalaivanan A.A., Manikandan K., Pradeep C., Experimental Analysis using Deep Learning Techniques for Safety and Riskless Transport-A Sustainable Mobility Environment for Post Covid-19, Proceedings of the 2021 6th International Conference on Inventive Computation Technologies (ICICT), pp. 980-984; Ray S., Das S., Sen A., An intelligent vision system for monitoring security and surveillance of ATM, Proceedings of the 2015 Annual IEEE India Conference (INDICON), pp. 1-5; Chen Q., Sang L., Face-mask recognition for fraud prevention using Gaussian mixture model, J. Vis. Commun. Image Represent, 55, pp. 795-801, (2018); Tomas J., Rego A., Viciano-Tudela S., Lloret J., Incorrect facemask-wearing detection using convolutional neural networks with transfer learning, Healthcare, 9, (2021); Qin B., Li D., Identifying facemask-wearing condition using image super-resolution with classification network to prevent COVID-19, Sensors, 20, (2020); Rudraraju S.R., Suryadevara N.K., Negi A., Face mask detection at the fog computing gateway, Proceedings of the 2020 15th Conference on Computer Science and Information Systems (FedCSIS), pp. 521-524; Cabani A., Hammoudi K., Benhabiles H., Melkemi M., MaskedFace-Net–A dataset of correctly/incorrectly masked face images in the context of COVID-19, Smart Health, 19, (2021); Asghar M.Z., Albogamy F.R., Al-Rakhami M.S., Asghar J., Rahmat M.K., Alam M.M., Lajis A., Nasir H.M., Facial Mask Detection Using Depthwise Separable Convolutional Neural Network Model During COVID-19 Pandemic, Front. Public Health, 10, (2022); Jeevan G., Zacharias G.C., Nair M.S., Rajan J., An empirical study of the impact of masks on face recognition, Pattern Recognit, 122, (2022); Azeem A., Sharif M., Raza M., Murtaza M., A survey: Face recognition techniques under partial occlusion, Int. Arab J. Inf. Technol, 11, pp. 1-10, (2014); He L., Li H., Zhang Q., Sun Z., Dynamic feature learning for partial face recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7054-7063; Li Y., Guo K., Lu Y., Liu L., Cropping and attention based approach for masked face recognition, Appl. Intell, 51, pp. 3012-3025, (2021); Nagrath P., Jain R., Madan A., Arora R., Kataria P., Hemanth J., SSDMNV2: A real time DNN-based face mask detection system using single shot multibox detector and MobileNetV2, Sustain. Cities Soc, 66, (2021); Wang Z., Wang G., Huang B., Xiong Z., Hong Q., Wu H., Yi P., Jiang K., Wang N., Pei Y., Et al., Masked face recognition dataset and application, arXiv, (2020); Yang S., Luo P., Loy C.C., Tang X., Wider face: A face detection benchmark, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5525-5533; Ge S., Li J., Ye Q., Luo Z., Detecting masked faces in the wild with lle-cnns, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2682-2690; Bu W., Xiao J., Zhou C., Yang M., Peng C., A cascade framework for masked face detection, Proceedings of the 2017 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM), pp. 458-462; Bhandary P., Face Mask Dataset Datset (FMDS); Ding F., Peng P., Huang Y., Geng M., Tian Y., Masked face recognition with latent part detection, Proceedings of the 28th ACM International Conference on Multimedia, pp. 2281-2289; Witkowski M., Medical Masks Dataset (MMD); Jangra A., Face Mask 12k Images Dataset; Makwana D., Face Mask Classification; Dey S.K., Howlader A., Deb C., MobileNet mask: A multi-phase face mask detection model to prevent person-to-person transmission of SARS-CoV-2, International Conference on Trends in Computational and Cognitive Engineering, pp. 603-613, (2021); Bhandary P., Simulated Masked Face Dataset; Queiroz L., Oliveira H., Yanushkevich S., Thermal-mask–a dataset for facial mask detection and breathing rate measurement, Proceedings of the 2021 International Conference on Information and Digital Technologies (IDT), pp. 142-151; Ullah N., Javed A., Ghazanfar M.A., Alsufyani A., Bourouis S., A novel DeepMaskNet model for face mask detection and masked facial recognition, J. King Saud Univ.-Comput. Inf. Sci, 34, pp. 9905-9914, (2022); Singh S., Ahuja U., Kumar M., Kumar K., Sachdeva M., Face mask detection using YOLOv3 and faster R-CNN models: COVID-19 environment, Multimed. Tools Appl, 80, pp. 19753-19768, (2021); Batagelj B., Peer P., Struc V., Dobrisek S., How to Correctly Detect Face-Masks for COVID-19 from Visual Information?, Appl. Sci, 11, (2021); Roy B., Nandy S., Ghosh D., Dutta D., Biswas P., Das T., MOXA: A deep learning based unmanned approach for real-time monitoring of people wearing medical masks, Trans. Indian Natl. Acad. Eng, 5, pp. 509-518, (2020); Irem Eyiokur F., Kemal Ekenel H., Waibel A., A Computer Vision System to Help Prevent the Transmission of COVID-19, arXiv, (2021); Wang B., Zhao Y., Chen C.P., Hybrid transfer learning and broad learning system for wearing mask detection in the covid-19 era, IEEE Trans. Instrum. Meas, 70, pp. 1-12, (2021); Jiang X., Gao T., Zhu Z., Zhao Y., Real-time face mask detection method based on YOLOv3, Electronics, 10, (2021); Hussain S., Yu Y., Ayoub M., Khan A., Rehman R., Wahid J.A., Hou W., IoT and deep learning based approach for rapid screening and face mask detection for infection spread control of COVID-19, Appl. Sci, 11, (2021); Nowrin A., Afroz S., Rahman M.S., Mahmud I., Cho Y.Z., Comprehensive review on facemask detection techniques in the context of covid-19, IEEE Access, 9, pp. 106839-106864, (2021); Mita T., Kaneko T., Hori O., Joint haar-like features for face detection, Proceedings of the Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1, 2, pp. 1619-1626; Jauhari A., Anamisa D., Negara Y., Detection system of facial patterns with masks in new normal based on the Viola Jones method, J. Phys. Conf. Ser, 1836, (2021); Viola P., Jones M., Rapid object detection using a boosted cascade of simple features, Proceedings of the 2001 IEEE Computer society Conference on Computer Vision and Pattern Recognition, CVPR 2001, 1; Chelbi S., Mekhmoukh A., A practical implementation of mask detection for COVID-19 using face detection and histogram of oriented gradients, Aust. J. Electr. Electron. Eng, 19, pp. 129-136, (2022); Yoon S.M., Kee S.C., Detection of Partially Occluded Face Using Support Vector Machines, Proceedings of the MVA, pp. 546-549; Sharifara A., Rahim M.S.M., Anisi Y., A general review of human face detection including a study of neural networks and Haar feature-based cascade classifier in face detection, Proceedings of the 2014 International Symposium on Biometrics and Security Technologies (ISBAST), pp. 73-78; Colombo A., Cusano C., Schettini R., Gappy PCA classification for occlusion tolerant 3D face detection, J. Math. Imaging Vis, 35, pp. 193-207, (2009); Ichikawa K., Mita T., Hori O., Kobayashi T., Component-based face detection method for various types of occluded faces, Proceedings of the 2008 3rd International Symposium on Communications, Control and Signal Processing, pp. 538-543; Thom N., Hand E.M., Facial attribute recognition: A survey, Computer Vision: A Reference Guide, pp. 1-13, (2020); Bhandari M., Shahi T.B., Siku B., Neupane A., Explanatory classification of CXR images into COVID-19, Pneumonia and Tuberculosis using deep learning and XAI, Comput. Biol. Med, 150, (2022); Li J., Jin K., Zhou D., Kubota N., Ju Z., Attention mechanism-based CNN for facial expression recognition, Neurocomputing, 411, pp. 340-350, (2020); Sayed A.N., Himeur Y., Bensaali F., Deep and transfer learning for building occupancy detection: A review and comparative analysis, Eng. Appl. Artif. Intell, 115, (2022); Himeur Y., Elnour M., Fadli F., Meskin N., Petri I., Rezgui Y., Bensaali F., Amira A., Next-generation energy systems for sustainable smart cities: Roles of transfer learning, Sustain. Cities Soc, 85, (2022); Koklu M., Cinar I., Taspinar Y.S., CNN-based bi-directional and directional long-short term memory network for determination of face mask, Biomed. Signal Process. Control, 71, (2022); Militante S.V., Dionisio N.V., Deep Learning Implementation of Facemask and Physical Distancing Detection with Alarm Systems, Proceedings of the 2020 Third International Conference on Vocational Education and Electrical Engineering (ICVEE), pp. 1-5; Chavda A., Dsouza J., Badgujar S., Damani A., Multi-stage cnn architecture for face mask detection, Proceedings of the 2021 6th International Conference for Convergence in Technology (I2CT), pp. 1-8; Din N.U., Javed K., Bae S., Yi J., A novel GAN-based network for unmasking of masked face, IEEE Access, 8, pp. 44276-44287, (2020); Geng M., Peng P., Huang Y., Tian Y., Masked face recognition with generative data augmentation and domain constrained ranking, Proceedings of the 28th ACM International Conference on Multimedia, pp. 2246-2254; Ge S., Li C., Zhao S., Zeng D., Occluded face recognition in the wild by identity-diversity inpainting, IEEE Trans. Circuits Syst. Video Technol, 30, pp. 3387-3397, (2020); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Computer Vision—ECCV 2016. ECCV 2016, 9905, (2016); Anithadevi N., Abinisha J., Akalya V., Haripriya V., An Improved SSD Object Detection Algorithm For Safe Social Distancing and Face Mask Detection In Public Areas Through Intelligent Video Analytics, Proceedings of the 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT), pp. 1-7; Wang Z., Wang P., Louis P.C., Wheless L.E., Huo Y., Wearmask: Fast in-browser face mask detection with serverless edge computing for covid-19, arXiv, (2021); Loey M., Manogaran G., Taha M.H.N., Khalifa N.E.M., Fighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection, Sustain. Cities Soc, 65, (2021); Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Pi Y., Nath N.D., Sampathkumar S., Behzadan A.H., Deep Learning for Visual Analytics of the Spread of COVID-19 Infection in Crowded Urban Environments, Nat. Hazards Rev, 22, (2021); Ahmed I., Ahmad M., Rodrigues J.J., Jeon G., Din S., A deep learning-based social distance monitoring framework for COVID-19, Sustain. Cities Soc, 65, (2021); Shalini G., Margret M.K., Niraimathi M.S., Subashree S., Social Distancing Analyzer Using Computer Vision and Deep Learning, J. Phys. Conf. Ser, 1916, (2021); Widiatmoko F., Berchmans H.J., Setiawan W., Computer Vision and Deep Learning Approach for Social Distancing Detection During COVID-19 Pandemic, Ph.D. Thesis, (2021); Wu P., Li H., Zeng N., Li F., FMD-Yolo: An efficient face mask detection method for COVID-19 prevention and control in public, Image Vis. Comput, 117, (2022); Basu A., Ali M.F., COVID-19 Face Mask Recognition with Advanced Face Cut Algorithm for Human Safety Measures, Proceedings of the 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT), pp. 1-5; Nagaraj P., Phebe G.S., Singh A., A Novel Technique to Classify Face Mask for Human Safety, Proceedings of the 2021 Sixth International Conference on Image Information Processing (ICIIP), 6, pp. 235-239; Anthoniraj S., Face Mask Detection with Computer Vision & Deep Learning, Proceedings of the 2021 International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA), pp. 1-4; Subhash S., Sneha K., Ullas A., Raj D., A COVID-19 Safety Web Application to Monitor Social Distancing and Mask Detection, Proceedings of the 2021 IEEE 9th Region 10 Humanitarian Technology Conference (R10-HTC), pp. 1-6; Setyawan N., Putri T.S.N.P., Al Fikih M., Kasan N., Comparative Study of CNN and YOLOv3 in Public Health Face Mask Detection, Proceedings of the 2021 8th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI), pp. 354-358; Liu S., Agaian S.S., COVID-19 face mask detection in a crowd using multi-model based on YOLOv3 and hand-crafted features, Proceedings of the Multimodal Image Exploitation and Learning 2021; Gawde B.B., A Fast, Automatic Risk Detector for COVID-19, Proceedings of the 2020 IEEE Pune Section International Conference (PuneCon), pp. 146-151; He J., Mask detection device based on YOLOv3 framework, Proceedings of the 2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE), pp. 268-271; Aswal V., Tupe O., Shaikh S., Charniya N.N., Single camera masked face identification, Proceedings of the 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 57-60; Ren X., Liu X., Mask wearing detection based on YOLOv3, J. Phys. Conf. Ser, 1678, (2020); Darawsheh A., Siam A.A., Shaar L.A., Odeh A., High-performance Detection and Predication Safety System using HUAWEI Atlas 200 DK AI Developer Kit, Proceedings of the 2022 2nd International Conference on Computing and Information Technology (ICCIT), pp. 213-216; Kumar K.S., Kumar G.A., Rajendra P.P., Gatti R., Kumar S.S., Nataraja N., Face Mask Detection and Temperature Scanning for the Covid-19 Surveillance System, Proceedings of the 2021 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT), pp. 985-989; Rakhsith L., Karthik B., Nithish D.A., Kumar V.K., Anusha K., Face Mask and Social Distancing Detection for Surveillance Systems, Proceedings of the 2021 5th International Conference on Trends in Electronics and Informatics (ICOEI), pp. 1056-1065; Prabha P.A., Karthikeyan G., Kuttralanathan K., Venkatesun M.M., Intelligent Mask Detection Using Deep Learning Techniques, J. Phys. Conf. Ser, 1916, (2021); Zhang K., Jia X., Wang Y., Zhang H., Cui J., Detection System of Wearing Face Masks Normatively Based on Deep Learning, Proceedings of the 2021 International Conference on Control Science and Electric Power Systems (CSEPS), pp. 35-39; Amin P.N., Moghe S.S., Prabhakar S.N., Nehete C.M., Deep Learning Based Face Mask Detection and Crowd Counting, Proceedings of the 2021 6th International Conference for Convergence in Technology (I2CT), pp. 1-5; Xiang Y., Yang H., Hu R., Hsu C.Y., Comparison of the Deep Learning Methods Applied on Human Eye Detection, Proceedings of the 2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA), pp. 314-318; Avanzato R., Beritelli F., Russo M., Russo S., Vaccaro M., YOLOv3-Based Mask and Face Recognition Algorithm for Individual Protection Applications; Bhuiyan M.R., Khushbu S.A., Islam M.S., A deep learning based assistive system to classify COVID-19 face mask for human safety with YOLOv3, Proceedings of the 2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT), pp. 1-5; Li C., Cao J., Zhang X., Robust deep learning method to detect face masks, Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture, pp. 74-77; Vinh T.Q., Anh N.T.N., Real-Time Face Mask Detector Using YOLOv3 Algorithm and Haar Cascade Classifier, Proceedings of the 2020 International Conference on Advanced Computing and Applications (ACOMP), pp. 146-149; Glowacka N., Ruminski J., Face with Mask Detection in Thermal Images Using Deep Neural Networks, Sensors, 21, (2021); Mahurkar R.R., Gadge N.G., Real-time COVID-19 Face Mask Detection with YOLOv4, Proceedings of the 2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC), pp. 1250-1255; Protik A.A., Rafi A.H., Siddique S., Real-time Personal Protective Equipment (PPE) Detection Using YOLOv4 and TensorFlow, Proceedings of the 2021 IEEE Region 10 Symposium (TENSYMP), pp. 1-6; Prasad P., Chawla A., Mohana, Facemask Detection to Prevent COVID-19 Using YOLOv4 Deep Learning Model, Proceedings of the 2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS), pp. 382-388; Qin Z., Guo Z., Lin Y., An Implementation of Face Mask Detection System Based on YOLOv4 Architecture, Proceedings of the 2022 14th International Conference on Computer Research and Development (ICCRD), pp. 207-213; Gupta A., Thapar D., Deb S., Smart Camera for Enforcing Social Distancing, Proceedings of the 2021 IEEE International Symposium on Smart Electronic Systems (iSES) (Formerly iNiS), pp. 349-354; Ubaid M.T., Khan M.Z., Rumaan M., Arshed M.A., Khan M.U.G., Darboe A., COVID-19 SOP’s Violations Detection in Terms of Face Mask Using Deep Learning, Proceedings of the 2021 International Conference on Innovative Computing (ICIC), pp. 1-8; Vella S., Scerri D., Vision-based Health Protocol Observance System for Small Rooms, Proceedings of the 2021 IEEE 11th International Conference on Consumer Electronics (ICCE-Berlin), pp. 1-6; Mokeddem M.L., Belahcene M., Bourennane S., Yolov4FaceMask: COVID-19 Mask Detector, Proceedings of the 2021 1st International Conference On Cyber Management And Engineering (CyMaEn), pp. 1-6; Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271; Cao Z., Shao M., Xu L., Mu S., Qu H., MaskHunter: Real-time object detection of face masks during the COVID-19 pandemic, IET Image Process, 14, pp. 4359-4367, (2020); Liu S., Qi L., Qin H., Shi J., Jia J., Path aggregation network for instance segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8759-8768; He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell, 37, pp. 1904-1916, (2015); Rahim A., Maqbool A., Rana T., Monitoring social distancing under various low light conditions with deep learning and a single motionless time of flight camera, PLoS ONE, 16, (2021); Rodriguez C.R., Luque D., La Rosa C., Esenarro D., Pandey B., Deep learning applied to capacity control in commercial establishments in times of COVID-19, Proceedings of the 2020 12th International Conference on Computational Intelligence and Communication Networks (CICN), pp. 423-428; Pandya S., Sur A., Solke N., COVIDSAVIOR: A Novel Sensor-Fusion and Deep Learning Based Framework for Virus Outbreaks, Front. Public Health, 9, (2021); Gola A., Panesar S., Sharma A., Ananthakrishnan G., Singal G., Mukhopadhyay D., MaskNet: Detecting Different Kinds of Face Mask for Indian Ethnicity, Proceedings of the International Advanced Computing Conference, pp. 492-503, (2020); Kumar A., Kalia A., Verma K., Sharma A., Kaushal M., Scaling up face masks detection with YOLO on a novel dataset, Optik, 239, (2021); Jocher G., Stoken A., Borovec J., Changyu L., Hogan A., Diaconu L., Ingham F., Poznanski J., Fang J., Yu L., Et al., ultralytics/yolov5: v3. 1-Bug Fixes and Performance Improvements. Version v3. 2020; Volume 1; Yap M.H., Hachiuma R., Alavi A., Brungel R., Cassidy B., Goyal M., Zhu H., Ruckert J., Olshansky M., Huang X., Et al., Deep learning in diabetic foot ulcers detection: A comprehensive evaluation, Comput. Biol. Med, 135, (2021); Walia I.S., Kumar D., Sharma K., Hemanth J.D., Popescu D.E., An Integrated Approach for Monitoring Social Distancing and Face Mask Detection Using Stacked ResNet-50 and YOLOv5, Electronics, 10, (2021); Ottakath N., Elharrouss O., Almaadeed N., Al-Maadeed S., Mohamed A., Khattab T., Abualsaud K., ViDMASK dataset for face mask detection with social distance measurement, Displays, 73, (2022); Jiang M., Fan X., Yan H., Retinamask: A face mask detector, arXiv, (2020); Deng J., Guo J., Ververas E., Kotsia I., Zafeiriou S., Retinaface: Single-shot multi-level face localisation in the wild, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5203-5212; Zhu R., Yin K., Xiong H., Tang H., Yin G., Masked Face Detection Algorithm in the Dense Crowd Based on Federated Learning, Wirel. Commun. Mob. Comput, 2021, (2021); Nguyen Quoc H., Truong Hoang V., Real-time human ear detection based on the joint of yolo and retinaface, Complexity, 2021, (2021); Addagarla S.K., Chakravarthi G.K., Anitha P., Real time multi-scale facial mask detection and classification using deep transfer learning techniques, Int. J. Adv. Trends Comput. Sci. Eng, 9, pp. 4402-4408, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Gathani J., Shah K., Detecting masked faces using region-based convolutional neural network, Proceedings of the 2020 IEEE 15th International Conference on Industrial and Information Systems (ICIIS), pp. 156-161; Meivel S., Devi K.I., Maheswari S.U., Menaka J.V., Real time data analysis of face mask detection and social distance measurement using Matlab, Mater. Today Proc, (2021); Zhang J., Han F., Chun Y., Chen W., A novel detection framework about conditions of wearing face mask for helping control the spread of covid-19, IEEE Access, 9, pp. 42975-42984, (2021); Sahraoui Y., Kerrache C.A., Korichi A., Nour B., Adnane A., Hussain R., DeepDist: A Deep-Learning-Based IoV Framework for Real-Time Objects and Distance Violation Detection, IEEE Internet Things Mag, 3, pp. 30-34, (2020); Gupta P., Sharma V., Varma S., A novel algorithm for mask detection and recognizing actions of human, Expert Syst. Appl, 198, (2022); Joshi A.S., Joshi S.S., Kanahasabai G., Kapil R., Gupta S., Deep learning framework to detect face masks from video footage, Proceedings of the 2020 12th International Conference on Computational Intelligence and Communication Networks (CICN), pp. 435-440; Loey M., Manogaran G., Taha M.H.N., Khalifa N.E.M., A hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic, Measurement, 167, (2021); Sethi S., Kathuria M., Kaushik T., Face mask detection using deep learning: An approach to reduce risk of Coronavirus spread, J. Biomed. Inform, 120, (2021); Snyder S.E., Husari G., Thor: A Deep Learning Approach for Face Mask Detection to Prevent the COVID-19 Pandemic, Proceedings of the SoutheastCon 2021, pp. 1-8; Yang C.W., Phung T.H., Shuai H.H., Cheng W.H., Mask or Non-Mask? Robust Face Mask Detector via Triplet-Consistency Representation Learning, ACM Trans. Multimed. Comput. Commun. Appl. (TOMM), 18, pp. 1-20, (2022); Kumar T.A., Rajmohan R., Pavithra M., Ajagbe S.A., Hodhod R., Gaber T., Automatic face mask detection system in public transportation in smart cities using IoT and deep learning, Electronics, 11, (2022); Bansal A., Dhayal S., Mishra J., Grover J., COVID-19 Outbreak: Detecting face mask types in real time, J. Inf. Optim. Sci, 43, pp. 357-370, (2022); Gupta S., Sreenivasu S., Chouhan K., Shrivastava A., Sahu B., Potdar R.M., Novel face mask detection technique using machine learning to control COVID’19 pandemic, Mater. Today Proc, (2021); Mundial I.Q., Hassan M.S.U., Tiwana M.I., Qureshi W.S., Alanazi E., Towards facial recognition problem in COVID-19 pandemic, Proceedings of the 2020 4rd International Conference on Electrical, Telecommunication and Computer Engineering (ELTICOM), pp. 210-214; Lin H., Tse R., Tang S.K., Chen Y., Ke W., Pau G., Near-realtime face mask wearing recognition based on deep learning, Proceedings of the 2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC), pp. 1-7; Cao Z., Simon T., Wei S.E., Sheikh Y., Realtime multi-person 2d pose estimation using part affinity fields, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7291-7299; Asif S., Wenhui Y., Tao Y., Jinhai S., Amjad K., Real Time Face Mask Detection System using Transfer Learning with Machine Learning Method in the Era of Covid-19 Pandemic, Proceedings of the 2021 4th International Conference on Artificial Intelligence and Big Data (ICAIBD), pp. 70-75; Habib S., Alsanea M., Aloraini M., Al-Rawashdeh H.S., Islam M., Khan S., An Efficient and Effective Deep Learning-Based Model for Real-Time Face Mask Detection, Sensors, 22, (2022); Sen S., Sawant K., Face mask detection for covid_19 pandemic using pytorch in deep learning, IOP Conf. Ser. Mater. Sci. Eng, 1070, (2021); Sanjaya S.A., Rakhmawan S.A., Face Mask Detection Using MobileNetV2 in The Era of COVID-19 Pandemic, Proceedings of the 2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI), pp. 1-5; Boulila W., Alzahem A., Almoudi A., Afifi M., Alturki I., Driss M., A Deep Learning-based Approach for Real-time Facemask Detection, arXiv, (2021); Lad A.M., Mishra A., Rajagopalan A., Comparative Analysis of Convolutional Neural Network Architectures for Real Time COVID-19 Facial Mask Detection, J. Phys. Conf. Ser, 1969, (2021); Nayak R., Manohar N., Computer-Vision based Face Mask Detection using CNN, Proceedings of the 2021 6th International Conference on Communication and Electronics Systems (ICCES), pp. 1780-1786; Taneja S., Nayyar A., Nagrath P., Face Mask Detection Using Deep Learning During COVID-19, Second International Conference on Computing, Communications, and Cyber-Security, pp. 39-51, (2021); Kayali D., Dimililer K., Sekeroglu B., Face Mask Detection and Classification for COVID-19 using Deep Learning, Proceedings of the 2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA), pp. 1-6; Singh R., Singh I., Kapoor A., Chawla A., Gupta A., Co-Yudh: A Convolutional Neural Network (CNN)-Inspired Platform for COVID Handling and Awareness, Sn Comput. Sci, 3, (2022); Aadithya V., Balakumar S., Bavishprasath M., Raghul M., Malathi P., Comparative Study Between MobilNet Face-Mask Detector and YOLOv<sub>3</sub> Face-Mask Detector, Sustainable Communication Networks and Application, pp. 801-809, (2022); Talahua J.S., Buele J., Calvopina P., Varela-Aldas J., Facial recognition system for people with and without face mask in times of the covid-19 pandemic, Sustainability, 13, (2021); Kumar A., A cascaded deep-learning-based model for face mask detection, Data Technol. Appl, (2022); Al-Hamid A.A., Kim T., Park T., Kim H., Optimization of Object Detection CNN With Weight Quantization and Scale Factor Consolidation, Proceedings of the 2021 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia), pp. 1-5; Bharathi S., Hari K., Senthilarasi M., Sudhakar R., An Automatic Real-Time Face Mask Detection using CNN, Proceedings of the 2021 Smart Technologies, Communication and Robotics (STCR), pp. 1-5; Jaisharma K., Nithin A., A Deep Learning Based Approach for Detection of Face Mask Wearing using YOLO V3-tiny Over CNN with Improved Accuracy, Proceedings of the 2022 International Conference on Business Analytics for Technology and Security (ICBATS), pp. 1-5; Liu G., Zhang Q., Mask Wearing Detection Algorithm Based on Improved Tiny YOLOv3, Int. J. Pattern Recognit. Artif. Intell, 35, (2021); Jiang X., Xiang F., Lv M., Wang W., Zhang Z., Yu Y., YOLOv3-Slim for Face Mask Recognition, J. Phys. Conf. Ser, 1771, pp. 1-9, (2021); Sathyamurthy K.V., Rajmohan A.S., Tejaswar A.R., Kavitha V., Manimala G., Realtime Face Mask Detection Using TINY-YOLO V4, Proceedings of the 2021 4th International Conference on Computing and Communications Technologies (ICCCT), pp. 169-174; Zhao Z., Hao K., Ma X., Liu X., Zheng T., Xu J., Cui S., SAI-YOLO: A Lightweight Network for Real-Time Detection of Driver Mask-Wearing Specification on Resource-Constrained Devices, Comput. Intell. Neurosci, 2021, (2021); Han Z., Huang H., Fan Q., Li Y., Li Y., Chen X., SMD-YOLO: An efficient and lightweight detection method for mask wearing status during the COVID-19 pandemic, Comput. Methods Programs Biomed, 221, (2022); Anand R., Das J., Sarkar P., Comparative Analysis of YOLOv4 and YOLOv4-tiny Techniques towards Face Mask Detection, Proceedings of the 2021 International Conference on Computational Performance Evaluation (ComPE), pp. 803-809; Kumar A., Kalia A., Kalia A., ETL-YOLO v4: A face mask detection algorithm in era of COVID-19 pandemic, Optik, 259, (2022); Hraybi S., Rizk M., Examining YOLO for Real-Time Face-Mask Detection, Proceedings of the 4th Smart Cities Symposium (SCS 2021); Zhu J., Wang J., Wang B., Lightweight mask detection algorithm based on improved YOLOv4-tiny, Chin. J. Liq. Cryst. Disp, 36, pp. 1525-1534, (2021); Farman H., Khan T., Khan Z., Habib S., Islam M., Ammar A., Real-Time Face Mask Detection to Ensure COVID-19 Precautionary Measures in the Developing Countries, Appl. Sci, 12, (2022); Aydemir E., Yalcinkaya M.A., Barua P.D., Baygin M., Faust O., Dogan S., Chakraborty S., Tuncer T., Acharya U.R., Hybrid deep feature generation for appropriate face mask use detection, Int. J. Environ. Res. Public Health, 19, (2022); Chowdary G.J., Punn N.S., Sonbhadra S.K., Agarwal S., Face mask detection using transfer learning of inceptionv3, Proceedings of the International Conference on Big Data Analytics, pp. 81-90, (2020); Inamdar M., Mehendale N., Real-Time Face Mask Identification Using Facemasknet Deep Learning Network, (2020); Rahman M.M., Manik M.M.H., Islam M.M., Mahmud S., Kim J.H., An automated system to limit COVID-19 using facial mask detection in smart city network, Proceedings of the 2020 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS), pp. 1-5; Mohan P., Paul A.J., Chirania A., A tiny CNN architecture for medical face mask detection for resource-constrained endpoints, Innovations in Electrical and Electronic Engineering, pp. 657-670, (2021); Yu J., Zhang W., Face mask wearing detection algorithm based on improved YOLO-v4, Sensors, 21, (2021); Vrigkas M., Kourfalidou E.A., Plissiti M.E., Nikou C., FaceMask: A New Image Dataset for the Automated Identification of People Wearing Masks in the Wild, Sensors, 22, (2022)","Y. Himeur; Department of Computer Science and Engineering, Qatar University, Doha P.O. Box 2713, Qatar; email: yhimeur@ud.ac.ae","","MDPI","","","","","","20798954","","","","English","Systems","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85149232389"
"Wang B.; Wang Z.; Sun X.; Wang H.; Fu K.","Wang, Bing (57216234840); Wang, Zhirui (56071876600); Sun, Xian (34875643000); Wang, Hongqi (55689018000); Fu, Kun (7202283802)","57216234840; 56071876600; 34875643000; 55689018000; 7202283802","DMML-Net: Deep Metametric Learning for Few-Shot Geographic Object Segmentation in Remote Sensing Imagery","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","5","10.1109/TGRS.2021.3116672","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117114886&doi=10.1109%2fTGRS.2021.3116672&partnerID=40&md5=adf3020c5f1a53534f4773b3ec276d1f","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, School of Electronic, Electrical and Communication Engineering, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, School of Electronic, Electrical and Communication Engineering, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, School of Electronic, Electrical and Communication Engineering, Beijing, 100190, China","Wang B., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China, Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China, University of Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese Academy of Sciences, School of Electronic, Electrical and Communication Engineering, Beijing, 100190, China; Wang Z., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China, Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; Sun X., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China, Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China, University of Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese Academy of Sciences, School of Electronic, Electrical and Communication Engineering, Beijing, 100190, China; Wang H., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China, Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China; Fu K., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China, Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Network Information System Technology (NIST), Beijing, 100190, China, University of Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese Academy of Sciences, School of Electronic, Electrical and Communication Engineering, Beijing, 100190, China","Geographic object segmentation is a fundamental yet challenging problem for remote sensing image interpretation. The prevalent paradigm to solve this problem is to train deep neural networks on massive labeled samples. Although remarkable achievements have been attained, these methods suffer from the severe dependence on the large-scale dataset and require a long training process with high computation burden. To address these issues, a deep metametric learning framework, named DMML-Net, consisting of the metametric learner and the base-metric learner, is proposed for few-shot geographic object segmentation. First, DMML-Net formulates the segmentation as the metric-based pixel classification and develops a deep feature pyramid comparison network as the architecture of the metric learner for multiscale metric learning. Benefiting from this design, the segmentation can be efficiently solved, as well as being robust to deal with the scale variations of geographic objects. Second, an affinity-based fusion mechanism is introduced to adaptively reweight and fuse the semantic information across samples, effectively calibrating the deviation of prototypes induced by the intraclass variations. Third, considering the impact of the large interclass distribution divergences, DMML-Net presents a metametric training paradigm to provide the metric model with flexible scalability for fast adaptation to novel tasks. After metatraining, DMML-Net can be applied for the few-shot segmentation tasks of novel geographic objects with only a few gradient steps on the small training set. Experimental results on two benchmark remote sensing datasets demonstrate the validity and the superiority of our method in low-shot conditions where there are only one to ten labeled samples. 1558-0644 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Image segmentation; Measurement; Object segmentation; Remote sensing; Semantics; Task analysis; Training","Deep neural networks; Job analysis; Remote sensing; Semantics; Few-shot learning; Few-shot segmentation; Images segmentations; Metalearning; Metric learning; Objects segmentation; Remote-sensing; Semantic segmentation; Semantic segmentation.; Shot segmentation; Task analysis; algorithm; detection method; remote sensing; satellite imagery; segmentation; Image segmentation","","","","","National Natural Science Foundation of China, (10.13039/501100001809)","","Ienco D., Interdonato R., Gaetano R., Minh D.H.T., Combining Sentinel-1 and Sentinel-2 satellite image time series for land cover mapping via a multi-source deep learning architecture, ISPRS J. Photogramm. Remote Sens., 158, pp. 11-22, (2019); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on convolutional neural networks (CNN) in vegetation remote sensing, ISPRS J. Photogramm. Remote Sens., 173, pp. 24-49, (2021); Wurm M., Stark T., Zhu X.X., Weigand M., Taubenbock H., Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks, ISPRS J. Photogramm. Remote Sens., 150, pp. 59-69, (2019); Dey V., Zhang Y., Zhong M., A review on image segmentation techniques with remote sensing perspective, Proc. ISPRS TC VII Symp. Years, 38, pp. 31-42, (2010); Hossain M.D., Chen D., Segmentation for object-based image analysis (OBIA): A review of algorithms and challenges from remote sensing perspective, ISPRS J. Photogram. Remote Sens., 150, pp. 115-134, (2019); Kotaridis I., Lazaridou M., Remote sensing image segmentation advances: A meta-analysis, ISPRS J. Photogramm. Remote Sens., 173, pp. 309-322, (2021); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Boston, MA, USA, pp. 3431-3440, (2015); Yue K., Yang L., Li R., Hu W., Zhang F., Li W., Tree-UNet: Adaptive tree convolutional neural networks for subdecimeter aerial image segmentation, ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13, (2019); Marmanis D., Schindler K., Wegner J.D., Galliani S., Datcu M., Stilla U., Classification with An Edge: Improving Semantic Image Segmentation with Boundary Detection, (2016); Paisitkriangkrai S., Sherrah J., Janney P., Hengel Den A.Van, Semantic labeling of aerial and satellite imagery, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 9, 7, pp. 2868-2881, (2016); Sun X., Wang P., Wang C., Liu Y., Fu K., PBNet: Part-based convolutional neural network for complex composite object detection in remote sensing imagery, ISPRS J. Photogramm. Remote Sens., 173, pp. 50-65, (2021); Caruana R., Lawrence S., Giles C.L., Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping, Proc. Adv. Neural Inf. Process. Syst., Denver, CO, USA, pp. 402-408, (2000); Thrun S., Pratt L.Y., Learning to learn: Introduction and overview, Learning to Learn. Boston, MA, USA: Spring, pp. 3-17, (1998); Fei-Fei L., Fergus R., Perona P., One-shot learning of object categories, IEEE Trans. Pattern Anal. Mach. Intell., 28, 4, pp. 594-611, (2006); Sun X., Wang B., Wang Z., Li H., Li H., Fu K., Research progress on few-shot learning for remote sensing image interpretation, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 14, pp. 2387-2402, (2021); Wang K., Liew J.H., Zou Y., Zhou D., Feng J., PANet: Few-shot image semantic segmentation with prototype alignment, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, Oct./Nov, pp. 9196-9205, (2019); Zhang C., Lin G., Liu F., Yao R., Shen C., CANet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Long Beach, CA, USA, pp. 5217-5226, (2019); Snell J., Swersky K., Zemel R.S., Prototypical networks for few-shot learning, Proc. 31st Int. Conf. Neural Inf. Process. Syst., Long Beach, CA, USA, pp. 4077-4087, (2017); Zamir S.W., Et al., ISAID: A large-scale dataset for instance segmentation in aerial images, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 28-37, (2019); Zhao H., Shi J., Qi X., Wang X., Jia J., Pyramid scene parsing network, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Honolulu, HI, USA, pp. 6230-6239, (2017); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell., 40, 4, pp. 834-848, (2017); Peng C., Zhang X., Yu G., Luo G., Sun J., Large kernel matters- Improve semantic segmentation by global convolutional network, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Honolulu, HI, USA, pp. 1743-1751, (2017); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs, (2014); Wang X., Girshick R., Gupta A., He K., Non-local neural networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7794-7803, (2018); Huang Z., Wang X., Huang L., Huang C., Wei Y., Liu W., CCNet: Criss-cross attention for semantic segmentation, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, pp. 603-612, (2019); Li X., Zhong Z., Wu J., Yang Y., Lin Z., Liu H., Expectationmaximization attention networks for semantic segmentation, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Seoul, South Korea, pp. 9166-9175, (2019); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent., Munich, Germany, pp. 234-241, (2015); Lin G., Milan A., Shen C., Reid I., RefineNet: Multi-path refinement networks for high-resolution semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Honolulu, HI, USA, pp. 5168-5177, (2017); Chen L.C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoderdecoder with atrous separable convolution for semantic image segmentation, Proc. Eur. Conf. Comput. Vis. (ECCV), Munich, Germany, pp. 833-851, (2018); Penatti O.A.B., Nogueira K., Santos J.A.D., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Boston, MA, USA, pp. 44-51, (2015); Volpi M., Tuia D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 55, 2, pp. 881-893, (2016); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., High-resolution aerial image labeling with convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 55, 12, pp. 7092-7103, (2017); Mou L., Hua Y., Zhu X.X., A relation-augmented fully convolutional network for semantic segmentation in aerial scenes, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Long Beach, CA, USA, pp. 12408-12417, (2019); Chen W., Jiang Z., Wang Z., Cui K., Qian X., Collaborative global-local networks for memory-efficient segmentation of ultra-high resolution images, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 8924-8933, (2019); Ding L., Zhang J., Bruzzone L., Semantic segmentation of large-size VHR remote sensing images using a two-stage multiscale training architecture, IEEE Trans. Geosci. Remote Sens., 58, 8, pp. 5367-5376, (2020); Nogueira K., Mura M.D., Chanussot J., Schwartz W.R., Santos J.A.D., Dynamic multicontext segmentation of remote sensing images based on convolutional networks, IEEE Trans. Geosci. Remote Sens., 57, 10, pp. 7503-7520, (2019); Wang Y., Yao Q., Kwok J.T., Ni L.M., Generalizing from a few examples: A survey on few-shot learning, ACM Comput. Surv., 53, 3, pp. 1-34, (2020); Lu J., Gong P., Ye J., Zhang C., Learning from Very Few Samples: A Survey, (2020); Cao T., Law M.T., Fidler S., A Theoretical Analysis of the Number of Shots in Few-shot Learning, (2020); Vinyals O., Blundell C., Lillicrap T., Kavukcuoglu K., Wierstra D., Matching networks for one shot learning, Proc. Adv. Neural Inf. Process. Syst., Barcelona, Spain, pp. 3630-3638, (2016); Sung F., Yang Y., Zhang L., Xiang T., Torr P.H.S., Hospedales T.M., Learning to compare: Relation network for few-shot learning, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Salt Lake City, UT, USA, pp. 1199-1208, (2018); Ravi S., Larochelle H., Optimization as a model for few-shot learning, Proc. 5th Int. Conf. Learn. Represent. (ICLR), Toulon, France, pp. 1-11, (2016); Hochreiter S., Schmidhuber J., Long short-term memory, Neural Comput., 9, 8, pp. 1735-1780, (1997); Finn C., Abbeel P., Levine S., Model-agnostic meta-learning for fast adaptation of deep networks, Proc. 34th Int. Conf. Mach. Learn. (ICML), Sydney, NSW, Australia, pp. 1126-1135, (2017); Cheng G., Li R., Lang C., Han J., Task-wise attention guided part complementary learning for few-shot image classification, Sci. China Inf. Sci., 64, 2, (2021); Cheng G., Yang C., Yao X., Guo L., Han J., When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs, IEEE Trans. Geosci. Remote Sens., 56, 5, pp. 2811-2821, (2018); Li H., Et al., RS-MetaNet: Deep metametric learning for few-shot remote sensing scene classification, IEEE Trans. Geosci. Remote Sens., 59, 8, pp. 6983-6994, (2021); Li L., Han J., Yao X., Cheng G., Guo L., DLA-MatchNet for few-shot remote sensing image scene classification, IEEE Trans. Geosci. Remote Sens., 59, 9, pp. 7844-7853, (2021); Cheng G., Et al., Prototype-CNN for few-shot object detection in remote sensing images, IEEE Trans. Geosci. Remote Sens., Early Access, Jul. 29, (2021); Li X., Deng J., Fang Y., Few-shot object detection on remote sensing images, IEEE Trans. Geosci. Remote Sens., Early Access, Feb. 24, (2021); Wang L., Bai X., Gong C., Zhou F., Hybrid inference network for few-shot SAR automatic target recognition, IEEE Trans. Geosci. Remote Sens., Early Access, Jan. 22, (2021); Cheng G., Et al., SPNet: Siamese-prototype network for few-shot remote sensing image scene classification, IEEE Trans. Geosci. Remote Sens., Early Access, Jul. 29, (2021); Liu Y., Zhang L., Han Z., Chen C., Integrating knowledge distillation with learning to rank for few-shot scene classification, IEEE Trans. Geosci. Remote Sens., Early Access, Jul. 15, (2021); Niu C., Zhang J., Wang Q., Liang J., Weakly supervised semantic segmentation for joint key local structure localization and classification of aurora image, IEEE Trans. Geosci. Remote Sens., 56, 12, pp. 7133-7146, (2018); Wang S., Chen W., Xie S.M., Azzari G., Lobell D.B., Weakly supervised deep learning for segmentation of remote sensing imagery, Remote Sens., 12, 2, (2020); Tan K., Li E., Du Q., Du P., An efficient semi-supervised classification approach for hyperspectral imagery, ISPRS J. Photogramm. Remote Sens., 97, pp. 36-45, (2014); Romaszewski M., Glomb P., Cholewa M., Semi-supervised hyperspectral classification from a small number of training samples using a co-training approach, ISPRS J. Photogramm. Remote Sens., 121, pp. 60-76, (2016); Sun X., Shi A., Huang H., Mayer H., BAS4Net: Boundaryaware semi-supervised semantic segmentation network for very high resolution remote sensing images, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 13, pp. 5398-5413, (2020); Pan S.J., Yang Q., A survey on transfer learning, IEEE Trans. Knowl. Data Eng., 22, 10, pp. 1345-1359, (2010); Xu Q., Yuan X., Ouyang C., Class-aware domain adaptation for semantic segmentation of remote sensing images, IEEE Trans. Geosci. Remote Sens., Early Access, Nov. 17, (2020); Tasar O., Giros A., Tarabalka Y., Alliez P., Clerc S., DAugNet: Unsupervised, multisource, multitarget, and life-long domain adaptation for semantic segmentation of satellite images, IEEE Trans. Geosci. Remote Sens., 59, 2, pp. 1067-1081, (2021); Shaban A., Bansal S., Liu Z., Essa I., Boots B., One-shot Learning for Semantic Segmentation, (2017); Zhang X., Wei Y., Yang Y., Huang T.S., SG-one: Similarity guidance network for one-shot semantic segmentation, IEEE Trans. Cybern., 50, 9, pp. 3855-3865, (2020); Xia G.-S., Et al., DOTA: A large-scale dataset for object detection in aerial images, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3974-3983, (2018); Wang J., Et al., Deep high-resolution representation learning for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 43, 10, pp. 3349-3364, (2021); Russakovsky O., Et al., ImageNet large scale visual recognition challenge, Int. J. Comput. Vis., 115, 3, pp. 211-252, (2015); Cheng G., Han J., Lu X., Remote sensing image scene classification: Benchmark and state of the art, Proc. IEEE, 105, 10, pp. 1865-1883, (2017); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Las Vegas, NV, USA, pp. 770-778, (2016); Chen L.-C., Papandreou G., Schroff F., Adam H., Rethinking Atrous Convolution for Semantic Image Segmentation, (2017); Gidaris S., Komodakis N., Dynamic few-shot visual learning without forgetting, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Salt Lake City, UT, USA, pp. 4367-4375, (2018); Hinton G., Vinyals O., Dean J., Distilling the Knowledge in A Neural Network, (2015); Oreshkin B.N., Lopez P.R., Lacoste A., TADAM: Task dependent adaptive metric for improved few-shot learning, Proc. 32nd Int. Conf. Neural Inf. Process. Syst., Montreal, QC, Canada, pp. 719-729, (2018)","X. Sun; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100190, China; email: sunxian@aircas.ac.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85117114886"
"Farjon G.; Huijun L.; Edan Y.","Farjon, Guy (57210638288); Huijun, Liu (58237906200); Edan, Yael (7004434501)","57210638288; 58237906200; 7004434501","Deep-learning-based counting methods, datasets, and applications in agriculture: a review","2023","Precision Agriculture","","","","","","","0","10.1007/s11119-023-10034-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163144652&doi=10.1007%2fs11119-023-10034-8&partnerID=40&md5=bbb08b1bd0668341be09a0f534c5da34","Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; College of Metrological Technology and Engineering, China Jiliang University, Hangzhou, China","Farjon G., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; Huijun L., College of Metrological Technology and Engineering, China Jiliang University, Hangzhou, China; Edan Y., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel","The number of objects is considered an important factor in a variety of tasks in the agricultural domain. Automated counting can improve farmers’ decisions regarding yield estimation, stress detection, disease prevention, and more. In recent years, deep learning has been increasingly applied to many agriculture-related applications, complementing conventional computer-vision algorithms for counting agricultural objects. This article reviews progress in the past decade and the state of the art for counting methods in agriculture, focusing on deep-learning methods. It presents an overview of counting algorithms, metrics, platforms and sensors, a list of all publicly available datasets, and an in-depth discussion of various deep-learning methods used for counting. Finally, it discusses open challenges in object counting using deep learning and gives a glimpse into new directions and future perspectives for counting research. The review reveals a major leap forward in object counting in agriculture in the past decade, led by the penetration of deep learning methods into counting platforms. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep learning; Precision agriculture; Visual counting","","","","","","Marcus Endowment Fund; Ministry of Science, ICT and Future Planning, MSIP, (20187); Ben-Gurion University of the Negev, BGU","This research was partially supported by the Phenomics Consortium, Research Innovation Authority Grant, from the Ministry of Science Grant Number 20187 and from Ben-Gurion University of the Negev through the Agricultural, Biological and Cognitive Robotics Initiative, the Marcus Endowment Fund, and the W. Gunther Plaut Chair in Manufacturing Engineering. ","Afonso M., Fonteijn H., Fiorentin F.S., Lensink D., Mooij M., Faber N., Polder G., Wehrens R., Tomato fruit detection and counting in greenhouses using deep learning, Frontiers in Plant Science, 11, (2020); Albuquerque P.L.F., Garcia V., Junior A.D.S.O., Lewandowski T., Detweiler C., Goncalves A.B., Costa C.S., Naka M.H., Pistori H., Automatic live fingerlings counting using computer vision, Computers and Electronics in Agriculture, 167, (2019); Alharbi N., Zhou J., Wang W., Automatic counting of wheat spikes from wheat growth images, (2018); Almaazmi A., Palm trees detecting and counting from high-resolution worldview-3 satellite images in United Arab Emirates, Remote Sensing for Agriculture, Ecosystems, and Hydrology XX, International Society for Optics and Photonics, (2018); Anderson N.T., Walsh K.B., Koirala A., Wang Z., Amaral M.H., Dickinson G.R., Sinha P., Robson A.J., Estimation of fruit load in Australian mango orchards using machine vision, Agronomy, 11, (2021); Bach S., Binder A., Montavon G., Klauschen F., Muller K.R., Samek W., On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation, PLoS ONE, 10, (2015); Bao W., Lin Z., Hu G., Liang D., Huang L., Zhang X., Method for wheat ear counting based on frequency domain decomposition of msvf-isct, Information Processing in Agriculture, (2022); Barbedo J.G.A., Koenigkan L.V., Perspectives on the use of unmanned aerial systems to monitor cattle, Outlook on Agriculture, 47, pp. 214-222, (2018); Bellocchio E., Ciarfuglia T.A., Costante G., Valigi P., Weakly supervised fruit counting for yield estimation using spatial consistency, IEEE Robotics and Automation Letters, 4, pp. 2348-2355, (2019); Bellocchio E., Costante G., Cascianelli S., Fravolini M.L., Valigi P., Combining domain adaptation and spatial consistency for unseen fruits counting: A quasi-unsupervised approach, IEEE Robotics and Automation Letters, 5, pp. 1079-1086, (2020); Bhattarai U., Karkee M., A weakly-supervised approach for flower/fruit counting in apple orchards, Computers in Industry, 138, (2022); Brereton P., Kitchenham B.A., Budgen D., Turner M., Khalil M., Lessons from applying the systematic literature review process within the software engineering domain, Journal of Systems and Software, 80, pp. 571-583, (2007); Drone services for plant water-status mapping, In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, pp. 8527-8530, (2021); Cao L., Xiao Z., Liao X., Yao Y., Wu K., Mu J., Li J., Pu H., Automated chicken counting in surveillance camera environments based on the point supervision algorithm: Lc-densefcn, Agriculture, 11, (2021); Chen C.H., Kung H.Y., Hwang F.J., Deep learning techniques for agronomy applications, Agronomy, 9, 3, (2019); Chen I.T., Lin H.Y., Detection, counting and maturity assessment of cherry tomatoes using multi-spectral images and machine learning techniques, In VISIGRAPP (5: VISAPP), pp. 759-766, (2020); Darwin B., Dharmaraj P., Prince S., Popescu D.E., Hemanth D.J., Recognition of bloom/yield in crop images using deep learning models for smart agriculture: A review, Agronomy, 11, (2021); David E., Serouart M., Smith D., Madec S., Velumani K., Liu S., Wang X., Pinto F., Shafiee S., Tahir I.S., Et al., Global wheat head detection 2021: An improved dataset for benchmarking wheat head detection methods, Plant Phenomics, (2021); Dhaka V.S., Meena S.V., Rani G., Sinwar D., Ijaz M.F., Wozniak M., Et al., A survey of deep convolutional neural networks applied for prediction of plant leaf diseases, Sensors, 21, (2021); Dijkstra K., Loosdrecht J., Schomaker L.R., Wiering M.A., Centroidnet: A deep neural network for joint object localization and counting, Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 585-601, (2018); Dobrescu A., Valerio Giuffrida M., Tsaftaris S.A., Leveraging multiple datasets for deep leaf counting, In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 2072-2079, (2017); Understanding deep neural networks for regression in leaf counting, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops., (2019); Everingham M., Van Gool L., Williams C.K., Winn J., Zisserman A., The pascal visual object classes (voc) challenge, International Journal of Computer Vision, 88, pp. 303-338, (2010); Farjon G., Itzhaky Y., Khoroshevsky F., Bar-Hillel A., Leaf counting: Fusing network components for improved accuracy, Frontiers in Plant Science, 12, (2021); Farjon G., Krikeb O., Hillel A.B., Alchanatis V., Detection and counting of flowers on apple trees for better chemical thinning decisions, Precision Agriculture, pp. 1-19, (2019); Gao F., Fang W., Sun X., Wu Z., Zhao G., Li G., Zhang Q., A novel apple fruit detection and counting methodology based on deep learning and trunk tracking in modern orchard, Computers and Electronics in Agriculture, 197, (2022); Gao F., Fu L., Zhang X., Majeed Y., Li R., Karkee M., Zhang Q., Multi-class fruit-on-plant detection for apple in snap system using faster r-CNN, Computers and Electronics in Agriculture, 176, (2020); Gebbers R., Adamchuk V.I., Precision agriculture and food security, Science, 327, pp. 828-831, (2010); Gene-Mola J., Sanz-Cortiella R., Rosell-Polo J.R., Morros J.R., Ruiz-Hidalgo J., Vilaplana V., Gregorio E., Fuji-SfM dataset: A collection of annotated images and point clouds for fuji apple detection and location using structure-from-motion photogrammetry, Data in Brief, 30, (2020); Gene-Mola J., Vilaplana V., Rosell-Polo J.R., Morros J.R., Ruiz-Hidalgo J., Gregorio E., Kfuji RGB-ds database: Fuji apple multi-modal images for fruit detection with color, depth and range-corrected ir data, Data in Brief, 25, (2019); Gomez A.S., Aptoula E., Parsons S., Bosilj P., Deep regression versus detection for counting in robotic phenotyping, IEEE Robotics and Automation Letters, 6, pp. 2902-2907, (2021); Gutierrez S., Wendel A., Underwood J., Ground based hyperspectral imaging for extensive mango yield estimation, Computers and Electronics in Agriculture, 157, pp. 126-135, (2019); Hani N., Roy P., Isler V., Apple counting using convolutional neural networks, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2559-2565, (2018); Hani N., Roy P., Isler V., A comparative study of fruit detection and counting methods for yield mapping in apple orchards, Journal of Field Robotics, 37, pp. 263-282, (2020); Harel B., Parmet Y., Edan Y., Maturity classification of sweet peppers using image datasets acquired in different times, Computers in Industry, 121, (2020); Harel B., van Essen R., Parmet Y., Edan Y., Viewpoint analysis for maturity classification of sweet peppers, Sensors, 20, (2020); Hassler S.C., Baysal-Gurel F., Unmanned aircraft system (UAS) technology and applications in agriculture, Agronomy, 9, (2019); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); He L., Fang W., Zhao G., Wu Z., Fu L., Li R., Majeed Y., Dhupia J., Fruit yield prediction and estimation in orchards: A state-of-the-art comprehensive review for both direct and indirect methods, Computers and Electronics in Agriculture, 195, (2022); Hemming J., Ruizendaal J., Hofstee J.W., Van Henten E.J., Fruit detectability analysis for different camera positions in sweet-pepper, Sensors, 14, pp. 6032-6044, (2014); Hobbs J., Paull R., Markowicz B., Rose G., Flowering density estimation from aerial imagery for automated pineapple flower counting, In AI for Social Good Workshop., (2020); Hollings T., Burgman M., van Andel M., Gilbert M., Robinson T., Robinson A., How do you find the green sheep? A critical review of the use of remotely sensed imagery to detect and count animals, Methods in Ecology and Evolution, 9, pp. 881-892, (2018); Hong S.J., Nam I., Kim S.Y., Kim E., Lee C.H., Ahn S., Park I.K., Kim G., Automatic pest counting from pheromone trap images using deep learning object detectors for Matsucoccus thunbergianae monitoring, Insects, 12, (2021); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Jayasinghe C., Badenhorst P., Jacobs J., Spangenberg G., Smith K., Image-based high-throughput phenotyping for the estimation of persistence of perennial ryegrass (Lolium perenne L.)—a review, Grass and Forage Science, 76, pp. 321-339, (2021); Jiang Y., Li C., Paterson A.H., Robertson J.S., Deepseedling: Deep convolutional network and Kalman filter for plant seedling detection and counting in the field, Plant Methods, 15, pp. 1-19, (2019); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Computers and Electronics in Agriculture, 147, pp. 70-90, (2018); Kendall A., Gal Y., What uncertainties do we need in Bayesian deep learning for computer vision?, In Advances in Neural Information Processing Systems, pp. 5574-5584, (2017); Kestur R., Meduri A., Narasipura O., Mangonet: A deep semantic segmentation architecture for a method to detect and count mangoes in an open orchard, Engineering Applications of Artificial Intelligence, 77, pp. 59-69, (2019); Kim D.W., Yun H.S., Jeong S.J., Kwon Y.S., Kim S.G., Lee W.S., Kim H.J., Modeling and testing of growth status for Chinese cabbage and white radish with UAV-based RGB imagery, Remote Sensing, 10, (2018); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning–method overview and review of use for fruit detection and yield estimation, Computers and Electronics in Agriculture, 162, pp. 219-234, (2019); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, (2012); The oil radish growth dataset for semantic segmentation and yield estimation, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019); Kurtser P., Edan Y., Statistical models for fruit detectability: Spatial and temporal analyses of sweet peppers, Biosystems Engineering, 171, pp. 272-289, (2018); Lac L., Keresztes B., Louargant M., Donias M., Da Costa J.P., An annotated image dataset of vegetable crops at an early stage of growth for proximal sensing applications, Data in Brief, 42, (2022); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988, (2017); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, In European Conference on Computer Vision, pp. 740-755, (2014); Linker R., A procedure for estimating the number of green mature apples in night-time orchard images using light distribution and its application to yield estimation, Precision Agriculture, 18, pp. 59-75, (2017); Liu S., Zeng X., Whitty M., 3dbunch: A novel IOS-smartphone application to evaluate the number of grape berries per bunch using image analysis techniques, IEEE Access, 8, pp. 114663-114674, (2020); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, In: European Conference on Computer Vision, pp. 21-37, (2016); Maheswari P., Raja P., Apolo-Apolo O.E., Perez-Ruiz M., Intelligent fruit yield estimation for orchards using deep learning based semantic segmentation techniques—a review, Frontiers in Plant Science, 12, (2021); Malambo L., Popescu S., Ku N.W., Rooney W., Zhou T., Moore S., A deep learning semantic segmentation-based approach for field-level sorghum panicle counting, Remote Sensing, 11, (2019); Mavridou E., Vrochidou E., Papakostas G.A., Pachidis T., Kaburlasos V.G., Machine vision systems in precision agriculture for crop farming, Journal of Imaging, 5, (2019); Mokrane A., Braham A.C., Cherki B., UAV coverage path planning for supporting autonomous fruit counting systems, 2019 International Conference on Applied Automation and Industrial Diagnostics (ICAAID), pp. 1-5, (2019); Mosley L., Pham H., Bansal Y., Hare E., Image-based sorghum head counting when you only look once., (2020); Nellithimaru A.K., Kantor G.A., Rols: Robust object-level slam for grape counting. In, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops., (2019); Ni X., Li C., Jiang H., Takeda F., Deep learning image segmentation and extraction of blueberry fruit traits associated with harvestability and yield, Horticulture Research, 7, (2020); Oghaz M.M.D., Razaak M., Kerdegari A., Argyriou V., Remagnino P., Scene and environment monitoring using aerial imagery and deep learning, In 2019 15Th International Conference on Distributed Computing in Sensor Systems (DCOSS), pp. 362-369, (2019); Osco L.P., de Arruda M.D.S., Goncalves D.N., Dias A., Batistoti J., de Souza M., Gomes F.D.G., Ramos A.P.M., de Castro Jorge L.A., Liesenberg V., Et al., A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery, ISPRS Journal of Photogrammetry and Remote Sensing, 174, pp. 1-17, (2021); Osco L.P., De Arruda M.D.S., Junior J.M., Da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS Journal of Photogrammetry and Remote Sensing, 160, pp. 97-106, (2020); Palacios F., Bueno G., Salido J., Diago M.P., Hernandez I., Tardaguila J., Automated grapevine flower detection and quantification method based on computer vision and deep learning from on-the-go imaging using a mobile sensing platform under field conditions, Computers and Electronics in Agriculture, 178, (2020); Rahimzadeh M., Attar A., Detecting and counting pistachios based on deep learning, Iran Journal of Computer Science, 5, pp. 69-81, (2022); Rahnemoonfar M., Dobbs D., Yari M., Starek M.J., Discountnet: Discriminating and counting network for real-time counting and localization of sparse objects in high-resolution UAV imagery, Remote Sensing, 11, (2019); Rashid M., Bari B.S., Yusup Y., Kamaruddin M.A., Khan N., A comprehensive review of crop yield prediction using machine learning approaches with special emphasis on palm oil yield prediction, IEEE Access, 9, pp. 63406-63439, (2021); Redmon J., Farhadi A., Yolov3: An incremental improvement., (2018); Ren S., He K., Girshick R., Sun J., Faster r-CNN: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, (2015); Ringdahl O., Kurtser P., Edan Y., Strategies for selecting best approach direction for a sweet-pepper harvesting robot, In Annual Conference Towards Autonomous Robotic Systems (, pp. 516-525, (2017); Ruiz-Garcia L., Lunadei L., Barreiro P., Robla I., A review of wireless sensor technologies and applications in agriculture and food industry: State of the art and current trends, Sensors, 9, pp. 4728-4750, (2009); Sadeghi-Tehran P., Virlet N., Ampe E.M., Reyns P., Hawkesford M.J., Deepcount: in-field automatic quantification of wheat spikes using simple linear iterative clustering and deep convolutional neural networks, Frontiers in Plant Science, 10, (2019); Saleem M.H., Potgieter J., Arif K.M., Automation in agriculture by machine and deep learning techniques: A review of recent developments, Precision Agriculture, 22, pp. 2053-2091, (2021); Santoro F., Tarantino E., Figorito B., Gualano S., D'Onghia A.M., A tree counting algorithm for precision agriculture tasks, International Journal of Digital Earth, 6, pp. 94-102, (2013); ). Deep learning applications in agriculture: A short review, In Iberian Robotics Conference, pp. 139-151, (2019); Santos T.T., de Souza L.L., dos Santos A.A., Avila S., Grape detection, segmentation, and tracking using deep neural networks and three-dimensional association, Computers and Electronics in Agriculture, 170, (2020); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2014); Soares V.H.A., Ponti M.A., Goncalves R.A., Campello R.J., Cattle counting in the wild with geolocated aerial images in large pasture areas, Computers and Electronics in Agriculture, 189, (2021); Springenberg J.T., Dosovitskiy A., Brox T., Riedmiller M., Striving for simplicity: The all convolutional net., (2014); Syazwani R.W.N., Asraf H.M., Amin M.M.S., Dalila K.N., Automated image identification, detection and fruit counting of top-view pineapple crown using machine learning, Alexandria Engineering Journal, 61, pp. 1265-1276, (2022); Tan M., Pang R., Le Q.V., Efficientdet: Scalable and efficient object detection, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10781-10790, (2020); Tenorio G.L., Caarls W., Automatic visual estimation of tomato cluster maturity in plant rows, Machine Vision and Applications, 32, pp. 1-18, (2021); Tian M., Guo H., Chen H., Wang Q., Long C., Ma Y., Automated pig counting using deep learning, Computers and Electronics in Agriculture, 163, (2019); Tong P., Han P., Li S., Li N., Bu S., Li Q., Li K., Counting trees with point-wise supervised segmentation network, Engineering Applications of Artificial Intelligence, 100, (2021); Tu S., Pang J., Liu H., Zhuang N., Chen Y., Zheng C., Wan H., Xue Y., Passion fruit detection and counting based on multiple scale faster r-CNN using RGB-D images, Precision Agriculture, 21, pp. 1072-1091, (2020); Vermote E.F., Skakun S., Becker-Reshef I., Saito K., Remote sensing of coconut trees in Tonga using very high spatial resolution worldview-3 data, Remote Sensing, 12, (2020); Villacres J., Viscaino M., Delpiano J., Vougioukas S., Cheein F.A., Apple orchard production estimation using deep learning strategies: A comparison of tracking-by-detection algorithms, Computers and Electronics in Agriculture, 204, (2023); Vitzrabin E., Edan Y., Adaptive thresholding with fusion using a RGBD sensor for red sweet-pepper detection, Biosystems Engineering, 146, pp. 45-56, (2016); Vitzrabin E., Edan Y., Changing task objectives for improved sweet pepper detection for robotic harvesting, IEEE Robotics and Automation Letters, 1, pp. 578-584, (2016); Westling F., Underwood J., Bryson M., A procedure for automated tree pruning suggestion using lidar scans of fruit trees, Computers and Electronics in Agriculture, 187, (2021); Wosner O., Farjon G., Bar-Hillel A., Object detection in agricultural contexts: A multiple resolution benchmark and comparison to human, Computers and Electronics in Agriculture, 189, (2021); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network, Remote Sensing, 11, (2019); Xiong H., Cao Z., Lu H., Madec S., Liu L., Shen C., Tasselnetv2: In-field counting of wheat spikes with context-augmented local regression networks, Plant Methods, 15, (2019); Xu B., Wang W., Falzon G., Kwan P., Guo L., Chen G., Tait A., Schneider D., Automated cattle counting using mask r-CNN in quadcopter vision system, Computers and Electronics in Agriculture, 171, (2020); Yamamoto K., Guo W., Yoshioka Y., Ninomiya S., On plant detection of intact tomato fruits using image analysis and machine learning methods, Sensors, 14, pp. 12191-12206, (2014); Yang Y., Ramanan D., Articulated human detection with flexible mixtures of parts, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 12, pp. 2878-2890, (2012); Zhang C., Zhang K., Ge L., Zou K., Wang S., Zhang J., Li W., A method for organs classification and fruit counting on pomegranate trees based on multi-features fusion and support vector machine by 3d point cloud, Scientia Horticulturae, 278, (2021); Zhang L., Li W., Liu C., Zhou X., Duan Q., Automatic fish counting method using image density grading and local regression, Computers and Electronics in Agriculture, 179, (2020); Zhang Q., Liu Y., Gong C., Chen Y., Yu H., Applications of deep learning for dense scenes analysis in agriculture: A review, Sensors, 20, (2020); Zhong Y., Gao J., Lei Q., Zhou Y., A vision-based counting and recognition system for flying insects in intelligent agriculture, Sensors, 18, (2018); Zivkovic Z., Van Der Heijden F., Efficient adaptive density estimation per image pixel for the task of background subtraction, Pattern Recognition Letters, 27, pp. 773-780, (2006)","G. Farjon; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; email: guyfar@post.bgu.ac.il","","Springer","","","","","","13852256","","PREAF","","English","Precis. Agric.","Review","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85163144652"
"Ghali R.; Akhloufi M.A.; Jmal M.; Mseddi W.S.; Attia R.","Ghali, Rafik (57210113619); Akhloufi, Moulay A. (23979609400); Jmal, Marwa (56382403800); Mseddi, Wided Souidene (8521975900); Attia, Rabah (6701758383)","57210113619; 23979609400; 56382403800; 8521975900; 6701758383","Wildfire segmentation using deep vision transformers","2021","Remote Sensing","13","17","3527","","","","35","10.3390/rs13173527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114555660&doi=10.3390%2frs13173527&partnerID=40&md5=1ab728dd0457a4b3f0d5909faee06180","Perception, Robotics and Intelligent Machines Research Group (PRIME), Department of Computer Science, Université de Moncton, 18 Antonine-Maillet Ave, Moncton, E1A 3E9, NB, Canada; SERCOM Laboratory, Ecole Polytechnique de Tunisie, Université de Carthage, La Marsa, 77-1054, Tunisia; Telnet Innovation Labs, Telnet Holding, Parc Elghazela des Technologies de la Communication, Ariana, 2088, Tunisia","Ghali R., Perception, Robotics and Intelligent Machines Research Group (PRIME), Department of Computer Science, Université de Moncton, 18 Antonine-Maillet Ave, Moncton, E1A 3E9, NB, Canada, SERCOM Laboratory, Ecole Polytechnique de Tunisie, Université de Carthage, La Marsa, 77-1054, Tunisia; Akhloufi M.A., Perception, Robotics and Intelligent Machines Research Group (PRIME), Department of Computer Science, Université de Moncton, 18 Antonine-Maillet Ave, Moncton, E1A 3E9, NB, Canada; Jmal M., Telnet Innovation Labs, Telnet Holding, Parc Elghazela des Technologies de la Communication, Ariana, 2088, Tunisia; Mseddi W.S., SERCOM Laboratory, Ecole Polytechnique de Tunisie, Université de Carthage, La Marsa, 77-1054, Tunisia; Attia R., SERCOM Laboratory, Ecole Polytechnique de Tunisie, Université de Carthage, La Marsa, 77-1054, Tunisia","In this paper, we address the problem of forest fires’ early detection and segmentation in order to predict their spread and help with fire fighting. Techniques based on Convolutional Networks are the most used and have proven to be efficient at solving such a problem. However, they remain limited in modeling the long-range relationship between objects in the image, due to the intrinsic locality of convolution operators. In order to overcome this drawback, Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures. They have recently been used to determine the global dependencies between input and output sequences using the self-attention mechanism. In this context, we present in this work the very first study, which explores the potential of vision Transformers in the context of forest fire segmentation. Two vision-based Transformers are used, TransUNet and MedT. Thus, we design two frameworks based on the former image Transformers adapted to our complex, non-structured environment, which we evaluate using varying backbones and we optimize for forest fires’ segmentation. Extensive evaluations of both frameworks revealed a performance superior to current methods. The proposed approaches achieved a state-of-the-art performance with an F1-score of 97.7% for TransUNet architecture and 96.0% for MedT architecture. The analysis of the results showed that these models reduce fire pixels mis-classifications thanks to the extraction of both global and local features, which provide finer detection of the fire’s shape. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Fire segmentation; Forest fire detection; MedT; TransUNet; Vision transformer; Wildfires","Architecture; Convolution; Convolutional neural networks; Deforestation; Fire extinguishers; Fire hazards; Image segmentation; Network architecture; Attention mechanisms; Convolution operators; Convolutional networks; Image transformers; Input and outputs; Sequence prediction; State-of-the-art performance; Structured environment; Fires","","","","","Natural Sciences and Engineering Research Council of Canada, NSERC, (RGPIN-2018-06233)","This research was enabled in part by support provided by the Natural Sciences and Engineering Research Council of Canada (NSERC), funding reference number RGPIN-2018-06233.","Dimitropoulos S., Fighting fire with science, Nature, 576, pp. 328-329, (2019); Gaur A., Singh A., Kumar A., Kulkarni K.S., Lala S., Kapoor K., Srivastava V., Kumar A., Mukhopadhyay S.C., Fire Sensing Technologies: A Review, IEEE Sens. J, 19, pp. 3191-3202, (2019); Kuutti S., Bowden R., Jin Y., Barber P., Fallah S., A Survey of Deep Learning Applications to Autonomous Vehicle Control, IEEE Trans. Intell. Transp. Syst, 22, pp. 712-733, (2021); Tian Y., Luo P., Wang X., Tang X., Deep Learning Strong Parts for Pedestrian Detection, Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1904-1912; Perez-Hernandez F., Tabik S., Lamas A., Olmos R., Fujita H., Herrera F., Object Detection Binary Classifiers methodology based on deep learning to identify small objects handled similarly: Application in video surveillance, Knowl.-Based Syst, 194, (2020); Nawaratne R., Alahakoon D., De Silva D., Yu X., Spatiotemporal Anomaly Detection Using Deep Learning for Real-Time Video Surveillance, IEEE Trans. Ind. Inform, 16, pp. 393-402, (2020); Gaur A., Singh A., Kumar A., Kumar A., Kapoor K., Video flame and smoke based fire detection algorithms: A literature review, Fire Technol, 56, pp. 1943-1980, (2020); Ghali R., Jmal M., Souidene Mseddi W., Attia R., Recent Advances in Fire Detection and Monitoring Systems: A Review, Proceedings of the 18th International Conference on Sciences of Electronics, Technologies of Information and Telecommunications (SETIT’18), 1, pp. 332-340, (2018); Ott M., Edunov S., Grangier D., Auli M., Scaling Neural Machine Translation, (2018); Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention Is All You Need, (2017); Devlin J., Chang M., Lee K., Toutanova K., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, (2018); Han K., Wang Y., Chen H., Chen X., Guo J., Liu Z., Tang Y., Xiao A., Xu C., Xu Y., Et al., A Survey on Visual Transformer, (2020); Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S., End-to-End Object Detection with Transformers, Computer Vision—ECCV, pp. 213-229, (2020); Ding M., Yang Z., Hong W., Zheng W., Zhou C., Yin D., Lin J., Zou X., Shao Z., Yang H., Et al., CogView: Mastering Text-to-Image Generation via Transformers; Yang F., Yang H., Fu J., Lu H., Guo B., Learning Texture Transformer Network for Image Super-Resolution, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5791-5800; Valanarasu J.M.J., Oza P., Hacihaliloglu I., Patel V.M., Medical Transformer: Gated Axial-Attention for Medical Image Segmentation; Chen J., Lu Y., Yu Q., Luo X., Adeli E., Wang Y., Lu L., Yuille A.L., Zhou Y., TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation; Khan S., Naseer M., Hayat M., Zamir S.W., Khan F.S., Shah M., Transformers in Vision: A Survey; Sudre C.H., Li W., Vercauteren T., Ourselin S., Jorge Cardoso M., Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 240-248, (2017); Toulouse T., Rossi L., Campana A., Celik T., Akhloufi M.A., Computer vision for wildfire research: An evolving image dataset for processing and analysis, Fire Saf. J, 92, pp. 188-194, (2017); Akhloufi M.A., Tokime R.B., Elassady H., Wildland fires detection and segmentation using deep learning. Pattern recognition and tracking xxix, Int. Soc. Opt. Photonics Proc. SPIE, 10649, (2018); Dzigal D., Akagic A., Buza E., Brdjanin A., Dardagan N., Forest Fire Detection based on Color Spaces Combination, Proceedings of the 11th International Conference on Electrical and Electronics Engineering (ELECO), pp. 595-599; Qin X., Zhang Z., Huang C., Dehghan M., Zaiane O.R., Jagersand M., U2-Net: Going deeper with nested U-structure for salient object detection, Pattern Recognit, 106, (2020); Yesilkaynak V.B., Sahin Y.H., Unal G.B., EfficientSeg: An Efficient Semantic Segmentation Network, (2020); Horng W.B., Peng J.W., Chen C.Y., A new image-based real-time flame detection method using color analysis, Proceedings of the IEEE Networking, Sensing and Control, pp. 100-105; Celik T., Demirel H., Fire detection in video sequences using a generic color model, Fire Saf. J, 44, pp. 147-158, (2009); Chen T.H., Wu P.H., Chiou Y.C., An early fire-detection method based on image processing, Proceedings of the International Conference on Image Processing, 3, pp. 1707-1710, (2004); Collumeau J.F., Laurent H., Hafiane A., Chetehouna K., Fire scene segmentations for forest fire characterization: A comparative study, Proceedings of the 18th IEEE International Conference on Image Processing, pp. 2973-2976; Chino D.Y.T., Avalhais L.P.S., Rodrigues J.F., Traina A.J.M., BoWFire: Detection of Fire in Still Images by Integrating Pixel Color and Texture Analysis, Proceedings of the 28th SIBGRAPI Conference on Graphics, Patterns and Images, pp. 95-102; Chen J., He Y., Wang J., Multi-feature fusion based fast video flame detection, Build. Environ, 45, pp. 1113-1122, (2010); Jamali M., Karimi N., Samavi S., Saliency Based Fire Detection Using Texture and Color Features, Proceedings of the 28th Iranian Conference on Electrical Engineering (ICEE), pp. 1-5; Ko B.C., Cheong K.H., Nam J.Y., Fire detection based on vision sensor and support vector machines, Fire Saf. J, 44, pp. 322-329, (2009); Foggia P., Saggese A., Vento M., Real-Time Fire Detection for Video-Surveillance Applications Using a Combination of Experts Based on Color, Shape, and Motion, IEEE Trans. Circuits Syst. Video Technol, 25, pp. 1545-1556, (2015); Khondaker A., Khandaker A., Uddin J., Computer Vision-based Early Fire Detection Using Enhanced Chromatic Segmentation and Optical Flow Analysis Technique, Int. Arab. J. Inf. Technol. (IAJIT), 17, pp. 947-953, (2020); Emmy Prema C., Vinsley S.S., Suresh S., Efficient Flame Detection Based on Static and Dynamic Texture Analysis in Forest Fire Detection, Fire Technol, 54, pp. 255-288, (2018); Wang T., Shi L., Yuan P., Bu L., Hou X., A new fire detection method based on flame color dispersion and similarity in consecutive frames, Proceedings of the Chinese Automation Congress (CAC), pp. 151-156; Ajith M., Martinez-Ramon M., Unsupervised Segmentation of Fire and Smoke From Infra-Red Videos, IEEE Access, 7, pp. 182381-182394, (2019); Gonzalez A., Zuniga M.D., Nikulin C., Carvajal G., Cardenas D.G., Pedraza M.A., Fernandez C.A., Munoz R.I., Castro N.A., Rosales B.F., Et al., Accurate fire detection through fully convolutional network, Proceedings of the 7th Latin American Conference on Networked and Electronic Media (LACNEM), pp. 1-6; Dang-Ngoc H., Nguyen-Trung H., Evaluation of Forest Fire Detection Model using Video captured by UAVs, Proceedings of the 19th International Symposium on Communications and Information Technologies (ISCIT), pp. 513-518; Muhammad K., Ahmad J., Lv Z., Bellavista P., Yang P., Baik S.W., Efficient Deep CNN-Based Fire Detection and Localization in Video Surveillance Applications, IEEE Trans. Syst. Man Cybern. Syst, 49, pp. 1419-1434, (2019); Mlich J., Koplik K., Hradis M., Zemcik P., Fire segmentation in Still images, International Conference on Advanced Concepts for Intelligent Vision Systems, pp. 27-37, (2020); Harkat H., Nascimento J., Bernardino A., Fire segmentation using a DeepLabv3+ architecture. Image and Signal Processing for Remote Sensing XXVI, Int. Soc. Opt. Photonics Proc. SPIE, 11533, pp. 134-145, (2020); Bochkov V.S., Kataeva L.Y., wUUNet: Advanced Fully Convolutional Neural Network for Multiclass Fire Segmentation, Symmetry, 13, (2021); Li P., Zhao W., Image fire detection algorithms based on convolutional neural networks, Case Stud. Therm. Eng, 19, (2020); Xu R., Lin H., Lu K., Cao L., Liu Y., A Forest Fire Detection System Based on Ensemble Learning, Forests, 12, (2021); Khan R.A., Uddin J., Corraya S., Real-time fire detection using enhanced color segmentation and novel foreground extraction, Proceedings of the 4th International Conference on Advances in Electrical Engineering (ICAEE), pp. 488-493; Zhao Z.Q., Zheng P., Xu S.T., Wu X., Object Detection With Deep Learning: A Review, IEEE Trans. Neural Netw. Learn. Syst, 30, pp. 3212-3232, (2019); Pan Z., Xu J., Guo Y., Hu Y., Wang G., Deep Learning Segmentation and Classification for Urban Village Using a Worldview Satellite Image Based on U-Net, Remote Sens, 12, (2020); Bragilevsky L., Bajic I.V., Deep learning for Amazon satellite image analysis, Proceedings of the IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM), pp. 1-5; Abdollahi A., Pradhan B., Shukla N., Chakraborty S., Alamri A., Deep Learning Approaches Applied to Remote Sensing Datasets for Road Extraction: A State-Of-The-Art Review, Remote Sens, 12, (2020); Bakator M., Radosav D., Deep Learning and Medical Diagnosis: A Review of Literature, Multimodal Technol. Interact, 2, (2018); Minaee S., Boykov Y.Y., Porikli F., Plaza A.J., Kehtarnavaz N., Terzopoulos D., Image Segmentation Using Deep Learning: A Survey, IEEE Trans. Pattern Anal. Mach. Intell, (2021); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, Adv. Neural Inf. Process. Syst, 25, pp. 1097-1105, (2012); Iandola F.N., Moskewicz M.W., Ashraf K., Han S., Dally W.J., Keutzer K., SqueezeNet: AlexNet-level accuracy with 50× fewer parameters and <1 MB model size, (2016); Xing Y., Zhong L., Zhong X., An Encoder-Decoder Network Based FCN Architecture for Semantic Segmentation, Wirel. Commun. Mob. Comput, 2020, (2020); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Medical Image Computing and Computer-Assisted Intervention—MICCAI, pp. 234-241, (2015); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2015); Chen L.C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell, 40, pp. 834-848, (2018); Chen L., Papandreou G., Schroff F., Adam H., Rethinking Atrous Convolution for Semantic Image Segmentation, (2017); Chen L.C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation, Proceedings of the European Conference on Computer Vision (ECCV), pp. 801-818; Xiao J., Hays J., Ehinger K.A., Oliva A., Torralba A., SUN database: Large-scale scene recognition from abbey to zoo, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3485-3492; Dai J., Li Y., He K., Sun J., R-FCN: Object Detection via Region-based Fully Convolutional Networks, (2016); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Proceedings of the European conference on computer vision (ECCV), pp. 21-37; Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Jocher G., Stoken A., Chaurasia A., Borovec J., Chanvichet V., Kwon Y., TaoXie S., Changyu L., Abhiram V., Skalski P., Et al., Yolov5, (2021); Tan M., Pang R., Le Q.V., EfficientDet: Scalable and Efficient Object Detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10781-10790; Tan M., Le Q., EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Proceedings of the 36th International Conference on Machine Learning, pp. 6105-6114; Girdhar R., Carreira J., Doersch C., Zisserman A., Video Action Transformer Network, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 244-253; Ye L., Rochan M., Liu Z., Wang Y., Cross-Modal Self-Attention Network for Referring Image Segmentation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10502-10511; He X., Chen Y., Lin Z., Spatial-Spectral Transformer for Hyperspectral Image Classification, Remote Sens, 13, (2021); Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, (2020); Sun C., Shrivastava A., Singh S., Gupta A., Revisiting Unreasonable Effectiveness of Data in Deep Learning Era, Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 843-852; Touvron H., Cord M., Douze M., Massa F., Sablayrolles A., Jegou H., Training data-efficient image transformers & distillation through attention; Chuvieco E., Mouillot F., van der Werf G.R., San Miguel J., Tanase M., Koutsias N., Garcia M., Yebra M., Padilla M., Gitas I., Et al., Historical background and current developments for mapping burned area from satellite Earth observation, Remote Sens. Environ, 225, pp. 45-64, (2019); van der Werf G.R., Randerson J.T., Giglio L., van Leeuwen T.T., Chen Y., Rogers B.M., Mu M., van Marle M.J.E., Morton D.C., Collatz G.J., Et al., Global fire emissions estimates during 1997–2016, Earth Syst. Sci. Data, 9, pp. 697-720, (2017); Giglio L., Boschetti L., Roy D.P., Humber M.L., Justice C.O., The Collection 6 MODIS burned area mapping algorithm and product, Remote Sens. Environ, 217, pp. 72-85, (2018); Key C.H., Benson N.C., Landscape assessment (LA), FIREMON: Fire Effects Monitoring and Inventory System, 164, pp. LA1-L55, (2006); Roy D., Boschetti L., Trigg S., Remote sensing of fire severity: Assessing the performance of the normalized burn ratio, IEEE Geosci. Remote Sens. Lett, 3, pp. 112-116, (2006); Miller J.D., Thode A.E., Quantifying burn severity in a heterogeneous landscape with a relative version of the delta Normalized Burn Ratio (dNBR), Remote Sens. Environ, 109, pp. 66-80, (2007); Frampton W.J., Dash J., Watmough G., Milton E.J., Evaluating the capabilities of Sentinel-2 for quantitative estimation of biophysical variables in vegetation, ISPRS J. Photogramm. Remote Sens, 82, pp. 83-92, (2013); Zheng Z., Wang J., Shan B., He Y., Liao C., Gao Y., Yang S., A New Model for Transfer Learning-Based Mapping of Burn Severity, Remote Sens, 12, (2020); Rebecca G., Tim D., Warwick H., Luke C., A remote sensing approach to mapping fire severity in south-eastern Australia using sentinel 2 and random forest, Remote Sens. Environ, 240, (2020); Zanetti M., Marinelli D., Bertoluzza M., Saha S., Bovolo F., Bruzzone L., Magliozzi M.L., Zavagli M., Costantini M., A high resolution burned area detector for Sentinel-2 and Landsat-8, Proceedings of the 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), pp. 1-4; Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geosci. Remote Sens. Lett, 14, pp. 778-782, (2017); Marcos D., Volpi M., Kellenberger B., Tuia D., Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models, ISPRS J. Photogramm. Remote Sens, 145, pp. 96-107, (2018); Zhang Q., Yuan Q., Zeng C., Li X., Wei Y., Missing Data Reconstruction in Remote Sensing Image With a Unified Spa-tial–Temporal–Spectral Deep Convolutional Neural Network, IEEE Trans. Geosci. Remote Sens, 56, pp. 4274-4288, (2018); Kampffmeyer M., Salberg A.B., Jenssen R., Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 680-688, (2016); Jeppesen J.H., Jacobsen R.H., Inceoglu F., Toftegaard T.S., A cloud detection algorithm for satellite imagery based on deep learning, Remote Sens. Environ, 229, pp. 247-259, (2019); Pinto M.M., Libonati R., Trigo R.M., Trigo I.F., DaCamara C.C., A deep learning approach for mapping and dating burned areas using temporal sequences of satellite images, ISPRS J. Photogramm. Remote Sens, 160, pp. 260-274, (2020); Hochreiter S., Schmidhuber J., Long Short-Term Memory, Neural Comput, 9, pp. 1735-1780, (1997); Farasin A., Colomba L., Garza P., Double-Step U-Net: A Deep Learning-Based Approach for the Estimation of Wildfire Damage Severity through Sentinel-2 Satellite Data, Appl. Sci, 10, (2020); Rahmatov N., Paul A., Saeed F., Seo H., Realtime fire detection using CNN and search space navigation, J. Real-Time Image Process, 18, pp. 1331-1340, (2021); Khennou F., Ghaoui J., Akhloufi M.A., Forest fire spread prediction using deep learning, Geospatial Informatics XI. Int. Soc. Opt. Photonics Proc. SPIE, 11733, pp. 106-117, (2021); Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Howard A., Sandler M., Chu G., Chen L.C., Chen B., Tan M., Wang W., Zhu Y., Pang R., Vasudevan V., Et al., Searching for MobileNetV3, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1314-1324; Paszke A., Gross S., Massa F., Lerer A., Bradbury J., Chanan G., Killeen T., Lin Z., Gimelshein N., Antiga L., Et al., PyTorch: An Imperative Style, High-Performance Deep Learning Library, (2019)","M.A. Akhloufi; Perception, Robotics and Intelligent Machines Research Group (PRIME), Department of Computer Science, Université de Moncton, Moncton, 18 Antonine-Maillet Ave, E1A 3E9, Canada; email: moulay.akhloufi@umoncton.ca","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114555660"
"Wang Y.; Liao Y.; Yang J.; Wang H.; Zhao Y.; Zhang C.; Xiao B.; Xu F.; Gao Y.; Xu M.; Zheng J.","Wang, Yu (45161879700); Liao, Yibing (57827531300); Yang, Jiamei (58161124900); Wang, Hui (56595830200); Zhao, Yuxuan (57737449500); Zhang, Chengyu (57737871800); Xiao, Bende (57222489826); Xu, Fei (58161187100); Gao, Yifan (57224996678); Xu, Mingzhu (57225863756); Zheng, Jianbin (36976907500)","45161879700; 57827531300; 58161124900; 56595830200; 57737449500; 57737871800; 57222489826; 58161187100; 57224996678; 57225863756; 36976907500","An FPGA-based online reconfigurable CNN edge computing device for object detection","2023","Microelectronics Journal","137","","105805","","","","0","10.1016/j.mejo.2023.105805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154036952&doi=10.1016%2fj.mejo.2023.105805&partnerID=40&md5=fb7537a46306fcb8a59a59a6a4118a76","School of Information Engineering, Wuhan University of Technology, China; Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China","Wang Y., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Liao Y., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Yang J., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Wang H., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Zhao Y., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Zhang C., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Xiao B., School of Information Engineering, Wuhan University of Technology, China, Hubei Key Laboratory of Broadband Wireless Communication and Sensor Networks, Wuhan University of Technology, China; Xu F., School of Information Engineering, Wuhan University of Technology, China; Gao Y., School of Information Engineering, Wuhan University of Technology, China; Xu M., School of Information Engineering, Wuhan University of Technology, China; Zheng J., School of Information Engineering, Wuhan University of Technology, China","Edge devices offer advantages such as low computation latency and high data security for executing convolutional neural networks (CNNs). However, deploying CNNs on resource-constrained devices is challenging due to the high computational intensity of CNNs and limited hardware on-chip resources. This hinders the application of deep learning techniques on edge devices. To address this issue, this paper proposes a reconfigurable CNN edge computing system based on Field-Programmable Gate Array (FPGA) for target detection tasks. The system utilizes the pipeline structure of FPGAs to speed up network computation and employs off-chip memory to store network models. Thus, the need for tiling techniques with high intermediate cache requirements was circumvented in this system. Additionally, we developed a parallel data scheduling model to reduce storage access cost delay on network computation efficiency. Our model achieves comparable efficiency with on-chip storage-based works when using off-chip storage. Online reconfigurable design enables the system to configure network structure and parameters at runtime to achieve different target recognition. This provides greater flexibility for use cases with frequently changing requirements. The proposed system was implemented on a Spartan-6 XC6SLX150 FPGA platform and was applied to pedestrian and vehicle classification tasks. To evaluate performance, speed, power consumption, and average intersection over union (IoU) were separately measured. Our system achieved a detection speed of 16 frames per second (FPS) on the Spartan-6, with a power consumption rate of 0.79 W and an average IoU of 41.2%. Remarkably, the system demonstrated a 178% speed increase and a 60% power consumption reduction compared to the FPGA-based FitNN implementation, while classification accuracy was reduced by only 2.52%. © 2023 Elsevier Ltd","Convolutional neural network; FPGA; Object detection; Reconfigurable","Computing power; Convolution; Convolutional neural networks; Deep learning; Digital storage; Edge computing; Electric power utilization; Energy efficiency; Field programmable gate arrays (FPGA); Integrated circuit design; Object recognition; Convolutional neural network; Edge computing; Field programmables; Field-programmable gate array; Network computations; Network edges; Objects detection; Programmable gate array; Reconfigurable; Spartan-6; Object detection","","","","","National innovation and entrepreneurship training program; Ministry of Education of the People's Republic of China, MOE","Manuscript received 13 Mar 2023; revised 10 Apr 2023; accepted 20 Apr 2023. This work was supported in part by the Ministry of Education of the People's Republic of China, National innovation and entrepreneurship training program for college students. ","Xu X., Zhang X., Yu B., Et al., Dac-sdc low power object detection challenge for uav applications, IEEE Trans. Pattern Anal. Mach. Intell., 43, 2, pp. 392-403, (2019); Vinh T.Q., Hai D.V., Optimizing convolutional neural network accelerator on low-cost FPGA, J. Circ. Syst. Comput., 30, 11, (2021); Gidaris S.; Mani V.R.S., Saravanaselvan A., Arumugam N., Performance comparison of CNN, QNN and BNN deep neural networks for real-time object detection using ZYNQ FPGA node, Microelectron. J., 119, (2022); Liu H., Panahi A., Andrews D., Et al., An FPGA-based upper-limb rehabilitation device for gesture recognition and motion evaluation using multi-task recurrent neural networks, IEEE Sensor. J., 22, 4, pp. 3605-3615 Aug, (2022); Hung G.L., Sahimi M.S.B., Samma H., Et al., Faster R-CNN deep learning model for pedestrian detection from drone images, SN Comput. Sci., 1, 2, pp. 1-9, (2020); Olaverri-Monreal C., Promoting trust in self-driving vehicles, Nat. Electron., 3, 6, pp. 292-294, (2020); Rojas-Perez L.O., Martinez-Carranza J., Deeppilot: a cnn for autonomous drone racing, Sensors, 20, 16, (2020); Chen C., Liu B., Wan S., Et al., An edge traffic flow detection scheme based on deep learning in an intelligent transportation system, IEEE Trans. Intell. Transport. Syst., 22, 3, pp. 1840-1852 Nov, (2020); Li X., Huang H., Chen T., Et al., A hardware-efficient computing engine for FPGA-based deep convolutional neural network accelerator, Microelectron. J., 128, (2022); Mittal S., A survey of FPGA-based accelerators for convolutional neural networks, Neural Comput. Appl., 32, 4, pp. 1109-1139 Feb, (2020); Dinelli G., Meoni G., Rapuano E., Et al., (2019); Wang E., Davis J.J., Zhao R., Et al., Deep neural network approximation for custom hardware: where we've been, where we're going, ACM Comput. Surv., 52, 2, pp. 1-39, (2019); Li Y., Liu Z., Xu K., Et al., A GPU-outperforming FPGA accelerator architecture for binary convolutional neural networks, ACM J. Emerg. Technol. Comput. Syst., 14, 2, pp. 1-16, (2018); Abdelouahab K., Pelcat M., Serot J., Et al., Accelerating CNN Inference on FPGAs: A Survey, (2018); Ranawaka P., Ekpanyapong M., Tavares A., Et al., Application specific architecture for hardware accelerating HOG-SVM to achieve high throughput on HD frames; Yih M., Ota J.M., Owens J.D., Et al., Fpga versus gpu for speed-limit-sign recognition; Zhang C., Li P., Sun G., Et al.; Li G., Liu Z., Li F., Et al., Block convolution: toward memory-efficient inference of large-scale CNNs on FPGA, IEEE Trans. Comput. Aided Des. Integrated Circ. Syst., 41, 5, pp. 1436-1447 Apr, (2021); Dinelli G., Meoni G., Rapuano E., Et al., MEM-OPT: a scheduling and data re-use system to optimize on-chip memory usage for CNNs on-board FPGAs, IEEE J. Emerg. Selected Topics Circuits Syst., 10, 3, pp. 335-347, (2020); Wang P., Cheng J., Accelerating convolutional neural networks for mobile applications; Han S., Pool J., Tran J., Et al., Learning both weights and connections for efficient neural network, Adv. Neural Inf. Process. Syst., 28, (2015); Han S., Mao H., Dally W.J., Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, (2015); Qiu J., Wang J., Yao S., Et al., Going deeper with embedded fpga platfor-m for convolutional neural network; Guo K., Sui L., Qiu J., Et al., Angel-eye: a complete design flow for mapping CNN onto embedded FPGA, IEEE Trans. Comput. Aided Des. Integrated Circ. Syst., 37, 1, pp. 35-47, (2018); Chen T., Du Z., Sun N., Et al., Diannao: a small-footprint high-throughput accelerator for ubiquitous machine-learning, Comput. Architect. News, 42, 1, pp. 269-284, (2014); Uijlings J.R., Van De Sande K.E., Gevers T., Et al., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Tian Y., Yang G., Wang Z., Et al., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agric., 157, pp. 417-426, (2019); Bharati P., Pramanik A., Deep Learning Techniques—R-CNN to Mask R-CNN: a Survey,” Computational Intelligence in Pattern Recognition, pp. 657-668, (2020); Girshick R., Fast r-cnn; Jiang H., Learned-Miller E., Face detection with the faster R-CNN; Ren S., He K., Girshick R., Et al., Faster r-cnn: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149 Jun, (2017); Felzenszwalb P.F., Huttenlocher D.P., Efficient graph-based image segmentation, Int. J. Comput. Vis., 59, 2, pp. 167-181, (2004); Farrukh F.U.D., Zhang C., Jiang Y., Et al., Power efficient tiny yolo cnn using reduced hardware resources based on booth multiplier and wallace tree adders, IEEE Open J. Circuit. Syst., 1, pp. 76-87, (2020); Yu X., Wang Y., Miao J., Et al., A data-center FPGA acceleration platfo-rm for convolutional neural networks; Zhao R., Song W., Zhang W., Et al., Accelerating binarized convolutional neural networks with software-programmable FPGAs; Wanta D., Smolik W.T., Kryszyn J., Et al., A run-time reconfiguration method for an FPGA-based electrical capacitance tomography system, Electronics, 11, 4, (2022); Dinelli G., Meoni G., Rapuano E., Et al., Advantages and limitations of fully on-chip CNN FPGA-based hardware accelerator; Nguyen D.T., Nguyen T.N., Kim H., Et al., A high-throughput and power-efficient FPGA implementation of YOLO CNN for object detection, IEEE Trans. Very Large Scale Integr. Syst., 27, 8, pp. 1861-1873 Aug, (2019); Benelli G., Meoni G., Fanucci L., A low power keyword spotting algorithm for memory constrained embedded systems; Pandir Y., Yildirim A., Analytical approach for the fractional differential equations by using the extended tanh method, Waves Random Complex Media, 28, 3, pp. 399-410, (2018); Srivastava Y., Murali V.; Zou D., Cao Y., Zhou D., Et al., Gradient descent optimizes over-parameterized deep ReLU networks, Mach. Learn., 109, 3, pp. 467-492, (2020); LeCun Y., Bottou L., Bengio Y., Et al., Gradient-based learning applied to document recognition, Proc. IEEE, 86, 11, pp. 2278-2324 Apr, (1998); Xception F.C., Deep learning with depthwise separable convolute-ons; Flohr F., Gavrila D., PedCut: an iterative framework for pedestri-an segmentation combining shape models and multiple data cues; Ding W., Huang Z., Huang Z., Et al., Designing efficient accelerator of depthwise separable convolutional neural network on FPGA, J. Syst. Architect., 97, pp. 278-286, (2019); Iandola F., Keutzer K., Small Neural Nets Are Beautiful: Enabling E-Mbedded Systems with Small Deep-Neural-Network Architectures, (2018); Lin H., Wang Y., Zhou J., Et al., Research and design of open convolutional neural network based on FPGA; Zhang Z., Mahmud M.P., Kouzani A.Z., FitNN: A Low-Resource FPGA-Based CNN Accelerator for Drones, (2022)","J. Zheng; School of Information Engineering, Wuhan University of Technology, China; email: jbzheng@whut.edu.cn","","Elsevier Ltd","","","","","","00262692","","MICEB","","English","Microelectron J","Article","Final","","Scopus","2-s2.0-85154036952"
"Jiang K.; Zhang W.; Liu J.; Liu F.; Xiao L.","Jiang, Kaixuan (57855016200); Zhang, Wenhua (57206485064); Liu, Jia (56376104500); Liu, Fang (57091781700); Xiao, Liang (57020500800)","57855016200; 57206485064; 56376104500; 57091781700; 57020500800","Joint Variation Learning of Fusion and Difference Features for Change Detection in Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4709918","","","","1","10.1109/TGRS.2022.3226778","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144771242&doi=10.1109%2fTGRS.2022.3226778&partnerID=40&md5=a8b81c35c67227346e915e09bceecd53","Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China","Jiang K., Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China; Zhang W., Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China; Liu J., Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China; Liu F., Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China; Xiao L., Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China","Remote sensing (RS) image change detection (CD) is an Earth observation technique for detecting surface changes in the same area during a period. With the rapid development of deep learning, various deep neural networks especially Siamese ones have been widely used in the field of CD. However, they have the deficiency of insufficient contextual information aggregation, thus resulting in false and missed detections, and it is difficult to refine the detection of change edges. To alleviate these problems and obtain more accurate results, we propose an efficient self-weighted spatial-temporal attention network (SSANet). In contrast to the Siamese structure, our network is a novel joint learning framework composed of fusion subnetwork, difference subnetwork, and decoder. Fusion subnetwork is used to extract multiscale object features where we propose a multicore channel-aligning attention (MCA) module to capture the long-range semantic information for multiscale context aggregation. Difference subnetwork is used to extract the difference variation features, where we propose a feature differential reconfiguration (FDR) module to learn the temporal change information. FDR can effectively filter change information and reconstruct features to improve the perception of changed regions. To better balance the MCA and FDR modules, an asymmetric weighting (AW) module is proposed in the decoder to self-weight the multiscale features and generate the change map. Experiments demonstrate the efficiency of proposed subnetworks and modules, and the state-of-the-art performance of SSANet. © 1980-2012 IEEE.","Asymmetric weighting (AW) module; change detection (CD); deep learning; feature differential reconfiguration (FDR) module; multicore channel-aligning attention (MCA) module","Change detection; Convolution; Data mining; Decoding; Deep neural networks; Feature extraction; Job analysis; Remote sensing; Asymmetric weighting module; Change detection; Decoding; Deep learning; Feature differential reconfiguration module; Features extraction; Multi-core channel-aligning attention module; Multi-cores; Neural-networks; Task analysis; algorithm; detection method; image classification; satellite imagery; Semantics","","","","","Jiangsu Funding Program for Excellent Postdoctoral Talent, (2022ZB266); Jiangsu Provincial Social Developing Project, (BE2018727); National Major Research Plan of China, (2016YFF0103604); National Natural Science Foundation of China, NSFC, (61571230, 61802190, 61871226, 61906093, 62276133); China Postdoctoral Science Foundation, (2022M711637); Natural Science Foundation of Jiangsu Province, (BK20220948); Fundamental Research Funds for the Central Universities, (30918011104, 30920021134, JSGP202204); Jiangsu Key Laboratory of Spectral Imaging and Intelligence Sense, (JSGP202101)","This work was supported in part by the National Nature Science Foundation of China under Grant 62276133, Grant 61906093, Grant 61871226, Grant 61802190, and Grant 61571230; in part by the Natural Science Foundation of Jiangsu Province, China, under Grant BK20220948; in part by China Postdoctoral Science Foundation under Grant 2022M711637; in part by the Jiangsu Funding Program for Excellent Postdoctoral Talent under Grant 2022ZB266; in part by the Open Research Fund in 2021 of Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense under Grant JSGP202101; in part by Jiangsu Provincial Social Developing Project under Grant BE2018727; in part by the National Major Research Plan of China under Grant 2016YFF0103604; and in part by the Fundamental Research Funds for the Central Universities under Grant JSGP202204, Grant 30918011104, and Grant 30920021134.","Singh A., Review article digital change detection techniques using remotely-sensed data, Int. J. Remote Sens., 10, 6, pp. 989-1003, (1989); Gong M., Zhao J., Liu J., Miao Q., Jiao L., A survey on change detection in synthetic aperture radar imagery, J. Comput. Res. Develop., 53, 1, (2016); Chen C.-F., Et al., Multi-decadal mangrove forest change detection and prediction in honduras, central America, with Landsat imagery and a Markov chain model, Remote Sens, 5, 12, pp. 6408-6426, (2013); Gong M., Zhao J., Liu J., Miao Q., Jiao L., Change detection in synthetic aperture radar images based on deep neural networks, IEEE Trans. Neural Netw. Learn. Syst., 27, 1, pp. 125-138, (2016); Han M., Change detection of land use and land cover in an urban region with SPOT-5 images and partial Lanczos extreme learning machine, J. Appl. Remote Sens., 4, 1, (2010); Zhao J., Gong M., Liu J., Jiao L., Deep learning to classify difference image for image change detection, Proc. Int. Joint Conf. Neural Netw. (IJCNN), pp. 411-417, (2014); Rogan J., Chen D., Remote sensing technology for mapping and monitoring land-cover and land-use change, Progr. Planning, 61, 4, pp. 301-325, (2004); Johnson R.D., Kasischke E.S., Change vector analysis: A technique for the multispectral monitoring of land cover and condition, Int. J. Remote Sens., 19, 3, pp. 411-426, (1998); Nielsen A.A., Conradsen K., Simpson J.J., Multivariate alteration detection (MAD) and MAF postprocessing in multispectral, bitemporal image data: New approaches to change detection studies, Remote Sens. Environ., 64, 1, pp. 1-19, (1998); Nemmour H., Chibani Y., Multiple support vector machines for land cover change detection: An application for mapping urban extensions, ISPRS J. Photogram. Remote Sens., 61, 2, pp. 125-133, (2006); Im J., Jensen J.R., A change detection model based on neighborhood correlation image analysis and decision tree classification, Remote Sens. Environ., 99, 3, pp. 326-340, (2005); Verdier G., Ferreira A., Adaptive Mahalanobis distance and knearest neighbor rule for fault detection in semiconductor manufacturing, IEEE Trans. Semicond. Manuf., 24, 1, pp. 59-68, (2011); Seo D., Kim Y., Eo Y., Lee M., Park W., Fusion of SAR and multispectral images using random forest regression for change detection, ISPRS Int. J. Geo-Inf., 7, 10, (2018); Zhang X., Chen X., Li F., Yang T., Change detection method for high resolution remote sensing images using deep learning, Acta Geodaetica Cartographica Sinica, 46, 8, (2017); Zhou L., Cao G., Li Y., Shang Y., Change detection based on conditional random field with region connection constraints in high-resolution remote sensing images, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 9, 8, pp. 3478-3488, (2016); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2117-2125, (2017); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2818-2826, (2016); Wang Y., Et al., Mask DeepLab: End-to-end image segmentation for change detection in high-resolution remote sensing images, Int. J. Appl. Earth Observ. Geoinf., 104, (2021); Arabi M.E.A., Karoui M.S., Djerriri K., Optical remote sensing change detection through deep Siamese network, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 5041-5044, (2018); Villa M., Dardenne G., Nasan M., Letissier H., Hamitouche C., Stindel E., FCN-based approach for the automatic segmentation of bone surfaces in ultrasound images, Int. J. Comput. Assist. Radiol. Surgery, 13, 11, pp. 1707-1716, (2018); Ronneberger O., Philipp F., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent. Cham, pp. 234-241, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Zhan Y., Fu K., Yan M., Sun X., Wang H., Qiu X., Change detection based on deep Siamese convolutional network for optical aerial images, IEEE Geosci. Remote Sens. Lett., 14, 10, pp. 1845-1849, (2017); Jiang H., Hu X., Li K., Zhang J., Gong J., Zhang M., PGA-SiamNet: Pyramid feature-based attention-guided Siamese network for remote sensing orthoimagery building change detection, Remote Sens, 12, 3, (2020); Wang M., Tan K., Jia X., Wang X., Chen Y., A deep Siamese network with hybrid convolutional feature extraction module for change detection based on multi-sensor remote sensing images, Remote Sens, 12, 2, (2020); Chen H., Wu C., Du B., Zhang L., Wang L., Change detection in multisource VHR images via deep Siamese convolutional multiple-layers recurrent neural network, IEEE Trans. Geosci. Remote Sens., 58, 4, pp. 2848-2864, (2020); Lei T., Et al., Difference enhancement and spatial–spectral nonlocal network for change detection in VHR remote sensing images, IEEE Trans. Geosci. Remote Sens., 60, pp. 1-13, (2022); Bandara W.G.C., Patel V.M., A transformer-based Siamese network for change detection, (2022); Woo S., Park J., Lee J.-Y., Kweon I.S., CBAM: Convolutional block attention module, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 3-19, (2018); Hofmann P., Strobl J., Blaschke T., Kux H., Detecting informal settlements from QuickBird data in Rio de Janeiro using an object based approach, Object-Based Image Analysis, pp. 531-553, (2008); Coppin P.R., Bauer M.E., Digital change detection in forest ecosystems with remote sensing imagery, Remote Sens. Rev., 13, 3-4, pp. 207-234, (1996); Jackson R., Spectral indices in n-space, Remote Sens. Environ., 13, 5, pp. 409-421, (1983); Gong M., Liang Y., Shi J., Ma W., Ma J., Fuzzy C-means clustering with local information and kernel metric for image segmentation, IEEE Trans. Image Process., 22, 2, pp. 573-584, (2013); Ghosh A., Mishra N.S., Ghosh S., Fuzzy clustering algorithms for unsupervised change detection in remote sensing images, Inf. Sci., 181, 4, pp. 699-715, (2011); Gong M., Su L., Jia M., Chen W., Fuzzy clustering with a modified MRF energy function for change detection in synthetic aperture radar images, IEEE Trans. Fuzzy Syst., 22, 1, pp. 98-109, (2014); Crist E.P., A TM tasseled cap equivalent transformation for reflectance factor data, Remote Sens. Environ., 17, 3, pp. 301-306, (1985); Bruzzone L., Prieto D.F., A minimum-cost thresholding technique for unsupervised change detection, Int. J. Remote Sens., 21, 18, pp. 3539-3544, (2000); Jianya G., Haigang S., Guorui M., Qiming Z., A review of multi-temporal remote sensing data change detection algorithms, Int. Arch. Photogramm., Remote Sens. Spatial Inf. Sci., 37, B7, pp. 757-762, (2008); Wang Q., Yuan Z., Du Q., Li X., 2-D CNN framework for hyperspectral image change detection, IEEE Trans. Geosci. Remote Sens., 57, 1, pp. 3-13, (2018); Zhu X.X., Et al., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geosci. Remote Sens. Mag., 5, 4, pp. 8-36, (2018); Mou L., Bruzzone L., Zhu X.X., Learning spectral–spatial–temporal features via a recurrent convolutional neural network for change detection in multispectral imagery, IEEE Trans. Geosci. Remote Sens., 57, 2, pp. 924-935, (2019); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3431-3440, (2015); Hussain M., Chen D., Cheng A., Wei H., Stanley D., Change detection from remotely sensed images: From pixel-based to object-based approaches, ISPRS J. Photogramm. Remote Sens., 80, pp. 91-106, (2013); Peng D., Zhang Y., Guan H., End-to-end change detection for high resolution satellite images using improved UNet++, Remote Sens, 11, 11, (2019); Caye Daudt R., Le Saux B., Boulch A., Fully convolutional Siamese networks for change detection, Proc. 25th IEEE Int. Conf. Image Process. (ICIP), pp. 4063-4067, (2018); Chen H., Shi Z., A spatial–temporal attention-based method and a new dataset for remote sensing image change detection, Remote Sens, 12, 10, (2020); Peng X., Zhong R., Li Z., Li Q., Optical remote sensing image change detection based on attention mechanism and image difference, IEEE Trans. Geosci. Remote Sens., 59, 9, pp. 7296-7307, (2021); Chen H., Qi Z., Shi Z., Remote sensing image change detection with transformers, IEEE Trans. Geosci. Remote Sens., 60, pp. 1-14, (2022); Fang S., Li K., Shao J., Li Z., SNUNet-CD: A densely connected Siamese network for change detection of VHR images, IEEE Geosci. Remote Sens. Lett., 19, pp. 1-5, (2022); Shi Q., Liu M., Li S., Liu X., Wang F., Zhang L., A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection, IEEE Trans. Geosci. Remote Sens., 60, pp. 1-16, (2022); Yang L., Chen Y., Song S., Li F., Huang G., Deep Siamese networks based change detection with remote sensing images, Remote Sens, 13, 17, (2021); Zhu X., Hu H., Lin S., Dai J., Deformable ConvNets v2: More deformable, better results, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1-9, (2019); Bourdis N., Marraud D., Sahbi H., Constrained optical flow for aerial image change detection, Proc. IEEE Int. Geosci. Remote Sens. Symp., pp. 4176-4179, (2011); Selvaraju R.R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D., Grad-CAM: Visual explanations from deep networks via gradient-based localization, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 618-626, (2017)","J. Liu; Nanjing University of Science and Technology, Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, 210094, China; email: OMEGALiuJ@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85144771242"
"You J.; Zhang R.; Lee J.","You, Jie (57204458966); Zhang, Ruirui (57392402900); Lee, Joonwhoan (55870620900)","57204458966; 57392402900; 55870620900","A deep learning-based generalized system for detecting pine wilt disease using RGB-based UAV images","2022","Remote Sensing","14","1","150","","","","13","10.3390/rs14010150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122044677&doi=10.3390%2frs14010150&partnerID=40&md5=4bc97ccf8f3690c28f97fc350bfe0488","Department of Computer Engineering, Jeonbuk National University, Jeonju-si, 54896, South Korea; School of Computer Science and Engineering, Cangzhou Normal University, Cangzhou, 061001, China","You J., Department of Computer Engineering, Jeonbuk National University, Jeonju-si, 54896, South Korea; Zhang R., School of Computer Science and Engineering, Cangzhou Normal University, Cangzhou, 061001, China; Lee J., Department of Computer Engineering, Jeonbuk National University, Jeonju-si, 54896, South Korea","Pine wilt is a devastating disease that typically kills affected pine trees within a few months. In this paper, we confront the problem of detecting pine wilt disease. In the image samples that have been used for pine wilt disease detection, there is high ambiguity due to poor image resolution and the presence of “disease-like” objects. We therefore created a new dataset using large-sized orthophotographs collected from 32 cities, 167 regions, and 6121 pine wilt disease hotspots in South Korea. In our system, pine wilt disease was detected in two stages: n the first stage, the disease and hard negative samples were collected using a convolutional neural network. Because the diseased areas varied in size and color, and as the disease manifests differently from the early stage to the late stage, hard negative samples were further categorized into six different classes to simplify the complexity of the dataset. Then, in the second stage, we used an object detection model to localize the disease and “disease-like” hard negative samples. We used several image augmentation methods to boost system performance and avoid overfitting. The test process was divided into two phases: a patch-based test and a real-world test. During the patch-based test, we used the test-time augmentation method to obtain the average prediction of our system across multiple augmented samples of data, and the prediction results showed a mean average precision of 89.44% in five-fold cross validation, thus representing an increase of around 5% over the alternative system. In the real-world test, we collected 10 orthophotographs in various resolutions and areas, and our system successfully detected 711 out of 730 potential disease spots. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","GIS application visualization; Hard negative mining; Object detection; Pine wilt disease dataset; Test-time augmentation","Aircraft detection; Convolutional neural networks; Deep learning; Geographic information systems; Image resolution; Large dataset; Object recognition; Statistical tests; Unmanned aerial vehicles (UAV); G.I.S. applications; GIS application; GIS application visualization; Hard negative mining; Negative minings; Negative samples; Pine wilt disease; Pine wilt disease dataset; Test time; Test-time augmentation; Object detection","","","","","","","Khan M.A., Ahmed L., Mandal P.K., Smith R., Haque M., Modelling the dynamics of Pine Wilt Disease with asymptomatic carriers and optimal control, Sci. Rep, 10, pp. 1-15, (2020); Hirata A., Nakamura K., Nakao K., Kominami Y., Tanaka N., Ohashi H., Takano K.T., Takeuchi W., Matsui T., Potential distribution of pine wilt disease under future climate change scenarios, PLoS ONE, 12, (2017); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., Hyyppa J., Saari H., Polonen I., Imai N.N., Individual tree detection and classification with UAV-based photogrammetric point clouds and hyperspectral imaging, Remote Sens, 9, (2017); Nezami S., Khoramshahi E., Nevalainen O., Polonen I., Honkavaara E., Tree species classification of drone hyperspectral and rgb imagery with deep learning convolutional neural networks, Remote Sens, 12, (2020); Sothe C., Dalponte M., de Almeida C.M., Schimalski M.B., Lima C.L., Liesenberg V., Miyoshi G.T., Tommaselli A.M.G., Tree species classification in a highly diverse subtropical forest integrating UAV-based photogrammetric point cloud and hyperspectral data, Remote Sens, 11, (2019); Egli S., Hopke M., CNN-Based Tree Species Classification Using High Resolution RGB Image Data from Automated UAV Observations, Remote Sens, 12, (2020); Maldonado-Ramirez S.L., Schmale D.G., Shields E.J., Bergstrom G.C., The relative abundance of viable spores of Gibberella zeae in the planetary boundary layer suggests the role of long-distance transport in regional epidemics of Fusarium head blight, Agric. For. Meteorol, 132, pp. 20-27, (2005); Nguyen H.T., Caceres M.L.L., Moritake K., Kentsch S., Shu H., Diez Y., Individual Sick Fir Tree (Abies mariesii) Identification in Insect Infested Forests by Means of UAV Images and Deep Learning, Remote Sens, 13, (2021); Arantes B.H.T., Moraes V.H., Geraldine A.M., Alves T.M., Albert A.M., da Silva G.J., Castoldi G., Spectral detection of nematodes in soybean at flowering growth stage using unmanned aerial vehicles, Ciência Rural, 51, (2021); Xiao Y., Dong Y., Huang W., Liu L., Ma H., Wheat Fusarium Head Blight Detection Using UAV-Based Spectral and Texture Features in Optimal Window Size, Remote Sens, 13, (2021); Takenaka Y., Katoh M., Denga S., Cheunga K., Detecting forests damaged by pine wilt disease at the individual tree level using airborne laser data and worldview-2/3 images over two seasons, Proceedings of the International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLII-3/W3, (2017); Yu R., Luo Y., Zhou Q., Zhang X., Wu D., Ren L., A machine learning algorithm to detect pine wilt disease using UAV-based hyperspectral imagery and LiDAR data at the tree level, Int. J. Appl. Earth Obs. Geoinf, 101, (2021); Deng X., Tong Z., Lan Y., Huang Z., Detection and Location of Dead Trees with Pine Wilt Disease Based on Deep Learning and UAV Remote Sensing, AgriEngineering, 2, pp. 294-307, (2020); Zhao Z.Q., Zheng P., tao Xu S., Wu X., Object detection with deep learning: A review, IEEE Trans. Neural Netw. Learn. Syst, 30, pp. 3212-3232, (2019); Sung K.K., Poggio T., Example-based learning for view-based human face detection, IEEE Trans. Pattern Anal. Mach. Intell, 20, pp. 39-51, (1998); Felzenszwalb P.F., Girshick R.B., McAllester D., Ramanan D., Object detection with discriminatively trained part-based models, IEEE Trans. Pattern Anal. Mach. Intell, 32, pp. 1627-1645, (2009); Iordache M.D., Mantas V., Baltazar E., Pauly K., Lewyckyj N., A Machine Learning Approach to Detecting Pine Wilt Disease Using Airborne Spectral Imagery, Remote Sens, 12, (2020); Syifa M., Park S.J., Lee C.W., Detection of the Pine Wilt Disease Tree Candidates for Drone Remote Sensing Using Artificial Intelligence Techniques, Engineering, 6, pp. 919-926, (2020); Wu B., Liang A., Zhang H., Zhu T., Zou Z., Yang D., Tang W., Li J., Su J., Application of conventional UAV-based high-throughput object detection to the early diagnosis of pine wilt disease by deep learning, For. Ecol. Manag, 486, (2021); Qin J., Wang B., Wu Y., Lu Q., Zhu H., Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms, Remote Sens, 13, (2021); Li F., Liu Z., Shen W., Wang Y., Wang Y., Ge C., Sun F., Lan P., A Remote Sensing and Airborne Edge-Computing Based Detection System for Pine Wilt Disease, IEEE Access, 9, pp. 66346-66360, (2021); Son M.H., Lee W.K., Lee S.H., Cho H.K., Lee J.H., Natural spread pattern of damaged area by pine wilt disease using geostatistical analysis, J. Korean Soc. For. Sci, 95, pp. 240-249, (2006); Park Y.S., Chung Y.J., Moon Y.S., Hazard ratings of pine forests to a pine wilt disease at two spatial scales (individual trees and stands) using self-organizing map and random forest, Ecol. Inform, 13, pp. 40-46, (2013); Lee J.B., Kim E.S., Lee S.H., An analysis of spectral pattern for detecting pine wilt disease using ground-based hyperspectral camera, Korean J. Remote Sens, 30, pp. 665-675, (2014); Kim S.R., Lee W.K., Lim C.H., Kim M., Kafatos M.C., Lee S.H., Lee S.S., Hyperspectral analysis of pine wilt disease to determine an optimal detection index, Forests, 9, (2018); Lee S., Park S.J., Baek G., Kim H., Lee C.W., Detection of damaged pine tree by the pine wilt disease using UAV Image, Korean J. Remote Sens, 35, pp. 359-373, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2016); Girshick R., Fast r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); Redmon J., Farhadi A., Yolov3: An incremental improvement, (2018); Bochkovskiy A., Wang C.Y., Liao H.Y.M., Yolov4: Optimal speed and accuracy of object detection, (2020); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, European Conference on Computer Vision, (2016); Liu S., Huang D., Receptive field block net for accurate and fast object detection, Proceedings of the European Conference on Computer Vision (ECCV), (2018); Shrivastava A., Gupta A., Girshick R., Training region-based object detectors with online hard example mining, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, Commun. ACM, 60, pp. 84-90, (2017); DeVries T., Taylor G.W., Improved regularization of convolutional neural networks with cutout, (2017); Solovyev R., Wang W., Gabruseva T., Weighted boxes fusion: Ensembling boxes from different object detection models, Image Vis. Comput, 107, (2021); Bodla N., Singh B., Chellappa R., Davis L.S., Soft-NMS–improving object detection with one line of code, Proceedings of the IEEE International Conference on Computer Vision, (2017); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, European Conference on Computer Vision, (2014); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, Proceedings of the IEEE International Conference on Computer Vision, (2017); Janssen V., Understanding coordinate reference systems, datums and transformations, Int. J. Geoinform, 5, pp. 41-53, (2009); Abdollahnejad A., Panagiotidis D., Tree Species Classification and Health Status Assessment for a Mixed Broadleaf-Conifer Forest with UAS Multispectral Imaging, Remote Sens, 12, (2020)","J. Lee; Department of Computer Engineering, Jeonbuk National University, Jeonju-si, 54896, South Korea; email: chlee@jbnu.ac.kr","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122044677"
"Jojoa Acosta M.F.; Caballero Tovar L.Y.; Garcia-Zapirain M.B.; Percybrooks W.S.","Jojoa Acosta, Mario Fernando (57221310706); Caballero Tovar, Liesle Yail (57221302691); Garcia-Zapirain, Maria Begonya (35732954700); Percybrooks, Winston Spencer (24778386600)","57221310706; 57221302691; 35732954700; 24778386600","Melanoma diagnosis using deep learning techniques on dermatoscopic images","2021","BMC Medical Imaging","21","1","6","","","","59","10.1186/s12880-020-00534-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098789515&doi=10.1186%2fs12880-020-00534-8&partnerID=40&md5=e968e59ea90d312b4f1fb58a17d6b42f","eVida Research Laboratory, University of Deusto, Avda. Universidades 24, Bilbao, 48007, Spain; Department of Electrical and Electronics Engineering, Universidad del Norte, Km.5 Vía Puerto Colombia, Barranquilla, Colombia","Jojoa Acosta M.F., eVida Research Laboratory, University of Deusto, Avda. Universidades 24, Bilbao, 48007, Spain; Caballero Tovar L.Y., eVida Research Laboratory, University of Deusto, Avda. Universidades 24, Bilbao, 48007, Spain; Garcia-Zapirain M.B., eVida Research Laboratory, University of Deusto, Avda. Universidades 24, Bilbao, 48007, Spain; Percybrooks W.S., Department of Electrical and Electronics Engineering, Universidad del Norte, Km.5 Vía Puerto Colombia, Barranquilla, Colombia","Background: Melanoma has become more widespread over the past 30 years and early detection is a major factor in reducing mortality rates associated with this type of skin cancer. Therefore, having access to an automatic, reliable system that is able to detect the presence of melanoma via a dermatoscopic image of lesions and/or skin pigmentation can be a very useful tool in the area of medical diagnosis. Methods: Among state-of-the-art methods used for automated or computer assisted medical diagnosis, attention should be drawn to Deep Learning based on Convolutional Neural Networks, wherewith segmentation, classification and detection systems for several diseases have been implemented. The method proposed in this paper involves an initial stage that automatically crops the region of interest within a dermatoscopic image using the Mask and Region-based Convolutional Neural Network technique, and a second stage based on a ResNet152 structure, which classifies lesions as either “benign” or “malignant”. Results: Training, validation and testing of the proposed model was carried out using the database associated to the challenge set out at the 2017 International Symposium on Biomedical Imaging. On the test data set, the proposed model achieves an increase in accuracy and balanced accuracy of 3.66% and 9.96%, respectively, with respect to the best accuracy and the best sensitivity/specificity ratio reported to date for melanoma detection in this challenge. Additionally, unlike previous models, the specificity and sensitivity achieve a high score (greater than 0.8) simultaneously, which indicates that the model is good for accurate discrimination between benign and malignant lesion, not biased towards any of those classes. Conclusions: The results achieved with the proposed model suggest a significant improvement over the results obtained in the state of the art as far as performance of skin lesion classifiers (malignant/benign) is concerned. © 2021, The Author(s).","Convolutional neural network; Deep learning; Mask R_CNN; Object classification; Object detection; Transfer learning","Deep Learning; Dermoscopy; Humans; Image Interpretation, Computer-Assisted; Melanoma; ROC Curve; Skin Neoplasms; classification; computer assisted diagnosis; diagnostic imaging; epiluminescence microscopy; human; melanoma; pathology; procedures; receiver operating characteristic; skin tumor","","","","","","","Melanoma: descripción General, (2019); Skin Cancer (Including Melanoma)—Patient Version, (2019); Detect Skin Cancer, (2019); Puede Detectarse Temprano El cáncer De Piel Tipo Melanoma, (2019); Lunares Comunes, Nevos displásicos Y El Riesgo De Melanoma-Instituto Nacional Del Cáncer, (2019); Maarouf M., Costello C.M., Gonzalez S., Angulo I., Curiel-Lewandrowski C.N., Shi V.Y., In vivo reflectance confocal microscopy: emerging role in noninvasive diagnosis and monitoring of eczematous dermatoses, Actas Dermo Sifiliog (Engl Ed), 110, 8, pp. 626-636, (2019); Ferrante di Ruffano L., Et al., Computer-assisted diagnosis techniques (dermoscopy and spectroscopy-based) for diagnosing skin cancer in adults, Cochrane Database Syst Rev, (2018); Dinnes J., Et al., Reflectance confocal microscopy for diagnosing cutaneous melanoma in adults, Cochrane Database Syst Rev, (2018); di Ruano F.L., Dermoscopy, with and without visual inspection, for diagnosing melanoma in adults (review), Cochrane Libr Cochrane Database Syst Rev, (2018); Dinnes J., Et al., Visual inspection for diagnosing cutaneous melanoma in adults, Cochrane Database Syst Rev, (2018); Harangi B., Skin lesion classification with ensembles of deep convolutional neural networks, J Biomed Inform, 86, pp. 25-32, (2018); Nida N., Irtaza A., Javed A., Yousaf M.H., Mahmood M.T., Melanoma lesion detection and segmentation using deep region based convolutional neural network and fuzzy C-means clustering, Int J Med Inform, (2019); Goyal M., Yap M.H., Automatic Lesion Boundary Segmentation in Dermoscopic Images with Ensemble Deep Learning Methods, (2019); Jafari M.H., Nasr-Esfahani E., Karimi N., Soroushmehr S.M.R., Samavi S., Najarian K., Extraction of skin lesions from non-dermoscopic images using deep learning, Int J Comput Assist Radiol Surg, (2016); CNN Architectures: Lenet, Alexnet, VGG, Googlenet, Resnet and More, (2019); Guo K., Xu T., Kui X., Zhang R., Chi T., iFusion: towards efficient intelligence fusion for deep learning from real-time and heterogeneous data, Inf Fusion, 51, pp. 215-223, (2019); Garcia-Arroyo J.L., Garcia-Zapirain B., Segmentation of skin lesions in dermoscopy images using fuzzy classification of pixels and histogram thresholding, Comput Methods Programs Biomed, (2019); Garnavi R., Aldeen M., Celebi M.E., Bhuiyan A., Dolianitis C., Varigos G., Automatic segmentation of dermoscopy images using histogram thresholding on optimal color channels, Int J Med Med Sci, 1, pp. 126-134, (2011); Schaefer G., Krawczyk B., Celebi M.E., Iyatomi H., An ensemble classification approach for melanoma diagnosis, Memetic Comput, (2014); Brodersen K.H., Ong C.S., Stephan K.E., Buhmann J.M., The balanced accuracy and its posterior distribution, 20Th International Conference on Pattern Recognition, pp. 3121-3124, (2010); Urbanowicz R.J., Moore J.H., ExSTraCS 2.0: description and evaluation of a scalable learning classifier system, Evol Intell, 8, 2-3, pp. 89-116, (2015); Velez D.R., Et al., A balanced accuracy function for epistasis modeling in imbalanced datasets using multifactor dimensionality reduction, Genet Epidemiol, 31, 4, pp. 306-315, (2007); Fawcett T., An introduction to ROC analysis, Pattern Recognit Lett, (2006); Imagenet.; Smith L.N., A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—learning Rate, Batch Size, Momentum, and Weight Decay, (2018); Automatic Computer-Based Diagnosis System for Dermoscopy Images; Covalic,” ISIC 2017: Skin lesion analysis towards melanoma detection, International Skin Imaging Collaboration, (2017); Navarrete-Dechent C., Dusza S.W., Liopyris K., Marghoob A.A., Halpern A.C., Marchetti M.A., Automated dermatological diagnosis: hype or reality?, J Invest Dermatol, 138, 10, pp. 2277-2279, (2018); Wong S.C., Gatt A., Stamatescu V., McDonnell M.D., Understanding Data Augmentation for Classification: When to Warp?, (2016); Xiang A., Wang F., Towards interpretable skin lesion classification with deep learning models, AMIA Annual Symposium Proceedings, 2019, pp. 1246-1255; Simard P.Y., Steinkraus D., Platt J.C., Best practices for convolutional neural networks applied to visual document analysis, Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings, (2003); Ciresan D.C., Meier U., Gambardella L.M., Schmidhuber J., Deep, big, simple neural nets for handwritten digit recognition, Neural Comput, (2010); Chawla N.V., Bowyer K.W., Hall L.O., Kegelmeyer W.P., SMOTE: synthetic minority over-sampling technique, J Artif Intell Res, (2002); Tschandl P., Et al., Human–computer collaboration for skin cancer recognition, Nat Med, (2020); Borgstein P.J., Meijer S., Van Diest P.J., Are locoregional cutaneous metastases in melanoma predictable, Ann Surg Oncol, 6, 3, pp. 315-321, (1999); Bann D.V., Chaikhoutdinov I., Zhu J., Andrews G., Satellite and in-transit metastatic disease in melanoma skin cancer: a retrospective review of disease presentation, treatment, and outcomes, Dermatol Surg, (2019); Dearborn F.M., Enfermedades De La Piel, (1999); Tsao H., Et al., Early detection of melanoma: reviewing the ABCDEs, J Am Acad Dermatol, (2015)","L.Y. Caballero Tovar; eVida Research Laboratory, University of Deusto, Bilbao, Avda. Universidades 24, 48007, Spain; email: lieslec@uninorte.edu.co","","BioMed Central Ltd","","","","","","14712342","","BMIMA","33407213","English","BMC Med. Imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85098789515"
"Ha N.T.; Tuan N.P.D.; Khanh T.H.; Du L.T.; Dinh T.C.; Sang D.N.; Son N.V.; Luan N.L.D.; Le N.T.","Ha, Nguyen Truc (58079922800); Tuan, Nguyen Phong Dinh (58079412000); Khanh, Tran Hoang (58078406500); Du, Le Thuong (58079412100); Dinh, Truong Cong (58079922900); Sang, Dinh Ngoc (58078150100); Son, Nguyen Van (58078150200); Luan, Nguyen Le Duy (57204362608); Le, Ngoc Thien (57191329232)","58079922800; 58079412000; 58078406500; 58079412100; 58079922900; 58078150100; 58078150200; 57204362608; 57191329232","Leveraging Deep Learning Model for Emergency Situations Detection on Urban Road Using Images from CCTV Cameras","2022","8th International Conference on Engineering and Emerging Technologies, ICEET 2022","","","","","","","0","10.1109/ICEET56468.2022.10007352","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146889013&doi=10.1109%2fICEET56468.2022.10007352&partnerID=40&md5=745ffa7ee40ea72b857f8b7d6c1ad7d8","University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam","Ha N.T., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Tuan N.P.D., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Khanh T.H., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Du L.T., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Dinh T.C., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Sang D.N., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Son N.V., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Luan N.L.D., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam; Le N.T., University of Architecture Ho Chi Minh City, Department of Urban Engineering, Ho Chi Minh City, Viet Nam","Deep Learning (DL) neural network has been proven as an innovative technology in computer vision in recent years. Many DL models for objective classification, objective detection, and object segmentation are successfully applied in many fields such as healthcare, farming, aquarium, and sports. In this study, we propose the DL training pipeline model to automatically detect many emergency issues on urban roads, including hazardous or fallen trees, flooding or blocked drains, garbage, open manhole, sinkhole, and traffic jam. The DL model extracts images from closed-circuit television (CCTV) cameras and detects these emergency issues to alert the authority immediately. In addition, we also propose the framework to implement our proposed DL model on CCTV cameras and drones to surveil the city's roads. Our proposed solution can be integrated into the existing CCTV system of the city and transforms it to become a smart CCTV system for a smart city.  © 2022 IEEE.","closed-circuit television (CCTV) cameras; Deep learning model; Emergency situation; MobileNetV2 model; smart cities","Deep learning; Learning systems; Object detection; Smart city; Traffic congestion; Video cameras; Closed circuit television; Closed-circuit television  camera; Deep learning model; Emergency situation; Innovative technology; Learning models; Learning neural networks; Mobilenetv2 model; Situation detection; Urban road; Timing circuits","","","","","","","Vershinina A.I., Volkova V.L., Smart cities: Challenges and opportunities, Espacios, 41, 15, pp. 23-28, (2020); Application of real-Time GIS analytics to support spatialintelligent decision-making in the era of big data for smart cities, EAI Endorsed Transactions on Smart Cities, 4; Costa D.G., Peixoto J.P.J., Jesus T.C., Portugal P., Vasques F., Rangel E., Peixoto M., A survey of emergencies management systems in smart cities, IEEE Access, 10, pp. 61843-61872, (2022); Costa D.G., Vasques F., Portugal P., Aguiar A., On the use of cameras for the detection of critical events in sensors-based emergency alerting systems, Journal of Sensor and Actuator Networks, 9, 4, (2020); Peppa M.V., Komar T., Xiao W., James P., Robson C., Xing J., Barr S., Towards an end-To-end framework of CCTV-based urban traffic volume detection and prediction, Sensors, 21, 2; Xu W., Ruiz-Juri N., Et al., Deep learning methods to leverage traffic monitoring cameras for pedestrian data applications, 26th ITS World Congress, Singapore, pp. 1-10, (2019); Mishra B., Thakker D., Mazumdar S., Simpson S., Neagu D., Using deep learning for IoT-enabled camera: A use case of flood monitoring, 6, pp. 235-240, (2019); Ramirez G., Salazar K., Barria V., Pinto O., Martin L.S., Carrasco R., Fuentealba D., Gatica G., Accident risk detection in urban trees using machine learning and fuzzy logic, Procedia Computer Science, 203, 2022, pp. 471-475; Ying W., Xu Z., Autonomous garbage detection for intelligent urban management, MATEC Web Conf, 232, (2018); Liu Y., Ge Z., Lv G., Wang S., Research on automatic garbage detection system based on deep learning and narrowband internet of things, Journal of Physics: Conference Series, 1069, (2018); Yun K., Kwon Y., Oh S., Moon J., Park J., Vision-based garbage dumping action detection for real-world surveillance platform, ETRI Journal, 41, 4, pp. 494-505, (2019); Andrijasevic U., Kocic J., Nesic V., Lid opening detection in manholes using RNN, 2020 28th Telecommunications Forum (TELFOR, 2020, pp. 1-4; Santos A., Marcato Junior J., Silva Andrade J.De, Pereira R., Matos D., Menezes G., Higa L., Eltner A., Ramos A.P., Osco L., Goncalves W., Storm-drain and manhole detection using the retinanet method, Sensors, 20, 16, (2020); Al-Smadi A.M., Alsmadi M.K., Baareh A., Emergent situations for smart cities: A survey, International Journal of Electrical and Computer Engineering (IJECE, 9, 6, pp. 4777-4787, (2019); Baller S.P., Jindal A., Chadha M., Gerndt M., Deepedgebench: Benchmarking deep neural networks on edge devices, 2021 IEEE International Conference on Cloud Engineering (IC2E), October 2021, pp. 1-12; Khan M.A., Safi A., Alvi B.A., Khan I.U., Drones for good in smart cities: A review, International Conference on Electrical, Electronics, Computers, Communication, Mechanical and Computing (EECCMC, pp. 1-7, (2018)","","","Institute of Electrical and Electronics Engineers Inc.","","8th International Conference on Engineering and Emerging Technologies, ICEET 2022","27 October 2022 through 28 October 2022","Kuala Lumpur","185970","","978-166549106-8","","","English","Int. Conf. Eng. Emerg. Technol., ICEET","Conference paper","Final","","Scopus","2-s2.0-85146889013"
"Rehman A.; Tariq S.; Farrakh A.; Ahmad M.; Javeid M.S.","Rehman, Abdur (58404084700); Tariq, Sharjeel (58297671400); Farrakh, Amina (58296352400); Ahmad, Munir (57226132376); Javeid, Muhammad Sheraz (57251468900)","58404084700; 58297671400; 58296352400; 57226132376; 57251468900","A Systematic Review of Machine Learning and Artificial Intelligence Methods to Tackle Climate Change Impacts","2023","2nd International Conference on Business Analytics for Technology and Security, ICBATS 2023","","","","","","","0","10.1109/ICBATS57792.2023.10111347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160701167&doi=10.1109%2fICBATS57792.2023.10111347&partnerID=40&md5=c1b74880379f14d2756454e18709dbc3","Punjab University College of Information Technology (PUCIT), College of Information Technology, Lahore, Pakistan; University of Engineering and Technology (UET), College of Information Technology, Lahore, Pakistan; Comsats University Islamabad, College of Information Technology, Islamabad, Pakistan; National College of Business Administration & Economics, College of Information Technology, Lahore, Pakistan; Applied Science Private Universit, Applied Science Research Center, Amman, 11937, Jordan","Rehman A., Punjab University College of Information Technology (PUCIT), College of Information Technology, Lahore, Pakistan; Tariq S., University of Engineering and Technology (UET), College of Information Technology, Lahore, Pakistan; Farrakh A., Comsats University Islamabad, College of Information Technology, Islamabad, Pakistan; Ahmad M., National College of Business Administration & Economics, College of Information Technology, Lahore, Pakistan; Javeid M.S., Applied Science Private Universit, Applied Science Research Center, Amman, 11937, Jordan","Weather prediction is vital in sustaining human beings and all living objects. It's been years since humans have been forecasting weather for growing desired crops and migrating from region to region. With this era of advancement and automation, machine learning and deep learning have advanced in all domains, and innovations are taking place for the betterment of beings. The field of weather is no longer far away from its advancements. Precise and timely weather predictions can save crops from being demolished and alarm farmers to protect their fields from getting wasted. Classic numerical weather prediction (NWP) has faced numerous challenges recently. It has built an ambiguity in the systematic data. Still, with the advancement of IOT and machine learning techniques like motion detection, speech recognition, and computer vision, it has been seen that these techniques can predict change in an environment more precisely and effectively. Currently, several innovations in weather forecasting employ machine learning and artificial neural network techniques, and these researchers see precise benchmarks. In this paper, we study the different methods of deep learning, machine learning, and advanced IOT devices for better prediction of weather and their comparative analysis.  © 2023 IEEE.","artificial neural network; deep learning; IOT; Machine learning; Python; weather forecast","Climate change; Crops; Deep learning; High level languages; Internet of things; Learning systems; Neural networks; Speech recognition; Weather forecasting; Artificial intelligence methods; Climate change impact; Deep learning; Human being; IOT; Machine-learning; Numerical weather prediction; Systematic Review; Weather forecast; Weather prediction; Python","","","","","","","Hasan N., Uddin M.T., Chowdhury N.K., Automated weather event analysis with machine learning, Proc. IEEE 2016 International Conference on Innovations in Science, Engineering, and Technology (ICISET), pp. 1-5, (2016); Lai L.L., Braun H., Zhang Q.P., Wu Q., Ma Y.N., Sun W.C., Yang L., Intelligent weather forecast, Proc. IEEE 2004 International Conference on Machine Learning and Cybernetics, pp. 4216-4221, (2004); Xingjian S., Chen Z., Wang H., Yeung D.-Y., Wong W.-K., Woo W.-C., Convolutional lstm network: a machine learning approach for precipitation nowcasting, Advances in Neural Information Processing Systems, NIPS, pp. 802-810, (2015); Salman A.G., Kanigoro B., Heryadi Y., Weather forecasting using deep learning techniques, 2015 international conference on advanced computer science and information systems (ICACSIS), pp. 281-285, (2015); Kim K.S., Lee J.B., Roh M.I., Han K.M., Lee G.H., Prediction of ocean weather based on denoising autoencoder and convolutional LSTM, Journal of Marine Science and Engineering, 8, 10, (2020); Bjerknes V., Volken E., Bronnimann S., The problem of weather prediction is considered from the viewpoints of mechanics and physics, Meteorologische Zeitschrift, 18, 6, (2009); Srivastava N., Mansimov E., Salakhudinov R., Unsupervised learning of video representations using lists, International Conference on Machine Learning, ICML, pp. 843-852, (2015); Wang B., Lu J., Yan Z., Luo H., Li T., Zheng Y., Zhang G., Deep uncertainty quantification: a machine learning approach for weather forecasting, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2087-2095, (2019); Frnda J., Durica M., Nedoma J., Zabka S., Martinek R., Kostelansky M., A weather forecast model accuracy analysis and ecmwf enhancement proposal by neural network, Sensors, 19, 23, (2019); Grover A., Kapoor A., Horvitz E., A deep hybrid model for weather forecasting, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, pp. 379-386, (2015); Ren X., Li X., Ren K., Song J., Xu Z., Deng K., Wang X., Deep learning-based weather prediction: a survey, Big Data Research, 23, (2021); Kim N., Na S.I., Park C.W., Huh M., Oh J., Ha K.J., Lee Y.W., An artificial intelligence approach to predicting corn yields under extreme weather conditions using satellite and meteorological data, Applied Sciences, 10, 11, (2020); Oses N., Azpiroz I., Marchi S., Guidotti D., Quartulli M., I Olaizola G., Analysis of Copernicus era5 climate reanalysis data as a replacement for weather station temperature measurements in machine learning models for olive phenology phase prediction, Sensors, 20, 21, (2020); Hasan N., Uddin M.T., Chowdhury N.K., Automated weather event analysis with machine learning, 2016 International Conference on Innovations in Science, Engineering, and Technology (ICISET), pp. 1-5, (2016); Bhawsar M., Tewari V., Khare P., A survey of weather forecasting based on machine learning and deep learning techniques, International Journal of Emerging Trends in Engineering Research, 9, 7, (2021); Barrera-Animas A.Y., Oyedele L.O., Bilal M., Akinosho T.D., Delgado J.M.D., Akanbi L.A., Rainfall prediction: A comparative analysis of modern machine learning algorithms for timeseries forecasting, Machine Learning with Applications, 7, (2022); Haq M.A., Baral P., Yaragal S., Pradhan B., Bulk processing of multi-temporal modis data, statistical analyses, and machine learning algorithms to understand climate variables in the Indian Himalayan region, Sensors, 21, 21, (2021); Quilting J.F., Grams C.M., EuLerian Identification of ascending AirStreams (ELIAS 2. 0) in numerical weather prediction and climate models-Part 1: Deep learning model Development, Geoscientific Model Development, 15, 2, pp. 715-730, (2022); Quilting J.F., Grams C.M., EuLerian Identification of ascending AirStreams (ELIAS 2. 0) in numerical weather prediction and climate models-Part 1: Deep learning model Development, Geoscientific Model Development, 15, 2, pp. 715-730, (2022); Liu Z., Wu D., Liu Y., Han Z., Lun L., Gao J., Cao G., Accuracy analyses and model comparison of machine learning adopted in building energy consumption prediction, Energy Exploration & Exploitation, 37, 4, pp. 1426-1451, (2019); Nestler G., Jackman A., 21st century emergency management, (2014)","A. Rehman; Punjab University College of Information Technology (PUCIT), College of Information Technology, Lahore, Pakistan; email: abdurrehman.sakhwat@pucit.edu.pk","","Institute of Electrical and Electronics Engineers Inc.","","2nd International Conference on Business Analytics for Technology and Security, ICBATS 2023","7 March 2023 through 8 March 2023","Dubai","188684","","979-835033564-4","","","English","Int. Conf. Bus. Anal. Technol. Secur., ICBATS","Conference paper","Final","","Scopus","2-s2.0-85160701167"
"Pathak S.P.; Patil D.S.; Patel S.","Pathak, Sujata P. (57208575861); Patil, Dr.Sonali (57985900800); Patel, Shailee (58272963400)","57208575861; 57985900800; 58272963400","Solar panel hotspot localization and fault classification using deep learning approach","2022","Procedia Computer Science","204","","","698","705","7","1","10.1016/j.procs.2022.08.084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142881665&doi=10.1016%2fj.procs.2022.08.084&partnerID=40&md5=bf8ce7a047ba901d5b82362dc548d91a","KJSCE, Mumbai, 400077, India","Pathak S.P., KJSCE, Mumbai, 400077, India; Patil D.S., KJSCE, Mumbai, 400077, India; Patel S., KJSCE, Mumbai, 400077, India","There has been an exponential increase in Photovoltaic energy over the last decade. The size and the complexity of photovoltaic solar power plants are increasing, and it requires advanced and robust condition monitoring systems for ensuring their reliability. To this aim, a novel method is addressed for fault detection in photovoltaic panels through processing of thermal images of solar panels captured by a thermographic camera. In this paper, two advanced convolutional neural network models are used wherein the task of the first model is to classify the type of fault affecting the panel and the task of the second model is to identify the region of interest of the faulty panel. Proposed approach uses F1 score as a metric to compare several classification models of which the ResNet-50 transfer learning model achieves the highest score of 85.37 %. Mean Average Precision is used as an evaluation metric for object detection models wherein the highest scoring model is Faster R-CNN with a score of 67 %. This paper puts forth an approach to facilitate early identification and fault localization in Solar Panels by minimizing the amount of manual labour involved in the process. © 2022 Elsevier B.V.. All rights reserved.","Faster R-CNN;ResNet-50; Photovoltaic; Thermal images","Condition monitoring; Convolutional neural networks; Deep learning; Fault detection; Image classification; Image segmentation; Object detection; Solar concentrators; Solar energy; Solar power generation; Solar power plants; Exponential increase; Fast R-CNN;; Fault classification; Hotspots; Learning approach; Localisation; Photovoltaics; Resnet-50; Solar panels; Thermal images; Solar panels","","","","","","","Aghaei M., Gandelli A., Grimaccia F., Leva S., Zich R.E., Ir real-time analyses for pv system monitoring by digital image processing techniques, 2015 International Conference on Event-based Control, Communication, and Signal Processing (Ebccsp), pp. 1-6, (2015); Breiman L., Random forests, Machine Learning, 45, 1, pp. 5-32, (2001); Chen T., Guestrin C., Xgboost: A scalable tree boosting system, Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, pp. 785-794, (2016); Chollet F., Xception: Deep Learning with Depthwise Separable Convolutions, (2017); Dunderdale C., Brettenny W., Clohessy C., Van Dyk E.E., Photovoltaic defect classification through thermal infrared imaging using a machine learning approach, Progress in Photovoltaics: Research and Applications, 28, 3, pp. 177-188, (2020); Et-Taleby A., Boussetta M., Benslimane M., Faults detection for photovoltaic field based on k-means, elbow,and average silhouette techniques through the segmentation of a thermal image, International Journal of Photoenergy, 2020, (2020); Greco A., Pironti C., Saggese A., Vento M., Vigilante V., A deep learning based approach for detecting panels in photovoltaic plants, Proceedings of the 3rd International Conference on Applications of Intelligent Systems, pp. 1-7, (2020); Guo G., Wang H., Bell D., Bi Y., Greer K., Knn model-based approach in classification, OTM Confederated International Conferences'' on the Move to Meaningful Internet Systems, pp. 986-996, (2003); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, (2015); Hearst M.A., Dumais S.T., Osuna E., Platt J., Scholkopf B., Support vector machines, IEEE Intelligent Systems and Their Applications, 13, 4, pp. 18-28, (1998); Henry C., Poudel S., Lee S.W., Jeong H., Automatic detection system of deteriorated pv modules using drone with thermal camera, Applied Sciences, 10, 11, (2020); Herraiz A.H., Marugan A.P., Marquez F.P.G., Photo- voltaic plant condition monitoring using thermal images analysis by convolutional neural network-based structure, Renewable Energy, 153, pp. 334-348, (2020); Huang G., Liu Z., Van Der Maaten L., Weinberger K.Q., Densely Connected Convolutional Networks, (2018); Jaffery Z.A., Dubey A.K., Haque A., Et al., Scheme for predictive fault diagnosis in photo-voltaic modules using thermal imaging, Infrared Physics & Technology, 83, pp. 182-187, (2017); Madeti S.R., Singh S., A comprehensive study on different types of faults and detection techniques for solar photovoltaic system, Solar Energy, 158, pp. 161-185, (2017); Phoolwani U.K., Sharma T., Singh A., Gawre S.K., Iot based solar panel analysis using thermal imaging, 2020 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS), pp. 1-5, (2020); Ren S., He K., Girshick R., Sun J., Faster R-cnn: Towards Real-time Object Detection with Region Proposal Networks, (2016); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2015); Tsanakas J.A., Ha L., Buerhop C., Faults and infrared thermographic diagnosis in operating c-si photovoltaic modules: A review of research and future challenges, Renewable and Sustainable Energy Reviews, 62, pp. 695-709, (2016); Wang L., Liu J., Guo X., Yang Q., Yan W., Online fault diagnosis of photovoltaic modules based on multiclass support vector machine, 2017 Chinese Automation Congress (CAC), pp. 4569-4574, (2017)","S.P. Pathak; KJSCE, Mumbai, 400077, India; email: sujatapathak@somaiya.edu","Cruz-Cunha M.M.; Mateus-Coelho N.","Elsevier B.V.","","2022 International Conference on Industry Sciences and Computer Science Innovation, iSCSi 2022","9 March 2022 through 11 March 2022","Porto","184213","18770509","","","","English","Procedia Comput. Sci.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142881665"
"Argyrou A.; Agapiou A.","Argyrou, Argyro (58001257000); Agapiou, Athos (35188628700)","58001257000; 35188628700","A Review of Artificial Intelligence and Remote Sensing for Archaeological Research","2022","Remote Sensing","14","23","6000","","","","8","10.3390/rs14236000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143829979&doi=10.3390%2frs14236000&partnerID=40&md5=6734de9a2fa99ecad8e9c3f9280e684d","Earth Observation Cultural Heritage Research Lab, Department of Civil Engineering and Geomatics, Faculty of Engineering and Technology, Cyprus University of Technology, Lemesos, 3036, Cyprus","Argyrou A., Earth Observation Cultural Heritage Research Lab, Department of Civil Engineering and Geomatics, Faculty of Engineering and Technology, Cyprus University of Technology, Lemesos, 3036, Cyprus; Agapiou A., Earth Observation Cultural Heritage Research Lab, Department of Civil Engineering and Geomatics, Faculty of Engineering and Technology, Cyprus University of Technology, Lemesos, 3036, Cyprus","The documentation and protection of archaeological and cultural heritage (ACH) using remote sensing, a non-destructive tool, is increasingly popular for experts around the world, as it allows rapid searching and mapping at multiple scales, rapid analysis of multi-source data sets, and dynamic monitoring of ACH sites and their environments. The exploitation of remote sensing data and their products have seen an increased use in recent years in the fields of archaeological science and cultural heritage. Different spatial and spectral analysis datasets have been applied to distinguish archaeological remains and detect changes in the landscape over time, and, in the last decade, archaeologists have adopted more thoroughly automated object detection approaches for potential sites. These approaches included, among others, object detection methods, such as those of machine learning (ML) and deep learning (DL) algorithms, as well as convolutional neural networks (CNN) and deep learning (DL) models using aerial and satellite images, airborne and spaceborne remote sensing (ASRS), multispectral, hyperspectral images, and active methods (synthetic aperture radar (SAR) and light detection and ranging radar (LiDAR)). Researchers also refer to the potential for archaeologists to explore such artificial intelligence (AI) approaches in various ways, such as identifying archaeological features and classifying them. Here, we present a review study related to the contributions of remote sensing (RS) and artificial intelligence in archaeology. However, a main question remains open in the field of research: the rate of positive contribution of remote sensing and artificial intelligence techniques in archaeological research. The scope of this study is to summarize the state of the art related to AI and RS for archaeological research and provide some further insights into the existing literature. © 2022 by the authors.","archaeology; artificial intelligence; remote sensing; site detection; surface detection","Antennas; Convolutional neural networks; Deep learning; Learning systems; Object detection; Object recognition; Optical radar; Radar imaging; Radar target recognition; Space-based radar; Spectrum analysis; Synthetic aperture radar; Tracking radar; Archeology; Cultural heritages; Data set; Multiple scale; Multisource data; Nondestructive tools; Rapid analysis; Remote-sensing; Site detection; Surface detection; Remote sensing","","","","","Cyprus University of Technology, CUT","This research was funded by the ENSURE project (innovative survey techniques for detection of surface and sub-surface archaeological remains), a Cyprus University of Technology internal funding.","Orengo H.A., Garcia-Molsosa A., A brave new world for archaeological survey: Automated machine learning-based potsherd detection using high-resolution drone imagery, J. Archaeol. Sci, 112, (2019); Renfrew C., Bahn P., Chapter 2, What is Left? The Variety of the Evidence, Archaeology, Theories, Methods and Practice, (1991); Alcock S.E., Cherry J.F., Introduction, Side-By-Side Survey. Comparative Regional Studies in the Mediterranean World, pp. 1-9, (2004); De Laet V., Paulissen E., Waelkens M., Methods for the extraction of archaeological features from very high-resolution Ikonos-2 remote sensing imagery, Hisar (southwest Turkey), J. Archaeol. Sci, 34, pp. 830-841, (2007); Barone P.M., Wueste E., Hodges R., Remote Sensing Materials for a Preliminary Archaeological Evaluation of the Giove Countryside (Terni, Italy), Remote Sens, 12, (2020); Tapete D., Remote Sensing and Geosciences for Archaeology, Geosciences, 8, (2018); Lasaponara R., Masini N., Satellite remote sensing in archaeology: Past, present and future perspectives, J. Archaeol. Sci, 38, pp. 1995-2002, (2011); Materazzi F., Pacifici M., Archaeological crop marks detection through drone multispectral remote sensing and vegetation indices: A new approach tested on the Italian pre-Roman city of Veii, J. Archaeol. Sci. Rep, 41, (2022); Czajlik Z., Cresnar M., Doneus M., Fera M., Kramberger A.H., Mele M., Researching archaeological landscapes across borders Strategies, methods and decisions for the 21st century Graz—Budapest, 2019, Diss. Archaeol, pp. 299-301, (2020); Hadjimitsis D.G., Agapiou A., Themistocleous K., Alexakis D.D., Sarris A., Remote Sensing for Archaeological Applications: Management, Documentation and Monitoring, Remote Sens. Environ. Integr. Approaches, 2013, pp. 57-95, (2013); Fountas S., Gemtos T., A [Undergraduate Textbook, (2015); Fischer W.A., Hemphill W.R., Kover A., Progress in remote sensing (1972–1976), Photogrammetria, 32, pp. 33-72, (1976); Davis D.S., Seeber K.E., Sanger M.C., Addressing the problem of disappearing cultural landscapes in archaeological research using multi-scalar survey, J. Isl. Coast. Archaeol, 16, pp. 524-540, (2021); Agapiou A., Sarris A., Beyond GIS Layering: Challenging the (Re)use and Fusion of Archaeological Prospection Data Based on Bayesian Neural Networks (BNN), Remote Sens, 10, (2018); Gallwey J., Eyre M., Tonkins M., Coggan J., Bringing lunar LiDAR back down to earth: Mapping our industrial heritage through deep transfer learning, Remote Sens, 11, (2019); Campbell J.B., Wynne R.H., Introduction to Remote Sensing, (2011); Davis D.S., Geographic disparity in machine intelligence approaches for archaeological remote sensing research, Remote Sens, 12, (2020); Sarris A., Kokkinou E., Soupios P., Papadopoulos E., Trigas V., Sepsa O., Gionis D., Iakovou M., Agapiou A., Satraki A., Et al., Geophysical investigations in Palaipafos, Cyprus, 36th Annual Conference on Computer Applications and Quantitative Methods in Archaeology, CAA, 2008 “On the Road to Reconstructing the Past, Budapest, (2008); Bicker S.H., Machine Learning Arrives in Archaeology, Camb. Univ. Press Behalf Soc. Am. Archaeol, 6, pp. 186-191, (2021); Bini M., Isola I., Zanchetta G., Ribolini A., Ciampalini A., Baneschi I., Mele D., D'Agata A.L., Identification of leveled archeological mounds (Höyük) in the alluvial plain of the Ceyhan River (Southern Turkey) by satellite remote-sensing analyses, Remote Sens, 10, (2018); Davis D.S., Sanger M.C., Lipo C.P., Automated mound detection using lidar and object-based image analysis in Beaufort County, South Carolina, Southeast. Archaeol, 38, pp. 23-37, (2019); Verschoof-van der Vaart W.B., Lambers K., Applying automated object detection in archaeological practice: A case study from the southern Netherlands, Archaeol. Prospect, 29, pp. 15-31, (2021); Albrecht C.M., Fisher C., Freitag M., Hamann H.F., Pankanti S., Pezzutti F., Rossi F., Learning and Recognizing Archeological Features from LiDAR Data, Proceedings of the 2019 IEEE International Conference on Big Data (Big Data); Orengo H.A., Garcia-Molsosa A., Berganzo-Besga I., Landauer J., Aliende P., Tres-Martinez S., New developments in drone-based automated surface survey: Towards a functional and effective survey system, Archaeol. Prospect, 28, pp. 519-526, (2021); Snitker G., Moser J.D., Southerlin B., Stewart C., Detecting historic tar kilns and tar production sites using high-resolution, aerial LiDAR-derived digital elevation models: Introducing the Tar KilnFeature Detection workflow (TKFD) using open-access R and FIJI software, J. Archaeol. Sci. Rep, 41, (2022); Borie C., Parcero-Oubina C., Kwon Y., Salazar D., Flores C., Olguin L., Andrade P., Beyond site detection: The role of satellite remote sensing in analysing archaeological problems. A case study in Lithic Resource Procurement in the Atacama Desert, Northern Chile, Remote Sens, 11, (2019); Davis D.S., Object-Based Image Analysis: A Review of Developments and Future Directions of Automated Feature Detection in Landscape Archaeology, Archaeol. Prospect, 26, pp. 155-163, (2018); Monna F., Magail J., Rolland T., Navarro N., Wilczek J., Gantulga J.O., Esin Y., Granjon L., Allard A.C., Chateau-Smith C., Machine learning for rapid mapping of archaeological structures made of dry stones–Example of burial monuments from the Khirgisuur culture, Mongolia, J. Cult. Herit, 43, pp. 118-128, (2020); Thabeng O.L., Merlo S., Adam E., High-Resolution Remote Sensing and Advanced Classification Techniques for the Prospection of Archaeological Sites’ Markers: The Case of Dung Deposits in the Shashi-Limpopo Confluence Area (Southern Africa), J. Archaeol. Sci, 102, pp. 48-60, (2019); Inomata T., Pinzon F., Ranchos J., Haraguchi T., Nasu H., Fernandez-Diaz J., Aoyama K., Yonenobu H., Archaeological Application of Airborne LiDAR with Object-Based Vegetation Classification and Visualization Techniques at the Lowland Maya Site, Remote Sens, 9, (2017); Cowley D., Verhoeven G., Traviglia A., Editorial for Special Issue: “Archaeological Remote Sensing in the 21st Century: (Re)Defining Practice and Theory, Remote Sens, 13, (2021); Luo L., Wang X., Guo H., Lasaponara R., Zong X., Masini N., Wang G., Shia P., Khatteli H., Chen F., Et al., Airborne and spaceborne remote sensing for archaeological and cultural heritage applications: A review of the century (1907–2017), Remote Sens. Environ, 232, (2019); Verhoeven G.J., Are We There Yet? A Review and Assessment of Archaeological Passive Airborne Optical Imaging Approaches in the Light of Landscape Archaeology, Geosciences, 7, (2017); Hritz C., Tracing settlement patterns and channel systems in southern Mesopotamia using remote sensing, J. Field Archaeol, 35, pp. 184-203, (2010); Abate N., Frisetti A., Marazzi F., Masini N., Lasaponara R., Multitemporal–Multispectral UAS Surveys for Archaeological Research: The Case Study of San Vincenzo Al Volturno (Molise, Italy), Remote Sens, 13, (2021); Aqdus S.A., Hanson W.S., Drummond J., The potential of hyperspectral and multi-spectral imagery to enhance ar-chaeological cropmark detection: A comparative study, J. Archaeol. Sci, 39, pp. 1915-1924, (2012); Abrams M., Comer D., Multispectral and hyperspectral technology and archaeological applications, Mapping Archaeological Landscapes from Space, pp. 57-71, (2013); Agapiou A., Hadjimitsis D.G., Sarris A., Georgopoulos A., Alexakis D.D., Optimum temporal and spectral window for monitoring crop marks over archaeological remains in the Mediterranean region, J. Archaeol. Sci, 40, pp. 1479-1492, (2013); Geser G., Impact of COVID-19 on Archaeology and Cultural Heritage. Salzburg Research 29 October 2021; Fitton T., Wynne-Jones S., Integrating legacy data for archaeological and remote survey at the 7–15th century site of Unguja Ukuu, Zanzibar, Proceedings of the CAA 2021, Programme and Abstracts, pp. 534-535; Budka J., Archaeological Team Building in Times of COVID-19. Munich University Attab to Ferka Survey Project Blog, (2020); Agapiou A., Lysandrou V., Remote sensing archaeology: Tracking and mapping evolution in European scientific literature from 1999 to 2015, J. Archaeol. Sci, 4, pp. 192-200, (2015); Agapiou A., Alexakis D., Sarris A., Hadjimitsis D.G., Evaluating the Potentials of Sentinel-2 for Archaeological Perspective, Remote Sens, 6, pp. 2176-2194, (2014); Alexakis D., Sarris A., Astaras T., Albanakis K., Integrated GIS, remote sensing and geo-morphologic approaches for the reconstruction of the landscape habitation of Thessaly during the neolithic period, J. Archaeol. Sci, 38, pp. 89-100, (2011); Hadjimitsis D.G., Agapiou A., Alexakis D.D., Sarris A., Exploring natural and anthropogenic risk for cultural heritage in Cyprus using remote sensing and GIS August 2011, Int. J. Digit. Earth, 6, pp. 115-142, (2013); Hadjimitsis D.G., Themistocleous K., Agapiou A., Clayton C.R.I., Multi-temporal study of archaeological sites in Cyprus using atmospheric corrected satellite remotely sensed data, Int. J. Archit. Comput, 7, pp. 121-138, (2009); Luo L., Wang X., Liu J., Guo H., Zong X., Ji W., Cao H., VHR GeoEye-1 imagery reveals an ancient water landscape at the Longcheng site, northern Chaohu Lake Basin (China), Int. J. Digit. Earth, 10, pp. 139-154, (2016); Brivio P.A., Pepe M., Tomasoni R., Multispectral and multiscale remote sensing data for archaeological prospecting in an alpine alluvial plain, J. Cult. Herit, 1, pp. 155-164, (2000); Aminzadeh B., Samani F., Identifying the boundaries of the historical site of Persepolis using remote sensing, Remote Sens. Environ, 102, pp. 52-62, (2006); Lasaponara R., Masini N., Detection of archaeological crop marks by using satellite Quick Bird multispectral imagery, J. Archaeol. Sci, 34, pp. 214-221, (2007); Evans D., Pottier C., Fletcher R., Hensley S., Tapley I., Milne A., Barbetti M., A comprehensive archaeological map of the world’s largest preindustrial settlement complex at Angkor, Cambodia, Proc. Natl. Acad. Sci. USA, 104, pp. 14277-14282, (2007); Garrison T.G., Houston S.D., Golden C., Inomata T., Nelson Z., Munson J., Evaluating the use of IKONOS satellite imagery in lowland Maya settlement archaeology, J. Archaeol. Sci, 35, pp. 2770-2777, (2008); Alexakis D., Sarris A., Astaras T., Albanakis K., Detection of Neolithic settlements in Thessaly (Greece) through multispectral and hyperspectral satellite imagery, Sensors, 9, pp. 1167-1187, (2009); Rajani M.B., Rajawat A.S., Potential of satellite-based sensors for studying distribution of archaeological sites along palaeochannels: Harappan sites a case study, J. Archaeol. Sci, 38, pp. 2010-2016, (2011); Agapiou A., Alexakis D.D., Hadjimitsis D.G., Spectral sensitivity of ALOS, ASTER, IKONOS, LANDSAT and SPOT satellite imagery intended for the detection of archaeological crop marks, Int. J. Digit. Earth, 7, pp. 351-372, (2014); Luo L., Wang X., Liu C., Guo H., Du X., Integrated RS, GIS and GPS approaches to archaeological prospecting in the Hexi Corridor, NW China: A case study of the royal road to ancient Dunhuang, J. Archaeol. Sci, 50, pp. 178-190, (2014); Kalayci T., Lasaponara R., Wainwright J., Masini N., Multispectral Contrast of Archaeological Features: A Quantitative Evaluation, Remote Sens, 11, (2019); Lasaponara R., Leucci G., Masini N., Persico R., Scardozzi G., Towards an operative use of remote sensing for exploring the past using satellite data: The case study of Hierapolis (Turkey), Remote Sens. Environ, 174, pp. 148-164, (2016); Atzberger C., Wess M., Doneus M., Verhoeven G., ARCTIS—A MATLAB<sup>®</sup> Toolbox for archaeological imaging spectroscopy, Remote Sens, 6, pp. 8617-8638, (2014); Doneus M., Verhoeven G., Atzberger C., Wess M., Rus M., New ways to extract archaeological information from hyperspectral pixels, J. Archaeol. Sci, 52, pp. 84-96, (2014); Moore E., Freeman T., Hensley S., Spaceborne and airborne radar at Angkor: Introducing new technology to the ancient site, Remote Sensing in Archaeology, pp. 185-216, (2007); Dore N., Patruno J., Pottier E., Crespi M., New research in polarimetric SAR technique for archaeological purposes using ALOS PALSAR data, Archaeol. Prospect, 20, pp. 79-87, (2013); Stewart C., Lasaponara R., Schiavon G., Multi-frequency, polarimetric SAR analysis for archaeological prospection, Int. J. Appl. Earth Obs. Geoinf, 28, pp. 211-219, (2014); Chen F., Masini N., Yang R., Milillo P., Feng D., Lasaponara R., A space view of radar archaeological marks: First applications of COSMO-SkyMed X-Band Data, Remote Sens, 7, pp. 24-50, (2015); Tapete D., Cigna F., Donoghue D., ‘Looting marks’ in space-borne SAR imagery: Measuring rates of archaeological looting in Apamea (Syria) with TerraSAR-X Staring Spotlight, Remote Sens. Environ, 178, pp. 42-58, (2016); Stewart C., Montanaro R., Sala M., Riccardi P., Feature extraction in the north Sinai Desert using spaceborne synthetic aperture radar: Potential archaeological applications, Remote Sens, 8, (2016); Challis K., Howard A.J., A review of trends within archaeological remote sensing in alluvial environments, Archaeol. Prospect, 13, pp. 231-240, (2006); Chen F., Lasaponara R., Masini N., An overview of satellite synthetic aperture radar remote sensing in archaeology: From site detection to monitoring, J. Cult. Herit, 23, pp. 5-11, (2017); Tapete D., Cigna F., Trends and perspectives of space-borne SAR remote sensing for archaeological landscape and cultural heritage applications, J. Archaeol. Sci. Rep, 14, pp. 716-726, (2017); Punnee W.-A., An archaeological application of synthetic aperture radar (SAR) in Thailand, Geocarto Int, 10, pp. 65-69, (1995); Gade M., Kohlus J., Kost C., SAR Imaging of Archaeological Sites on Intertidal Flats in the German Wadden Sea, Geosciences, 7, (2017); Kadhim I., Abed F.M., The Potential of LiDAR and UAV-Photogrammetric Data Analysis to Interpret Archaeological Sites: A Case Study of Chun Castle in South-West England, Int. J. Geo.-Inf, 10, (2021); Traviglia A., Torsello A., Landscape Pattern Detection in Archaeological Remote Sensing, Geosciences, 7, (2017); Trier O.D., Cowley D.C., Waldeland A.U., Using deep neural networks on airborne laser scanning data: Results from a case study of semi-automatic mapping of archaeological topography on Arran, Scotland, Archaeol. Prospect, 26, pp. 165-175, (2019); Agapiou A., Vionis A., Papantoniou G., Detection of Archaeological Surface Ceramics Using Deep Learning Image-Based Methods and very High-Resolution UAV Imageries, Land, 10, (2021); Altaweel M., Khelifi A., Li Z., Squitieri A., Basmaji T., Ghazal M., Automated Archaeological Feature Detection Using Deep Learning on Optical UAV Imagery: Preliminary Results, Remote Sens, 14, (2022); Bickler S.H., Machine Learning Identification and Classification of Historic Ceramics. Archaeology in New Zealand, Res. Gate, 61, pp. 20-32, (2018); Bickler S.H., Prospects for Machine Learning for Shell Midden Analysis. Archaeology in New Zealand, Res. Gate, 61, pp. 48-58, (2018); Verschoof-van der Vaart W.B., Lambers K., Learning to Look at LiDAR: The Use of R-CNN in the Automated Detection of Archaeological Objects in LiDAR Data from the Netherlands, J. Comput. Appl. Archaeol, 2, pp. 31-40, (2019); Reese K.M., Deep learning artificial neural networks for non-destructive archaeological site dating, J. Archaeol. Sci, 132, (2021); Bonhage A., Eltaher M., Raab T., Breuss M., Raab A., Schneider A., A modified Mask region-based convolutional neural network approach for the automated detection of archaeological sites on high-resolution light detection and ranging-derived digital elevation models in the North German Lowland, Archaeol. Prospect, 28, pp. 177-186, (2021); Pawlowicz L.M., Downum C.E., Applications of deep learning to decorated ceramic typology and classification: A case study using Tusayan White Ware from Northeast Arizona, J. Archaeol. Sci, 130, (2021); Davis D.S., Defining what we study: The contribution of machine automation in archaeological research, Digit. Appl. Archaeol. Cult. Herit, 18, (2020); Olivier M., Verschoofvan der Vaart W., Implementing State of-the-Art Deep Learning Approaches for Archaeological Object Detection in Remotely—Sensed Data: The Results of Cross-Domain Collaboration, J. Comput. Appl. Archaeol, 4, pp. 274-289, (2021); Richards-Rissettoa F., Newtonb D., Al Zadjalic A., A 3D point cloud Deep Learning approach using Lidar to identify ancient Maya archaeological sites, Proceedings of the 28th CIPA Symposium “Great Learning & Digital Emotion; Berganzo-Besga I., Orengo H.A., Lumbreras F., Carrero-Pazos M., Fonte J., Vilas-Estevez B., Hybrid MSRM-Based Deep Learning and MultitemporalSentinel 2-Based Machine Learning Algorithm Detects Near 10k Archaeological Tumuli in North-Western Iberia, Remote Sens, 13, (2021); Verschoof-van der Vaart W.B., Lambers K., Kowalczyk W., Bourgeois Q.P., Combining Deep Learning and Location-Based Ranking for Large-Scale Archaeological Prospection of LiDAR Data from The Netherlands, ISPRS Int. J. Geo.-Inf, 9, (2020); Trier V.D., Reksten J.H., Loseth K., Automated mapping of cultural heritage in Norway from airborne lidar data using faster R-CNN, Int. J. Appl. Earth Obs. Geoinf, 95, (2021); Somrak M., Saso Dzeroski S., Kokalj Z., Learning to Classify Structures in ALS-Derived Visualizations of Ancient Maya Settlements with CNN, Remote Sens, 12, (2020); Maxwell A.E., Pourmohammadi P., Poyner J.D., Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data, Remote Sens, 12, (2020); Martin-Abadal M., Pinar-Molina M., Martorell-Torres A., Oliver-Codina G., Gonzalez-Cid Y., Underwater Pipe and Valve 3D Recognition Using Deep Learning Segmentation, J. Mar. Sci. Eng, 9, (2021); Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community, J. Appl. Remote Sens, 11, (2017); Fu T., Ma L., Li M., Johnson B.A., Using convolutional neural network to identify irregular segmentation objects from very high-resolution remote sensing imagery, J. Appl. Remote Sens, 12, (2018); Guyot A., Hubert-Moy L., Lorho T., Detecting neolithic burial mounds from lidar derived elevation data using a multi-scale approach and machine learning techniques, Remote Sens, 10, (2018); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, arXiv, (2014); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag, 4, pp. 22-40, (2016); Dominguez Rodrigo M., Cifuentes Alcobendas G., Jimenez Garcia B., Abellan N., Pizarro Monzo M., Organista E., Baquedano E., Artificial intelligence provides greater accuracy in the classification of modern and ancient bone surface modifications, Sci. Rep, 10, (2020); Caspari G., Crespo P., Convolutional Neural Networks for Archaeological Site Detection—Finding “Princely” Tombs, J. Archaeol. Sci, 110, (2019); Jamil A.H., Yakub F., Azizul Azizan A., Roslan S.A., Zaki S.A., Ahmad S.A., A Review on Deep Learning Application for Detection of Archaeological Structures, J. Adv. Res. Appl. Sci. Eng. Technol, 26, pp. 7-14, (2022); Mitchell M., Complexity. A Guided Tour, pp. 186-208, (2009); Casini L., Roccetti M., Delnevo G., Marchetti N., Orru V., The Barrier of meaning in archaeological data science, Proceedings of the SCIFI-IT’ 2020—4th Annual Science Fiction Prototyping Conference, pp. 61-65; Sharafi S., Fouladvand S., Simpson I., Alvarez J.A.B., Application of pattern recognition in detection of buried archaeological sites based on analysing environmental variables, Khorramabad Plain, West Iran, J. Archaeol. Sci. Rep, 8, pp. 206-215, (2016); Mehrnoush S., Mehrtash A., Khazraee E., Ur J., Deep Learning in Archaeological Remote Sensing: Automated Qanat Detection in the Kurdistan Region of Iraq, Remote Sens, 12, (2020); Bundzel M., Jascur M., Kovac M., Lieskovsky T., Sincak P., Tkacik T., Semantic Segmentation of Airborne LiDAR Data in Maya Archaeology, Remote Sens, 12, (2020); Orengo H.A., Conesa F.C., Garcia-Molsosa A., Lobo A., Green A.S., Madella M., Petrie C.A., Automated detection of archaeological mounds using machine learning classification of multi-sensor and multi-temporal satellite data, Proc. Natl. Acad. Sci. USA, 117, pp. 18240-18250, (2020); Davis D.S., Gaspari G., Lipo C.P., Sanger M.C., Deep learning reveals extent of Archaic Native American shell-ring building.practices, J. Archaeoogical Sci, 132, (2021); Luo L., Wang X., Guo H., Huadong Guo H., Lasaponara R., Shi P., Bachagha N., Li L., Yao Y., Masini N., Et al., Google Earth as a Powerful Tool for Archaeological and Cultural Heritage Applications: A Review, Remote Sens, 10, (2018); Barber M., A History of Aerial Photography and Archaeology. Mata Hari’s Glass Eye and Other Stories, A History of Aerial Photography and Archaeology, (2011); Hutson M., Has artificial intelligence become alchemy?, Science, 360, (2018); Cassar J., Chapter: Climate change and archaeological sites: Adaptation strategies, Cultural Heritage from Pollution to Climate Change, (2016); Burkea A., Peros M.C., Wren C.D., Pausata F.S.R., Riel-Salvatore J., Moine O., Vernal A., Kageyama M., Solene Boisard S., The archaeology of climate change: The case for cultural diversity, Proc. Natl. Acad. Sci. USA, 118, (2021)","A. Argyrou; Earth Observation Cultural Heritage Research Lab, Department of Civil Engineering and Geomatics, Faculty of Engineering and Technology, Cyprus University of Technology, Lemesos, 3036, Cyprus; email: ac.argyrou@edu.cut.ac.cy","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143829979"
"Saheed Y.K.; Usman A.A.; Sukat F.D.; Abdulrahman M.","Saheed, Yakub Kayode (57204126295); Usman, Aisha Abubakar (58195923900); Sukat, Favour Dirwokmwa (58195180300); Abdulrahman, Muftahu (58195473600)","57204126295; 58195923900; 58195180300; 58195473600","A novel hybrid autoencoder and modified particle swarm optimization feature selection for intrusion detection in the internet of things network","2023","Frontiers in Computer Science","5","","997159","","","","0","10.3389/fcomp.2023.997159","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153524300&doi=10.3389%2ffcomp.2023.997159&partnerID=40&md5=21be3d3e792a98a22c0e9501f3b9c39c","School of Information Technology and Computing, Adamawa State, Yola, Nigeria; American University of Nigeria, Adamawa State, Yola, Nigeria; African Centre of Excellence on Technology Enhanced Learning, Abuja, Nigeria; National Open University of Nigeria, Abuja, Nigeria","Saheed Y.K., School of Information Technology and Computing, Adamawa State, Yola, Nigeria, American University of Nigeria, Adamawa State, Yola, Nigeria; Usman A.A., School of Information Technology and Computing, Adamawa State, Yola, Nigeria, American University of Nigeria, Adamawa State, Yola, Nigeria; Sukat F.D., School of Information Technology and Computing, Adamawa State, Yola, Nigeria, American University of Nigeria, Adamawa State, Yola, Nigeria; Abdulrahman M., African Centre of Excellence on Technology Enhanced Learning, Abuja, Nigeria, National Open University of Nigeria, Abuja, Nigeria","The Internet of Things (IoT) represents a paradigm shift in which the Internet is connected to real objects in a range of areas, including home automation, industrial processes, human health, and environmental monitoring. The global market for IoT devices is booming, and it is estimated that there will be 50 billion connected devices by the end of 2025. This explosion of IoT devices, which can be expanded more easily than desktop PCs, has led to an increase in cyber-attacks involving IoT devices. To address this issue, it is necessary to create novel approaches for identifying attacks launched by hacked IoT devices. Due to the possibility that these attacks would succeed, Intrusion Detection Systems (IDS) are required. IDS' feature selection stage is widely regarded as the most essential stage. This stage is extremely time-consuming and labor-intensive. However, numerous machine learning (ML) algorithms have been proposed to enhance this stage to boost an IDS's performance. These approaches, however, did not produce desirable results in terms of accuracy and detection rate (DR). In this paper, we propose a novel hybrid Autoencoder and Modified Particle Swarm Optimization (HAEMPSO) for feature selection and deep neural network (DNN) for classification. The PSO with modification of inertia weight was utilized to optimize the parameters of DNN. The experimental analysis was performed on two realistic UNSW-NB15 and BoT-IoT datasets that are suitable for IoT environment. The findings obtained by analyzing the proposed HAEMPSO against the Generic attack in the UNSW-NB15 dataset gave an accuracy of 98.8%, and a DR of 99.9%. While the benign class revealed an accuracy of 99.9% and DR of 99.7%. In the BoT-IoT dataset, the DDoS HTTP attack revealed an accuracy of 99.22% and DR of 97.79%. While the benign class gave an accuracy of 97.54% and DR of 97.92%. In comparison with the state-of-the-art machine learning schemes, our proposed HAEMPSO-DNN achieved a competitive feat in terms of DR and accuracy. Copyright © 2023 Saheed, Usman, Sukat and Abdulrahman.","autoencoder (AE); deep neural network; Internet of Things; machine learning; particle swarm optimization","","","","","","","","Abbas N., Asim M., Tariq N., Baker T., Abbas S., A mechanism for securing IoT-enabled applications at the fog layer, J. Sens. Actuator Netw, 8, pp. 1-18, (2019); Ahanger T.A., Aljumah A., Atiquzzaman M., State-of-the-art survey of artificial intelligent techniques for IoT security, Comput. Netw, 19, (2022); Alterazi H.A., Kshirsagar P.R., Manoharan H., Selvarajan S., Alhebaishi N., Srivastava G., Et al., Prevention of cyber security with the internet of things using particle swarm optimization, Sensors, 22, (2022); Arshad J., Azad M.A., Abdeltaif M.M., Salah K., An intrusion detection framework for energy constrained IoT devices, Mech. Syst. Signal Process, 136, (2020); Askarzadeh A., A novel metaheuristic method for solving constrained engineering optimization problems: Crow search algorithm, Comput. Struct, 169, pp. 1-12, (2016); Aslahi-Shahri B.M., Rahmani R., Chizari M., Maralani A., Eslami M., Golkar M.J., Et al., A hybrid method consisting of GA and SVM for intrusion detection system, Neural Comput. Appl, 27, pp. 1669-1676, (2016); Atlam H.F., Alenezi A., Alassafi M.O., Alshdadi A.A., Wills G.B., “Security, cybercrime and digital forensics for IoT,”, Principles of Internet of Things (IoT) Ecosystem: Insight Paradigm. Intelligent Systems Reference Library, (2022); Atzori L., Iera A., Morabito G., The internet of things: a survey, Comput. Netw, 54, pp. 2787-2805, (2010); Aydin M.A., Zaim A.H., Ceylan K.G., A hybrid intrusion detection system design for computer network security, Comput. Electr. Eng, 35, pp. 517-526, (2009); Blanco R., Malagon P., Briongos S., Moya J.M., “Anomaly detection using gaussian mixture probability model to implement intrusion detection system,”, Hybrid Artificial Intelligent Systems. HAIS 2019. Lecture Notes in Computer Science, pp. 648-659, (2019); Chohra A., Shirani P., Karbab E.B., Debbabi M., CHAMELEON: Optimized feature selection using particle swarm optimization and ensemble methods for network anomaly detection, Comput. Secur, 117, (2022); Choudhary S., Kesswani N., Analysis of KDD-Cup'99, NSL-KDD and UNSW-NB15 Datasets using Deep Learning in IoT, Procedia Comput. Sci, 167, pp. 1561-1573, (2020); Chung Y.Y., Wahid N., A hybrid network intrusion detection system using simplified swarm optimization (SSO), Appl. Soft Comput. J, 12, pp. 3014-3022, (2012); Derhab A., Guerroumi M., Gumaei A., Maglaras L., Ferrag M.A., Mukherjee M., Et al., Blockchain and Random Subspace Learning-Based IDS for SDN-Enabled Industrial IoT Security, Sensors (Switzerland), 19, pp. 1-24, (2019); Elbasiony R.M., Sallam E.A., Eltobely T.E., Fahmy M.M., A hybrid network intrusion detection framework based on random forests and weighted k-means, Ain Shams Eng. J, 4, pp. 753-762, (2013); Fenanir S., Semchedine F., Baadache A., A Machine Learning-Based Lightweight Intrusion Detection System for the Internet of Things, Rev. d'Intelligence Artif, 33, pp. 203-211, (2019); Ferrag M.A., Derdour M., Mukherjee M., Derhab A., Maglaras L., Janicke H., Et al., Blockchain technologies for the internet of things: research issues and challenges, IEEE Internet Things J, 6, pp. 2188-2204, (2019); Ferrag M.A., Maglaras L., Deliverycoin: An IDS and blockchain-based delivery framework for drone-delivered services, Computers, 8, pp. 1-15, (2019); Ferrag M.A., Maglaras L., DeepCoin: a novel deep learning and blockchain-based energy exchange framework for smart grids, IEEE Trans. Eng. Manag, 67, pp. 1285-1297, (2020); Ferrag M.A., Maglaras L., Ahmim A., Derdour M., Janicke H., RDTIDS: Rules and decision tree-based intrusion detection system for internet-of-things networks, Futur. Internet, 12, pp. 1-14, (2020); Ferrag M.A., Maglaras L.A., Janicke H., Smith R., Deep Learning Techniques for Cyber Security Intrusion Detection : A Detailed Analysis, pp. 126-136, (2019); Govindarajan M., Chandrasekaran R., Intrusion detection using neural based hybrid classification methods, Comput. Netw, 55, pp. 1662-1671, (2011); Habib M., Aljarah I., Faris H., A modified multi-objective particle swarm optimizer-based lévy flight: an approach toward intrusion detection in internet of things, Arab. J. Sci. Eng, 45, pp. 6081-6108, (2020); Kayode Saheed Y., Idris Abiodun A., Misra S., Kristiansen Holone M., Colomo-Palacios R., A machine learning-based intrusion detection for detecting internet of things network attacks, Alexandria Eng. J, 61, pp. 9395-9409, (2022); Kevric J., Jukic S., Subasi A., An effective combining classifier approach using tree algorithms for network intrusion detection, Neural Comput. Appl, 28, pp. 1051-1058, (2017); Kim G., Lee S., Kim S., A novel hybrid intrusion detection method integrating anomaly detection with misuse detection, Expert Syst. Appl, 41, pp. 1690-1700, (2014); Lahasan B., Samma H., Optimized deep autoencoder model for internet of things intruder detection, IEEE Access, 10, pp. 8434-8448, (2022); Leo M., Battisti F., Carli M., Neri A., A federated architecture approach for Internet of Things security, 2014 Euro Med Telco Conference From Netw. Infrastructures to Netw. Fabr. Revolut. Edges, EMTC 2014, (2014); Li S., Li Y., Han W., Du X., Guizani M., Tian Z., Et al., Malicious mining code detection based on ensemble learning in cloud computing environment, Simul. Model. Pract. Theory, 113, (2021); Li S., Zhang Q., Wu X., Han W., Tian Z., Attribution classification method of APT malware in IoT Using Machine Learning Techniques, Secur. Commun. Netw, pp. 1-12, (2021); Li W., Meng W., Au M.H., Enhancing collaborative intrusion detection via disagreement-based semi-supervised learning in IoT environments, J. Netw. Comput. Appl, 161, pp. 1-9, (2020); Liang C., Shanmugam B., Azam S., Karim A., Islam A., Zamani M., Et al., Intrusion detection system for the internet of things based on blockchain and multi-agent systems, Electron, 9, pp. 1-27, (2020); Lin W.C., Ke S.W., Tsai C.F., CANN: An intrusion detection system based on combining cluster centers and nearest neighbors, Knowl. Based Syst, 78, pp. 13-21, (2015); Liu J., Yang D., Lian M., Li M., Research on intrusion detection based on particle swarm optimization in IoT, IEEE Access, 9, pp. 38254-38268, (2021); Liu W., Wang Z., Liu X., Zeng N., Liu Y., Alsaadi F.E., Et al., A survey of deep neural network architectures and their applications, Neurocomputing, 234, pp. 11-26, (2017); Marlow R., Kuriyakose S., Mesaros N., Han H.H., Tomlinson R., Faust S.N., Et al., A phase III, open-label, randomised multicentre study to evaluate the immunogenicity and safety of a booster dose of two different reduced antigen diphtheria-tetanus-acellular pertussis-polio vaccines, when co-administered with measles-mumps-rubella vacci, Vaccine, 36, pp. 2300-2306, (2018); Minh Dang L., Piran M.J., Han D., Min K., Moon H., A survey on internet of things and cloud computing for healthcare, Electron, 8, pp. 1-49, (2019); Moustafa N., Slay J., The evaluation of network anomaly detection systems: statistical analysis of the UNSW-NB15 data set and the comparison with the KDD99 data set, Inf. Secur. J, 25, pp. 18-31, (2016); Oh D., Kim D., Ro W.W., A malicious pattern detection engine for embedded security systems in the internet of things, Sensors (Switzerland), 14, pp. 24188-24211, (2014); Pongle P., Chavan G., Real time intrusion and wormhole attack detection in internet of things, Int. J. Comput. Appl, 121, pp. 1-9, (2015); Putra G.D., Dedeoglu V., Kanhere S.S., Jurdak R., Poster abstract: Towards scalable and trustworthy decentralized collaborative intrusion detection system for IoT, Proc. - 5th ACM/IEEE Conf. Internet Things Des. Implementation, IoTDI, 2020, pp. 256-257, (2020); Ramadan R.A., Yadav K., A novel hybrid intrusion detection system (Ids) for the detection of internet of things (IoT) network attacks, Ann. Emerg. Technol. Comput, 4, pp. 61-74, (2020); Ravi V., Chaganti R., Alazab M., Recurrent deep learning-based feature fusion ensemble meta-classifier approach for intelligent network intrusion detection system, Comput. Electr. Eng, 102, (2022); Raza S., Wallgren L., Voigt T., SVELTE: Real-time intrusion detection in the Internet of Things, Ad Hoc Networks, 11, pp. 2661-2674, (2013); Saheed Y.K., “Performance improvement of intrusion detection system for detecting attacks on internet of things and edge of things,”, Artificial Intelligence for Cloud and Edge Computing. Internet of Things, (2022); Saheed Y.K., “A binary firefly algorithm based feature selection method on high dimensional intrusion detection data,”, Illumination of Artificial Intelligence in Cybersecurity and Forensics. Lecture Notes on Data Engineering and Communications Technologies, (2022); Saheed Y.K., Arowolo M.O., Tosho A.U., An efficient hybridization of K-means and genetic algorithm based on support vector machine for cyber intrusion detection system, Int. J. Electr. Eng. Inform, 14, pp. 426-442, (2022); Saheed Y.K., Baba U.A., Raji M.A., “Big data analytics for credit card fraud detection using supervised machine learning models,”, Big Data Analytics in the Insurance Market (Emerald Studies in Finance, Insurance, and Risk Management), pp. 31-56, (2022); Saheed Y.K., Hamza-usman F.M., Feature Selection with IG-R for Improving Performance of Intrusion Detection System, Int. J. Commun. Netw. Inform. Secur, 12, pp. 338-344, (2020); Sedjelmaci H., Senouci S.M., Al-Bahri M., A lightweight anomaly detection technique for low-resource IoT devices: A game-theoretic methodology, 2016 IEEE Int. Conf. Commun. ICC, (2016); Shafiq M., Tian Z., Bashir A.K., Du X., Guizani M., CorrAUC: A Malicious Bot-IoT traffic detection method in IoT network using machine-learning techniques, IEEE Internet Things J, 8, pp. 3242-3254, (2021); Shafiq M., Tian Z., Sun Y., Du X., Guizani M., Selection of effective machine learning algorithm and Bot-IoT attacks traffic identification for internet of things in smart city, Futur. Gener. Comput. Syst, 107, pp. 433-442, (2020); Sicari S., Rizzardi A., Coen-Porisini A., 5G In the internet of things era: An overview on security and privacy challenges, Comput. Networks, 179, (2020); Singh S., Sharma K., Yoon B., Shojafar M., Cho G.H., Ra I.H., Et al., Convergence of blockchain and artificial intelligence in IoT network for the sustainable smart city, Sustain. Cities Soc, 63, (2020); Subham K.G., Meenakshi T., Jyoti G., Hybrid optimization and deep learning based intrusion detection system, Comput. Electr. Eng, 100, pp. 1-15, (2022); Thamilarasu G., Chawla S., Towards deep-learning-driven intrusion detection for the internet of things, Sensors (Switzerland), 19, (2019); Thanigaivelan N.K., Nigussie E., Kanth R.K., Virtanen S., Isoaho J., Distributed internal anomaly detection system for Internet-of-Things, 2016 13th IEEE Annu. Consum. Commun. Netw. Conf. CCNC 2016, pp. 319-320, (2016); Wang G., Hao J., Mab J., Huang L., A new approach to intrusion detection using Artificial Neural Networks and fuzzy clustering, Expert Syst. Appl, 37, pp. 6225-6232, (2010); Yan B., Han G., Effective feature extraction via stacked sparse autoencoder to improve intrusion detection system, IEEE Access, 6, pp. 41238-41248, (2018); Zhang W., Zhang Y., Intrusion detection model for industrial internet of things based on improved autoencoder, Comput. Intell. Neurosci, 27, (2022)","Y.K. Saheed; School of Information Technology and Computing, Yola, Adamawa State, Nigeria; email: yakubu.saheed@aun.edu.ng","","Frontiers Media S.A.","","","","","","26249898","","","","English","Frontier. Comput. Sci.","Article","Final","","Scopus","2-s2.0-85153524300"
"Jung S.; Heo H.; Park S.; Jung S.-U.; Lee K.","Jung, Sunguk (57888063000); Heo, Hyeonbeom (57888734400); Park, Sangheon (56044334200); Jung, Sung-Uk (37043905800); Lee, Kyungjae (55655680100)","57888063000; 57888734400; 56044334200; 37043905800; 55655680100","Benchmarking Deep Learning Models for Instance Segmentation","2022","Applied Sciences (Switzerland)","12","17","8856","","","","2","10.3390/app12178856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137941610&doi=10.3390%2fapp12178856&partnerID=40&md5=5fe1811553193ca13592c6d7637123d2","Department of Computer Science, Yong In University, Yongin, 17092, South Korea; Content Research Division, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; School of Artificial Intelligence, Yong In University, Yongin, 17092, South Korea","Jung S., Department of Computer Science, Yong In University, Yongin, 17092, South Korea; Heo H., Department of Computer Science, Yong In University, Yongin, 17092, South Korea; Park S., Content Research Division, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; Jung S.-U., Content Research Division, Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; Lee K., School of Artificial Intelligence, Yong In University, Yongin, 17092, South Korea","Instance segmentation has gained attention in various computer vision fields, such as autonomous driving, drone control, and sports analysis. Recently, many successful models have been developed, which can be classified into two categories: accuracy- and speed-focused. Accuracy and inference time are important for real-time applications of this task. However, these models just present inference time measured on different hardware, which makes their comparison difficult. This study is the first to evaluate and compare the performances of state-of-the-art instance segmentation models by focusing on their inference time in a fixed experimental environment. For precise comparison, the test hardware and environment should be identical; hence, we present the accuracy and speed of the models in a fixed hardware environment for quantitative and qualitative analyses. Although speed-focused models run in real-time on high-end GPUs, there is a trade-off between speed and accuracy when the computing power is insufficient. The experimental results show that a feature pyramid network structure may be considered when designing a real-time model, and a balance between the speed and accuracy must be achieved for real-time application. © 2022 by the authors.","convolutional neural networks; deep learning; image segmentation; instance segmentation; object detection","","","","","","ICT R&D program of MSIT; Ministry of Science, ICT and Future Planning, MSIP, (2020R1G1A1102041); Electronics and Telecommunications Research Institute, ETRI; National Research Foundation of Korea, NRF; Institute for Information and Communications Technology Promotion, IITP, (2021-0-00230)","This research was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2020R1G1A1102041); by Electronics and Telecommunications Research Institute (ETRI) grant funded by ICT R&D program of MSIT/IITP (2021-0-00230, Development of real·virtual environmental analysis based adaptive interaction technology).","Kirillov A., He K., Girshick R., Rother C., Dollar P., Panoptic segmentation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9404-9413; Liang T., Chu X., Liu Y., Wang Y., Tang Z., Chu W., Chen J., Ling H., Cbnetv2: A composite backbone network architecture for object detection, arXiv, (2021); Guo R., Niu D., Qu L., Li Z., SOTR: Segmenting Objects with Transformers, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 7157-7166; He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969; Lee Y., Park J., CenterMask: Real-Time Anchor-Free Instance Segmentation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Wang X., Zhang R., Kong T., Li L., Shen C., Solov2: Dynamic and fast instance segmentation, Adv. Neural Inf. Process. Syst, 33, pp. 17721-17732, (2020); Chen H., Sun K., Tian Z., Shen C., Huang Y., Yan Y., Blendmask: Top-down meets bottom-up for instance segmentation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8573-8581; Bolya D., Zhou C., Xiao F., Lee Y.J., Yolact: Real-time instance segmentation, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9157-9166; Liu H., Soto R.A.R., Xiao F., Lee Y.J., Yolactedge: Real-time instance segmentation on the edge, Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 9579-9585; Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, Proceedings of the European Conference on Computer Vision, pp. 740-755, (2014); Ignatov A., Timofte R., Kulik A., Yang S., Wang K., Baum F., Wu M., Xu L., Van Gool L., Ai benchmark: All about deep learning on smartphones in 2019, Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 3617-3635; Luo C., He X., Zhan J., Wang L., Gao W., Dai J., Comparison and Benchmarking of AI Models and Frameworks on Mobile Devices, arXiv, (2020); Bianco S., Cadene R., Celona L., Napoletano P., Benchmark Analysis of Representative Deep Neural Network Architectures, IEEE Access, 6, pp. 64270-64277, (2018); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I., Wojna Z., Song Y., Guadarrama S., Et al., Speed/accuracy trade-offs for modern convolutional object detectors, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7310-7311; Wang Y.E., Wei G.Y., Brooks D., Benchmarking TPU, GPU, and CPU platforms for deep learning, arXiv, (2019); Ammirato P., Poirson P., Park E., Kosecka J., Berg A.C., A dataset for developing and benchmarking active vision, Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 1378-1385; Perazzi F., Pont-Tuset J., McWilliams B., Van Gool L., Gross M., Sorkine-Hornung A., A benchmark dataset and evaluation methodology for video object segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 724-732; Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117-2125; Devlin J., Chang M.W., Lee K., Toutanova K., Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv, (2018); Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need, Adv. Neural Inf. Process. Syst, 30, pp. 5999-6009, (2017); Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Et al., An image is worth 16 × 16 words: Transformers for image recognition at scale, arXiv, (2020); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Adv. Neural Inf. Process. Syst, 28, pp. 91-99, (2015); Long J., Shelhamer E., Darrell T., Fully Convolutional Networks for Semantic Segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo B., Swin transformer: Hierarchical vision transformer using shifted windows, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022; Tian Z., Shen C., Chen H., He T., FCOS: Fully Convolutional One-Stage Object Detection, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9627-9636; Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988; Yang L., Fan Y., Xu N., Video instance segmentation, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5188-5197; Dosovitskiy A., Fischer P., Ilg E., Hausser P., Hazirbas C., Golkov V., Van Der Smagt P., Cremers D., Brox T., Flownet: Learning optical flow with convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, pp. 2758-2766; Wang X., Kong T., Shen C., Jiang Y., Li L., Solo: Segmenting objects by locations, Proceedings of the European Conference on Computer Vision, pp. 649-665, (2020); Wu Y., Kirillov A., Massa F., Lo W.Y., Girshick R., Detectron2, (2019); Cao J., Anwer R.M., Cholakkal H., Khan F.S., Pang Y., Shao L., Sipmask: Spatial information preservation for fast image and video instance segmentation, Proceedings of the European Conference on Computer Vision, pp. 1-18, (2020); Du W., Xiang Z., Chen S., Qiao C., Chen Y., Bai T., Real-time Instance Segmentation with Discriminative Orientation Maps, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7314-7323","K. Lee; School of Artificial Intelligence, Yong In University, Yongin, 17092, South Korea; email: kjlee@yiu.ac.kr","","MDPI","","","","","","20763417","","","","English","Appl. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137941610"
"Rukhovich D.I.; Koroleva P.V.; Rukhovich A.D.; Komissarov M.A.","Rukhovich, Dmitry I. (6504291031); Koroleva, Polina V. (15843738800); Rukhovich, Alexey D. (56145307100); Komissarov, Mikhail A. (56210687500)","6504291031; 15843738800; 56145307100; 56210687500","Informativeness of the Long-Term Average Spectral Characteristics of the Bare Soil Surface for the Detection of Soil Cover Degradation with the Neural Network Filtering of Remote Sensing Data","2023","Remote Sensing","15","1","124","","","","0","10.3390/rs15010124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145776089&doi=10.3390%2frs15010124&partnerID=40&md5=5eb27c9a5e8473beaa3b8d2132e86dd7","Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Ufa Institute of Biology UFRC RAS, Pr. Oktyabrya 69, Ufa, 450054, Russian Federation","Rukhovich D.I., Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Koroleva P.V., Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Rukhovich A.D., Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Komissarov M.A., Ufa Institute of Biology UFRC RAS, Pr. Oktyabrya 69, Ufa, 450054, Russian Federation","The long-term spectral characteristics of the bare soil surface (BSS) in the BLUE, GREEN, RED, NIR, SWIR1, and SWIR2 Landsat spectral bands are poorly studied. Most often, the RED and NIR spectral bands are used to analyze the spatial heterogeneity of the soil cover; in our opinion, it is outmoded and seems unreasonable. The study of multi-temporal spectral characteristics requires the processing of big remote sensing data based on artificial intelligence in the form of convolutional neural networks. The analysis of BSS belongs to the direct methods of analysis of the soil cover. Soil degradation can be detected by ground methods (field reconnaissance surveys), modeling, or digital methods, and based on the remote sensing data (RSD) analysis. Ground methods are laborious, and modeling gives indirect results. RSD analysis can be based on the principles of calculation of vegetation indices (VIs) and on the BSS identification. The calculation of VIs also provides indirect information about the soil cover through the state of vegetation. BSS analysis is a direct method for analyzing soil cover heterogeneity. In this work, the informativeness of the long-term (37 years) average spectral characteristics of the BLUE, GREEN, RED, NIR, SWIR1 and SWIR2 bands of the Landsat 4–8 satellites for detecting areas of soil degradation with recognition of the BSS using deep machine learning methods was estimated. The objects of study are the spectral characteristics of kastanozems (dark chestnut soils) in the south of Russia in the territory of the Morozovsky district of the Rostov region. Soil degradation in this area is mainly caused by erosion. The following methods were used: retrospective monitoring of soil and land cover, deep machine learning using convolutional neural networks, and cartographic analysis. Six new maps of the average long-term spectral brightness of the BSS have been obtained. The information content of the BSS for six spectral bands has been verified on the basis of ground surveys. The informativeness was determined by the percentage of coincidences of degradation facts identified during the RSD analysis, and those determined in the field. It has been established that the spectral bands line up in the following descending order of information content: RED, NIR, GREEN, BLUE, SWIR1, SWIR2. The accuracy of degradation maps by band was determined as: RED—84.6%, NIR—82.9%, GREEN—78.0%, BLUE—78.0%, SWIR1—75.5%, SWIR2—62.2%. © 2022 by the authors.","bare soil; deep machine learning; Landsat spectral bands; neural networks; soil degradation; soil water erosion","Convolution; Convolutional neural networks; Deep neural networks; Erosion; Infrared devices; Learning systems; Remote sensing; Soil moisture; Vegetation mapping; Bare soils; Deep machine learning; LANDSAT; Landsat spectral band; Machine-learning; Neural-networks; Soil degradation; Soil water; Soil water erosion; Spectral band; Water erosion; Landsat","","","","","Russian Science Foundation, RSF, (22-17-00071, FGUR-2022-0009)","The research was supported by Russian Science Foundation (project No. 22-17-00071, https://rscf.ru/project/22-17-00071/) (development of methodology for detection of soil degradation/erosion areas based on remote sensing data) and framework of state assignment No. FGUR-2022-0009 (field surveys and agrochemical analyses).","Ischenko T.A., All-Union Instruction on Soil Surveys and the Compilation of Large-Scale Soil Land Use Maps, (1973); Farifteh J., Van Der Meer F., Atzberger C., Carranza E.J.M., Quantitative analysis of salt-affected soil reflectance spectra: A comparison of two adaptive methods (PLSR and ANN), Remote Sens. Environ, 110, pp. 59-78, (2007); Higginbottom T.P., Symeonakis E., Assessing land degradation and desertification using vegetation index data: Current frameworks and future directions, Remote Sens, 6, pp. 9552-9575, (2014); Ibrahim Y.Z., Balzter H., Kaduk J., Tucker C.J., Land degradation assessment using residual trend analysis of GIMMS NDVI3g, soil moisture and rainfall in sub-Saharan west Africa from 1982 to 2012, Remote Sens, 7, pp. 5471-5494, (2015); Mendonca-Santos M.D.L., Dart R.O., Santos H.G., Coelho M.R., Berbara R.L.L., Lumbreras J.F., Digital soil mapping of topsoil organic carbon content of Rio de Janeiro state, Brazil, Digital Soil Mapping, pp. 255-266, (2010); Lozbenev N., Komissarov M., Zhidkin A., Gusarov A., Fomicheva D., Comparative assessment of digital and conventional soil mapping: A case study of the Southern Cis-Ural region, Russia, Soil Syst, 6, (2022); Glazunov G.P., Gendugov V.M., A full-scale model of wind erosion and its verification, Eurasian Soil Sci, 36, pp. 216-226, (2003); Larionov G.A., Dobrovol'skaya N.G., Krasnov S.F., Liu B.Y., The new equation for the relief factor in statistical models of water erosion, Eurasian Soil Sci, 36, pp. 1105-1113, (2003); Maltsev K.A., Yermolaev O.P., Potential soil loss from erosion on arable lands in the European part of Russia, Eurasian Soil Sci, 52, pp. 1588-1597, (2019); Sukhanovskii Y.P., Rainfall erosion model, Eurasian Soil Sci, 43, pp. 1036-1046, (2010); Shary P.A., Sharaya L.S., Mitusov A.V., Fundamental quantitative methods of land surface analysis, Geoderma, 107, pp. 1-32, (2002); Romanenkov V., Smith J., Smith P., Sirotenko O.D., Rukhovitch D.I., Romanenko I.A., Soil organic carbon dynamics of croplands in European Russia: Estimates from the “model of humus balance, Reg. Environ. Chang, 7, pp. 93-104, (2007); Rukhovich D.I., Koroleva P.V., Vilchevskaya E.V., Romanenkov V., Kolesnikova L.G., Constructing a spatially-resolved database for modelling soil organic carbon stocks of croplands in European Russia, Reg. Environ. Chang, 7, pp. 51-61, (2007); Xu H., Hu X., Guan H., Zhang B., Wang M., Chen S., Chen M., A remote sensing based method to detect soil erosion in forests, Remote. Sens, 11, (2019); Phinzi K., Ngetar N.S., Mapping soil erosion in a quaternary catchment in Eastern Cape using geographic information system and remote sensing, S. Afr. J. Geomat, 6, (2017); Eckert S., Husler F., Liniger H., Hodel E., Trend analysis of MODIS NDVI time series for detecting land degradation and regeneration in Mongolia, J. Arid. Environ, 113, pp. 16-28, (2015); Ayalew D.A., Deumlich D., Sarapatka B., Doktor D., Quantifying the sensitivity of NDVI-Based C factor estimation and potential soil erosion prediction using Spaceborne earth observation data, Remote Sens, 12, (2020); De Carvalho D.F., Durigon V.L., Antunes M.A.H., De Almeida W.S., Oliveira P.T.S., Predicting soil erosion using Rusle and NDVI time series from TM Landsat 5, Pesqui. Agropecuária Bras, 49, pp. 215-224, (2014); Yengoh G.T., Dent D., Olsson L., Tengberg A.E., Tucker C.J., Limits to the use of NDVI in land degradation assessment, Use of the Normalized Difference Vegetation Index (NDVI) to Assess Land Degradation at Multiple Scales, pp. 27-30, (2015); Khitrov N.B., Rukhovich D.I., Koroleva P.V., Kalinina N.V., Trubnikov A.V., Petukhov D.A., Kulyanitsa A.L., A study of the responsiveness of crops to fertilizers by zones of stable intra-field heterogeneity based on big satellite data analysis, Arch. Agron. Soil Sci, 66, pp. 1963-1975, (2020); Zhang Y., Walker J.P., Pauwels V.R.N., Sadeh Y., Assimilation of wheat and soil states into the APSIM-wheat crop model: A case study, Remote Sens, 14, (2022); Qi G., Chang C., Yang W., Gao P., Zhao G., Soil salinity inversion in coastal corn planting areas by the satellite-UAV-ground integration approach, Remote Sens, 13, (2021); Romano E., Bergonzoli S., Pecorella I., Bisaglia C., De Vita P., Methodology for the definition of durum wheat yield homogeneous zones by using satellite spectral indices, Remote Sens, 13, (2021); Iwahashi Y., Ye R., Kobayashi S., Yagura K., Hor S., Soben K., Homma K., Quantification of changes in rice production for 2003–2019 with MODIS LAI data in Pursat Province, Cambodia, Remote Sens, 13, (2021); Rukhovich D.I., Koroleva P.V., Rukhovich D.D., Kalinina N.V., The use of deep machine learning for the automated selection of remote sensing data for the determination of areas of arable land degradation processes distribution, Remote Sens, 13, (2021); Kulyanitsa A.L., Rukhovich D.I., Koroleva P.V., Vilchevskaya E.V., Kalinina N.V., Analysis of the informativity of big satellite precision-farming data processing for correcting large-scale soil maps, Eurasian Soil Sci, 53, pp. 1709-1725, (2020); Rukhovich D.I., Koroleva P.V., Kalinina N.V., Vilchevskaya E.V., Suleiman G.A., Chernousenko G.I., Detecting degraded arable land on the basis of remote sensing big data analysis, Eurasian Soil Sci, 54, pp. 161-175, (2021); Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., The informativeness of coefficients a and b of the soil line for the analysis of remote sensing materials, Eurasian Soil Sci, 49, pp. 831-845, (2016); Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Maps of averaged spectral deviations from soil lines and their comparison with traditional soil maps, Eurasian Soil Sci, 49, pp. 739-756, (2016); Kulyanitsa A.L., Rukhovich A.D., Rukhovich D.D., Koroleva P.V., Rukhovich D.I., Simakova M.S., The Application of the piecewise linear approximation to the spectral neighborhood of soil line for the analysis of the quality of normalization of remote sensing materials, Eurasian Soil Sci, 50, pp. 387-396, (2017); Koroleva P.V., Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Kulyanitsa A.L., Trubnikov A.V., Kalinina N.V., Simakova M.S., Location of bare soil surface and soil line on the RED–NIR spectral plane, Eurasian Soil Sci, 50, pp. 1375-1385, (2017); Koroleva P.V., Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Kulyanitsa A.L., Trubnikov A.V., Kalinina N.V., Simakova M.S., Characterization of soil types and subtypes in N-dimensional space of multitemporal (empirical) soil line, Eurasian Soil Sci, 51, pp. 1021-1033, (2018); Satellite Big Data: How It Is Changing the Face of Precision Farming; Koroleva P.V., Rukhovich D.I., Shapovalov D.A., Suleiman G.A., Dolinina E.A., Retrospective monitoring of soil waterlogging on arable land of Tambov oblast in 2018–1968, Eurasian Soil Sci, 52, pp. 834-852, (2019); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Chernousenko G.I., Vil'chevskaya E.V., Dolinina E.A., The influence of soil salinization on land use changes in Azov district of Rostov oblast, Eurasian Soil Sci, 50, pp. 276-295, (2017); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Chernousenko G.I., Vil'chevskaya E.V., Dolinina E.A., Rukhovich S.V., Methodology for comparing soil maps of different dates with the aim to reveal and describe changes in the soil cover (by the example of soil salinization monitoring), Eurasian Soil Sci, 49, pp. 145-162, (2016); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Vil'chveskaya E.V., Dolinina E.A., Rukhovich S.V., Retrospective analysis of changes in land uses on vertic soils of closed mesodepressions on the Azov plain, Eurasian Soil Sci, 48, pp. 1050-1075, (2015); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Vil'chevskaya E.V., Dolinina E.A., Rukhovich S.V., Impact of shelterbelts on the fragmentation of erosional networks and local soil waterlogging, Eurasian Soil Sci, 47, pp. 1086-1099, (2014); Zi Y., Xie F., Jiang Z., A cloud detection method for Landsat 8 images based on PCANet, Remote Sens, 10, (2018); Zeng X., Yang J., Deng X., An W., Li J., Cloud detection of remote sensing images on Landsat-8 by deep learning, Proceedings of the Tenth International Conference on Digital Image Processing (ICDIP 2018); Mateo-Garcia G., Gomez-Chova L., Convolutional neural networks for cloud screening: Transfer learning from Landsat-8 to Proba-V, Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 2103-2106; Shao Z., Pan Y., Diao C., Cai J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network, IEEE Trans. Geosci. Remote Sens, 57, pp. 4062-4076, (2019); Goodfellow I., Bengio Y., Courville A., Deep learning, (2016); Porzi L., Bulo S.R., Colovic A., Kontschieder P., Seamless scene segmentation, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8269-8278, (2019); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Zhou Z., Rahman Siddiquee M.M., Tajbakhsh N., Liang J., UNet++: A nested U-Net architecture for medical image segmentation, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11, (2018); Liu Y., Zhu Q., Cao F., Chen J., Lu G., High-resolution remote sensing image segmentation framework based on attention mechanism and adaptive weighting, ISPRS Int. J. Geo-Inf, 10, (2021); Zhang J., Zhu H., Wang P., Ling X., ATT squeeze U-Net: A lightweight network for forest fire detection and recognition, IEEE Access, 9, pp. 10858-10870, (2021); Sa I., Popovic M., Khanna R., Chen Z., Lottes P., Liebisch F., Nieto J., Stachniss C., Walter A., Siegwart R., WeedMap: A large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming, Remote Sens, 10, (2018); Lottes P., Behley J., Milioto A., Stachniss C., Fully convolutional networks with sequential information for robust crop and weed detection in precision farming, IEEE Robot. Autom. Lett, 3, pp. 2870-2877, (2018); Openshaw S., Geographical data mining: Key design issues, Proceedings of the 4th International Conference on GeoComputation; Hastie T.J., Tibshirani R., Friedman J.H., The Elements of Statistical Learning: Data Mining, Inference, and Prediction, (2008); Rukhovich D.I., Koroleva P.V., Rukhovich D.D., Rukhovich A.D., Recognition of the bare soil using deep machine learning methods to create maps of arable soil degradation based on the analysis of multi-temporal remote sensing data, Remote Sens, 14, (2022); Kauth R.J., Thomas G.S., The tasseled cap—A graphic description of the spectral-temporal development of agricultural crops as seen by LANDSAT, Proceedings of the Symposium on machine processing of remotely sensed data, (1976); Bajocco S., Ginaldi F., Savian F., Morelli D., Scaglione M., Fanchini D., Raparelli E., Bregaglio S.U.M., On the use of NDVI to estimate LAI in field crops: Implementing a conversion equation library, Remote Sens, 14, (2022); Dubbini M., Palumbo N., De Giglio M., Zucca F., Barbarella M., Tornato A., Sentinel-2 data and unmanned aerial system products to support crop and bare soil monitoring: Methodology based on a statistical comparison between remote sensing data with identical spectral bands, Remote Sens, 14, (2022); Lee K.-S., Cohen W.B., Kennedy R.E., Maiersperger T.K., Gower S.T., Hyperspectral versus multispectral data for estimating leaf area index in four different biomes, Remote Sens. Environ, 91, pp. 508-520, (2004); Darvishzadeh R., Atzberger C., Skidmore A.K., Abkar A.A., Leaf Area Index derivation from hyperspectral vegetation indices and the red edge position, Int. J. Remote Sens, 30, pp. 6199-6218, (2009); Bezuglova O.S., Nazarenko O.G., Ilyinskaya I.N., Land degradation dynamics in Rostov oblast, Arid Ecosyst, 10, pp. 93-97, (2020); Gaevaya E.A., Bezuglova O.S., Ilinskaya I.N., Taradin S.A., Nezhinskaya E.N., Mishchenko A.V., The experience in the implementation of adaptive-landscape systems of agriculture in Rostov Oblast, IOP Conf. Ser. Earth Environ. Sci, 629, (2021); Golosov V.N., Collins A.L., Dobrovolskaya N.G., Bazhenova O.I., Ryzhov Y.V., Sidorchuk A.Y., Soil loss on the arable lands of the forest-steppe and steppe zones of European Russia and Siberia during the period of intensive agriculture, Geoderma, 381, (2021); Gusarov A.V., Land-use/-cover changes and their effect on soil erosion and river suspended sediment load in different landscape zones of European Russia during 1970–2017, Water, 13, (2021); Litvin L.F., Kiryukhina Z.P., Krasnov S.F., Dobrovol'skaya N.G., Dynamics of agricultural soil erosion in European Russia, Eurasian Soil Sci, 50, pp. 1343-1352, (2017); Dokuchaev V.V., State Soil-Erosion Map of Russia (Asian Part), Scale 1:2,500,000, (2004); Beck H.E., Zimmermann N.E., McVicar T.R., Vergopolan N., Berg A., Wood E.F., Present and future Köppen-Geiger climate classification maps at 1–km resolution, Sci. Data, 5, pp. 180-214, (2018); Vysotskii G.N., Izbrannye Trudy (Selected Works), (1960); Selyaninov G.T., Methods of agricultural climatology, Agric. Meteorol, 22, pp. 4-20, (1930); Rukhovich D.I., Koroleva P.V., Vilchevskaya E.V., Kalinina N.V., Digital thematic cartography as a change in the available primary sources and ways of using them, Digital Soil Mapping: Theoretical and Experimental Studies, pp. 58-86, (2012); Bryzzhev A.V., Rukhovich D.I., Koroleva P.V., Kalinina N.V., Vilchevskaya E.V., Dolinina E.A., Rukhovich S.V., Organization of retrospective monitoring of the soil cover of Rostov oblast, Eurasian Soil Sci, 48, pp. 1029-1049, (2015); Shapovalov D.A., Koroleva P.V., Kalinina N.V., Rukhovich D.I., Suleiman G.A., Dolinina E.A., Differences in inventories of waterlogged territories in soil surveys of different years and in land management documents, Eurasian Soil Sci, 53, pp. 294-309, (2020); McCarty J.L., Ellicott E.A., Romanenkov V., Rukhovitch D., Koroleva P., Multi-year black carbon emissions from cropland burning in the Russian Federation, Atmos. Environ, 63, pp. 223-238, (2012); Rouse J.W., Haas R.H., Schell J.A., Deering D.W., Monitoring vegetation systems in the great plains with ERTS, Proceedings of the Third ERTS Symposium, 1, pp. 309-317, (1974); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, arXiv, (2015); Jadon S., A survey of loss functions for semantic segmentation, Proceedings of the 2020 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB), pp. 1-7; Kingma D.P., Ba J., Adam: A method for stochastic optimization, arXiv, (2014); Kohavi R., A study of cross-validation and bootstrap for accuracy estimation and model selection, Proceedings of the 14th international joint conference on Artificial intelligence-Volume 2 (IJCAI’95), pp. 1137-1143; Mullin M., Sukthankar R., Complete cross-validation for nearest neighbor classifiers, Proceedings of the Seventeenth International Conference on Machine Learning (ICML ’00), pp. 639-646; Soil Map of the Collective Farm Rodina, Morozovsky District, Rostov Region, Scale 1:25000, (1975); Arnold R., Blume H.P., Bockheim J., Boyadgiev T., Bridges E., Brinkman R., Broll G., Bronger A., Constantini E., Creutzberg D., Et al., World Reference Base for Soil Resources: IUSS Working Group WRB. FAO, (1998); (1993); Walkley A.J., Black I.A., Estimation of soil organic carbon by the chromic acid titration method, Soil Sci, 37, pp. 29-38, (1934); Egorov V.V., Classification and Diagnostics of Soils of the USSR (Russian Translations Series, 42), (1986); Vieira A.S., do Valle Junior R.F., Rodrigues V.S., da Silva Quinaia T.L., Mendes R.G., Valera C.A., Fernandes L.F.S., Pacheco F.A.L., Estimating water erosion from the brightness index of orbital images: A framework for the prognosis of degraded pastures, Sci. Total Environ, 776, (2021); Yuan Q., Shen H., Li T., Li Z., Li S., Jiang Y., Xu H., Tan W., Yang Q., Wang J., Et al., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ, 241, (2020); Cook K.L., An evaluation of the effectiveness of low-cost UAVs and structure from motion for geomorphic change detection, Geomorphology, 278, pp. 195-208, (2017); Rahmati O., Tahmasebipour N., Haghizadeh A., Pourghasemi H.R., Feizizadeh B., Evaluation of different machine learning models for predicting and mapping the susceptibility of gully erosion, Geomorphology, 298, pp. 118-137, (2017)","P.V. Koroleva; Dokuchaev Soil Science Institute, Moscow, Pyzhevsky Lane 7, 119017, Russian Federation; email: soilmap@yandex.ru","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145776089"
"Gao H.; Zhang Y.; Chen Z.; Xu F.; Hong D.; Zhang B.","Gao, Hongmin (34770690700); Zhang, Yitong (58168942900); Chen, Zhonghao (57222329315); Xu, Feng (58401864200); Hong, Danfeng (56108179600); Zhang, Bing (57210588483)","34770690700; 58168942900; 57222329315; 58401864200; 56108179600; 57210588483","Hyperspectral Target Detection via Spectral Aggregation and Separation Network with Target Band Random Mask","2023","IEEE Transactions on Geoscience and Remote Sensing","61","","5515516","","","","0","10.1109/TGRS.2023.3288739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163463903&doi=10.1109%2fTGRS.2023.3288739&partnerID=40&md5=53b6954009fede06a77548f2f949185d","Hohai University, College of Computer and Information, Department of Information, Nanjing, 211100, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Digital Earth Science, Beijing, 100094, China; University of Chinese Academy of Sciences, College of Resources and Environment, Beijing, 100049, China","Gao H., Hohai University, College of Computer and Information, Department of Information, Nanjing, 211100, China; Zhang Y., Hohai University, College of Computer and Information, Department of Information, Nanjing, 211100, China; Chen Z., Hohai University, College of Computer and Information, Department of Information, Nanjing, 211100, China; Xu F., Hohai University, College of Computer and Information, Department of Information, Nanjing, 211100, China; Hong D., Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Digital Earth Science, Beijing, 100094, China; Zhang B., Aerospace Information Research Institute, Chinese Academy of Sciences, Key Laboratory of Digital Earth Science, Beijing, 100094, China, University of Chinese Academy of Sciences, College of Resources and Environment, Beijing, 100049, China","Hyperspectral target detection (HTD) is a pixel-wise detection method based on limited prior targets and spectral differences, which has been widely studied and applied in many fields. Recently, deep learning (DL) plays an important role in hyperspectral imagery (HSI) processing. However, for HTD, the severe lack of class-balanced training sets is an enormous challenge. Meanwhile, it is difficult to suppress backgrounds while highlighting targets through the deep network. To address these issues, we propose a spectral aggregation and separation network (SASN) with a target band random mask (TBRM) for HTD in this article. For the training sets of SASN, a multifarious representative background selection strategy (MRBS) is first proposed to obtain a multifarious and representative background training set. Next, aiming at the notorious class imbalance, a data augmentation (DA) method, TBRM, is proposed to generate adequate target training set by repeating randomly zero-masking the spectral bands of a prior target. Subsequently, in the training of SASN, residual connection and squeeze-and-excitation (SE) channel attention mechanism are applied to fully extract high discriminative features and nonlinear ones in the spectra. Besides, to better separate the targets and backgrounds, a triplet-soft loss function is presented, which makes the training in the direction of spectral separation of background samples from both the prior target and target samples. During testing, the trained SASN distinguishes the spectral similarities and differences simultaneously for highlighting targets and suppressing backgrounds. Moreover, extensive experimental results validate that the proposed method has superior detection performances, background suppression capacity, and separability compared with ten cutting-edge HTD algorithms on six benchmark HSI datasets.  © 1980-2012 IEEE.","Data augmentation (DA); deep learning (DL); hyperspectral imagery (HSI); hyperspectral target detection (HTD)","Deep learning; Feature extraction; Hyperspectral imaging; Object recognition; Radar target recognition; Remote sensing; Data augmentation; Deep learning; Features extraction; Hyper-spectral imageries; Hyperspectral imagery; Hyperspectral target detection; Objects detection; Signal processing algorithms; algorithm; artificial neural network; benchmarking; data set; detection method; image processing; pixel; spectral analysis; Object detection","","","","","National Natural Science Foundation of China, NSFC, (62071168); China Postdoctoral Science Foundation, (2021M690885); Natural Science Foundation of Jiangsu Province, (BK20211201)","This work was supported in part by the National Natural Science Foundation of China under Grant 62071168, in part by the Natural Science Foundation of Jiangsu Province under Grant BK20211201, and in part by the China Postdoctoral Science Foundation under Grant 2021M690885.","Landgrebe D., Hyperspectral image data analysis, IEEE Signal Process. Mag, 19, 1, pp. 17-28, (2002); Rasti B., Et al., Feature extraction for hyperspectral imagery: The evolution from shallow to deep: Overview and toolbox, IEEE Geosci. Remote Sens. Mag, 8, 4, pp. 60-88, (2020); Manolakis D., Shaw G., Detection algorithms for hyperspectral imaging applications, IEEE Signal Process. Mag, 19, 1, pp. 29-43, (2002); Nasrabadi N.M., Hyperspectral target detection: An overview of current and future challenges, IEEE Signal Process. Mag, 31, 1, pp. 34-44, (2014); Lu B., Dao P., Liu J., He Y., Shang J., Recent advances of hyperspectral imaging technology and applications in agriculture, Remote Sens, 12, 16, (2020); Manolakis D., Truslow E., Pieper M., Cooley T., Brueggeman M., Detection algorithms in hyperspectral imaging systems: An overview of practical algorithms, IEEE Signal Process. Mag, 31, 1, pp. 24-33, (2014); Hou Y., Zhang Y., Yao L., Liu X., Wang F., Mineral target detection based on MSCPE-BSE in hyperspectral image, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 1614-1617, (2016); Zhu D., Du B., Zhang L., Binary-class collaborative representation for target detection in hyperspectral images, IEEE Geosci. Remote Sens. Lett, 16, 7, pp. 1100-1104, (2019); Chang C., Hyperspectral target detection: Hypothesis testing, signalto-noise ratio, and spectral angle theories, IEEE Trans. Geosci. Remote Sens, 60, (2022); Saito T., Toriwaki J.-I., New algorithms for Euclidean distance transformation of an n-dimensional digitized picture with applications, Pattern Recognit, 27, 11, pp. 1551-1565, (1994); Kruse F.A., Et al., The spectral image processing system (SIPS)-interactive visualization and analysis of imaging spectrometer data, Remote Sens. Environ, 44, 2-3, pp. 145-163, (1993); Chang C.-I., An information-theoretic approach to spectral variability, similarity, and discrimination for hyperspectral image analysis, IEEE Trans. Inf. Theory, 46, 5, pp. 1927-1932, (2000); Kraut S., Scharf L.L., The CFAR adaptive subspace detector is a scale-invariant GLRT, IEEE Trans. Signal Process, 47, 9, pp. 2538-2541, (1999); Manolakis D., Et al., The remarkable success of adaptive cosine estimator in hyperspectral target detection, Proc. SPIE, 8743, (2013); Manolakis D., Marden D., Shaw G.A., Hyperspectral image processing for automatic target detection applications, Lincoln Lab. J, 14, 1, pp. 79-116, (2003); Hong D., Yokoya N., Chanussot J., Zhu X.X., An augmented linear mixing model to address spectral variability for hyperspectral unmixing, IEEE Trans. Image Process, 28, 4, pp. 1923-1938, (2019); Hong D., Et al., Endmember-guided unmixing network (EGU-Net): A general deep learning framework for self-supervised hyperspectral unmixing, IEEE Trans. Neural Netw. Learn. Syst, 33, 11, pp. 6518-6531, (2022); Manolakis D.G., Shaw G.A., Keshava N., Comparative analysis of hyperspectral adaptive matched filter detectors, Proc. SPIE, 4049, pp. 2-17, (2000); Gao L., Yang B., Du Q., Zhang B., Adjusted spectral matched filter for target detection in hyperspectral imagery, Remote Sens, 7, 6, pp. 6611-6634, (2015); Sofer Y., Geva E., Rotman S.R., Improved covariance matrices for point target detection in hyperspectral data, Proc. IEEE Int. Conf. Microw., Commun., Antennas Electron. Syst, (2009); Farrand W., Mapping the distribution of mine tailings in the Coeur d'Alene River Valley, Idaho, through the use of a constrained energy minimization technique, Remote Sens. Environ, 59, 1, pp. 64-76, (1997); Chang C.-I., Et al., Generalized constrained energy minimization approach to subpixel target detection for multispectral imagery, Opt. Eng, 39, 5, pp. 1275-1281, (2000); Du Q., Ren H., Chang C.-I., A comparative study for orthogonal subspace projection and constrained energy minimization, IEEE Trans. Geosci. Remote Sens, 41, 6, pp. 1525-1529, (2003); Jiao X., Chang C.-I., Kernel-based constrained energy minimization (K-CEM), Proc. SPIE, 6966, pp. 523-533, (2008); Zou Z., Shi Z., Hierarchical suppression method for hyperspectral target detection, IEEE Trans. Geosci. Remote Sens, 54, 1, pp. 330-342, (2016); Chen Z., Et al., Global to local: A hierarchical detection algorithm for hyperspectral image target detection, IEEE Trans. Geosci. Remote Sens, 60, (2022); Bioucas-Dias J.M., Plaza A., Camps-Valls G., Scheunders P., Nasrabadi N., Chanussot J., Hyperspectral remote sensing data analysis and future challenges, IEEE Geosci. Remote Sens. Mag, 1, 2, pp. 6-36, (2013); Hong D., Et al., Interpretable hyperspectral artificial intelligence: When nonconvex modeling meets hyperspectral remote sensing, IEEE Geosci. Remote Sens. Mag, 9, 2, pp. 52-87, (2021); Bitar A.W., Cheong L., Ovarlez J., Sparse and low-rank matrix decomposition for automatic target detection in hyperspectral imagery, IEEE Trans. Geosci. Remote Sens, 57, 8, pp. 5239-5251, (2019); Ling Q., Li K., Li Z., Lin Z., Wang J., Hyperspectral detection and unmixing of subpixel target using iterative constrained sparse representation, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 15, pp. 1049-1063, (2022); Zhuang L., Gao L., Zhang B., Fu X., Bioucas-Dias J.M., Hyperspectral image denoising and anomaly detection based on low-rank and sparse representations, IEEE Trans. Geosci. Remote Sens, 60, (2022); Shi Y., Li J., Li Y., Gamba P., Hyperspectral target detection using a bilinear sparse binary hypothesis model, IEEE Trans. Geosci. Remote Sens, 60, (2022); Chen Y., Nasrabadi N.M., Tran T.D., Sparse representation for target detection in hyperspectral imagery, IEEE J. Sel. Topics Signal Process, 5, 3, pp. 629-640, (2011); Li W., Du Q., Zhang B., Combined sparse and collaborative representation for hyperspectral target detection, Pattern Recognit, 48, 12, pp. 3904-3916, (2015); Zhu D., Du B., Zhang L., Single-spectrum-driven binary-class sparse representation target detector for hyperspectral imagery, IEEE Trans. Geosci. Remote Sens, 59, 2, pp. 1487-1500, (2021); Sun X., Et al., Target detection through tree-structured encoding for hyperspectral images, IEEE Trans. Geosci. Remote Sens, 59, 5, pp. 4233-4249, (2021); Zhang B., Et al., Progress and challenges in intelligent remote sensing satellite systems, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 15, pp. 1814-1822, (2022); Hong D., Et al., SpectralFormer: Rethinking hyperspectral image classification with transformers, IEEE Trans. Geosci. Remote Sens, 60, (2022); Du J., Li Z., Sun H., CNN-based target detection in hyperspectral imagery, Proc. IEEE Int. Geosci. Remote Sens. Symp, pp. 2761-2764, (2018); Feng Z., Zhang J., Feng J., Spectral-spatial joint target detection of hyperspectral image based on transfer learning, Proc. IEEE Int. Geosci. Remote Sens. Symp, pp. 1770-1773, (2020); Zhang G., Zhao S., Li W., Du Q., Ran Q., Tao R., HTDNet: A deep convolutional neural network for target detection in hyperspectral imagery, Remote Sens, 12, 9, (2020); Zhu D., Du B., Zhang L., Two-stream convolutional networks for hyperspectral target detection, IEEE Trans. Geosci. Remote Sens, 59, 8, pp. 6907-6921, (2021); Rao W., Gao L., Qu Y., Sun X., Zhang B., Chanussot J., Siamese transformer network for hyperspectral image target detection, IEEE Trans. Geosci. Remote Sens, 60, (2022); Xie W., Zhang J., Lei J., Li Y., Jia X., Self-spectral learning with GAN based spectral-spatial target detection for hyperspectral image, Neural Netw, 142, pp. 375-387, (2021); Qin H., Xie W., Li Y., Jiang K., Lei J., Du Q., PTGAN: A proposal-weighted two-stage GAN with attention for hyperspectral target detection, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 4428-4431, (2021); Zhu D., Du B., Zhang L., How to construct a deep networkbased hyperspectral target detector?-A LSTM inspired method, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 3193-3196, (2021); Qin H., Xie W., Li Y., Du Q., HTD-VIT: Spectral-spatial joint hyperspectral target detection with vision transformer, Proc. IEEE Int. Geosci. Remote Sens. Symp, pp. 1967-1970, (2022); Ester M., Et al., A density-based algorithm for discovering clusters in large spatial databases with noise, Proc. KDD, 96, 34, pp. 226-231, (1996); Haixiang G., Yijing L., Shang J., Mingyun G., Yuanyue H., Bing G., Learning from class-imbalanced data: Review of methods and applications, Expert Syst. Appl, 73, pp. 220-239, (2017); Buda M., Maki A., Mazurowski M.A., A systematic study of the class imbalance problem in convolutional neural networks, (2017); Amin A., Et al., Comparing oversampling techniques to handle the class imbalance problem: A customer churn prediction case study, IEEE Access, 4, pp. 7940-7957, (2016); Lecun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, 11, pp. 2278-2324, (1998); Krizhevsky A., Sutskever I., Hinton G., ImageNet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Process. Syst, 25, 2, pp. 1097-1105, (2012); Wu R., Yan S., Shan Y., Dang Q., Sun G., Deep image: Scaling up image recognition, (2015); DeVries T., Taylor G.W., Improved regularization of convolutional neural networks with cutout, (2017); Zhang H., Cisse M., Dauphin Y.N., Lopez-Paz D., Mixup: Beyond empirical risk minimization, (2017); Yun S., Han D., Oh S.J., Chun S., Choe J., Yoo Y., CutMix: Regularization strategy to train strong classifiers with localizable features, (2019); Chen P., Liu S., Zhao H., Jia J., GridMask data augmentation, (2020); Balntas V., Johns E., Tang L., Mikolajczyk K., PN-Net: Conjoined triple deep network for learning local image descriptors, (2016); Green R.O., Et al., Imaging spectroscopy and the airborne visible/ infrared imaging spectrometer (AVIRIS), Remote Sens. Environ, 65, 3, pp. 227-248, (1998); Zhang Y., Wu K., Du B., Hu X., Multitask learning-based reliability analysis for hyperspectral target detection, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 12, 7, pp. 2135-2147, (2019); Kang X., Zhang X., Li S., Li K., Li J., Benediktsson J.A., Hyperspectral anomaly detection with attribute and edge-preserving filters, IEEE Trans. Geosci. Remote Sens, 55, 10, pp. 5600-5611, (2017); Yokoya N., Iwasaki A., Airborne hyperspectral data over Chikusei, Space Appl. Lab. Univ. Tokyo, Japan. Tech. Rep. SAL-2016-05-27, (2016); Xu Y., Wu Z., Chanussot J., Wei Z., Joint reconstruction and anomaly detection from compressive hyperspectral images using Mahalanobis distance-regularized tensor RPCA, IEEE Trans. Geosci. Remote Sens, 56, 5, pp. 2919-2930, (2018); Guo T., Luo F., Zhang L., Tan X., Liu J., Zhou X., Target detection in hyperspectral imagery via sparse and dense hybrid representation, IEEE Geosci. Remote Sens. Lett, 17, 4, pp. 716-720, (2020); Zhang Y., Du B., Zhang L., Wang S., A low-rank and sparse matrix decomposition-based Mahalanobis distance method for hyperspectral anomaly detection, IEEE Trans. Geosci. Remote Sens, 54, 3, pp. 1376-1389, (2016); Hu X., Et al., Hyperspectral anomaly detection using deep learning: A review, Remote Sens, 14, 9, (2022)","F. Xu; Hohai University, College of Computer and Information, Department of Information, Nanjing, 211100, China; email: xufeng@hhu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85163463903"
"Rakhmatuiln I.; Kamilaris A.; Andreasen C.","Rakhmatuiln, Ildar (57226603012); Kamilaris, Andreas (36189564000); Andreasen, Christian (7005358690)","57226603012; 36189564000; 7005358690","Deep neural networks to detectweeds from crops in agricultural environments in real-time: A review","2021","Remote Sensing","13","21","4486","","","","23","10.3390/rs13214486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120067412&doi=10.3390%2frs13214486&partnerID=40&md5=37a2e27ab61e42b73ba77205db55f1a2","Department of Power Plant Networks and Systems, South Ural State University, Chelyabinsk City, 454080, Russian Federation; CYENS Center of Excellence, Dimarchias Square 23, Nicosia, 1016, Cyprus; Department of Computer Science, University of Twente, Enschede, 7522 NB, Netherlands; Department of Plant and Environmental Sciences, University of Copenhagen, Højbakkegaard Allé 13, Taastrup, DK 2630, Denmark","Rakhmatuiln I., Department of Power Plant Networks and Systems, South Ural State University, Chelyabinsk City, 454080, Russian Federation; Kamilaris A., CYENS Center of Excellence, Dimarchias Square 23, Nicosia, 1016, Cyprus, Department of Computer Science, University of Twente, Enschede, 7522 NB, Netherlands; Andreasen C., Department of Plant and Environmental Sciences, University of Copenhagen, Højbakkegaard Allé 13, Taastrup, DK 2630, Denmark","Automation, including machine learning technologies, are becoming increasingly crucial in agriculture to increase productivity. Machine vision is one of the most popular parts of machine learning and has been widely used where advanced automation and control have been required. The trend has shifted from classical image processing and machine learning techniques to modern artificial intelligence (AI) and deep learning (DL) methods. Based on large training datasets and pre-trained models, DL-based methods have proven to be more accurate than previous traditional techniques. Machine vision has wide applications in agriculture, including the detection of weeds and pests in crops. Variation in lighting conditions, failures to transfer learning, and object occlusion constitute key challenges in this domain. Recently, DL has gained much attention due to its advantages in object detection, classification, and feature extraction. DL algorithms can automatically extract information from large amounts of data used to model complex problems and is, therefore, suitable for detecting and classifying weeds and crops. We present a systematic review of AI-based systems to detect weeds, emphasizing recent trends in DL. Various DL methods are discussed to clarify their overall potential, usefulness, and performance. This study indicates that several limitations obstruct the widespread adoption of AI/DL in commercial applications. Recommendations for overcoming these challenges are summarized. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning in agriculture; Machine vision for weed control; Precision agriculture; Robotic weed control; Weed detection","Classification (of information); Computer vision; Crops; Deep neural networks; Feature extraction; Large dataset; Object detection; Precision agriculture; Agricultural environments; Deep learning in agriculture; Learning methods; Machine learning technology; Machine vision for weed control; Machine-vision; Precision Agriculture; Real- time; Robotic weed control; Weed detection; Weed control","","","","","Deputy Ministry of Research, Innovation and Digital Policy; Horizon 2020 Framework Programme, H2020, (101000256, 739578); European Commission, EC","This review was mainly funded by the EU–project WeLASER “Sustainable Weed Management in Agriculture with Laser-Based Autonomous Tools,” Grant agreement ID: 101000256, funded under H2020-EU.3.2.1.1. AK received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No. 739578 and from the Government of the Republic of Cyprus through the Deputy Ministry of Research, Innovation and Digital Policy.","NSP-Weeds; Kudsk P., Streibig J.S., Herbicides and two edge-sword, Weed Res, 43, pp. 90-102, (2003); Harrison J.L., Pesticide Drift and the Pursuit of Environmental Justice, (2011); Lemtiri A., Colinet G., Alabi T., Cluzeau D., Zirbes L., Haubruge E., Francis F., Impacts of earthworms on soil components and dynamics. A review, Biotechnol. Agron. Soc. Environ, 18, pp. 121-133, (2014); Pannacci E., Farneselli M., Guiducci M., Tei F., Mechanical weed control in onion seed production, Crop. Prot, 135, (2020); Rehman T., Qamar U., Zaman Q.Z., Chang Y.K., Schumann A.W., Corscadden K.W., Development and field evaluation of a machine vision based in-season weed detection system for wild blueberry, Comput. Electron. Agric, 162, pp. 1-3, (2019); Rakhmatulin I., Andreasen C., A concept of a compact and inexpensive device for controlling weeds with laser beams, Agron, 10, (2020); Raj R., Rajiv P., Kumar P., Khari M., Feature based video stabilization based on boosted HAAR Cascade and representative point matching algorithm, Image Vis. Comput, 101, (2020); Kaur J., Sinha P., Shukla R., Tiwari V., Automatic Cataract Detection Using Haar Cascade Classifier, Data Intelligence Cognitive Informatics, (2021); Abouzahir A., Sadik M., Sabir E., Bag-of-visual-words-augmented Histogram of Oriented Gradients for efficient weed detection, Biosyst. Eng, 202, pp. 179-194, (2021); Che'Ya N., Dunwoody E., Gupta M., Assessment of Weed Classification Using Hyperspectral Reflectance and Optimal Multispectral UAV Imagery, Agronomy, 11, (2021); De Rainville F.M., Durand A., Fortin F.A., Tanguy K., Maldague X., Panneton B., Simard M.J., Bayesian classification and unsupervised learning for isolating weeds in row crops, Pattern Anal. Applic, 17, pp. 401-414, (2014); Islam N., Rashid M., Wibowo S., Xu C.Y., Morshed A., Wasimi S.A., Moore S., Rahman S.M., Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm, Agriculture, 11, (2021); Hung C., Xu Z., Sukkarieh S., Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV, Remote Sens, 6, pp. 12037-12054, (2014); Pourghassemi B., Zhang C., Lee J., On the Limits of Parallelizing Convolutional Neural Networks on GPUs, Proceedings of the SPAA ‘20: 32nd ACM Symposium on Parallelism in Algorithms and Architectures; Kulkarni A., Deshmukh G., Advanced Agriculture Robotic Weed Control System, Int. J. Adv. Res. Electr. Electron. Instrum. Eng, 2, (2013); Wang N., Zhang E., Dowell Y., Sun D., Design of an optical weed sensor using plant spectral characteristic, Am. Soc. Agric. Biol. Eng, 44, pp. 409-419, (2001); Gikunda P., Jouandeau N., Modern CNNs for IoT Based Farms, (2019); Jouandeau N., Gikunda P., State-Of-The-Art Convolutional Neural Networks for Smart Farms: A Review, Science and Information (SAI) Conference, (2017); Saleem M., Potgieter J., Arif K., Automation in Agriculture by Machine and Deep Learning Techniques: A Review of Recent Developments, Precis. Agric, 22, pp. 2053-2091, (2021); Kamilaris A., Prenafeta-Boldu F., A review of the use of convolutional neural networks in agriculture, J. Agric. Sci, 156, pp. 312-322, (2018); Jiang B., He J., Yang S., Fu H., Li H., Fusion of machine vision technology and AlexNet-CNNs deep learning network for the detection of postharvest apple pesticide residues, Artif. Intell. Agric, 1, pp. 1-8, (2019); Liu H., Lee S., Saunders C., Development of a machine vision system for weed detection during both of off-season, Amer. J. Agric. Biol. Sci, 9, pp. 174-193, (2014); Watchareeruetai U., Takeuchi Y., Matsumoto T., Kudo H., Ohnishi N., Computer Vision Based Methods for Detecting Weeds in Lawns, Mach. Vis. Applic, 17, pp. 287-296, (2006); Padmapriya S., Bhuvaneshwari P., Real time Identification of Crops, Weeds, Diseases, Pest Damage and Nutrient Deficiency, Internat. J. Adv. Res. Educ. Technol, 5, (2018); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning, Sci. Rep, 9, pp. 118-124, (2019); Downey D., Slaughter K., David C., Weeds accurately mapped using DGPS and ground-based vision identification, Calif. Agric, 58, pp. 218-221, (2004); Cun Y., Boser B., Dencker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation Applied to Handwritten Zip Code Recognition, Neural Comput, 1, pp. 541-551, (1989); Wen X., Jing H., Yanfeng S., Hui Z., Advances in Convolutional Neural Networks, Advances in Deep Learning, (2020); Gothai P., Natesan S., Weed Identification using Convolutional Neural Network and Convolutional Neural Network Architectures, Conference, Proceedings of the 2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC); Su W.-H., Crop plant signalling for real-time plant identification in smart farm: A systematic review and new concept in artificial intelligence for automated weed control, Artif. Intelli. Agric, 4, pp. 262-271, (2020); Li Y., Nie J., Chao X., Do we really need deep CNN for plant diseases identification?, Comput. Electron. Agric, 178, (2020); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, SPRS J. Photogram. Remote Sens, 173, pp. 24-49, (2021); O'Mahony N., Campbell S., Carvalho A., Harapanahalli S., Hernandez G.V., Krpalkova L., Riordan D., Walsh J., Deep Learning vs. Traditional Computer Vision, Advances in Computer Vision. CVC 2019. Advances in Intelligent Systems and Computing, 943, (2020); Wang A., Zhang W., Wei X., A review on weed detection using ground based machine vision and image processing techniques, Comput. Electron. Agric, 158, pp. 226-240, (2019); Dhillon A., Verma G., Convolutional neural network: A review of models, methodologies and applications to object detection, Prog. Artif. Intell, 9, pp. 85-112, (2020); Ren Y., Cheng X., Review of convolutional neural network optimization and training in image processing, Tenth International Symposium on Precision Engineering Measurements and Instrumentation 2018, (2019); Gorach T., Deep convolution neural networks—A review, Intern. Res. J. Eng. Technol, 5, pp. 439-452, (2018); Naranjo-Torres J., Mora M., Hernandez-Garcia R., Barrientos R., Review of Convolutional Neural Network Applied to Fruit Image Processing, Appl. Sci, 10, (2020); Jiao J., Zhao M., Lin J., Liang K., A comprehensive review on convolutional neural network in machine fault diagnosis, Neurocomputing, 417, pp. 36-63, (2020); He T., Kong R., Holmes A., Nguyen M., Sabuncu M.R., Eickhoff S.B., Bzdok D., Feng J., Yeo B.T.T., Deep neural networks and kernel regression achieve comparable accuracies for functional connectivity prediction of behaviour and demographics, NeuroImage, 206, (2020); Ma X., Kittikunakorn N., Sorman B., Xi H., Chen A., Marsh M., Mongeau A., Piche N., Williams R.O., Skomski D., Application of Deep Learning Convolutional Neural Networks for Internal Tablet Defect Detection: High Accuracy, Throughput, and Adaptability, J. Pharma. Sci, 109, pp. 1547-1557, (2020); Aydogan M., Karci A., Improving the accuracy using pre-trained word embeddings on deep neural networks for Turkish text classification, Phys. A Stat. Mech. Its Appl, 541, (2020); Agarwal M., Gupta S., Biswas K., Development of Efficient CNN model for Tomato crop disease identification, Sustain. Comput. Inform. Syst, 28, (2020); Boulent J., Foucher S., Theau J., Charles P., Convolutional Neural Networks for the Automatic Identification of Plant Diseases, Front. Plant Sci, 10, (2019); Jiang Y., Li C., Convolutional Neural Networks for Image-Based High Throughput Plant Phenotyping: A Review, Plant Phenomics 2020, 2020; Noon S., Amjad M., Qureshi M., Mannan A., Use of deep learning techniques for identification of plant leaf stresses: A review, Sustain. Comput. Inf. Systems, 28, (2020); Mishra S., Sachan R., Rajpal D., Deep Convolutional Neural Network based Detection System for Real-time Corn Plant Disease Recognition, Procedia Comput. Sci, 167, pp. 2003-2010, (2020); Badhan S.K., Dsilva D.M., Sonkusare R., Weakey S., Real-Time Weed Detection using Machine Learning and Stereo-Vision, Proceedings of the 2021 6th International Conference for Convergence in Technology (I2CT), pp. 1-5; Gai J., Plants Detection, Localization and Discrimination using 3D Machine Vision for Robotic Intra-row Weed Control, (2016); Gottardi M., A CMOS/CCD image sensor for 2D real time motion estimation, Sens. Actuators A Phys, 46, pp. 251-256, (1995); Helmers H., Schellenberg M., CMOS vs. CCD sensors in speckle interferometry, Opt. Laser Technol, 35, pp. 587-593, (2003); Silfhout R., Kachatkou A., Fibre-optic coupling to high-resolution CCD and CMOS image sensors, Nucl. Instr. Methods Phys. Res. Sect. A Accel. Spectrum. Detect. Ass. Equip, 597, pp. 266-269, (2008); Krishna B., Rekulapellim N., Kauda B.P., Materials Today: Proceedings. Comparison of different deep learning frameworks, Mater. Today Proc, (2020); Trung W., Maleki F., Romero F., Forghani R., Kadoury S., Overview of Machine Learning: Part 2: Deep Learning for Medical Image Analysis, Neuroimaging Clin. N. Am, 30, pp. 417-431, (2020); Wang P., Fan E., Wang P., Comparative analysis of image classification algorithms based on traditional machine learning and deep learning, Pattern Recognit. Lett, 141, pp. 61-67, (2021); Bui D., Tsangaratos P., Nguyen V., Liem N., Trinh P., Comparing the prediction performance of a Deep Learning Neural Network model with conventional machine learning models in landslide susceptibility assessment, CATENA, 188, (2020); Kamilaris A., Brik C., Karatsiolis S., Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles, Proceedings of the CAIP 2019, the Workshop on Deep-Learning Based Computer Vision for UAV, (2019); Barth R., IJsselmuiden J., Hemming J., Van Henten E.J., Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset, Comput. Electron. Agri, 144, pp. 284-296, (2018); Zichao J., A Novel Crop Weed Recognition Method Based on Transfer Learning from VGG16 Implemented by Keras, OP Conf. Ser. Mater. Sci. Eng, 677, (2019); Chen D., Lu Y., Yong S., Performance Evaluation of Deep Transfer Learning on Multiclass Identification of Common Weed Species in Cotton Production Systems; Espejo-Garcia B., Mylonas N., Athanasakos L., Spyros Fountas S., Vasilakoglou I., Towards weeds identification assistance through transfer learning, Comput. Electron. Agric, 171, (2020); Al-Qurran R., Al-Ayyoub M., Shatnawi A., Plant Classification in the Wild: A Transfer Learning Approach, Proceedings of the 2018 International Arab Conference on Information Technology (ACIT), pp. 1-5; Pajares G., Garcia-Santillam I., Campos Y., Montalo M., Machine-vision systems selection for agricultural vehicles: A guide, Imaging, 2, (2016); Shorten C., Khoshgoftaar T., A survey on Image Data Augmentation for Deep Learning, J. Big Data, 6, (2019); Zheng Y., Kong J., Jin X., Wang X., CropDeep: The Crop Vision Dataset for Deep-Learning-Based Classification and Detection in Precision Agriculture, Sensors, 19, (2019); Sudars K., Jasko J., Namatevsa I., Ozola L., Badaukis N., Dataset of annotated food crops and weed images for robotic computer vision control, Data Brief, 31, (2020); Cap Q.H., Tani H., Uga H., Kagiwada S., Lyatomi H., LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis; Zhu J., Park T., Isola P., Efros A., Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, (2020); Huang Z., Ke W., Huang D., Improving Object Detection with Inverted Attention, Proceedings of the 2020 IEEE Winter Conference on Applications of Computer Vision (WACV); He C., Lai S., Lam K., Object Detection with Relation Graph Inference, Proceedings of the ICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), (2019); Champ J., Mora-Fallas A., Goeau H., Mata-Montero E., Bonnet P., Joly A., Instance segmentation for the fine detection of crop and weed plants by precision agricultural robots, Appl. Plant Sci, 8, (2020); Lameski P., Zdravevski E., Trajkovik V., Kulakov A., Weed Detection Dataset with RGB Images Taken Under Variable Light Conditions, ICT Innovations 2017. Communications in Computer and Information Science, 778, (2017); Giselsson T.M., Jorgensen R.N., Jensen P.K., Dyrmann M., Midtiby H.S., A Public Image Database for Benchmark of Plant Seedling Classification Algorithms, (2017); Cicco M., Potena C., Grisetti G., Pretto A., Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection, (2016); Lu Y., Young S., A survey of public datasets for computer vision tasks in precision agriculture, Comput. Electron. Agric, 178, (2020); Faisal F., Hossain B., Emam H., Performance Analysis of Support Vector Machine and Bayesian Classifier for Crop and Weed Classification from Digital Images, World Appl. Sci, 12, pp. 432-440, (2011); Dyrmann M., Automatic Detection and Classification of Weed Seedlings under Natural Light Conditions, (2017); Chang C., Lin K., Smart Agricultural Machine with a Computer Vision Based Weeding and Variable-Rate Irrigation Scheme, Robotics, 7, (2018); Slaughter D.C., Giles D.K., Downey D., Autonomous robotic weed control systems: A review, Comput. Electron. Agric, 61, pp. 63-78, (2008); Abhisesh S., Machine Vision System for Robotic Apple Harvesting in Fruiting Wall Orchards, (2016); Qiu Q., Fan Z., Meng Z., Zhang Q., Cong Y., Li B., Wang N., Zhao C., Extended Ackerman Steering Principle for the coordinated movement control of a four wheel drive agricultural mobile robot, Comput. Electron. Agric, 152, pp. 40-50, (2018); Ren G., Lin T., Ying Y., Chowdhary G., Ting K.C., Agricultural robotics research applicable to poultry production: A review, Comput. Electron. Agric, 169, (2020); Asha R., Aman M., Pankaj M., Singh A., Robotics-automation and sensor based approaches in weed detection and control: A review, Intern. J. Chem. Stud, 8, pp. 542-550, (2020); Shinde A., Shukla M., Crop detection by machine vision for weed management, Intern. J. Adv. Eng. Technol, 7, pp. 818-826, (2014); Raja R., Nguyen T., Vuong V.L., Slaughter D.C., Fennimore S.A., RTD-SEPs: Real-time detection of stem emerging points and classification of crop-weed for robotic weed control in producing tomato, Biosyst. Eng, 195, pp. 152-171, (2020); Sirikunkitti S., Chongcharoen K., Yoongsuntia P., Ratanavis A., Progress in a Development of a Laser-Based Weed Control System, Proceedings of the 2019 Research, Invention, and Innovation Congress (RI2C), pp. 1-4; Mathiassen S., Bak T., Christensen S., Kudsk P., The effect of laser treatment as a weed control method, Biosyst. Eng, 95, pp. 497-505, (2006); Xiong Y., Ge Y., Liang Y., Blackmore S., Development of a prototype robot and fast path-planning algorithm for static laser weeding, Comput. Electron. Agric, 142, pp. 494-503, (2017); Marx C., Barcikowski S., Hustedt M., Haferkamp H., Rath T., Design and application of a weed damage model for laser-based weed control, Biosyst. Eng, 113, pp. 148-157, (2012); Libran-Embid F., Klaus F., Tscharntke T., Grass I., Unmanned aerial vehicles for biodiversity-friendly agricultural landscapes—A systematic review, Sci. Total Environ, 732, (2020); Boursianis A., Papadopoulou M., Diamantoulakis P., Liopa-Tsakalidi A., Barouchas P., Salahas G., Karagiannidis G., Wan S., Goudos S.K., Internet of Things (IoT) and Agricultural Unmanned Aerial Vehicles (UAVs) in smart farming: A comprehensive review, Internet Things, 7, (2020); Huang H., Deng J., Lan Y., Yang A., Deng X., Zhang L., A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery, PLoS ONE, 13, (2018); Hunter J., Gannon T.W., Richardson R.J., Yelverton F.H., Leon R.G., Integration of remote-weed mapping and an autonomous spraying unmanned aerial vehicle for site-specific weed management, Pest. Manag. Sci, 76, pp. 1386-1392, (2020); Cerro J., Ulloa C., Barrientos A., Rivas J., Unmanned Aerial Vehicles in Agriculture: A Survey, Agronomy, 11, (2021); Rasmussen J., Nielsen J., A novel approach to estimating the competitive ability of Cirsium arvense in cereals using unmanned aerial vehicle imagery, Weed Res, 60, pp. 150-160, (2020); Rijk L., Beedie S., Precision Weed Spraying using a Multirotor UAV, Proceedings of the10th International Micro-Air Vehicles Conference, (2018); Liang Y., Yang Y., Chao C., Low-Cost Weed Identification System Using Drones, Proceedings of the Seventh International Symposium on Computing and Networking Workshops (CANDARW), pp. 260-263; Zhang Q., Liu Y., Gong C., Chen Y., Yu H., Applications of deep learning for dense scenes, analysis in agriculture: A review, Sensors, 20, (2020); Asad M., Bais A., Weed Detection in Canola Fields Using Maximum Likelihood Classification and Deep Convolutional Neural Network, Inform. Process. Agric, 7, pp. 535-545, (2020); Shawky O., Hagag A., Dahshan E., Ismail M., Remote sensing image scene classification using CNN-MLP with data augmentation, Optik, 221, (2020); Zhuoyao Z., Lei S., Qiang H., Improved localization accuracy by LocNet for Faster R-CNN based text detection in natural scene images, Pattern Recognit, 96, (2019); Liakos K., Busato P., Moshou D., Pearson S., Bochtis D., Machine learning in agriculture: A review, Sensors, 18, (2018); Hasan A.S.M.M., Sohel F., Diepeveen D., Laga H., Jones M.G.K., A survey of deep learning techniques for weed detection from images, Comput. Electron. Agric, 184, (2021); Rehman T.U., Mahmud M.S., Chang Y.K., Jin J., Shin J., Current and future applications of statistical machine learning algorithms for agricultural machine vision systems, Comput. Electron. Agricult, 156, pp. 585-605, (2019); Osorio K., Puerto A., Pedraza C., Jamaica D., Rodriguez L., A Deep Learning Approach for Weed Detection in Lettuce Crops Using Multispectral Images, AgriEngineering, 2, (2020); Ferreira A.S., Freitas D.M., Goncalves da Silva G., Pistori H., Folhes M.T., Weed detection in soybean crops using ConvNets, Comput. Electron. Agric, 143, pp. 314-324, (2017); Santos L., Santos F.N., Oliveira P.M., Shinde P., Deep Learning Applications in Agriculture: A Short Review, Robot 2019: Fourth Iberian Robotics Conference. Advances in Intelligent Systems and Computing, 1092, (2019); Dokic K., Blaskovic L., Mandusic D., From machine learning to deep learning in agriculture—The quantitative review of trends, IOP Conf. Ser. Earth Environ. Sci, 614, (2020); Tian H., Wang T., Yadong Y., Qiao X., Li Y., Computer vision technology in agricultural automation —A review, Inform. Process. Agric, 7, pp. 1-19, (2020); Khaki S., Pham H., Han Y., Kuhl A., Convolutional Neural Networks for Image-Based Corn Kernel Detection and Counting; Yu J., Sharpe S., Schumann A., Boyd N., Deep learning for image-based weed detection in turfgrass, Eur. J. Agron, 104, pp. 78-84, (2019); Yu J., Schumann A., Cao Z., Sharpe S., Weed detection in perennial ryegrass with deep learning convolutional neural network, Front. Plant Sci, 10, (2019); Gao J., French A., Pound M., Deep convolutional neural networks for image based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, (2020); Scott S., Comparison of Object Detection and Patch-Based Classification Deep Learning Models on Mid- to Late-Season Weed Detection in UAV Imagery, Remote Sens, 12, (2020); Narvekar C., Rao M., Flower classification using CNN and transfer learning in CNN- Agriculture Perspective, Proceedings of the 3rd International Conference on Intelligent Sustainable Systems (ICISS), pp. 660-664; Sharma P., Berwal Y., Ghai W., Performance analysis of deep learning CNN models for disease detection in plants using image segmentation, Information. Process. Agric, 7, pp. 566-574, (2020); Du X., Lin T., Jin P., SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization, Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Koh J., Spangenberg G., Kant S., Automated Machine Learning for High-Throughput Image-Based Plant Phenotyping, Remote Sens, 13, (2021); Shah S., Wu W., Lu Q., AmoebaNet: An SDN-enabled network service for big data science, J. Netw. Comput. Appl, 119, pp. 70-82, (2018); Yao L., Xu H., Zhang W., SM-NAS: Structural-to-Modular Neural Architecture Search for Object Detection, Proc. AAAI Conf. Artif. Intell, 34, pp. 12661-12668, (2020); Jia X., Yang X., Yu X., Gao H., A Modified CenterNet for Crack Detection of Sanitary Ceramics, Proceedings of the IECON 2020—46th Annual Conference of the IEEE Industrial Electronics Society; Zhao K., Yan W.Q., Fruit Detection from Digital Images Using CenterNet, Geom. Vis, 1386, pp. 313-326, (2021); Xu M., Deng Z., Qi L., Jiang Y., Li H., Wang Y., Xing X., Fully convolutional network for rice seedling and weed image segmentation at the seedling stage in paddy fields, PLoS ONE, 14, (2019); Kong J., Wang H., Wang X., Jin X., Fang X., Lin S., Multi-stream hybrid architecture based on cross-level fusion strategy for fine-grained crop species recognition in precision agriculture, Comput. Electron. Agric, 185, (2021); Wosner O., Detection in Agricultural Contexts: Are We Close to Human Level? Computer Vision—ECCV 2020 Workshops, Lect. Notes Comput. Sci, (2020); Wu D., Lv S., Jiang M., Song H., Using channel pruning-based YOLO v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments, Comput. Electron. Agric, 178, (2020); Kuznetsova A., Maleva T., Soloviev V., Detecting Apples in Orchards Using YOLOv3 and YOLOv5 in General and Close-Up Images; Advances in Neural Networks—ISNN, (2020); Tian Y., Yang G., Wang Z., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agric, 157, pp. 417-426, (2019); Wu D., Wu Q., Yin X., BoJiang B., Wang H., He D., Song H., Lameness detection of dairy cows based on the YOLOv3 deep learning algorithm and a relative step size characteristic vector, Biosyst. Eng, 189, pp. 150-163, (2020); Waheed A., Goyal M., Gupta D., Khanna A., Hassanien A.E., Pandey H.M., An optimized dense convolutional neural network model for disease recognition and classification in corn leaf, Comput. Electron. Agric, 175, (2020); Atila U., Ucar M., Akyol K., Ucar E., Plant leaf disease classification using EfficientNet deep learning model, Ecol. Inform, 61, (2021); Pang Y., Shi Y., Gao S., Jiang F., Veeranampalayam-Sivakumar A.-N., Thomson L., Luck J., Liu C., Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery, Comput. Electron. Agric, 178, (2020); Liang F., Tian Z., Dong M., Cheng S., Sun L., Li H., Chen Y., Zhang G., Efficient neural network using pointwise convolution kernels with linear phase constraint, Neurocomputing, 423, pp. 572-579, (2021); Taravat A., Wagner M.P., Bonifacio R., Petit D., Advanced Fully Convolutional Networks for Agricultural Field Boundary Detection, Remote Sens, 13, (2021); Isufi E., Pocchiari M., Hanjalic A., Accuracy-diversity trade-off in recommender systems via graph convolutions, Inf. Process. Managem, 58, (2021); Wei Y., Gu K., Tan L., A positioning method for maize seed laser-cutting slice using linear discriminant analysis based on isometric distance measurement, Inf. Process. Agric, (2021); Koo J., Klabjan D., Utke J., Combined Convolutional and Recurrent Neural Networks for Hierarchical Classification of Images, (2019); Agarap A.F.M., An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification, (2017); Khaki S., Wang L., Archontoulis S., A CNN-RNN Framework for Crop Yield Prediction, Front. Plant Sci, 10, (2020); Dyrmann M., Jorgensen R.H., Midtiby H.S., RoboWeedSupport—Detection of weed locations in leaf occluded cereal crops using a fully convolutional neural network, Adv. Anim. Biosci, 8, pp. 842-847, (2017); Barth R., Hemming J., Henten V., Optimising realism of synthetic images using cycle generative adversarial networks for improved part segmentation, Comput. Electron. Agric, 173, (2020); Nguyen N., Tien D., Thanh D., An Evaluation of Deep Learning Methods for Small Object Detection, J. Electr. Comput. Eng, (2020); Chen C., Liu M., Tuzel O., Xiao J., R-CNN for Small Object Detection, Comput. Vis, (2017); Yu Y., Zhang K., Li Y., Zhang D., Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN, Comput. Electron. Agric, 163, (2019); Boukhris L., Abderrazak J., Besbes H., Tailored Deep Learning based Architecture for Smart Agriculture, Proceedings of the 2020 International Wireless Communications and Mobile Computing (IWCMC); Basodi S., Chunya C., Zhang H., Pan Y., Gradient Amplification: An efficient way to train deep neural networks; Kurniawan A., Administering NVIDIA Jetson Nano, IoT Projects with NVIDIA Jetson Nano, (2021); Kurniawan A., NVIDIA Jetson Nano, IoT Projects with NVIDIA Jetson Nano, (2021); Verucchi M., Brilli G., Sapienza D., Verasani M., Arena M., Gatti F., Capotondi A., Cavicchioli R., Bertogna M., Solieri M., A Systematic Assessment of Embedded Neural Networks for Object Detection, Proceedings of the 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA), pp. 937-944; Gasparovic M., Zrinjski M., Barkovic D., Radocaj D., An automatic method for weed mapping in oat fields based on UAV imagery, Comput. Electron. Agric, 173, (2020); Yano I.H., Alves J.R., Santiago W.E., Mederos B.J.T., Identification of weeds in sugarcane fields through images taken by UAV and random forest classifier, IFAC-Pap, 49, pp. 415-420, (2016); Zhou H., Zhang C., A Field Weed Density Evaluation Method Based on UAV Imaging and Modified U-Net, Remote Sens, 13, (2021); Bakhshipour A., Jafari A., Evaluation of support vector machine and artificial neural networks in weed detection using shape features, Comput. Electron. Agric, 145, pp. 153-160, (2018); Sudars K., Data For: Dataset of Annotated Food Crops and Weed Images for Robotic Computer Vision Control, Mendeley Data, (2021); Xu Y., He R., Gao Z., Li C., Zhai Y., Jiao Y., Weed density detection method based on absolute feature corner points in field, Agronomy, 10, (2020); Shorewala S., Ashfaque A.R.S., Verma U., Weed Density and Distribution Estimation for Precision Agriculture Using Semi-Supervised Learning","C. Andreasen; Department of Plant and Environmental Sciences, University of Copenhagen, Taastrup, Højbakkegaard Allé 13, DK 2630, Denmark; email: can@plen.ku.dk","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120067412"
"Duporge I.; Isupova O.; Reece S.; Macdonald D.W.; Wang T.","Duporge, Isla (57205185303); Isupova, Olga (57164749700); Reece, Steven (15923614700); Macdonald, David W. (7401463172); Wang, Tiejun (55709751800)","57205185303; 57164749700; 15923614700; 7401463172; 55709751800","Using very-high-resolution satellite imagery and deep learning to detect and count African elephants in heterogeneous landscapes","2021","Remote Sensing in Ecology and Conservation","7","3","","369","381","12","42","10.1002/rse2.195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097946970&doi=10.1002%2frse2.195&partnerID=40&md5=2001e22a19c8e6da669974ca6e94c30c","Wildlife Conservation Research Unit, Department of Zoology, University of Oxford, Recanati-Kaplan Centre, Tubney, United Kingdom; Department of Computer Science, University of Bath, Bath, United Kingdom; Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Faculty of Geo-information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands","Duporge I., Wildlife Conservation Research Unit, Department of Zoology, University of Oxford, Recanati-Kaplan Centre, Tubney, United Kingdom; Isupova O., Department of Computer Science, University of Bath, Bath, United Kingdom; Reece S., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Macdonald D.W., Wildlife Conservation Research Unit, Department of Zoology, University of Oxford, Recanati-Kaplan Centre, Tubney, United Kingdom; Wang T., Faculty of Geo-information Science and Earth Observation (ITC), University of Twente, Enschede, Netherlands","Satellites allow large-scale surveys to be conducted in short time periods with repeat surveys possible at intervals of <24 h. Very-high-resolution satellite imagery has been successfully used to detect and count a number of wildlife species in open, homogeneous landscapes and seascapes where target animals have a strong contrast with their environment. However, no research to date has detected animals in complex heterogeneous environments or detected elephants from space using very-high-resolution satellite imagery and deep learning. In this study, we apply a Convolution Neural Network (CNN) model to automatically detect and count African elephants in a woodland savanna ecosystem in South Africa. We use WorldView-3 and 4 satellite data –the highest resolution satellite imagery commercially available. We train and test the model on 11 images from 2014 to 2019. We compare the performance accuracy of the CNN against human accuracy. Additionally, we apply the model on a coarser resolution satellite image (GeoEye-1) captured in Kenya, without any additional training data, to test if the algorithm can generalize to an elephant population outside of the training area. Our results show that the CNN performs with high accuracy, comparable to human detection capabilities. The detection accuracy (i.e., F2 score) of the CNN models was 0.78 in heterogeneous areas and 0.73 in homogenous areas. This compares with the detection accuracy of the human labels with an averaged F2 score 0.77 in heterogeneous areas and 0.80 in homogenous areas. The CNN model can generalize to detect elephants in a different geographical location and from a lower resolution satellite. Our study demonstrates the feasibility of applying state-of-the-art satellite remote sensing and deep learning technologies for detecting and counting African elephants in heterogeneous landscapes. The study showcases the feasibility of using high resolution satellite imagery as a promising new wildlife surveying technique. Through creation of a customized training dataset and application of a Convolutional Neural Network, we have automated the detection of elephants in satellite imagery with accuracy as high as human detection capabilities. The success of the model to detect elephants outside of the training data site demonstrates the generalizability of the technique. © 2020 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.","Aerial Survey; Anthropocene; Conservation; Convolutional Neural Network; Endangered Species; Machine Learning; Object Detection; Wildlife Census","","","","","","CNN; Ralph Mistler Trust; Lloyd's Register Foundation, LRF; DigitalGlobe Foundation; European Space Agency, ESA","Funding text 1: We greatly appreciate the generous support by the DigitalGlobe Foundation (Maxar Technologies) and European Space Agency for awarding the image grants to support this work without which this research would not have been possible. We are also grateful to Hexagon Geospatial for providing a complementary license to ERDAS Imagine which enabled us to process the satellite imagery. We are grateful to all the human volunteer labellers for taking the time to label the images and giving us a point of comparison to the CNN. ID is grateful for a bequest to the Wildlife Conservation Research Unit, University of Oxford from the Ralph Mistler Trust which supported her to carry out this research. SR is grateful for funding from the LLoyd's Register Foundation through the Alan Turing Institute’s Data Centric Engineering programme. We are grateful to Sofia Minano Gonzalez and Hannah Cubaynes for their valuable comments on the manuscript. ; Funding text 2: We greatly appreciate the generous support by the DigitalGlobe Foundation (Maxar Technologies) and European Space Agency for awarding the image grants to support this work without which this research would not have been possible. We are also grateful to Hexagon Geospatial for providing a complementary license to ERDAS Imagine which enabled us to process the satellite imagery. We are grateful to all the human volunteer labellers for taking the time to label the images and giving us a point of comparison to the CNN. ID is grateful for a bequest to the Wildlife Conservation Research Unit, University of Oxford from the Ralph Mistler Trust which supported her to carry out this research. SR is grateful for funding from the LLoyd's Register Foundation through the Alan Turing Institute?s Data Centric Engineering programme. We are grateful to Sofia Minano Gonzalez and Hannah Cubaynes for their valuable comments on the manuscript.","Abileah R., Marine mammal census using space satellite imagery, U.S. Navy Journal of Underwater Acoustics, 52, (2002); Barber-Meyer S.M., Kooyman G.L., Ponganis P.J., Estimating the relative abundance of emperor penguins at inaccessible colonies using satellite imagery, Polar Biology, 30, 12, pp. 1565-1570, (2007); Barnes R., Beardsley K., Michelmore F., Barnes K.L., Alers M.P., Blom A., Estimating forest elephant numbers with dung counts and a geographic information system, The Journal of Wildlife Management, 61, 4, pp. 1384-1393, (1997); Barnosky A.D., Brown J.H., Daily G.C., Dirzo R., Ehrlich A.H., Ehrlich P.R., Et al., Introducing the Scientific Consensus on Maintaining Humanity’s Life Support Systems in the 21st Century: Information for Policy Makers, The Anthropocene Review, 1, 1, pp. 78-109, (2014); Borowicz A., Le H., Humphries G., Nehls G., Hoschle C., Kosarev V., Et al., Aerial-trained deep learning networks for surveying cetaceans from satellite imagery, PLoS One, 14, 10, (2019); Bowler E., Et al., Using deep learning to count albatrosses from space: Assessing results in light of ground truth uncertainty, Remote Sensing, 12, (2019); Bowley C., Et al., Detecting Wildlife in Unmanned Aerial Systems Imagery Using Convolutional Neural Networks Trained with an Automated Feedback Loop, 10860, (2018); Bruijning M., Et al., trackdem: Automated particle tracking to obtain population counts and size distributions from videos in r, Methods in Ecology and Evolution, 9, 4, pp. 965-973, (2018); Buma W.G., Lee S.-I., Multispectral image-based estimation of drought patterns and intensity around Lake Chad, Africa, Remote Sensing, 11, 21, (2019); Cao C., Wang B., Zhang W., Zeng X., Yan X., Feng Z., Et al., An Improved Faster R-CNN for Small Object Detection, IEEE Access, 7, pp. 106838-106846, (2019); Cardinale B., Impacts of Biodiversity Loss, Science, 336, pp. 552-553, (2012); Caughley G., Sinclair R., Scott-Kemmis D., Experiments in aerial survey, The Journal of Wildlife Management, 40, pp. 290-300, (1976); Chabot D., Bird D.M., Wildlife research and management methods in the 21st century: Where do unmanned aircraft fit in?, Journal of Unmanned Vehicle Systems, 3, 4, pp. 137-155, (2015); Christie K.S., Gilbert S.L., Brown C.L., Hatfield M., Hanson L., Unmanned aircraft systems in wildlife research: current and future applications of atransformative technology, Frontiers in Ecology and the Environment, 14, 5, pp. 241-251, (2016); Cubaynes H.C., Fretwell P.T., Bamford C., Gerrish L., Jackson J.A., Et al., Whales from space: Four mysticete species described using new VHR satellite imagery, Marine Mammal Science, 35, 2, pp. 466-491, (2018); Dascalu A., David E.O., Skin cancer detection by deep learning and sound analysis algorithms: A prospective clinical study of an elementary dermoscope, EBioMedicine, 43, pp. 107-113, (2019); de Carvalho Junior O., Guimaraes R.F., Silva C.R., Gomes R.A., Standardized Time-Series and Interannual Phenological Deviation: New Techniques for Burned-Area Detection Using Long-Term MODIS-NBR Dataset, Remote Sensing, 7, 6, pp. 6950-6985, (2015); Du Toit J.C.O., O'Connor T.G., Changes in rainfall pattern in the eastern Karoo, South Africa, over the past 123 years, Water SA, 40, 3, (2014); Dutta A., Zisserman A., The VIA Annotation Software for Images, Audio and Video, 2019, (2019); Eikelboom J.A.J., Wind J., van de Ven E., Kenana L.M., Schroder B., de Knegt H.J., Et al., Improving the precision and accuracy of animal population estimates with aerial image object detection, Methods in Ecology and Evolution, 10, 11, pp. 1875-1887, (2019); Emmanuel H., Di Vittorio M., Barnes R.F., Guenda W., Luiselli L., Detection of interannual population trends in seven herbivores from a West African savannah: a comparison between dung counts and direct counts of individuals, African Journal of Ecology, 55, pp. 609-617, (2017); Ferreira A.C., Silva L.R., Renna F., Brandl H.B., Renoult J.P., Farine D.R., Et al., Deep learning-based methods for individual recognition in small birds, Methods in Ecology and Evolution, (2020); Fretwell P.T., LaRue M.A., Morin P., Kooyman G.L., Wienecke B., Ratcliffe N., Et al., An emperor penguin population estimate: the first global, synoptic survey of a species from space, PLoS One, 7, 4, (2012); Fretwell P.T., Scofield P., Phillips R.A., Using super-high resolution satellite imagery to census threatened albatrosses, Ibis, 159, 3, pp. 481-490, (2017); Fretwell P.T., Staniland I.J., Forcada J., Whales from space: counting southern right whales by satellite, PLoS One, 9, 2, (2014); Fretwell P.T., Trathan P.N., Penguins from space: faecal stains reveal the location of emperor penguin colonies, Global Ecology and Biogeography, 18, pp. 543-552, (2009); Fullman T., Kiker G.A., Gaylard A., Southworth J., Waylen P., Kerley G.I., Et al., Elephants respond to resource trade-offs in an aseasonal system through daily and annual variability in resource selection, KOEDOE -African Protected Area Conservation and Science, pp. 1-21, (2017); Gara T.E.A., Understanding the effect of landscape fragmentation and vegetation productivity on elephant habitat utilization in Amboseli ecosystem, Kenya, African Journal of Ecology, 55, pp. 860-871, (2016); Ginosar S., Haas D., Brown T., Malik J., Detecting People in Cubist Art, (2014); Goncalves B.C., Spitzbart B., Lynch H.J., SealNet: A fully-automated pack-ice seal detection pipeline for sub-meter satellite imagery, Remote Sensing of Environment, 239, (2020); Gray P.C., Fleishman A.B., Klein D.J., McKown M.W., Bezy V.S., Lohmann K.J., Et al., A Convolutional neural network for detecting sea turtles in drone imagery, Methods in Ecology and Evolution, (2018); Guirado E., Tabik S., Rivas M.L., Alcaraz-Segura D., Herrera F., Whale counting in satellite and aerial images with deep learning, Scientific Reports, 9, 1, (2019); Gulshan V., Peng L., Coram M., Stumpe M.C., Wu D., Narayanaswamy A., Et al., Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs, JAMA, 316, 22, pp. 2402-2410, (2016); Hollings T., Burgman M., van Andel M., Gilbert M., Robinson T., Robinson A., How do you find the green sheep? A critical review of the use of remotely sensed imagery to detect and count animals, Methods in Ecology and Evolution, 9, 4, pp. 881-892, (2018); Hordiiuk D., Oliinyk I., Hnatushenko V., Maksymov K., Semantic Segmentation for Ships Detection from Satellite Imagery, in IEEE 39th International Conference on Electronics and Nanotechnology (ELNANO), pp. 454-457, (2019); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Et al., Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors, in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3296-3297, (2017); Hughes B.J., Martin G.R., Reynolds S.J., The use of Google EarthTMsatellite imagery to detect the nests of masked boobies Sula dactylatra, Wildlife Biology, 17, 2, pp. 210-216, (2011); Jachmann H., Comparison of Aerial Counts with Ground Counts for Large African Herbivores, British Ecological Society, 39, 5, pp. 841-852, (2002); Kakembo V., Smith J., Kerley G., A Temporal Analysis of Elephant-Induced Thicket Degradation in Addo Elephant National Park, Eastern Cape, South Africa, Rangeland Ecology & Management, 68, 6, pp. 461-469, (2015); Kao A.B., Berdahl A.M., Hartnett A.T., Lutz M.J., Bak-Coleman J.B., Ioannou C.C., Et al., Counteracting estimation bias and social influence to improve the wisdom of crowds, Journal of the Royal Society, Interface, 15, 141, (2018); Kellenberger B., Marcos D., Lobry S., Tuia D., Half a percent of labels is enough: efficient animal detection in UAV imagery using deep CNNs and active learning, IEEE Transactions on Geoscience and Remote Sensing, 57, 12, pp. 9524-9533, (2019); Kellenberger B., Marcos D., Tuia D., Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning, Remote Sensing of Environment, 216, pp. 139-153, (2018); Kiage L.M., Liu K.B., Walker N.D., Lam N., Huh O.K., Recent land-cover/use change associated with land degradation in the Lake Baringo catchment, Kenya, East Africa: evidence from Landsat TM and ETM+, International Journal of Remote Sensing, 28, 19, pp. 4285-4309, (2007); Koneff M.D., Royle J.A., Otto M.C., Wortham J.S., Bidwell J.K., A double-observer method to estimate detection rate during aerial waterfowl surveys, The Journal of Wildlife Management, 72, pp. 1641-1649, (2008); Krizhevsky A., Sutskever I., Hinton G., ImageNet Classification with Deep Convolutional Neural Networks, (2012); Kusimi J.M., Assessing land use and land cover change in the Wassa West District of Ghana using remote sensing, GeoJournal, 71, 4, pp. 249-259, (2008); LaRue M.A., Lynch H.J., Lyver P.O., Barton K., Ainley D.G., Pollard A., Et al., A method for estimating colony sizes of Adélie penguins using remote sensing imagery, Polar Biology, 37, 4, pp. 507-517, (2014); LaRue M.A., Rotella J.J., Garrott R.A., Siniff D.B., Ainley D.G., Stauffer G.E., Satellite imagery can be used to detect variation in abundance of Weddell seals (Leptonychotes weddellii) in Erebus Bay, Antarctica, Polar Biology, 34, 11, pp. 1727-1737, (2011); LaRue M.A., Stapleton S., Estimating the abundance of polar bears on Wrangel Island during late summer using high-resolution satellite imagery: a pilot study, Polar Biology, 41, 12, pp. 2621-2626, (2018); LaRue M.A., Stapleton S., Anderson M., Feasibility of using high-resolution satellite imagery to assess vertebrate wildlife populations, Conservation Biology, 31, 1, pp. 213-220, (2017); LaRue M.A., Stapleton S., Porter C., Atkinson S., Atwood T., Dyck M., Et al., Testing methods for using high-resolution satellite imagery to monitor polar bear abundance and distribution, Wildlife Society Bulletin, 39, 4, pp. 772-779, (2015); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Lin T., Microsoft COCO: Common Objects in Context, in Computer Vision – ECCV (2014), (2014); Loffler E., Margules C., Wombats Detected from Space, Remote Sensing of Environment, 9, pp. 47-56, (1980); Lu D., Mausel P., Brondizio E., Moran E., Change detection techniques, International Journal of Remote Sensing, 25, 12, pp. 2365-2401, (2010); Lynch H.J., LaRue M.A., First global census of the Adélie Penguin, The Auk, 131, 4, pp. 457-466, (2014); Lynch H.J., White R., Black A.D., Naveen R., Detection, differentiation, and abundance estimation of penguin species by high-resolution satellite imagery, Polar Biology, 35, 6, pp. 963-968, (2012); Maire F., Alvarez L.M., Hodgson A., Automating marine mammal detection in aerial images captured during wildlife surveys: A deep learning approach, (2015); Mairea F., Mejias L., Hodgson A., Duclos G., Detection of Dugongs from Unmanned Aerial Vehicles, (2013); McMahon C.R., Howe H., Van Den Hoff J., Alderman R., Brolsma H., Hindell M.A., Satellites, the all-seeing eyes in the sky: counting elephant seals from space, PLoS One, 9, 3, (2014); Meng Q., Meentemeyer R.K., Modeling of multi-strata forest fire severity using Landsat TM Data, International Journal of Applied Earth Observation and Geoinformation, 13, 1, pp. 120-126, (2011); Miao Z., Gaynor K.M., Wang J., Liu Z., Muellerklein O., Norouzzadeh M.S., Et al., Insights and approaches using deep learning to classify wildlife, Scientific Reports, 9, 1, (2019); Mierswa I., The Wisdom of Crowds: Best Practices for Data Prep & Machine Learning Derived from Millions of Data Science Workflows, (2016); Mulero-Pazmany M., Jenni-Eiermann S., Strebel N., Sattler T., Negro J.J., Tablado Z., Unmanned aircraft systems as a new source of disturbance for wildlife: a systematic review, PLoS One, 12, 6, (2017); Olczak J., Fahlberg N., Maki A., Razavian A.S., Jilert A., Stark A., Et al., Artificial intelligence for analyzing orthopedic trauma radiographs, Acta Orthopaedica, 88, 6, pp. 581-586, (2017); Oliveira E.R., Disperati L., Cenci L., Gomes Pereira L., Alves F.L., Multi-index image differencing method (MINDED) for Flood Extent Estimations, Remote Sensing, 11, 11, (2019); Pang J., Li C., Shi J., Xu Z., Feng H., CNN: Fast tiny object detection in large-scale remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, 57, 8, pp. 5512-5524, (2019); Petersen S., Et al., Using machine learning to accelerate ecological research, (2019); Poulsen J.R., Koerner S.E., Moore S., Medjibe V.P., Blake S., Clark C.J., Et al., Poaching empties critical Central African wilderness of forest elephants, Current Biology, 27, 4, pp. R134-R135, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, pp. 1137-1149, (2017); Romero J.R., Roncallo P.F., Akkiraju P.C., Ponzoni I., Echenique V.C., Carballido J.A., Using classification algorithms for predicting durum wheat yield in the province of Buenos Aires, Computers and Electronics in Agriculture, 96, pp. 173-179, (2013); Rulinda C.M., Bijker W., Stein A., Image mining for drought monitoring in eastern Africa using Meteosat SEVIRI data, International Journal of Applied Earth Observation and Geoinformation, 12, pp. S63-S68, (2010); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Et al., ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); Sasamal S.K., Chaudhury S.B., Samal R.N., Pattanaik A.K., QuickBird spots flamingos off Nalabana Island, Chilika Lake, India, International Journal of Remote Sensing, 29, 16, pp. 4865-4870, (2008); Schlossberg S., Chase M.J., Griffin C.R., Testing the Accuracy of Aerial Surveys for Large Mammals: An Experiment with African Savanna Elephants (Loxodonta africana), PLoS One, 11, 10, (2016); Schmidhuber J., Deep learning in neural networks: an overview, Neural Netw, 61, pp. 85-117, (2015); Schneider S., Greenberg S., Taylor G.W., Kremer S.C., Three critical factors affecting automated image species recognition performance for camera traps, Ecology and Evolution, 10, 7, pp. 3503-3517, (2020); Schneider S., Taylor G., Kremer S., Deep Learning Object Detection Methods for Ecological Camera Trap Data; Sharma N., Blumenstein M., Scully-Power P., Shark detection from aerial imagery using region-based CNN, a study, (2018); Shiu Y., Wrege P.H., Keen S., Rowland E.D., Large-scale automatic acoustic monitoring of African forest elephants’ calls in the terrestrial acoustic recordings, The Journal of the Acoustical Society of America, 135, 5, (2014); Sibanda M., Murwira A., Cotton fields drive elephant habitat fragmentation in the Mid Zambezi Valley, Zimbabwe, International Journal of Applied Earth Observation and Geoinformation, 19, pp. 286-297, (2012); Skogen K., Helland H., Kaltenborn B., Concern about climate change, biodiversity loss, habitat degradation and landscape change: Embedded in different packages of environmental concern?, Journal for Nature Conservation, 44, pp. 12-20, (2018); Smit J., Pozo R.A., Cusack J.J., Nowak K., Jones T., Using camera traps to study the age–sex structure and behaviour of crop-using elephants Loxodonta africana in Udzungwa Mountains National Park, Tanzania, Oryx, 53, 2, pp. 368-376, (2017); Soltis J., King L., Vollrath F., Douglas-Hamilton I., Accelerometers and simple algorithms identify activity budgets and body orientation in African elephants Loxodonta africana, Endangered Species Research, (2016); Stapleton S., LaRue M., Lecomte N., Atkinson S., Garshelis D., Porter C., Et al., Polar bears from space: assessing satellite imagery as a tool to track Arctic wildlife, PLoS One, 9, 7, (2014); Strigl D., Kofler K., Podlipnig S., Performance and Scalability of GPU-Based Convolutional Neural Networks, (2010); Szegedy C., Ioffe S., Vanhoucke V., Alemi A., Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, (2017); Tambling C., Druce D.J., Hayward M.W., Castley J.G., Adendorff J., Kerley G.I., Spatial and temporal changes in group dynamics and range use enable anti-predator responses in African buffalo, Ecology, 93, pp. 1297-1304, (2012); Thito K., Wolski P., Murray-Hudson M., Mapping inundation extent, frequency and duration in the Okavango Delta from 2001 to 2012, African Journal of Aquatic Science, 41, 3, pp. 267-277, (2016); Torney C.J., Lloyd-Jones D.J., Chevallier M., Moyer D.C., Maliti H.T., Mwita M., Et al., A comparison of deep learning and citizen science techniques for counting wildlife in aerial survey images, Methods in Ecology and Evolution, 10, 6, pp. 779-787, (2019); Toshihiro M., Horimoto H., Ishihara T., Kofuji K., Autonomous Tracking of Sea Turtles based on Multibeam Imaging Sonar: Toward Robotic Observation of Marine Life, Science Direct, 52, pp. 86-90, (2019); LabelImg, (2015); van Wilgen N.J., Goodall V., Holness S., Chown S.L., McGeoch M.A., Rising temperatures and changing rainfall patterns in South Africa's national parks, International Journal of Climatology, 36, 2, pp. 706-721, (2016); Velasco M., A Quickbird’s eye view on marmots, in International Institute for Geo-information science and Earth Observation, (2009); Weinstein B.G., A computer vision for animal ecology, Journal of Animal Ecology, 87, 3, pp. 533-545, (2018); Wickler W., Seibt U., Aimed Object-throwing by a Wild African Elephant in an interspecific encounter, Ethology, 103, 5, pp. 365-368, (1997); Willi M., Pitman R.T., Cardoso A.W., Locke C., Swanson A., Boyer A., Et al., Identifying animal species in camera trap images using deep learning and citizen science, Methods in Ecology and Evolution, 10, 1, pp. 80-91, (2018); Witharana C., LaRue M.A., Lynch H.J., Benchmarking of data fusion algorithms in support of earth observation based Antarctic wildlife monitoring, ISPRS Journal of Photogrammetry and Remote Sensing, 113, pp. 124-143, (2016); Xue Y., Wang T., Skidmore A.K., Automatic Counting of Large Mammals from Very-high Resolution Panchromatic Satellite Imagery, Remote Sensing, 9, 9, (2017); Yang Z., Wang T., Skidmore A.K., de Leeuw J., Said M.Y., Freer J., Spotting East African mammals in open savannah from space, PLoS One, 9, 12, (2014)","I. Duporge; Wildlife Conservation Research Unit, Department of Zoology, University of Oxford, Recanati-Kaplan Centre, Tubney, United Kingdom; email: Isla.duporge@zoo.ox.ac.uk","","John Wiley and Sons Inc","","","","","","20563485","","","","English","Remote Sens. Ecol. Conserv.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097946970"
"Aburasain R.Y.; Edirisinghe E.A.; Zamim M.Y.","Aburasain, R.Y. (57218716991); Edirisinghe, E.A. (6701576984); Zamim, M.Y. (57844560700)","57218716991; 6701576984; 57844560700","A Coarse-to-Fine Multi-class Object Detection in Drone Images Using Convolutional Neural Networks","2022","Lecture Notes in Networks and Systems","440 LNNS","","","12","33","21","0","10.1007/978-3-031-11432-8_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135897194&doi=10.1007%2f978-3-031-11432-8_2&partnerID=40&md5=5a5a90ce37d9798ab31181fcabbf70ae","Department of Computer Science, Jazan University, Jazan, Saudi Arabia; Department of Computer Science, Loughborough University, Loughborough, United Kingdom; Ministry of Education, Jazan, Saudi Arabia","Aburasain R.Y., Department of Computer Science, Jazan University, Jazan, Saudi Arabia; Edirisinghe E.A., Department of Computer Science, Loughborough University, Loughborough, United Kingdom; Zamim M.Y., Ministry of Education, Jazan, Saudi Arabia","Multi-class object detection has a rapid evolution in the last few years with the rise of deep Convolutional Neural Networks (CNNs) learning based, in particular. However, the success approaches are based on high resolution ground level images and extremely large volume of data as in COCO and VOC datasets. On the other hand, the availability of the drones has been increased in the last few years and hence several new applications have been established. One of such is understanding drone footage by analysing, detecting, recognizing different objects in the covered area. In this study conducted, a collection of large images captured by a drone flying at a fixed altitude in a desert area located within the United Arab Emirates (UAE) is given and it is utilised for training and evaluating the CNN networks to be investigated. Three state-of-the-art CNN architectures, namely SSD-500 with VGGNet-16 meta-architecture, SSD-500 with ResNet meta-architecture and YOLO-V3 with Darknet-53 are optimally configured, re-trained, tested and evaluated for the detection of three different classes of objects in the captured footage, namely, palm trees, group-of-animals/cattle and animal sheds in farms. Our preliminary experiments revealed that YOLO-V3 outperformed SSD-500 with VGGNet-16 by a large margin and has a considerable improvement as compared to using SSD-500 with ResNet. Therefore, it has been selected for further investigation, aiming to propose an efficient coarse-to-fine object detection model for multi-class object detection in drone images. To this end, the impact of changing the activation function of the hidden units and the pooling type in the pooling layer has been investigated in detail. In addition, the impact of tuning the learning rate and the selection of the most effective optimization method for general hyper-parameters tuning is also investigated. The result demonstrated that the multi-class object detector developed has precision of 0.99, a recall of 0.94 and an F-score of 0.96, proving the efficiency of the multi-class object detection network developed. © 2022, The Author(s).","Convolution neural networks; Drones; Multiclass object detection; Unmanned aerial vehicles","","","","","","","","Lin T.-Y., Et al., Microsoft coco: Common objects in context, European Conference on Computer Vision, pp. 740-755, (2014); Everingham M., van Gool L., Williams C.K., Winn J., Zisserman A., The pascal visual object classes (voc) challenge, Int. J. Comput. Vis., 88, 2, pp. 303-338, (2010); Drone-Based Dataset for Desert Area. Falcon Eye Drones Ltd, (2017); Aburasain R.Y., Edirisinghe E.A., Albatay A., Drone-based cattle detection using deep neural networks, Intellisys 2020. AISC, 1250, pp. 598-611, (2021); Aburasain R.Y., Edirisinghe E.A., Albatay A., Palm tree detection in drone images using deep convolutional neural networks: Investigating the effective use of YOLO V3. In: Conference on Multimedia, Interaction, Design and Innovation, pp, 21–36, (2020); Voulodimos A., Doulamis N., Doulamis, Protopapadakis, E.: Deep Learning for Computer Vision: A Brief review’, Comput. Intell. Neurosci, (2018); Lecun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, 11, pp. 2278-2324, (1998); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Zeiler M.D., Fergus R., Visualizing and understanding convolutional networks, European Conference on Computer Vision, pp. 818-833, (2014); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Arxiv Prepr. Arxiv, 140, (2014); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Szegedy C., Ioffe S., Vanhoucke V., Alemi A., Inception-v4, Inception-ResNet and the Impact of Residual Connections on, Learning, (2016); Howard A.G., Et al., Mobilenets: Efficient convolutional neural networks for mobile vision applications, Arxiv Prepr, (2017); Fei-Fei L., Deng J., Li K., ImageNet: Constructing a large-scale image database, J. Vis., 9, 8, (2009); Szegedy C., Et al., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826, (2016); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Liu W., Et al., Ssd: Single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Uijlings J.R., van De Sande K.E., Gevers T., Smeulders A.W., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Redmon J., Darknet: Open Source Neural Networks in C, (2013); Redmon J., Farhadi A., Yolov3: An Incremental Improvement Arxiv Prepr. Arxiv180402767, (2018); Radovic M., Adarkwa O., Wang Q., Object recognition in aerial images using convolutional neural networks, J. Imaging, 3, 2, (2017); Ikshwaku S., Srinivasan A., Varghese A., Gubbi J., Railway corridor monitoring using deep drone vision, Computational Intelligence: Theories, Applications and Future Directions-Volume II, pp. 361-372, (2019); Al-Sa'd M.F., Al-Ali A., Mohamed A., Khattab T., Erbad A., RF-based drone detection and identification using deep learning approaches: An initiative towards a large open source drone database, Future Gener. Comput. Syst., 100, pp. 86-97, (2019); Varghese A., Gubbi J., Sharma H., Balamuralidhar P., Power infrastructure monitoring and damage detection using drone captured images, International Joint Conference on Neural Networks (IJCNN), pp. 1681-1687, (2017); Shao W., Kawakami R., Yoshihashi R., You S., Kawase H., Naemura T., Cattle detection and counting in UAV images based on convolutional neural networks, Int. J. Remote Sens., 41, 1, pp. 31-52, (2020); Kellenberger B., Volpi M., Tuia D., Fast animal detection in UAV images using convolutional neural networks, IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 866-869, (2017); Malek S., Bazi Y., Alajlan N., Alhichri H., Melgani F., Efficient framework for palm tree detection in UAV images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7, 12, pp. 4692-4703, (2014); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Wang Y., Zhu X., Wu B., Automatic detection of individual oil palm trees from UAV images using HOG features and an SVM classifier, Int. J. Remote Sens., 40, 19, pp. 7356-7370, (2019); Al Mansoori S., Kunhu A., Al Ahmad H., Automatic palm trees detection from multi-spectral UAV data using normalized difference vegetation index and circular Hough transform, High-Performance Computing in Geoscience and Remote Sensing VIII, 10792, (2018); Almaazmi A., Palm trees detecting and counting from high-resolution WorldView-3 satellite images in United Arab Emirates. In: Remote Sensing for Agriculture, Ecosystems, and Hydrology XX, vol. 10783, p, 107831M, (2018); Freudenberg M., Nolke N., Agostini A., Urban K., Worgotter F., Kleinn C., Large scale palm tree detection in high resolution satellite images using U-Net, Remote Sens, 11, 3, (2019); Mubin N.A., Nadarajoo E., Shafri H.Z.M., Hamedianfar A., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method, Int. J. Remote Sens., 40, 19, pp. 7500-7515, (2019); Zortea M., Nery M., Ruga B., Carvalho L.B., Bastos A.C., Oil-palm tree detection in aerial images combining deep learning classifiers, IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 657-660, (2018); Yousif H., Yuan J., Kays R., He Z., Fast human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification, IEEE International Symposium on Circuits and Systems (ISCAS), pp. 1-4, (2017); Gomez Villa A., Salazar A., Vargas F., Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks, Ecol. Inform., 41, pp. 24-32, (2017); Norouzzadeh M.S., Et al., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proc. Natl. Acad. Sci., 115, 25, pp. E5716-E5725, (2018); Rivas A., Chamoso P., Gonzalez-Briones A., Corchado J.M., Detection of cattle using drones and convolutional neural networks, Sensors, 18, 7, (2018); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Kim B.K., Kang H.-S., Park S.-O., Drone classification using convolutional neural networks with merged doppler images, IEEE Geosci. Remote Sens. Lett., 14, 1, pp. 38-42, (2016); von Luxburg U., A tutorial on spectral clustering, Stat. Comput., 17, 4, pp. 395-416, (2007); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications; Yoon I., Anwar A., Rakshit T., Raychowdhury A., Transfer and online reinforcement learning in STT-MRAM based embedded systems for autonomous drones, Design, Automation Test in Europe Conference Exhibition (DATE), pp. 1489-1494, (2019); Aburasain R.Y., Application of convolutional neural networks in object detection, re-identification and recognition, Loughborough University, (2020); Polyak B.T., Some methods of speeding up the convergence of iteration methods, USSR Comput. Math. Math. Phys., 4, 5, pp. 1-17, (1964); Kurbiel T., Khaleghian S., Training of Deep Neural Networks based on Distance Measures using RMSProp, Arxiv170801911 Cs Stat, (2017); Siemieniec R., Mente R., Jantscher W., 600 V Power Device Technologies for Highly Efficient Power supplies,º in 2021 23Rd European Conference on Power Electronics and Applications","R.Y. Aburasain; Department of Computer Science, Jazan University, Jazan, Saudi Arabia; email: raburasain@jazanu.edu.sa","Biele C.; Kacprzyk J.; Owsiński J.W.; Kopeć W.; Romanowski A.; Sikorski M.","Springer Science and Business Media Deutschland GmbH","","9th Machine Intelligence and Digital Interaction Conference, MIDI 2021","9 December 2021 through 10 December 2021","Virtual, Online","281399","23673370","978-303111431-1","","","English","Lect. Notes Networks Syst.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85135897194"
"Türkdamar M.U.; Taşyürek M.; Öztürk C.","Türkdamar, Mehmet Uğur (57986057900); Taşyürek, Murat (57215586146); Öztürk, Celal (23091274400)","57986057900; 57215586146; 23091274400","Field Detection from Satellite Images with Deep Learning Methods","2023","6th International Conference on Inventive Computation Technologies, ICICT 2023 - Proceedings","","","","1","8","7","0","10.1109/ICICT57646.2023.10134299","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163449346&doi=10.1109%2fICICT57646.2023.10134299&partnerID=40&md5=1d6154742a3ee36f1c889e43ef3316d8","Department of Computer Engineering, Niğde Ömer Halisdemir University, Niğde, Turkey; Department of Computer Engineering, Kayseri University, Kayseri, Turkey; Department of Computer Engineering, Erciyes University, Kayseri, Turkey","Türkdamar M.U., Department of Computer Engineering, Niğde Ömer Halisdemir University, Niğde, Turkey; Taşyürek M., Department of Computer Engineering, Kayseri University, Kayseri, Turkey; Öztürk C., Department of Computer Engineering, Erciyes University, Kayseri, Turkey","Today, effective production is interrupted unless land ownership disputes are resolved. The state cannot make the necessary investments due to these disputes not being concluded, and the borders of the fields remain unclear. Artificial intelligence-based methods can be suggested to eliminate disagreements and uncertainty. By using convolutional neural network (CNN) based deep learning networks in which image data are meaningful, areas with primary importance in crop production have been identified in this study. With the CNN networks used by computer vision technology, meaningful information can be extracted from the image. Field detection processes were carried out in this study by using deep learning networks that learn from data. As remote sensing studies gain speed, the number of deep learning studies also increases. For this purpose, satellite images were first collected from the Google Earth website, and then these collected images were used in Faster R-CNN and SSD training, which gained a reputation for accuracy and speed. It is aimed to provide more efficient production and resolve disputes by detecting the fields from satellite images. From two different networks running, SSD outperformed Faster R-CNN in terms of both accuracy and run time. With an f1 score of %97.32, SSD gave Faster R-CNN %3.18 superiority. In the field object results in the test images, the SSD outperformed by detecting 12 more fields. In terms of run times, the SSD performed faster detections with a difference of 285.5ms in the experiments tried in one-third of the test images. © 2023 IEEE.","CNN; crop field detection; deep learning; faster region-CNN; satellite images; single shot detector","Convolutional neural networks; Cultivation; Deep learning; Learning systems; Remote sensing; Satellites; Convolutional neural network; Crop field detection; Crop fields; Deep learning; Fast region-convolutional neural network; Field detections; Satellite images; Single shot detector; Single-shot; Crops","","","","","","","Ozturk C., Tasyurek M., Turkdamar M.U., Transfer learning and fine-tuned transfer learning methods' effectiveness analyse in the cnnbased deep learning models, Concurrency and Computation: Practice and Experience, 35, 4; Berezina K., Ciftci O., Cobanoglu C., Robots, artificial intelligence, and service automation in restaurants, Robots, artificial intelligence, and service automation in travel, tourism and hospitality, (2019); Lu C., Tang X., Surpassing human-level face verification performance on lfw with gaussianface, Twenty-ninth AAAI conference on artificial intelligence, (2015); Spanaki K., Karafili E., Sivarajah U., Despoudi S., Irani Z., Artificial intelligence and food security: swarm intelligence of agritech drones for smart agrifood operations, Production Planning & Control, 33, 16, pp. 1498-1516, (2022); Biehl K., Danis D., Migration studies in Turkey from a gender perspective, (2020); Emrah A., Arzu K.A.N., Support Program for Rural Development Investments: The Case of Rural Economic Infrastructure Projects in Bolu in 2021, Kirsehir Ahi Evran University Journal of the Faculty of Agriculture, 2, 2, pp. 121-132; Aydin B., Ozkan E., Evaluation of fertilizer and soil analysis support in terms of producers: The case of Kırklareli province, Turkish Journal of Agriculture and Natural Sciences, 4, 3, pp. 302-310, (2017); Ozdemir Z., Supporting practices and results in Turkish agriculture, Istanbul University Faculty of Economics Journal, 47, pp. 1-4, (1989); Tuna Y., State intervention in agricultural product prices and the history of intervention price policy in Turkey, Istanbul University, Faculty of Economics, Journal, 47, pp. 1-4, (2011); Ulker E., Yuksel O., Ergul S., Status of Forage Crops Agriculture, Seed Production and Foreign Trade in Our Country, Uşak University Journal of Science and Natural Sciences, 4, 2, pp. 127-138; Sahin G., Cabuk S.N., Cetin M., The change detection in coastal settlements using image processing techniques: a case study of korfez, Environmental Science and Pollution Research, 29, 10, pp. 15172-15187, (2022); Cienciala A., Sobolewska-Mikulska K., Sobura S., Credibility of the cadastral data on land use and the methodology for their verification and update, Land Use Policy, 102, (2021); Li W., Wang D., Li M., Gao Y., Wu J., Yang X., Field detection of tiny pests from sticky trap images using deep learning in agricultural greenhouse, Computers and Electronics in Agriculture, 183, (2021); Chen F., Zhang Y., Zhang J., Liu L., Wu K., Rice false smut detection and prescription map generation in a complex planting environment, with mixed methods, based on near earth remote sensing, Remote Sensing, 14, 4, (2022); Dallaqua F., Rosa R., Schultz B., Faria L., Rodrigues T., Oliveira C., Kieser M., Malhotra V., Dwyer T., Wolfe D., Forest plantation detection through deep semantic segmentation, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 43, pp. 77-84, (2022); Mahakalanda I., Demotte P., Perera I., Meedeniya D., Wijesuriya W., Rodrigo L., Deep learning-based prediction for stand age and land utilization of rubber plantation, Application of Machine Learning in Agriculture, pp. 131-156, (2022); Jin X., Che J., Chen Y., Weed identification using deep learning and image processing in vegetable plantation, IEEE Access, 9, pp. 10940-10950, (2021); Lee H., Ho H.W., Zhou Y., Deep learning-based monocular obstacle avoidance for unmanned aerial vehicle navigation in tree plantations, Journal of Intelligent & Robotic Systems, 101, 1, pp. 1-18, (2021); Liu X., Ghazali K.H., Han F., Mohamed I.I., Automatic detection of oil palm tree from uav images based on the deep learning method, Applied Artificial Intelligence, 35, 1, pp. 13-24, (2021); Fu L., Duan J., Zou X., Lin J., Zhao L., Li J., Yang Z., Fast and accurate detection of banana fruits in complex background orchards, IEEE Access, 8, pp. 196835-196846, (2020); Singh P., Verma A., Alex J.S.R., Disease and pest infection detection in coconut tree through deep learning techniques, Computers and electronics in agriculture, 182, (2021); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255, (2009); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in neural information processing systems, 28, (2015); Howard A.G., Some improvements on deep convolutional neural network based image classification, (2013); Chorowski J.K., Bahdanau D., Serdyuk D., Cho K., Bengio Y., Attention-based models for speech recognition, Advances in neural information processing systems, 28, (2015); Kocer B., New approaches to transfer learning, (2012); Turkdamar, Ugur M., Tasyurek M., Ozturk C., Helmet detection on the construction site with transfer-learned and non-transfer-learning deep networks, Niğde Ömer Halisdemir University Journal of Engineering Sciences, 12, 1, pp. 39-51, (2023); Skalski P., Make Sense, (2019); Paszke A., Gross S., Chintala S., Chanan G., Yang E., DeVito Z., Lin Z., Desmaison A., Antiga L., Lerer A., Automatic differentiation in pytorch, (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, European conference on computer vision, pp. 740-755, (2014); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255, (2009)","","","Institute of Electrical and Electronics Engineers Inc.","","6th International Conference on Inventive Computation Technologies, ICICT 2023","26 April 2023 through 28 April 2023","Lalitpur","189117","","979-835039849-6","","","English","Int. Conf. Inven. Comput. Technol., ICICT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85163449346"
"Maiti S.; Maji D.; Kumar Dhara A.; Sarkar G.","Maiti, Souvik (57204425820); Maji, Debasis (57191162467); Kumar Dhara, Ashis (57209854712); Sarkar, Gautam (7005482767)","57204425820; 57191162467; 57209854712; 7005482767","Automatic detection and segmentation of optic disc using a modified convolution network","2022","Biomedical Signal Processing and Control","76","","103633","","","","5","10.1016/j.bspc.2022.103633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126891740&doi=10.1016%2fj.bspc.2022.103633&partnerID=40&md5=9574995d10247983c7c6fad851be393a","Department of Electrical Engineering, Jadavpur University, Kolkata, 700032, India; Department of Electrical Engineering, Haldia Institute of Technology, West Bengal, 721657, India; Department of Electrical Engineering, National Institute of Technology, West Bengal, Durgapur, 713209, India","Maiti S., Department of Electrical Engineering, Jadavpur University, Kolkata, 700032, India; Maji D., Department of Electrical Engineering, Haldia Institute of Technology, West Bengal, 721657, India; Kumar Dhara A., Department of Electrical Engineering, National Institute of Technology, West Bengal, Durgapur, 713209, India; Sarkar G., Department of Electrical Engineering, Jadavpur University, Kolkata, 700032, India","Diabetic Retinopathy refers to an eye disease commonly found in patients suffering from prolonged diabetes mellitus and can eventually cause blindness resulting from leaking of retinal blood vessels. Early detection of any abnormalities in the optic disc can help the ophthalmologists in effective treatment of the patient. Segmentation of optic disc is also necessary for screening glaucoma using optic cup-to-disc ratio. Automatic screening of fundus images using computerized techniques can be of great help in detection of several retinal pathologies. The study presents a deep learning approach for detecting and segmenting the optic disc automatically from the fundus images using a modified convolutional network. The most powerful encoder structure has been identified by experimenting on seven encoder networks namely VGG11, VGG13, VGG16, VGG19, Resnet34, Densenet121 and InceptionV3. In this paper, the VGG16 framework is considered as an encoder, while the decoder is designed as a symmetric structure of that of the encoder to perform object segmentation more effectively. The Convolutional Long Short Term Memory is incorporated in the encoder framework for better convergence and to obtain high accuracy. The efficiency of this work is verified by applying it over a large variation of fundus images from databases namely MESSIDOR, DRIVE, DIARETDB0, DIARETDB1, CHASE-DB1, IDRiD and STARE. Intersection-over-Union, Dice Coefficient, Accuracy and Sensitivity have been computed for evaluating the effectiveness of the work. This algorithm has attained an overall accuracy of 99.44%. The experimental results indicate the superiority of this model on segmentation operation. © 2022 Elsevier Ltd","Convolutional neural network; Diabetic retinopathy; Optic disc","Blood vessels; Convolution; Convolutional neural networks; Deep learning; Diagnosis; Eye protection; Image segmentation; Patient treatment; Signal encoding; Automatic Detection; Automatic segmentations; Convolutional neural network; Diabetes mellitus; Diabetic retinopathy; Eye disease; Fundus image; Optic disks; Patient's suffering; Retinal blood vessels; accuracy; Article; classification algorithm; controlled study; convolutional neural network; eye fundus; human; image segmentation; learning; memory; optic disk; prediction; training; Ophthalmology","","","","","","","Guo Y., Liu Y.U., Georgiou T., Lew M.S., A review of semantic segmentation using deep neural networks, Int. J. Multimed. Inf. Retr., 7, 2, pp. 87-93, (2018); Garcia-Garcia A., Orts-Escolano S., Oprea S., Villena-Martinez V., Martinez-Gonzalez P., Garcia-Rodriguez J., A survey on deep learning techniques for image and video semantic segmentation, Appl. Soft Comput., 70, pp. 41-65, (2018); Garcia-Lamont F., Cervantes J., Lopez A., Rodriguez L., Segmentation of images by color features: a survey, Neurocomputing., 292, pp. 1-27, (2018); Erkaymaz O., Ozer M., Perc M., Performance of small-world feedforward neural networks for the diagnosis of diabetes, Appl. Math. Comput., 311, pp. 22-28, (2017); Surucu M., Isler Y., Perc M., Kara R., Convolutional neural networks predict the onset of paroxysmal atrial fibrillation: theory and applications, Chaos An Interdiscip. J. Nonlinear Sci., 31, (2021); Wang S.-H., Zhou Q., Yang M., Zhang Y.-D., ADVIAN: Alzheimer's disease VGG-inspired attention network based on convolutional block attention module and multiple way data augmentation, Front. Aging Neurosci., 13, (2021); Wang S.-H., Khan M.A., Zhang Y.-D., VISPNN: VGG-Inspired Stochastic Pooling Neural Network, C. Mater. Contin., 70, pp. 3081-3097, (2022); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, pp. 3431-3440, (2015); Zhou Y., Xie L., Shen W., Wang Y., Fishman E.K., Yuille A.L., A fixed-point model for pancreas segmentation in abdominal CT scans, pp. 693-701, (2017); Li Y., Qi H., Dai J., Ji X., Wei Y., Fully convolutional instance-aware semantic segmentation, pp. 2359-2367, (2017); Drozdzal M., Vorontsov E., Chartrand G., Kadoury S., Pal C., The importance of skip connections in biomedical image segmentation, pp. 179-187, (2016); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, pp. 234-241, (2015); Iglovikov V.I., Rakhlin A., Kalinin A.A., Shvets A.A., Paediatric bone age assessment using deep convolutional neural networks, pp. 300-308, (2018); Hasan M.K., Alam M.A., Elahi M.T.E., Roy S., Marti R., DRNet: segmentation and localization of optic disc and Fovea from diabetic retinopathy image, Artif. Intell. Med., 111, (2021); Korznikov K.A., Kislov D.E., Altman J., Dolezal J., Vozmishcheva A.S., Krestov P.V., Using U-net-like deep convolutional neural networks for precise tree recognition in very high resolution RGB (Red, Green, Blue) satellite images, Forests., 12, (2021); Yao W., Zeng Z., Lian C., Tang H., Pixel-wise regression using U-Net and its application on pansharpening, Neurocomputing., 312, pp. 364-371, (2018); pp. 424-432, (2016); Iglovikov V., Seferbekov S., Buslaev A., Shvets A., Ternausnetv2: Fully convolutional network for instance segmentation, pp. 233-237, (2018); Sherstinsky A., Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network, Phys. D Nonlinear Phenom., 404, (2020); Li J., Lin X., Che H., Li H., Qian X., Pancreas segmentation with probabilistic map guided bi-directional recurrent UNet, Phys. Med. Biol., 66, (2021); Zeng T., Wu B., Zhou J., Davidson I., Ji S., Recurrent encoder-decoder networks for time-varying dense prediction, pp. 1165-1170, (2017); Bai W., Suzuki H., Qin C., Tarroni G., Oktay O., Matthews P.M., Rueckert D., Recurrent neural networks for aortic image sequence segmentation with sparse annotations, Int. Conf. Med. Image Comput. Comput. Interv., pp. 586-594, (2018); Girshick R., pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Alom M.Z., Yakopcic C., Taha T.M., Asari V.K., Nuclei segmentation with recurrent residual convolutional neural networks based U-Net (R2U-Net), pp. 228-233, (2018); Xu K., Ba J., Kiros R., Cho K., Courville A., Salakhudinov R., Zemel R., Bengio Y., Show, attend and tell: Neural image caption generation with visual attention, pp. 2048-2057, (2015); Vinyals O., Kaiser L., Koo T., Petrov S., Sutskever I., Hinton G., Grammar as a foreign language, Adv. Neural Inf. Process. Syst., 28, pp. 2773-2781, (2015); Sak H., Senior A., Beaufays F., (2014); Zhao J., Deng F., Cai Y., Chen J., Long short-term memory-Fully connected (LSTM-FC) neural network for PM2. 5 concentration prediction, Chemosphere., 220, pp. 486-492, (2019); Liu Y., Zheng H., Feng X., Chen Z., Short-term traffic flow prediction with Conv-LSTM, pp. 1-6, (2017); Lotter W., Kreiman G., Cox D., (2016); Stollenga M.F., Byeon W., Liwicki M., Schmidhuber J., Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation, Adv. Neural Inf. Process. Syst., 28, pp. 2998-3006, (2015); Simonyan K., Zisserman A., (2014); Balakrishna C., Dadashzadeh S., Soltaninejad S., (2018); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, pp. 770-778, (2016); Huang G., Liu Z., Van Der Maaten L., Weinberger K.Q., pp. 4700-4708, (2017); Demir A., Yilmaz F., Kose O., Early detection of skin cancer using deep learning architectures: resnet-101 and inception-v3, pp. 1-4, (2019); Kwon S., CLSTM: deep feature-based speech emotion recognition using the hierarchical ConvLSTM network, Mathematics., 8, (2020); Arbelle A., Raviv T.R., Microscopy cell segmentation via convolutional LSTM networks, pp. 1008-1012, (2019); Zapata-Impata B.S., Gil P., Torres F., Learning spatio temporal tactile features with a ConvLSTM for the direction of slip detection, Sensors., 19, (2019); Lecun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE., 86, 11, pp. 2278-2324, (1998); Sudre C.H., Li W., Vercauteren T., Ourselin S., Cardoso M.J., Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations, pp. 240-248, (2017); Mehta S., Mercan E., Bartlett J., Weaver D., Elmore J.G., Shapiro L., Y-Net: joint segmentation and classification for diagnosis of breast biopsy images, Int. Conf. Med. Image Comput. Comput. Interv., pp. 893-901, (2018); Seo H., Huang C., Bassenne M., Xiao R., Xing L., Modified U-Net (mU-Net) with incorporation of object-dependent high level features for improved liver and liver-tumor segmentation in CT images, IEEE Trans. Med. Imaging., 39, 5, pp. 1316-1325, (2020); Zoph B., Vasudevan V., Shlens J., Le Q.V., Learning transferable architectures for scalable image recognition, pp. 8697-8710, (2018); Zhang X., Li Z., Change C., Loy D., pp. 718-726, (2017); Hu J., Shen L., Sun G., pp. 7132-7141, (2018); Cubuk E.D., Zoph B., Mane D., Vasudevan V., Le Q.V., (2018); Tan M., Le Q., Efficientnet: Rethinking model scaling for convolutional neural networks, pp. 6105-6114, (2019); Huang Y., Cheng Y., Bapna A., Firat O., Chen D., Chen M., Lee H., Ngiam J., Le Q.V., Wu Y., Gpipe: efficient training of giant neural networks using pipeline parallelism, Adv. Neural Inf. Process. Syst., 32, (2019); Decenciere E., Zhang X., Cazuguel G., Lay B., Cochener B., Trone C., Gain P., Ordonez R., Massin P., Erginay A., Charton B., Klein J.-C., Feedback on a publicly distributed image database: the Messidor database, Image Anal. Stereol., 33, 3, (2014); Staal J., Abramoff M.D., Niemeijer M., Viergever M.A., van Ginneken B., Ridge-based vessel segmentation in color images of the retina, IEEE Trans. Med. Imaging., 23, 4, pp. 501-509, (2004); Kauppi T., Kalesnykiene V., Kamarainen J.-K., Lensu L., Sorri I., Uusitalo H., Kalviainen H., Pietila J., DIARETDB0: evaluation database and methodology for diabetic retinopathy algorithms, Mach. Vis. Pattern Recognit. Res. Group, Lappeenranta Univ. Technol. Finl., 73, pp. 1-17, (2006); Kalviainen R., Uusitalo H., DIARETDB1 diabetic retinopathy database and evaluation protocol, (2007); Zhang J., Dashtbozorg B., Bekkers E., Pluim J.P.W., Duits R., ter Haar Romeny B.M., Robust retinal vessel segmentation via locally adaptive derivative frames in orientation scores, IEEE Trans. Med. Imaging., 35, 12, pp. 2631-2644, (2016); Porwal P., Pachade S., Kamble R., Kokare M., Deshmukh G., Sahasrabuddhe V., Meriaudeau F., Indian diabetic retinopathy image dataset (IDRiD): a database for diabetic retinopathy screening research, Data., 3, (2018); Hoover A., Goldbaum M., Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels, IEEE Trans. Med. Imaging., 22, 8, pp. 951-958, (2003); Rehman Z.U., Naqvi S.S., Khan T.M., Arsalan M., Khan M.A., Khalil M.A., Multi-parametric optic disc segmentation using superpixel based feature classification, Expert Syst. Appl., 120, pp. 461-473, (2019); Morales S., Naranjo V., Angulo J., Alcaniz M., Automatic detection of optic disc based on PCA and mathematical morphology, IEEE Trans. Med. Imaging., 32, 4, pp. 786-796, (2013); Abdullah M., Fraz M.M., Barman S.A., Localization and segmentation of optic disc in retinal images using circular Hough transform and grow-cut algorithm, PeerJ., 4, (2016); Fan Z., Rong Y., Cai X., Lu J., Li W., Lin H., Chen X., Optic disk detection in fundus image based on structured learning, IEEE J. Biomed. Heal. Informatics., 22, 1, pp. 224-234, (2018); Zahoor M.N., Fraz M.M., Fast optic disc segmentation in retina using polar transform, IEEE Access., 5, pp. 12293-12300, (2017); Nija K.S., Anupama C.P., Gopi V.P., Anitha V.S., Automated segmentation of optic disc using statistical region merging and morphological operations, Phys. Eng. Sci. Med., 43, 3, pp. 857-869, (2020); Roychowdhury S., Koozekanani D.D., Kuchinka S.N., Parhi K.K., Optic disc boundary and vessel origin segmentation of fundus images, IEEE J. Biomed. Heal. Informatics., 20, 6, pp. 1562-1574, (2016); Abdullah A.S., Ozok Y.E., Rahebi J., A novel method for retinal optic disc detection using bat meta-heuristic algorithm, Med. Biol. Eng. Comput., 56, 11, pp. 2015-2024, (2018); Ramani R.G., Shanthamalar J.J., Improved image processing techniques for optic disc segmentation in retinal fundus images, Biomed. Signal Process. Control., 58, (2020)","S. Maiti; Department of Electrical Engineering, Jadavpur University, Kolkata, 700032, India; email: maitisouvikhaldia@gmail.com","","Elsevier Ltd","","","","","","17468094","","","","English","Biomed. Signal Process. Control","Article","Final","","Scopus","2-s2.0-85126891740"
"Muhtar D.; Zhang X.; Xiao P.; Li Z.; Gu F.","Muhtar, Dilxat (57712195400); Zhang, Xueliang (48762261800); Xiao, Pengfeng (55722986100); Li, Zhenshi (57222320949); Gu, Feng (58159789200)","57712195400; 48762261800; 55722986100; 57222320949; 58159789200","CMID: A Unified Self-Supervised Learning Framework for Remote Sensing Image Understanding","2023","IEEE Transactions on Geoscience and Remote Sensing","61","","5607817","","","","0","10.1109/TGRS.2023.3268232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153520170&doi=10.1109%2fTGRS.2023.3268232&partnerID=40&md5=7c6221c709c7c18c756f03e8d0712690","Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China","Muhtar D., Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China; Zhang X., Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China; Xiao P., Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China; Li Z., Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China; Gu F., Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China","Self-supervised learning (SSL) has gained wide-spread attention in the remote sensing (RS) and Earth observation (EO) communities owing to its ability to learn task-agnostic representations without human-annotated labels. Nevertheless, most existing RS SSL methods are limited to learning either global semantic separable or local spatial perceptible representations. We argue that this learning strategy is suboptimal in the realm of RS since the required representations for different RS downstream tasks are often varied and complex. In this study, we proposed a unified SSL framework that is better suited for RS image representation learning. The proposed SSL framework, contrastive mask image distillation (CMID), is capable of learning representations with both global semantic separability and local spatial perceptibility by combining contrastive learning (CL) with masked image modeling (MIM) in a self-distillation way. Furthermore, our CMID learning framework is architecture-agnostic, which is compatible with both convolutional neural networks (CNNs) and vision transformers (ViTs), allowing CMID to be easily adapted to a variety of deep learning (DL) applications for RS understanding. Comprehensive experiments have been carried out on four downstream tasks (i.e., scene classification, semantic segmentation, object detection, and change detection) and the results show that models pretrained using CMID achieve a better performance than other state-of-the-art SSL methods on multiple downstream tasks. The code and pretrained models will be made available at https://github.com/NJU-LHRS/official-CMIDhttps://github.com/NJU-LHRS/official-CMID to facilitate SSL research and speed up the development of RS images DL applications.  © 1980-2012 IEEE.","Contrastive learning (CL); deep learning (DL); masked image modeling (MIM); remote sensing (RS) pretraining; self-supervised learning (SSL)","Convolution; Deep learning; Image reconstruction; Neural networks; Object detection; Object recognition; Remote sensing; Semantic Segmentation; Supervised learning; Contrastive learning; Convolutional neural network; Deep learning; Image modeling; Images reconstruction; Masked image modeling; Pre-training; Remote sensing pretraining; Remote-sensing; Self-supervised learning; Task analysis; data set; image classification; numerical model; remote sensing; supervised learning; Semantics","","","","","","","Saleem M.H., Potgieter J., Arif K.M., Automation in agriculture by machine and deep learning techniques: A review of recent developments, Precis. Agricult, 22, 6, pp. 2053-2091, (2021); Zheng Y.-Y., Kong J.-L., Jin X.-B., Wang X.-Y., Zuo M., CropDeep: The crop vision dataset for deep-learning-based classification and detection in precision agriculture, Sensors, 19, 5, (2019); Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens, 117, pp. 11-28, (2016); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS J. Photogramm. Remote Sens, 159, pp. 296-307, (2020); Zhu X.X., Et al., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geosci. Remote Sens. Mag, 5, 4, pp. 8-36, (2018); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Yuan Q., Et al., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ, 241, (2020); Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community, J. Appl. Remote Sens, 11, 4, (2017); Wang Y., Albrecht C.M., Braham N.A.A., Mou L., Zhu X.X., Self-supervised learning in remote sensing: A review, (2022); Jing L., Tian Y., Self-supervised visual feature learning with deep neural networks: A survey, IEEE Trans. Pattern Anal. Mach. Intell, 43, 11, pp. 4037-4058, (2020); Chen X., Fan H., Girshick R., He K., Improved baselines with momentum contrastive learning, (2020); Misra I., Van Der Maaten L., Self-supervised learning of pretextinvariant representations, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 6707-6717, (2020); Grill J.-B., Et al., Bootstrap your own latent-A new approach to selfsupervised learning, Proc. Adv. Neural Inf. Process. Syst, 33, pp. 21271-21284, (2020); Sun X., Et al., RingMo: A remote sensing foundation model with masked image modeling, IEEE Trans. Geosci. Remote Sens., (2022); Manas O., Lacoste A., Giro-I-Nieto X., Vazquez D., Rodriguez P., Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9414-9423, (2021); Ayush K., Et al., Geography-aware self-supervised learning, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 10181-10190, (2021); Jaiswal A., Babu A.R., Zadeh M.Z., Banerjee D., Makedon F., A survey on contrastive self-supervised learning, Technologies, 9, 1, (2020); Kang J., Fernandez-Beltran R., Duan P., Liu S., Plaza A.J., Deep unsupervised embedding for remotely sensed images based on spatially augmented momentum contrast, IEEE Trans. Geosci. Remote Sens, 59, 3, pp. 2598-2610, (2021); Swope A.M., Rudelis X.H., Story K.T., Representation learning for remote sensing: An unsupervised sensor fusion approach, (2021); Li W., Chen K., Chen H., Shi Z., Geographical knowledge-driven representation learning for remote sensing images, IEEE Trans. Geosci. Remote Sens, 60, (2022); Chen Y., Bruzzone L., Self-supervised SAR-optical data fusion of Sentinel-1/-2 images, IEEE Trans. Geosci. Remote Sens, 60, (2022); Yuan Y., Lin L., Liu Q., Hang R., Zhou Z.-G., SITS-former: A pre-trained spatio-spectral-temporal representation model for Sentinel-2 time series classification, Int. J. Appl. Earth Observ. Geoinf, 106, (2022); Cong Y., Et al., SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery, (2022); Scheibenreif L., Hanna J., Mommert M., Borth D., Self-supervised vision transformers for land-cover segmentation and classification, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 1422-1431, (2022); Wang D., Et al., Advancing plain vision transformer toward remote sensing foundation model, IEEE Trans. Geosci. Remote Sens, 61, (2023); Dosovitskiy A., Et al., An image is worth 16×16 words: Transformers for image recognition at scale, (2020); Wang D., Zhang J., Du B., Xia G.-S., Tao D., An empirical study of remote sensing pretraining, IEEE Trans. Geosci. Remote Sens., (2022); Aleissaee A.A., Et al., Transformers in remote sensing: A survey, (2022); He K., Chen X., Xie S., Li Y., Dollar P., Girshick R., Masked autoencoders are scalable vision learners, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 16000-16009, (2022); Xie Z., Et al., SimMIM: A simple framework for masked image modeling, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 9653-9663, (2022); Zhou Q., Yu C., Luo H., Wang Z., Li H., MimCo: Masked image modeling pre-training with contrastive teacher, Proc. 30th ACM Int. Conf. Multimedia, pp. 4487-4495, (2022); Huang Z., Et al., Contrastive masked autoencoders are stronger vision learners, (2022); Tao C., Et al., Siamese image modeling for self-supervised vision representation learning, (2022); Cheng G., Han J., Lu X., Remote sensing image scene classification: Benchmark and state of the art, Proc. IEEE, 105, 10, pp. 1865-1883, (2017); Li W., Wei W., Zhang L., GSDet: Object detection in aerial images based on scale reasoning, IEEE Trans. Image Process, 30, pp. 4599-4609, (2021); Muhtar D., Zhang X., Xiao P., Index your position: A novel self-supervised learning method for remote sensing images semantic segmentation, IEEE Trans. Geosci. Remote Sens, 60, (2022); Liu J., Huang X., Zheng J., Liu Y., Li H., MixMAE: Mixed and masked autoencoder for efficient pretraining of hierarchical vision transformers, (2022); Zhang L., Song J., Gao A., Chen J., Bao C., Ma K., Be your own teacher: Improve the performance of convolutional neural networks via self distillation, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 3713-3722, (2019); Caron M., Et al., Emerging properties in self-supervised vision transformers, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9650-9660, (2021); Xiao T., Wang X., Efros A.A., Darrell T., What should not be contrastive in contrastive learning, (2020); Jing L., Zhu J., LeCun Y., Masked Siamese ConvNets, (2022); Stojnic V., Risojevic V., Self-supervised learning of remote sensing scene representations using contrastive multiview coding, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) Workshops, pp. 1182-1191, (2021); Liu X., Et al., Self-supervised learning: Generative or contrastive, IEEE Trans. Knowl. Data Eng, 35, 1, pp. 857-876, (2023); Van Den Oord A., Li Y., Vinyals O., Representation learning with contrastive predictive coding, (2018); Chen T., Kornblith S., Norouzi M., Hinton G., A simple framework for contrastive learning of visual representations, Proc. Int. Conf. Mach. Learn, pp. 1597-1607, (2020); Chen X., He K., Exploring simple Siamese representation learning, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 15750-15758, (2021); Zbontar J., Jing L., Misra I., LeCun Y., Deny S., Barlow Twins: Self-supervised learning via redundancy reduction, Proc. Int. Conf. Mach. Learn, pp. 12310-12320, (2021); Caron M., Misra I., Mairal J., Goyal P., Bojanowski P., Joulin A., Unsupervised learning of visual features by contrasting cluster assignments, Proc. Adv. Neural Inf. Process. Syst, 33, pp. 9912-9924, (2020); Chen X., Xie S., He K., An empirical study of training selfsupervised vision transformers, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9640-9649, (2021); Wang X., Zhang R., Shen C., Kong T., Li L., Dense contrastive learning for self-supervised visual pre-training, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3024-3033, (2021); Xie Z., Lin Y., Zhang Z., Cao Y., Lin S., Hu H., Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 16684-16693, (2021); Pinheiro P.O.O., Almahairi A., Benmalek R., Golemo F., Courville A.C., Unsupervised learning of dense visual representations, Proc. Adv. Neural Inf. Process. Syst, 33, pp. 4489-4500, (2020); Liu S., Li Z., Sun J., Self-EMD: Self-supervised object detection without ImageNet, (2020); Xu Y., Zhang Q., Zhang J., Tao D., RegionCL: Can simple region swapping contribute to contrastive learning?, (2021); Xiao T., Reed C.J., Wang X., Keutzer K., Darrell T., Region similarity representation learning, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 10539-10548, (2021); Van Gansbeke W., Vandenhende S., Georgoulis S., Van Gool L., Revisiting contrastive methods for unsupervised learning of visual representations, Proc. Adv. Neural Inf. Process. Syst, 34, pp. 16238-16250, (2021); Chen M., Et al., Generative pretraining from pixels, Proc. Int. Conf. Mach. Learn, pp. 1691-1703, (2020); Bao H., Dong L., Piao S., Wei F., BEiT: BERT pre-training of image transformers, (2021); Ramesh A., Et al., Zero-shot text-to-image generation, Proc. Int. Conf. Mach. Learn, pp. 8821-8831, (2021); Dong X., Et al., PeCo: Perceptual codebook for BERT pre-training of vision transformers, (2021); Zhou J., Et al., iBOT: Image BERT pre-training with online tokenizer, (2021); Wei C., Fan H., Xie S., Wu C.-Y., Yuille A., Feichtenhofer C., Masked feature prediction for self-supervised visual pre-training, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 14668-14678, (2022); Wei Y., Et al., Contrastive learning rivals masked image modeling in fine-tuning via feature distillation, (2022); Li Y., Xie S., Chen X., Dollar P., He K., Girshick R., Benchmarking detection transfer learning with vision transformers, (2021); Tian K., Jiang Y., Diao Q., Lin C., Wang L., Yuan Z., Designing BERT for convolutional networks: Sparse and hierarchical masked modeling, (2023); Tian Y., Krishnan D., Isola P., Contrastive multiview coding, Proc. 16th Eur. Conf. Comput. Vis.-ECCV. Glasgow, U.K.: Springer, pp. 776-794, (2020); Jung H., Oh Y., Jeong S., Lee C., Jeon T., Contrastive selfsupervised learning with smoothed representation for remote sensing, IEEE Geosci. Remote Sens. Lett, 19, pp. 1-5, (2022); Saha S., Ebel P., Zhu X., Self-supervised multisensor change detection, IEEE Trans. Geosci. Remote Sens, 60, (2022); Li H., Et al., Global and local contrastive self-supervised learning for semantic segmentation of HR remote sensing images, IEEE Trans. Geosci. Remote Sens, 60, (2022); Li S., Et al., Architecture-agnostic masked image modeling-From ViT back to CNN, (2022); Jiang L., Dai B., Wu W., Loy C.C., Focal frequency loss for image reconstruction and synthesis, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 13919-13929, (2021); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Liu Z., Et al., Swin transformer: Hierarchical vision transformer using shifted windows, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 10012-10022, (2021); MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark, (2020); Xie X., Zhou P., Li H., Lin Z., Yan S., Adan: Adaptive Nesterov momentum algorithm for faster optimizing deep models, (2022); Xiao T., Liu Y., Zhou B., Jiang Y., Sun J., Unified perceptual parsing for scene understanding, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 418-434, (2018); Long Y., Et al., On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-AID, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 4205-4230, (2021); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Proc. 18th SIGSPATIAL Int. Conf. Adv. Geograph. Inf. Syst., pp. 270-279, (2010); Cubuk E.D., Zoph B., Shlens J., Le Q.V., Randaugment: Practical automated data augmentation with a reduced search space, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 702-703, (2020); Zhang H., Cisse M., Dauphin Y.N., Lopez-Paz D., mixup: Beyond empirical risk minimization, (2017); Yun S., Han D., Chun S., Oh S.J., Yoo Y., Choe J., CutMix: Regularization strategy to train strong classifiers with localizable features, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 6023-6032, (2019); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2818-2826, (2016); Zhong Z., Zheng L., Kang G., Li S., Yang Y., Random erasing data augmentation, Proc. AAAI Conf. Artif. Intell, 34, 7, pp. 13001-13008, (2020); Xia G.-S., Et al., DOTA: A large-scale dataset for object detection in aerial images, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 3974-3983, (2018); Zhou Y., Et al., MMRotate: A rotated object detection benchmark using PyTorch, Proc. 30th ACM Int. Conf. Multimedia, pp. 7331-7334, (2022); Xie X., Cheng G., Wang J., Yao X., Han J., Oriented R-CNN for object detection, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 3520-3529, (2021); Lebedev M.A., Vizilter Y.V., Vygolov O.V., Knyaz V.A., Rubis A.Y., Change detection in remote sensing images using conditional adversarial networks, Int. Arch. Photogramm., Remote Sens. Spatial Inf. Sci, 42, 2, pp. 565-571, (2018); Ji S., Wei S., Lu M., Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set, IEEE Trans. Geosci. Remote Sens, 57, 1, pp. 574-586, (2019); Chen H., Qi Z., Shi Z., Remote sensing image change detection with transformers, IEEE Trans. Geosci. Remote Sens, 60, (2022); MMSelfSup: OpenMMLab Self-Supervised Learning Toolbox and Benchmark, (2021); Li A.C., Efros A.A., Pathak D., Understanding collapse in non-contrastive Siamese representation learning, Proc. Eur. Conf. Comput. Vis. Springer, pp. 490-505, (2022); Liu B., Wang M., Foroosh H., Tappen M., Pensky M., Sparse convolutional neural networks, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 806-814, (2015); Liu G., Et al., Partial convolution based padding, (2018); Hinton G.E., Roweis S., Stochastic neighbor embedding, Proc. Adv. Neural Inf. Process. Syst, 15, pp. 1-8, (2002); Assran M., Et al., Masked Siamese networks for label-efficient learning, Proc. 17th Eur. Conf. Comput. Vis.-ECCV. Tel Aviv, Israel: Springer, pp. 456-473, (2022)","X. Zhang; Nanjing University, Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Nanjing, 210023, China; email: zxl@nju.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85153520170"
"Albaghdadi M.F.; Manaa M.E.","Albaghdadi, Mustafa Fahem (57900111200); Manaa, Mehdi Ebady (56304393200)","57900111200; 56304393200","Unmanned aerial vehicles and machine learning for detecting objects in real time","2022","Bulletin of Electrical Engineering and Informatics","11","6","","3490","3497","7","0","10.11591/eei.v11i6.4185","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139171885&doi=10.11591%2feei.v11i6.4185&partnerID=40&md5=b1de48a575d3232f9dab84f2e884e9cf","Department of Information Networks, College of Information Technology, University of Babylon, Babylon, Iraq","Albaghdadi M.F., Department of Information Networks, College of Information Technology, University of Babylon, Babylon, Iraq; Manaa M.E., Department of Information Networks, College of Information Technology, University of Babylon, Babylon, Iraq","An unmanned aerial vehicle (UAV) image recognition system in real-time is proposed in this study. To begin, the you only look once (YOLO) detector has been retrained to better recognize objects in UAV photographs. The trained YOLO detector makes a trade-off between speed and precision in object recognition and localization to account for four typical moving entities caught by UAVs (cars, buses, trucks, and people). An additional 1500 UAV photographs captured by the embedded UAV camera are fed into the YOLO, which uses those probabilities to estimate the bounding box for the entire image. When it comes to object detection, the YOLO competes with other deep-learning frameworks such as the faster region convolutional neural network. The proposed system is tested on a wild test set of 1500 UAV photographs with graphics processing unit GPU acceleration, proving that it can distinguish objects in UAV images effectively and consistently in real-time at a detection speed of 60 frames per second. © 2022, Institute of Advanced Engineering and Science. All rights reserved.","Object detection; Real-time system; UAV; YOLO","","","","","","","","Alfeo A. L., Cimino M. G. C. A., Francesco N. D., Lega M., Vaglini G., Design and simulation of the emergent behavior of small drones swarming for distributed target localization, Journal of Computational Science, 29, pp. 19-33, (2018); Vidal V. F., Honorio L. M., Santos M. F., Silva M. F., Cerqueira A. S., Oliveira E. J., UAV vision aided positioning system for location and landing, 2017 18th International Carpathian Control Conference (ICCC), 168, pp. 228-233, (2017); Alheeti K. M. A., Al-Ani M. S., Al-Aloosy A. K. N., Alzahrani A., Rukan D. A. S., Intelligent mobile detection of cracks in concrete utilising an unmanned aerial vehicle, Bulletin of Electrical Engineering and Informatics, 11, 1, pp. 176-184, (2022); Wakabayashi M., Okuno H. G., Kumon M., Multiple Sound Source Position Estimation by Drone Audition Based on Data Association Between Sound Source Localization and Identification, IEEE Robotics and Automation Letters, 5, 2, pp. 782-789, (2020); Tullu A., Endale B., Wondosen A., Hwang H.-Y., Machine Learning Approach to Real-Time 3D Path Planning for Autonomous Navigation of Unmanned Aerial Vehicle, Applied Sciences, 11, 10, (2021); Al-Sheary A., Almagbile A., Crowd Monitoring System Using Unmanned Aerial Vehicle (UAV), Journal of Civil Engineering and Architecture, 11, 11, (2017); Saha H., Et al., A low cost fully autonomous GPS (Global Positioning System) based quad copter for disaster management, 2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC), 2018, pp. 654-660, (2018); Lee J., Wang J., Crandall D., Sabanovic S., Fox G., Real-Time, Cloud-Based Object Detection for Unmanned Aerial Vehicles, 2017 First IEEE International Conference on Robotic Computing (IRC), pp. 36-43, (2017); Hsieh M.-R., Lin Y.-L., Hsu W. H., Drone-Based Object Counting by Spatially Regularized Regional Proposal Network, 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 4165-4173, (2017); Baeck P. J., Lewyckyj N., Beusen B., Horsten W., Pauly K., Drone based near real-time human detection with geographic localization, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences-ISPRS Archives, 42, 3/W8, pp. 49-53, (2019); Lu Y., Wang Z., Tang Z., Javidi T., Target Localization with Drones using Mobile CNNs, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2566-2573, (2018); Zhang H., Wang G., Lei Z., Hwang J.-N., Eye in the Sky, Proceedings of the 27th ACM International Conference on Multimedia, 1, pp. 899-907, (2019); Albanese A., Sciancalepore V., Costa-Perez X., SARDO: An Automated Search-and-Rescue Drone-Based Solution for Victims Localization, IEEE Transactions on Mobile Computing, 21, 9, pp. 3312-3325, (2022); Singhal H., Rajpure S., Badgujar R., Dangle P., Patil P., Guide P., Identifying and Detecting Real-Time Objects Using Drone Camera, 4, 12, pp. 39-42, (2019); Estrada M. A. R., Ndoma A., The uses of unmanned aerial vehicles –UAV’s-(or drones) in social logistic: Natural disasters response and humanitarian relief aid, Procedia Computer Science, 149, pp. 375-383, (2019); Li Z., Road Aerial Object Detection Based on Improved YOLOv5, Journal of Physics: Conference Series, 2171, 1, (2022); Qadir Z., Ullah F., Munawar H. S., Al-Turjman F., Addressing disasters in smart cities through UAVs path planning and 5G communications: A systematic review, Computer Communications, 168, pp. 114-135, (2021); McRae J. N., Gay C. J., Nielsen B. M., Hunt A. P., Using an Unmanned Aircraft System (Drone) to Conduct a Complex High Altitude Search and Rescue Operation: A Case Study, Wilderness & Environmental Medicine, 30, 3, pp. 287-290, (2019); Nawaz H., Ali H. M., Massan S. R., A study of mobility models for UAV Communication Networks, 3C Tecnología_Glosas de innovación aplicadas a la pyme, pp. 276-297, (2019); Pal S., Chawan P. P. M., Real-time object Detection using Deep Learning: A survey, International Research Journal of Engineering and Technology (IRJET), pp. 397-399, (2019); Sorbelli F. B., Das S. K., Pinotti C. M., Silvestri S., Range based algorithms for precise localization of terrestrial objects using a drone, Pervasive and Mobile Computing, 48, pp. 20-42, (2018); Kulaib A. R., Shubair R. M., Al-Qutayri M. A., Ng J. W. P., An overview of localization techniques for Wireless Sensor Networks, 2011 International Conference on Innovations in Information Technology, pp. 167-172, (2011); Nawaz H., Ali H. M., Massan S.-R., Applications of unmanned aerial vehicles: a review, 3C Tecnología_Glosas de innovación aplicadas a la pyme, pp. 85-105, (2019); Sehree N. A., Khidhir A. M., Olive trees cases classification based on deep convolutional neural network from unmanned aerial vehicle imagery, Indonesian Journal of Electrical Engineering and Computer Science, 27, 1, (2022); Yang T., Cabani A., Chafouk H., A Survey of Recent Indoor Localization Scenarios and Methodologies, Sensors, 21, 23, (2021); Golcarenarenji G., M.-Alpiste I., Wang Q., -Calero J. M. A., Efficient Real-Time Human Detection Using Unmanned Aerial Vehicles Optical Imagery, International Journal of Remote Sensing, 42, 7, pp. 2440-2462, (2021)","M.F. Albaghdadi; Department of Information Network, College of Information Technology, University of Babylon, Al Hillah, 20th Street, Babylon, Iraq; email: mustafa.net.msc@student.uobabylon.edu.iq","","Institute of Advanced Engineering and Science","","","","","","20893191","","","","English","Bull. Electr. Eng. Inform.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139171885"
"Yeşilmen S.; Tatar B.","Yeşilmen, Seda (54797088700); Tatar, Bahadır (57837189500)","54797088700; 57837189500","Efficiency of convolutional neural networks (CNN) based image classification for monitoring construction related activities: A case study on aggregate mining for concrete production","2022","Case Studies in Construction Materials","17","","e01372","","","","6","10.1016/j.cscm.2022.e01372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135719934&doi=10.1016%2fj.cscm.2022.e01372&partnerID=40&md5=c6e5f33d797e1c52d2d39887040ab752","Department of Civil Engineering, Çankaya University, Turkey","Yeşilmen S., Department of Civil Engineering, Çankaya University, Turkey; Tatar B., Department of Civil Engineering, Çankaya University, Turkey","Monitoring construction activities is an important task for efficiency in construction site operations thus the topic received a fair amount of attention in the literature. Optimizing construction site operations by monitoring and detecting various tasks is dependent on the size of the construction field, which determines the tools that can be used for the job. A monitoring task can be performed with high efficiency through image classification algorithms by training the algorithms to detect construction machinery. If the area of monitoring is larger, such as the task of detecting construction related operations in a large infrastructural construction, using drone images might become inefficient. We aimed to take a first step towards a cost-efficient monitoring system specifically for construction activities that cover large territories. Consequently, satellite image classification has been performed for construction machinery detection in this study. We utilized different versions of well-established convolutional neural network architectures as backbone for the transfer learning method and their performances are evaluated. Finally, the best performing models are determined as DenseNet161 and ResNet101 with 0.919 and 0.903 test accuracies, respectively. DenseNet161 model was discussed in terms of its accuracy and efficiency in a case study to detect illegal aggregate mining activity through the basin of Thamirabarani River. © 2022 The Authors","Aggregate mining; Computer vision; Concrete production; Convolutional neural networks; Remote sensing; Sustainability","Computer vision; Concrete aggregates; Construction equipment; Construction industry; Convolution; Deep learning; Efficiency; Image classification; Learning systems; Machinery; Monitoring; Network architecture; Object detection; Remote sensing; Aggregate mining; Case-studies; Concrete productions; Construction activities; Construction machinery; Construction sites; Convolutional neural network; Monitoring construction; Remote-sensing; Site operations; Convolutional neural networks","","","","","","","Atangana Njock P.G., Shen S.-L., Zhou A., Modoni G., Artificial neural network optimized by differential evolution for predicting diameters of jet grouted columns, J. Rock Mech. Geotech. Eng., 13, pp. 1500-1512, (2021); Congro M., de Alencar Monteiro V.M., Brandao A.L., dos Santos B.F., Roehl D., Silva F.D.A., Prediction of the residual flexural strength of fiber reinforced concrete using artificial neural networks, Constr. Build. Mater., 303, (2021); Adesanya E., Aladejare A., Adediran A., Lawal A., Illikainen M., Predicting shrinkage of alkali-activated blast furnace-fly ash mortars using artificial neural network (ANN), Cem. Concr. Compos., 124, (2021); de Almeida Pereira G.H., Fusioka A.M., Nassu B.T., Minetto R., Active fire detection in Landsat-8 imagery: a large-scale dataset and a deep-learning study, ISPRS J. Photogramm. Remote Sens., 178, pp. 171-186, (2021); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: common objects in context, (2014); Deng J., Dong W., Socher R., Li L.-J., Li K., Li F.-F., ImageNet: a large-scale hierarchical image database, (2009); Praveen Gujjar J., Prasanna Kumar H.R., Chiplunkar N.N., Image classification and prediction using transfer learning in colab notebook, Glob. Transit. Proc., 2, 2, pp. 382-385, (2021); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, (2012); Lowe D.G., Object recognition from local scale-invariant features, (1999); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2015); Ioffe S., Szegedy C., Batch normalization: accelerating deep network training by reducing internal covariate shift, 37, (2015); Xie S., Girshick R., Dollar P., Tu Z., He K., Aggregated residual transformations for deep neural networks, (2017); Jeelani I., Gheisari M., Safety challenges of UAV integration in construction: conceptual analysis and future research roadmap, Saf. Sci., 144, (2021); Zeng T., Wang J., Cui B., Wang X., Wang D., Zhang Y., The equipment detection and localization of large-scale construction jobsite by far-field construction surveillance video based on improving YOLOv3 and Grey wolf optimizer improving extreme learning machine, Constr. Build. Mater., 291, (2021); Nogueira K., Penatti O.A.B., dos Santos J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification, Pattern Recognit., 61, pp. 539-556, (2017); Khan S.N., (2020); Javadi S., M D., M. I P., Vehicle detection in aerial images based on 3D depth maps and deep neural networks, IEEE Access, 9, pp. 8381-8391, (2021); Chen J., Wang H., Wang S., He E., Zhang T., Wang L., Convolutional neural network with transfer learning approach for detection of unfavorable driving state using phase coherence image, Expert Syst. Appl., 187, (2021); Arabi S., Haghighat A.K., Sharma A., A deep‐learning‐based computer vision solution for construction vehicle detection, Comput.-Aided Civ. Infrastruct. Eng., 35, pp. 753-767, (2020); Fang W., Ding L., Zhong B., Love P.E., Luo H., Automated detection of workers and heavy equipment on construction sites: a convolutional neural network approach, Adv. Eng. Inform., 37, pp. 139-149, (2018); Guo Y., Xu Y., Li S., Dense construction vehicle detection based on orientation-aware feature fusion convolutional neural network, Autom. Constr., 112, (2020); Seites-Rundlett W., Bashar M.Z., Torres-Machi C., Corotis R.B., Combined evidence model to enhance pavement condition prediction from highly uncertain sensor data, Reliab. Eng. Syst. Saf., 217, (2022); Cusson D., Trischuk K., Hebert D., Hewus G., Gara M., Ghuman P., Satellite-based InSAR monitoring of highway bridges: validation case study on the north channel bridge in Ontario, Canada, Transp. Res. Rec., 2672, 45, pp. 76-86, (2018); Chen J., Tang P., Rakstad T., Patrick M., Zhou X., Augmenting a deep-learning algorithm with canal inspection knowledge for reliable water leak detection from multispectral satellite images, Adv. Eng. Inform., 46, (2020); Sharma D.K., Malikov V., Parygin D., Golubev A., Lozhenitsina A., Sadovnikova N., GPU-card performance research in satellite imagery classification problems using machine learning, Procedia Comput. Sci., 178, pp. 55-64, (2020); Castelluccio M., Poggi G., Sansone C., Verdoliva L., Land use classification in remote sensing images by convolutional neural networks, arXiv Prepr., (2015); Cheng G., Xie X., Han J., Guo L., Xia G.-S., Remote sensing image scene classification meets deep learning: challenges, methods, benchmarks, and opportunities, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 13, pp. 3735-3756, (2020); Mayra J., Keski-Saari S., Kivinen S., Tanhuanpaa T., Hurskainen P., Kullberg P., Poikolainen L., Viinikka A., Tuominen S., Kumpula T., Vihervaara P., Tree species classification from airborne hyperspectral and LiDAR data using 3D convolutional neural networks, Remote Sens. Environ., 256, (2021); Sharma A., Liu X., Yang X., Shi D., A patch-based convolutional neural network for remote sensing image classification, Neural Netw., 95, pp. 19-28, (2017); Obeng E.A., Oduro K.A., Obiri B.D., Abukari H., Guuroh R.T., Djagbletey G.D., Appiah-Korang J., Appiah M., Impact of illegal mining activities on forest ecosystem services: local communities’ attitudes and willingness to participate in restoration activities in Ghana, Heliyon, 5, 10, (2019); Duan H., Cao Z., Shen M., Liu D., Xiao Q., Detection of illicit sand mining and the associated environmental effects in China's fourth largest freshwater lake using daytime and nighttime satellite images, Sci. Total Environ., 647, pp. 606-618, (2019); Hackney C.R., Darby S.E., Parsons D.R., Leyland J., Best J.L., Aalto R., Nicholas A.P., Houseago R.C., River bank instability from unsustainable sand mining in the lower Mekong River, Nat. Sustain., 3, pp. 217-225, (2020); Schnebele E., Tanyu B.F., Cervone G., Waters N., Review of remote sensing methodologies for pavement management and assessment, Eur. Transp. Res. Rev., 7, pp. 1-19, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 770-778, (2016); He K., Zhang X., Ren S., Sun J., Identity mappings in deep residual networks, Eur. Conf. Comput. Vis., (2016); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., MobileNets: efficient convolutional neural networks for mobile vision applications, CoRR, 16, (2017); Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.-C., MobileNetV2: inverted residuals and linear bottlenecks, (2018); Huang G., Liu Z., (2017); Tan M., Le Q.V., Efficientnet: rethinking model scaling for convolutional neural networks, Int. Conf. Mach. Learn., pp. 6105-6114, (2019); Giglioni V., Garcia-Macias E., Venanzi I., Ierimonti L., Ubertini F., The use of receiver operating characteristic curves and precision-versus-recall curves as performance metrics in unsupervised structural damage classification under changing environment, Eng. Struct., 246, (2021); Setjo C.H., Achmad B., (2017); Elavenil S., Livingston J., Parameswari K., Case study on illegal sand mining in Tamil Nadu: alternate solution by replacing natural sand by M-sand, Int. J. Mech. Prod. Eng. Res. Dev. (IJMPERD), 7, pp. 279-284, (2017)","S. Yeşilmen; Department of Civil Engineering, Çankaya University, Ankara, Turkey; email: syesilmen@cankaya.edu.tr","","Elsevier Ltd","","","","","","22145095","","","","English","Case Stud. Constr. Mater.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135719934"
"Chi H.; Sun J.; Zhang C.; Miao C.","Chi, Huanzhao (56003176800); Sun, Jing (55547135861); Zhang, Cheng (57199503054); Miao, Changsheng (37089452300)","56003176800; 55547135861; 57199503054; 37089452300","Remote sensing data processing and analysis for the identification of geological entities","2023","Acta Geophysica","71","3","","1565","1577","12","1","10.1007/s11600-022-00871-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135595517&doi=10.1007%2fs11600-022-00871-y&partnerID=40&md5=de28026797a94fb494a32acde46427be","School of Prospecting & Surveying Engineering, Changchun Institute of Technology, Jinlin, Changchun, 130021, China; Jilin Provincial Key Laboratory of Changbai Historical Culture and VR Reconstruction Technology, Jinlin, Changchun, 130021, China; Qingdao Institute of Marine Geology, CGS, Shandong, Qingdao, 266071, China; Laboratory for Marine Mineral Resources, Qingdao National Laboratory for Marine Science and Technology, Shandong, Qingdao, 266071, China","Chi H., School of Prospecting & Surveying Engineering, Changchun Institute of Technology, Jinlin, Changchun, 130021, China, Jilin Provincial Key Laboratory of Changbai Historical Culture and VR Reconstruction Technology, Jinlin, Changchun, 130021, China; Sun J., Qingdao Institute of Marine Geology, CGS, Shandong, Qingdao, 266071, China, Laboratory for Marine Mineral Resources, Qingdao National Laboratory for Marine Science and Technology, Shandong, Qingdao, 266071, China; Zhang C., School of Prospecting & Surveying Engineering, Changchun Institute of Technology, Jinlin, Changchun, 130021, China, Jilin Provincial Key Laboratory of Changbai Historical Culture and VR Reconstruction Technology, Jinlin, Changchun, 130021, China; Miao C., School of Prospecting & Surveying Engineering, Changchun Institute of Technology, Jinlin, Changchun, 130021, China","As Earth observation technology has advanced, the volume of remote sensing big data has grown rapidly, offering significant obstacles to efficient and effective processing and analysis. A convolutional neural network refers to a neural network that covers convolutional calculations. It is a form of deep learning, and convolutional neural networks have characterization learning characteristics, which can classify information into different data. Remote Sensing Data Processing from various sensors has been attracting with more information in Remote Sensing. Remote sensing data is generally adjusted and refined through image processing. Image processing techniques, such as filtering and feature detection, are ideal for dealing with the high-dimensionality of geographically distributed systems. The geological entity is a term in geological work which refers to the product of geological processes that occupy a certain space in the Earth’s crust and are different from other materials. They are of different sizes and are divided into different types according to their size. It mainly focuses on improving classification accuracy and accurately describing scattering types. For geological entity recognition, this paper proposed a Deep Convolutional Neural Network Polarized Synthetic Aperture Radar (DCNN-PSAR). It is expected to use deep convolutional neural network technology and polarized SAR technology to explore new methods of geological entities and improve geological recognition capabilities. With the help of Multimodal Remote Sensing Data Processing, it is now possible to characterize and identify the composition of the Earth’s surface from orbital and aerial platforms. This paper proposes a ground object classification algorithm for polarized SAR images based on a fully convolutional network, which realizes the geological classification function and overcomes the shortcomings of too long. The evaluation of DCNN-PSAR shows that the accuracy of the water area is showing a rising trend, and the growth rate is relatively fast in the early stage, which directly changes from 0.14 to 0.6. Still, the increase is slower in the later stage. DCNN-PSAR achieves the highest quality of remote sensing data extraction. © 2022, The Author(s) under exclusive licence to Institute of Geophysics, Polish Academy of Sciences & Polish Academy of Sciences.","Data processing; Deep convolutional neural network; Entity recognition; Geological body; Multimodal; Remote sensing; Synthetic aperture radar","Antennas; Classification (of information); Convolution; Convolutional neural networks; Data handling; Deep neural networks; Earth (planet); Geology; Image processing; Orbits; Remote sensing; Space-based radar; Convolutional neural network; Data processing and analysis; Deep convolutional neural network; Earth Observation Technology; Entity recognition; Geological bodies; Geological-entities; Multi-modal; Remote sensing data; Remote-sensing; artificial neural network; data processing; data set; image processing; machine learning; remote sensing; synthetic aperture radar; Synthetic aperture radar","","","","","Department of Science and Technology of Jilin Province, (20200403068SF, 20210101108JC)","This article has received funding from the Science and Technology Department Project of Jilin Province (Grant No.20200403068SF and 20210101108JC). ","Abo El-Hassan M., Awadalla K.H., Hussein K.F., Shaped-beam circularly polarized antenna array of linear elements for satellite and SAR applications, Wirel Pers Commun, 110, 2, pp. 605-619, (2020); Acharya U.R., Fujita H., Lih O.S., Adam M., Tan J.H., Chua C.K., Automated detection of coronary artery disease using different durations of ECG segments with convolutional neural network, Knowl-Based Syst, 132, pp. 62-71, (2017); Adriano B., Yokoya N., Xia J., Miura H., Liu W., Matsuoka M., Koshimura S., Learning from multimodal and multitemporal earth observation data for building damage mapping, ISPRS J Photogramm Remote Sens, 175, pp. 132-143, (2021); Anquez P., Pellerin J., Irakarama M., Cupillard P., Levy B., Caumon G., Automatic correction and simplification of geological maps and cross-sections for numerical simulations, CR Geosci, 351, 1, pp. 48-58, (2019); Chauhan S., Srivastava H.S., Patel P., Crop height estimation using RISAT-1 hybrid-polarized synthetic aperture radar data, IEEE J Sel Top Appl Earth Obs Remote Sens, 12, 8, pp. 2928-2933, (2019); Chen Y.H., Krishna T., Emer J.S., Sze V., Eyeriss: an energy-efficient reconfigurable accelerator for deep convolutional neural networks, IEEE J Solid-State Circuits, 52, 1, pp. 127-138, (2016); Chen H., Zhang Y., Kalra M.K., Lin F., Chen Y., Liao P., Wang G., Low-dose CT with a residual encoder-decoder convolutional neural network, IEEE Trans Med Imaging, 36, 12, pp. 2524-2535, (2017); El-Hassan M.A., Hussein K.F.A., Farahat A.E., Awadalla K.H., Shaped-beam circularly-polarized practical antenna array for land imaging SAR systems, Appl Comput Electromagn Soc J (ACES), pp. 642-653, (2019); Hong D., Hu J., Yao J., Chanussot J., Zhu X.X., Multimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model, ISPRS J Photogramm Remote Sens, 178, pp. 68-80, (2021); Hou Y., Li Z., Wang P., Li W., Skeleton optical spectra-based action recognition using convolutional neural networks, IEEE Trans Circuits Syst Video Technol, 28, 3, pp. 807-811, (2018); Ilerbaig J., Archives as sediments: metaphors of deposition and archival thinking, Arch Sci, 21, 1, pp. 83-95, (2021); Kruthiventi S.S., Ayush K., Babu R.V., Deepfix: a fully convolutional neural network for predicting human eye fixations, IEEE Trans Image Process, 26, 9, pp. 4446-4456, (2017); Li H., Perrie W., Sea ice characterization and classification using hybrid polarimetry SAR, IEEE J Sel Top Appl Earth Obs Remote Sens, 9, 11, pp. 4998-5010, (2016); Li H., Mouche A., Wang H., Stopa J.E., Chapron B., Polarization dependence of azimuth cutoff from quad-pol SAR images, IEEE Trans Geosci Remote Sens, 57, 12, pp. 9878-9887, (2019); Liu M., Shi J., Li Z., Li C., Zhu J., Liu S., Towards a better analysis of deep convolutional neural networks, IEEE Trans Visual Comput Graphics, 23, 1, pp. 91-100, (2017); Lu F., Wu F., Hu P., Peng Z., Kong D., Automatic 3D liver location and segmentation via convolutional neural network and graph cut, Int J Comput Assist Radiol Surg, 12, 2, pp. 171-182, (2017); Mahdianpari M., Motagh M., Akbari V., Mohammadimanesh F., Salehi B., A Gaussian random field model for de-speckling of multi-polarized synthetic aperture radar data, Adv Space Res, 64, 1, pp. 64-78, (2019); Manning P.M., The Hyde we live in: Stevenson, evolution, and the anthropogenic fog, Vic Lit Cult, 46, 1, pp. 181-199, (2018); McCann M.T., Jin K.H., Unser M., Convolutional neural networks for inverse problems in imaging: a review, IEEE Signal Process Mag, 34, 6, pp. 85-95, (2017); Peng W., Cao Y., Shen C., Et al., Temporal pyramid pooling based convolutional neural networks for action recognition, IEEE Trans Multimed, 27, 12, pp. 2613-2622, (2017); Schirrmeister R.T., Springenberg J.T., Fiederer L.D.J., Glasstetter M., Eggensperger K., Tangermann M., Ball T., Deep learning with convolutional neural networks for EEG decoding and visualization, Hum Brain Mapp, 38, 11, pp. 5391-5420, (2017); Shen W., Zhou M., Yang F., Yu D., Dong D., Yang C., Tian J., Multi-crop convolutional neural networks for lung nodule malignancy suspiciousness classification, Pattern Recogn, 61, pp. 663-673, (2017); Sprohnle K., Fuchs E.M., Pelizari P.A., Object-based analysis and fusion of optical and SAR satellite data for dwelling detection in refugee camps, IEEE J Sel Top Appl Earth Obs Remote Sens, 10, 5, pp. 1780-1791, (2017); Sun G., Kong Y., Jia X., Zhang A., Rong J., Ma H., Synergistic use of optical and dual-polarized SAR data with multiple kernel learning for urban impervious surface mapping, IEEE J Sel Top Appl Earth Obs Remote Sens, 12, 1, pp. 223-236, (2018); Wang P., Cao Y., Shen C., Liu L., Shen H.T., Temporal pyramid pooling-based convolutional neural network for action recognition, IEEE Trans Circuits Syst Video Technol, 27, 12, pp. 2613-2622, (2016); Wang X., Shao Y., Tian W., Li K., On the classification of mixed floating pollutants on the Yellow Sea of China by using a quad-polarized SAR image, Front Earth Sci, 12, 2, pp. 373-380, (2018); Ye X., Lin M., Zheng Q., Yuan X., Liang C., Zhang B., Zhang J., A typhoon wind-field retrieval method for the dual-polarization SAR imagery, IEEE Geosci Remote Sens Lett, 16, 10, pp. 1511-1515, (2019); Yu K., Salzmann M., Second-Order Convolutional Neural Networks, (2017); Zhang B., Mouche A., Lu Y., Perrie W., Zhang G., Wang H., A geophysical model function for wind speed retrieval from C-band HH-polarized synthetic aperture radar, IEEE Geosci Remote Sens Lett, 16, 10, pp. 1521-1525, (2019); Zhang X., Zhao Y., Xie J., Li C., Hu Z., Geological big data acquisition based on speech recognition, Multimed Tools Appl, 79, 33, pp. 24413-24428, (2020)","J. Sun; Qingdao Institute of Marine Geology, CGS, Qingdao, Shandong, 266071, China; email: sjsjing@126.com","","Springer Science and Business Media Deutschland GmbH","","","","","","18956572","","","","English","Acta Geophys.","Article","Final","","Scopus","2-s2.0-85135595517"
"Shu Q.; Pan J.; Zhang Z.; Wang M.","Shu, Qidi (57226639455); Pan, Jun (55459046800); Zhang, Zhuoer (57899263100); Wang, Mi (57205635094)","57226639455; 55459046800; 57899263100; 57205635094","DPCC-Net: Dual-perspective change contextual network for change detection in high-resolution remote sensing images","2022","International Journal of Applied Earth Observation and Geoinformation","112","","102940","","","","4","10.1016/j.jag.2022.102940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138458756&doi=10.1016%2fj.jag.2022.102940&partnerID=40&md5=d6c1e2585703a2d9e63b1d33df12cfda","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","Shu Q., State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Pan J., State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Zhang Z., State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Wang M., State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","Change detection in remote sensing images plays an important role in observing earth surface. Over the past few years, deep learning has been widely used in image analysis due to its powerful feature extraction capability, which has shown great potential for change detection task. However, current methods still have difficulties in identifying complex changes due to the insufficient exploration of temporal information. In addition, the complex contextual information of high-resolution images further limits the accuracy. To clarify the temporal information for complex changes and acquire relational contexts of high-resolution images, a dual-perspective change contextual network (DPCC-Net) is proposed for change detection in high-resolution remote sensing images. The presented method emphasizes the process of extraction and optimization of change features by bi-temporal feature fusion and contextual modeling. Firstly, a siamese network is used to extract bi-temporal features. Then, a novel dual-perspective fusion (DPF) is proposed, which takes bi-temporal features as reference respectively and obtains two sets of change features from each temporal perspective, thereby increasing the sensitivity to change related information and changes in complex scenes can be better identified. Next, a change context module (CCM) is proposed to incorporate abundant contexts to change features. CCM considers the relation and similarity between each pixel and its contextual pixels, thereby facilitating the integrity of change objects. The quantitative and qualitative results on three change detection datasets indicate that DPCC-Net achieves state-of-the-art performance. The code of DPCC-Net will be released at: https://github.com/SQD1/DPCC-Net. © 2022 The Author(s)","Change detection; Contextual representation; Feature fusion; Fully convolutional network (FCN)","artificial neural network; detection method; image resolution; pixel; qualitative analysis; quantitative analysis; remote sensing","","","","","Key Project of Hubei Provincial Natural Science Foundation in China, (2020CFA001); Key Research and Development Plan Project of Hubei Province in China, (2020BIB006); National Natural Science Foundation of China, NSFC, (41971422)","This work was supported in part by the National Natural Science Foundation of China under Grant 41971422; in part by the Key Research and Development Plan Project of Hubei Province in China under Grant 2020BIB006; in part by the Key Project of Hubei Provincial Natural Science Foundation in China under Grant 2020CFA001, and by the LIESMARS Special Research Funding.","Alcantarilla P.F., Stent S., Ros G., Arroyo R., Gherardi R., Street-view change detection with deconvolutional networks, Autonom. Rob., 42, 7, pp. 1301-1322, (2018); Bovolo F., Bruzzone L., A theoretical framework for unsupervised change detection based on change vector analysis in the polar domain, IEEE Trans. Geosci. Remote Sens., 45, 1, pp. 218-236, (2006); Chen G., Hay G.J., Carvalho L.M., Wulder M.A., Object-based change detection, Int. J. Remote Sens., 33, 14, pp. 4434-4457, (2012); Chen H., Qi Z., Shi Z., Remote sensing image change detection with transformers, IEEE Trans. Geosci. Remote Sens., 60, pp. 1-14, (2022); Chen H., Shi Z., A Spatial-Temporal Attention-Based Method and a New Dataset for Remote Sensing Image Change Detection, Remote Sens., 12, 10, (2020); Chen H., Wu C., Du B., Zhang L., Deep siamese multi-scale convolutional network for change detection in multi-temporal VHR images, 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images, pp. 1-4, (2019); Chen L.-C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-decoder with atrous separable convolution for semantic image segmentation, Proceedings of the European conference on computer vision, pp. 801-818, (2018); Chen P., Zhang B., Hong D., Chen Z., Yang X., Li B., FCCDN: Feature constraint network for VHR image change detection, ISPRS J. Photogramm. Remote Sens., 187, pp. 101-119, (2022); Daudt R.C., Le Saux B., Boulch A., Fully convolutional siamese networks for change detection, 2018 25th IEEE International Conference on Image Processing (ICIP), IEEE, pp. 4063-4067, (2018); Deng J., Wang K., Deng Y., Qi G., PCA-based land-use change detection and analysis using multitemporal and multisensor satellite data, Int. J. Remote Sens., 29, 16, pp. 4823-4838, (2008); Diakogiannis F.I., Waldner F., Caccetta P., Looking for change? Roll the Dice and demand Attention, Remote Sens., 13, 18, (2021); Diakogiannis F.I., Waldner F., Caccetta P., Wu C., ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data, ISPRS J. Photogramm. Remote Sens., 162, pp. 94-114, (2020); Ding Q., Shao Z., Huang X., Altan O., DSA-Net: A novel deeply supervised attention-guided network for building change detection in high-resolution remote sensing images, Int. J. Appl. Earth Obs. Geoinf., 105, (2021); Fang S., Li K., Shao J., Li Z., SNUNet-CD: A densely connected siamese network for change detection of VHR images, IEEE Geosci. Remote Sens. Lett., 19, pp. 1-5, (2021); Gong M., Zhan T., Zhang P., Miao Q., Superpixel-based difference representation learning for change detection in multispectral remote sensing images, IEEE Trans. Geosci. Remote Sens., 55, 5, pp. 2658-2673, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Hussain M., Chen D., Cheng A., Wei H., Stanley D., Change detection from remotely sensed images: From pixel-based to object-based approaches, ISPRS J. Photogramm. Remote Sens., 80, pp. 91-106, (2013); Ji S., Shen Y., Lu M., Zhang Y., Building instance change detection from large-scale aerial images using convolutional neural networks and simulated samples, Remote Sens., 11, 11, (2019); Jiang H., Hu X., Li K., Zhang J., Gong J., Zhang M., PGA-SiamNet: Pyramid feature-based attention-guided siamese network for remote sensing orthoimagery building change detection, Remote Sens., 12, 3, (2020); Lebedev M.A., Vizilter Y.V., Vygolov O.V., Knyaz V.A., Rubis A.Y., Change detection in remote sensing images using conditional adversarial networks, Int. Arch. Photogramm. Remote Sens. Spatial Inform. Sci., XLII-2, pp. 565-571, (2018); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Leichtle T., Geiss C., Wurm M., Lakes T., Taubenbock H., Unsupervised change detection in VHR remote sensing imagery–an object-based clustering approach in a dynamic urban environment, Int. J. Appl. Earth Obs. Geoinf., 54, pp. 15-27, (2017); Liu R., Jiang D., Zhang L., Zhang Z., Deep depthwise separable convolutional network for change detection in optical aerial images. IEEE J. Sel. Top. in Appl, Earth Observ. Remote Sens., 13, pp. 1109-1118, (2020); Liu T., Yang L., Lunga D., Change detection using deep learning approach with object-based image analysis, Remote Sensing Environ., 256, (2021); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, (2015); Marpu P.R., Gamba P., Canty M.J., Improving change detection results of IR-MAD by eliminating strong changes, IEEE Geosci. Remote Sens. Lett., 8, 4, pp. 799-803, (2011); Pang S., Zhang A., Hao J., Liu F., Chen J., SCA-CDNet: a robust siamese correlation-and-attention-based change detection network for bitemporal VHR images, Int. J. Remote Sens., 1-22, (2021); Peng D., Zhang Y., Guan H., End-to-end change detection for high resolution satellite images using improved UNet++, Remote Sens., 11, 11, (2019); Shi Q., Liu M., Li S., Liu X., Wang F., Zhang L., A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection, (2021); Shi W., Zhang M., Zhang R., Chen S., Zhan Z., Change detection based on artificial intelligence: State-of-the-art and challenges, Remote Sens., 12, 10, (2020); Song L., Xia M., Jin J., Qian M., Zhang Y., SUACDNet: Attentional change detection network based on siamese U-shaped structure, Int. J. Appl. Earth Obs. Geoinf., 105, (2021); Sudre C.H., Li W., Vercauteren T., Ourselin S., Cardoso M.J., Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, (2017); Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need, Adv. Neural Inform. Process. Syst., pp. 5998-6008, (2017); Vega P.J.S., da Costa G.A.O.P., Feitosa R.Q., Adarme M.X.O., de Almeida C.A., Heipke C., Rottensteiner F., An unsupervised domain adaptation approach for change detection and its application to deforestation mapping in tropical biomes, ISPRS J. Photogramm. Remote Sens., 181, pp. 113-128, (2021); Walter V., Object-based classification of remote sensing data for change detection, ISPRS J. Photogramm. Remote Sens., 58, 3-4, pp. 225-238, (2004); Wang D., Chen X., Jiang M., Du S., Xu B., Wang J., ADS-Net: An Attention-Based deeply supervised network for remote sensing image change detection, Int. J. Appl. Earth Obs. Geoinf., 101, (2021); Wang F., Xu Y.J., Comparison of remote sensing change detection techniques for assessing hurricane damage to forests, Environ. Monit. Assess, 162, 1, pp. 311-326, (2010); Wang X., Girshick R., Gupta A., He K., Non-local neural networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7794-7803, (2018); Yang J., Weisberg P.J., Bristow N.A., Landsat remote sensing approaches for monitoring long-term tree cover dynamics in semi-arid woodlands: Comparison of vegetation indices and spectral mixture analysis, Rem. Sens. Environ., 119, pp. 62-71, (2012); Yuan Y., Chen X., Wang J., Object-contextual representations for semantic segmentation, Proceedings of the European Conference on Computer Vision, pp. 173-190, (2020); Yuan Y., Huang L., Guo J., Zhang C., Chen X., Wang J., (2018); Zanetti M., Bovolo F., Bruzzone L., Rayleigh-Rice mixture parameter estimation via EM algorithm for change detection in multispectral images, IEEE Trans. Image Process., 24, 12, pp. 5004-5016, (2015); Zhan Y., Fu K., Yan M., Sun X., Wang H., Qiu X., Change detection based on deep siamese convolutional network for optical aerial images, IEEE Geosci. Remote Sens. Lett., 14, 10, pp. 1845-1849, (2017); Zhang C., Yue P., Tapete D., Jiang L., Shangguan B., Huang L., Liu G., A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images, ISPRS J. Photogramm. Remote Sens., 166, pp. 183-200, (2020); Zhang H., Zhang H., Wang C., Xie J., Co-occurrent features in semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 548-557, (2019); Zhang M., Shi W., A feature difference convolutional neural network-based change detection method, IEEE Trans. Geosci. Remote Sens., 58, 10, pp. 7232-7246, (2020); Zhang Y., Peng D., Huang X., Object-based change detection for VHR images based on multiscale uncertainty analysis, IEEE Geosci. Remote Sens. Lett., 15, 1, pp. 13-17, (2017); Zhao H., Shi J., Qi X., Wang X., Jia J., Pyramid scene parsing network, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2881-2890, (2017); Zheng Z., Wan Y., Zhang Y., Xiang S., Peng D., Zhang B., CLNet: Cross-layer convolutional neural network for change detection in optical remote sensing imagery, ISPRS J. Photogramm. Remote Sens., 175, pp. 247-267, (2021)","J. Pan; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; email: panjun1215@whu.edu.cn","","Elsevier B.V.","","","","","","15698432","","","","English","Int. J. Appl. Earth Obs. Geoinformation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138458756"
"Yuan H.; Huang K.; Ren C.; Xiong Y.; Duan J.; Yang Z.","Yuan, Haotian (57354574400); Huang, Kekun (43261321100); Ren, Chuanxian (25928080300); Xiong, Yongzhu (56249705800); Duan, Jieli (37123703800); Yang, Zhou (55522618700)","57354574400; 43261321100; 25928080300; 56249705800; 37123703800; 55522618700","Pomelo Tree Detection Method Based on Attention Mechanism and Cross-Layer Feature Fusion","2022","Remote Sensing","14","16","3902","","","","3","10.3390/rs14163902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137771388&doi=10.3390%2frs14163902&partnerID=40&md5=2f9e8827210d4a536b67e3f52ad17340","School of Engineering, South China Agricultural University, Guangzhou, 510642, China; School of Mathematics, Jiaying University, Meizhou, 514015, China; Guangdong Provincial Key Laboratory of Conservation and Precision Utilization of Characteristic Agricultural Resources in Mountainous Areas, Jiaying University, Meizhou, 514015, China; School of Mathematics, Sun Yat-sen University, Guangzhou, 510275, China; School of Geography and Tourism, Jiaying University, Meizhou, 514015, China","Yuan H., School of Engineering, South China Agricultural University, Guangzhou, 510642, China; Huang K., School of Mathematics, Jiaying University, Meizhou, 514015, China, Guangdong Provincial Key Laboratory of Conservation and Precision Utilization of Characteristic Agricultural Resources in Mountainous Areas, Jiaying University, Meizhou, 514015, China; Ren C., School of Mathematics, Sun Yat-sen University, Guangzhou, 510275, China; Xiong Y., Guangdong Provincial Key Laboratory of Conservation and Precision Utilization of Characteristic Agricultural Resources in Mountainous Areas, Jiaying University, Meizhou, 514015, China, School of Geography and Tourism, Jiaying University, Meizhou, 514015, China; Duan J., School of Engineering, South China Agricultural University, Guangzhou, 510642, China; Yang Z., School of Engineering, South China Agricultural University, Guangzhou, 510642, China, Guangdong Provincial Key Laboratory of Conservation and Precision Utilization of Characteristic Agricultural Resources in Mountainous Areas, Jiaying University, Meizhou, 514015, China","Deep learning is the subject of increasing research for fruit tree detection. Previously developed deep-learning-based models are either too large to perform real-time tasks or too small to extract good enough features. Moreover, there has been scarce research on the detection of pomelo trees. This paper proposes a pomelo tree-detection method that introduces the attention mechanism and a Ghost module into the lightweight model network, as well as a feature-fusion module to improve the feature-extraction ability and reduce computation. The proposed method was experimentally validated and showed better detection performance and fewer parameters than some state-of-the-art target-detection algorithms. The results indicate that our method is more suitable for pomelo tree detection. © 2022 by the authors.","attention mechanism; convolutional neural network; object detection; pomelo tree detection; remote-sensing image","Convolutional neural networks; Deep learning; Feature extraction; Forestry; Object recognition; Orchards; Remote sensing; Attention mechanisms; Convolutional neural network; Cross layer; Detection methods; Features fusions; Fruit trees; Objects detection; Pomelo tree detection; Remote sensing images; Tree detections; Object detection","","","","","Guangdong Province Special Project in Key Fields for Universities, (2020ZDZX3044); Open Research Projects of Zhejiang Lab, (2021KH0AB08); National Natural Science Foundation of China, NSFC, (61906046, 61976104, 61976229); Natural Science Foundation of Guangdong Province, (2020A1515010702); Science and Technology Planning Project of Guangdong Province, (2020B121201013)","This work is supported by the National Natural Science Foundation of China under Grants 61976104, 61906046, and 61976229, the Natural Science Foundation of Guangdong Province under Grant 2020A1515010702, the Guangdong Province Special Project in Key Fields for Universities under Grant 2020ZDZX3044, the Open Research Projects of Zhejiang Lab under Grant 2021KH0AB08 and the Science and Technology Program of Guangdong Province under Grant 2020B121201013: Guangdong Provincial Key Laboratory of Conservation and Precision Utilization of Characteristic Agricultural Resources in Mountainous Areas.","Morton J.F., Fruits of Warm Climates, (1987); Jimenez-Brenes F.M., Lopez-Granados F., De Castro A., Torres-Sanchez J., Serrano N., Pena J., Quantifying pruning impacts on olive tree architecture and annual canopy growth by using UAV-based 3D modelling, Plant Methods, 13, (2017); Castillo-Ruiz F.J., Jimenez-Jimenez F., Blanco-Roldan G.L., Sola-Guirado R.R., Agueera-Vega J., Castro-Garcia S., Analysis of fruit and oil quantity and quality distribution in high-density olive trees in order to improve the mechanical harvesting process, Span. J. Agric. Res, 13, (2015); Garcia-Ruiz F., Sankaran S., Maja J.M., Lee W.S., Rasmussen J., Ehsani R., Comparison of two aerial imaging platforms for identification of Huanglongbing-infected citrus trees, Comput. Electron. Agric, 91, pp. 106-115, (2013); Zhang C., Valente J., Kooistra L., Guo L., Wang W., Orchard management with small unmanned aerial vehicles: A survey of sensing and analysis approaches, Precis. Agric, 22, pp. 2007-2052, (2021); Barbagallo S., Consoli S., Russo A., A one-layer satellite surface energy balance for estimating evapotranspiration rates and crop water stress indexes, Sensors, 9, pp. 1-21, (2009); Salgadoe A.S.A., Robson A.J., Lamb D.W., Dann E.K., Searle C., Quantifying the severity of phytophthora root rot disease in avocado trees using image analysis, Remote Sens, 10, (2018); Moran M.S., Inoue Y., Barnes E., Opportunities and limitations for image-based remote sensing in precision crop management, Remote Sens. Environ, 61, pp. 319-346, (1997); Wal T., Abma B., Viguria A., Previnaire E., Zarco-Tejada P.J., Serruys P., Valkengoed E.V., Voet P., Fieldcopter: Unmanned aerial systems for crop monitoring services, Precision Agriculture ’13, pp. 169-175, (2013); Ochoa K.S., Guo Z., A framework for the management of agricultural resources with automated aerial imagery detection, Comput. Electron. Agric, 162, pp. 53-69, (2019); Swetnam T.L., Falk D.A., Application of metabolic scaling theory to reduce error in local maxima tree segmentation from aerial LiDAR, For. Ecol. Manag, 323, pp. 158-167, (2014); Yang J., He Y., Caspersen J.P., Jones T.A., Delineating individual tree crowns in an uneven-aged, mixed broadleaf forest using multispectral watershed segmentation and multiscale fitting, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 10, pp. 1390-1401, (2016); Jing L., Hu B., Noland T., Li J., An individual tree crown delineation method based on multi-scale segmentation of imagery, ISPRS J. Photogramm. Remote Sens, 70, pp. 88-98, (2012); Srestasathiern P., Rakwatin P., Oil palm tree detection with high resolution multi-spectral satellite imagery, Remote Sens, 6, pp. 9749-9774, (2014); Dos Santos A.M., Mitja D., Delaitre E., Demagistri L., de Souza Miranda I., Libourel T., Petit M., Estimating babassu palm density using automatic palm tree detection with very high spatial resolution satellite images, J. Environ. Manag, 193, pp. 40-51, (2017); Pu R., Landry S., A comparative analysis of high spatial resolution IKONOS and WorldView-2 imagery for mapping urban tree species, Remote Sens. Environ, 124, pp. 516-533, (2012); Hung C., Bryson M., Sukkarieh S., Multi-class predictive template for tree crown detection, ISPRS J. Photogramm. Remote Sens, 68, pp. 170-183, (2012); Dalponte M., Orka H.O., Ene L.T., Gobakken T., Naesset E., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data, Remote Sens. Environ, 140, pp. 306-317, (2014); Lopez-Lopez M., Calderon R., Gonzalez-Dugo V., Zarco-Tejada P.J., Fereres E., Early detection and quantification of almond red leaf blotch using high-resolution hyperspectral and thermal imagery, Remote Sens, 8, (2016); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., Hyyppa J., Saari H., Polonen I., Imai N.N., Et al., Individual tree detection and classification with UAV-based photogrammetric point clouds and hyperspectral imaging, Remote Sens, 9, (2017); Wang Y., Zhu X., Wu B., Automatic detection of individual oil palm trees from UAV images using HOG features and an SVM classifier, Int. J. Remote Sens, 40, pp. 7356-7370, (2019); Huang K.K., Ren C.X., Liu H., Lai Z.R., Yu Y.F., Dai D.Q., Hyperspectral image classification via discriminant Gabor ensemble filter, IEEE Trans. Cybern, 52, pp. 8352-8365, (2021); Albetis J., Duthoit S., Guttler F., Jacquin A., Goulard M., Poilve H., Feret J.B., Dedieu G., Detection of Flavescence dorée grapevine disease using unmanned aerial vehicle (UAV) multispectral imagery, Remote Sens, 9, (2017); Lei S., Luo J., Tao X., Qiu Z., Remote Sensing Detecting of Yellow Leaf Disease of Arecanut Based on UAV Multisource Sensors, Remote Sens, 13, (2021); Zhang Y., Wa S., Liu Y., Zhou X., Sun P., Ma Q., High-Accuracy Detection of Maize Leaf Diseases CNN Based on Multi-Pathway Activation Function Module, Remote Sens, 13, (2021); Nofrizal A.Y., Sonobe R., Yamashita H., Seki H., Mihara H., Morita A., Ikka T., Evaluation of a One-Dimensional Convolution Neural Network for Chlorophyll Content Estimation Using a Compact Spectrometer, Remote Sens, 14, (2022); Milioto A., Lottes P., Stachniss C., Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in CNNs, Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 2229-2235; Potena C., Nardi D., Pretto A., Fast and accurate crop and weed identification with summarized train sets for precision agriculture, Proceedings of the International Conference on Intelligent Autonomous Systems, pp. 105-121; Milella A., Marani R., Petitti A., Reina G., In-field high throughput grapevine phenotyping with a consumer-grade depth camera, Comput. Electron. Agric, 156, pp. 293-306, (2019); Qi X., Dong J., Lan Y., Zhu H., Method for Identifying Litchi Picking Position Based on YOLOv5 and PSPNet, Remote Sens, 14, (2022); Huang B., Zhao B., Song Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote-sensing imagery, Remote Sens. Environ, 214, pp. 73-86, (2018); Huang K.K., Ren C.X., Liu H., Lai Z.R., Yu Y.F., Dai D.Q., Hyperspectral image classification via discriminative convolutional neural network with an improved triplet loss, Pattern Recognit, 112, (2021); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote-sensing images, Remote Sens, 9, (2016); Pibre L., Chaumon M., Subsol G., Lenco D., Derras M., How to deal with multi-source data for tree detection based on deep learning, Proceedings of the 2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pp. 1150-1154; Wu J., Yang G., Yang H., Zhu Y., Li Z., Lei L., Zhao C., Extracting apple tree crown information from remote imagery using deep learning, Comput. Electron. Agric, 174, (2020); Zheng J., Fu H., Li W., Wu W., Yu L., Yuan S., Tao W.Y.W., Pang T.K., Kanniah K.D., Growing status observation for oil palm trees using Unmanned Aerial Vehicle (UAV) images, ISPRS J. Photogramm. Remote Sens, 173, pp. 95-121, (2021); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Adv. Neural Inf. Process. Syst, 28, pp. 91-99, (2015); Osco L.P., De Arruda M., Junior J.M., Da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens, 160, pp. 97-106, (2020); Zheng J., Wu W., Yu L., Fu H., Coconut Trees Detection on the Tenarunga Using High-Resolution Satellite Images and Deep Learning, Proceedings of the 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, pp. 6512-6515; Zheng J., Wu W., Yuan S., Fu H., Li W., Yu L., Multisource-domain generalization-based oil palm tree detection using very-high-resolution (vhr) satellite images, IEEE Geosci. Remote Sens. Lett, 19, pp. 1-5, (2021); Zheng J., Fu H., Li W., Wu W., Zhao Y., Dong R., Yu L., Cross-regional oil palm tree counting and detection via a multi-level attention domain adaptation network, ISPRS J. Photogramm. Remote Sens, 167, pp. 154-177, (2020); Pang J., Chen K., Shi J., Feng H., Ouyang W., Lin D., Libra r-cnn: Towards balanced learning for object detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 821-830; Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Proceedings of the European Conference on Computer Vision, pp. 21-37; Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788; Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Wang C.Y., Bochkovskiy A., Liao H.Y.M., Scaled-yolov4: Scaling cross stage partial network, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13029-13038; Ge Z., Liu S., Wang F., Li Z., Sun J., Yolox: Exceeding yolo series in 2021, arXiv, (2021); Liu R., Tao F., Liu X., Na J., Leng H., Wu J., Zhou T., RAANet: A Residual ASPP with Attention Framework for Semantic Segmentation of High-Resolution Remote Sensing Images, Remote Sens, 14, (2022); Han Z., Hu W., Peng S., Lin H., Zhang J., Zhou J., Wang P., Dian Y., Detection of Standing Dead Trees after Pine Wilt Disease Outbreak with Airborne Remote Sensing Imagery by Multi-Scale Spatial Attention Deep Learning and Gaussian Kernel Approach, Remote Sens, 14, (2022); Gong H., Mu T., Li Q., Dai H., Li C., He Z., Wang W., Han F., Tuniyazi A., Li H., Et al., Swin-Transformer-Enabled YOLOv5 with Attention Mechanism for Small Object Detection on Satellite Images, Remote Sens, 14, (2022); Li X., Pan J., Xie F., Zeng J., Li Q., Huang X., Liu D., Wang X., Fast and accurate green pepper detection in complex backgrounds via an improved Yolov4-tiny model, Comput. Electron. Agric, 191, (2021); Yu J., Wu T., Zhou S., Pan H., Zhang X., Zhang W., An SAR Ship Object Detection Algorithm Based on Feature Information Efficient Representation Network, Remote Sens, 14, (2022); Cheng G., Yang C., Yao X., Guo L., Han J., When deep learning meets metric learning: Remote-sensing image scene classification via learning discriminative CNNs, IEEE Trans. Geosci. Remote Sens, 56, pp. 2811-2821, (2018); Han K., Wang Y., Tian Q., Guo J., Xu C., Xu C., Ghostnet: More features from cheap operations, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1580-1589; Woo S., Park J., Lee J.Y., Kweon I.S., Cbam: Convolutional block attention module, Proceedings of the European Conference on Computer Vision (ECCV), pp. 3-19; Li M., Zhai Y.M., Luo Y.W., Ge P.F., Ren C.X., Enhanced transport distance for unsupervised domain adaptation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13936-13944; Liu J.J., Hou Q., Cheng M.M., Feng J., Jiang J., A simple pooling-based design for real-time salient object detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3917-3926; Tan M., Pang R., Le Q.V., Efficientdet: Scalable and efficient object detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10781-10790; Zamboni P., Junior J.M., Silva J., Miyoshi G.T., Matsubara E.T., Nogueira K., Goncalves W.N., Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images, Remote Sens, 13, (2021)","Z. Yang; School of Engineering, South China Agricultural University, Guangzhou, 510642, China; email: yangzhou@scau.edu.cn","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137771388"
"Osio A.A.; Le H.-A.; Ayugi S.; Onyango F.; Odwe P.; Lefevre S.","Osio, A.A. (57218827748); Le, H.-A. (55440782700); Ayugi, S. (57668178700); Onyango, F. (57667520300); Odwe, P. (57215532945); Lefevre, S. (57203070803)","57218827748; 55440782700; 57668178700; 57667520300; 57215532945; 57203070803","Detection of degraded acacia tree species using deep neural networks on uav drone imagery","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","455","462","7","0","10.5194/isprs-Annals-V-3-2022-455-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132009450&doi=10.5194%2fisprs-Annals-V-3-2022-455-2022&partnerID=40&md5=d080e12723f4ffe04798691fc5ebcd5a","Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; IRISA, Universite Bretagne Sud (UBS), Vannes, France","Osio A.A., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Le H.-A., IRISA, Universite Bretagne Sud (UBS), Vannes, France; Ayugi S., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Onyango F., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Odwe P., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Lefevre S., IRISA, Universite Bretagne Sud (UBS), Vannes, France","Deep-learning-based image classification and object detection has been applied successfully to tree monitoring. However, studies of tree crowns and fallen trees, especially on flood inundated areas, remain largely unexplored. Detection of degraded tree trunks on natural environments such as water, mudflats, and natural vegetated areas is challenging due to the mixed colour image backgrounds. In this paper, Unmanned Aerial Vehicles (UAVs), or drones, with embedded RGB cameras were used to capture the fallen Acacia Xanthophloea trees from six designated plots around Lake Nakuru, Kenya. Motivated by the need to detect fallen trees around the lake, two well-established deep neural networks, i.e. Faster Region-based Convolution Neural Network (Faster R-CNN) and Retina-Net were used for fallen tree detection. A total of 7,590 annotations of three classes on 256×256 image patches were used for this study. Experimental results show the relevance of deep learning in this context, with Retina-Net model achieving 38.9% precision and 57.9% recall.  © Authors 2022.","Acacia degradation; Deep Learning; Object Detection; UAV","Aircraft detection; Antennas; Drones; Floods; Lakes; Object detection; Object recognition; Acacium degradation; Colour image; Deep learning; Fallen tree; Images classification; Mudflats; Natural environments; Objects detection; Tree crowns; Tree species; Deep neural networks","","","","","Kenya National Council for Science Technology & Innovation; Kenya Wildlife Services; Providence Health Care, PHC; National Research Fund, Kenya, NRF; Campus France","The authors acknowledge: Kenya National Research Fund (K-NRF) and Campus France through Pamoja PHC, Kenya National Council for Science Technology & Innovation (K-NACOSTI) and Kenya Wildlife Services for providing permit to enable Drone Surveys in Lake Nakuru.","Alon A. S., Festijo E. D., Juanico D. E. O., Tree detection using genus-specific retinanet from or-thophoto for segmentation access of airborne lidar data, 2019 IEEE 6th International Conference on Engineering Technologies and Applied Sciences (ICETAS), pp. 1-6, (2019); Ampatzidis Y., Partel V., Meyering B., Albrecht U., Citrus rootstock evaluation utilizing UAV-based remote sensing and artificial intelligence, Computers and Electronics in Agriculture, 164, (2019); Bisson P. A., Quinn T. P., Reeves G. H., Gregory S. V., Best management practices, cumulative effects, and long-Term trends in fish abundance in pacific northwest river systems, Watershed management, pp. 189-232, (1992); Cowden M. M., A study of the current range and habitat of fuzzy sandozi conks (Bridgeoporus nobilissimus) throughout Pacific Northwest forests, (2002); Dalponte M., Ene L. T., Gobakken T., Nasset E., Gianelle D., Predicting selected forest stand characteristics with multispectral ALS data, Remote Sensing, 10, 4, (2018); Davidson N., Dinesen L., Fennessy S., Finlayson C., Grillas P., Grobicki A., McInnes R., Stroud D., A review of the adequacy of reporting to the Ramsar Convention on change in the ecological character of wetlands, Marine and Freshwater Research, 71, 1, pp. 117-126, (2019); Galidaki G., Zianis D., Gitas I., Radoglou K., Karathanassi V., Tsakiri-Strati M., Woodhouse I., Mallinis G., Vegetation biomass estimation with remote sensing: focus on forest and other wooded land over the Mediterranean ecosystem, International Journal of Remote Sensing, 38, 7, pp. 1940-1966, (2017); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Gkioxari G., Girshick R., Malik J., Contextual action recognition with r*cnn, Proceedings of the IEEE International Conference on Computer Vvision, pp. 1080-1088, (2015); Gorelick N., Hancher M., Dixon M., Ilyushchenko S., Thau D., Moore R., Google Earth Engine: Planetary-scale geospatial analysis for everyone, Remote sensing of Environment, 202, pp. 18-27, (2017); Hagele M., Seegerer P., Lapuschkin S., Bockmayr M., Samek W., Klauschen F., Muller K.-R., Binder A., Resolving challenges in deep learning-based analyses of histopathological images using explanation methods, Scientific reports, 10, 1, pp. 1-12, (2020); Harmon M. E., Franklin J. F., Swanson F. J., Sollins P., Gregory S., Lattin J., Anderson N., Cline S., Aumen N., Sedell J., Et al., Ecology of coarse woody debris in temperate ecosystems, Advances in ecological research, 15, pp. 133-302, (1986); Iradukunda P., Sang J. K., Nyadawa M. O., Maina C.W., Sedimentation effect on the storage capacity in lake Nakuru, Kenya, Journal of Sustainable Research in Engineering, 5, 3, pp. 149-158, (2020); Jiang S., Yao W., Heurich M., Et al., Dead wood detection based on semantic segmentation of vhr aerial cir imagery using optimized fcn-densenet, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, pp. 127-133, (2019); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision (ICCV), (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C. L., Microsoft COCO: Common objects in context, European Conference on Computer Vision, pp. 740-755, (2014); Manzanera J. A., Garc?a-Abril A., Pascual C., Tejera R., Martin-Fernandez S., Tokola T., Valbuena R., Fusion of airborne LiDAR and multispectral sensors reveals synergic capabilities in forest structure characterization, GIScience & Remote Sensing, 53, 6, pp. 723-738, (2016); Mubea K., Menz G., Monitoring land-use change in Nakuru (Kenya) using multi-sensor satellite data, (2012); Naik P., Dalponte M., Bruzzone L., Prediction of Forest Aboveground Biomass Using Multitemporal Multispectral Remote Sensing Data, Remote Sensing, 13, 7, (2021); Ngweno C. C., Mwasi S. M., Kairu J. K., Distribution, density and impact of invasive plants in Lake Nakuru National Park, Kenya, African Journal of Ecology, 48, 4, pp. 905-913, (2010); Norden B., Gotmark F., Ryberg M., Paltto H., Allmer J., Partial cutting reduces species richness of fungi on woody debris in oak-rich forests, Canadian Journal of Forest Research, 38, 7, pp. 1807-1816, (2008); Nzimande N., Mutanga O., Kiala Z., Sibanda M., Mapping the spatial distribution of the yellowwood tree (Podocarpus henkelii) in the Weza-Ngele forest using the newly launched Sentinel-2 multispectral imager data, South African Geographical Journal, 103, 2, pp. 204-222, (2021); Odada E., Raini J., Ndetei R., Experiences and lessons learned brief, Lake Nakuru, (2004); Osio A., Lefevre S., Object-Based Change Detection on Acacia Xanthophloea Species Degradation Along Lake Nakuru Riparian Reserve, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 43, pp. 347-352, (2021); Osio A., Lefevre S., Ogao P., Ayugi S., Obiabased monitoring of riparian vegetation applied to the identification of degraded acacia xanthophloea along lake nakuru, kenya, GEOBIA 2018-From pixels to ecosystems and global sustainability, pp. 18-22, (2018); Osio A., Pham M., Lefevre S., Spatial Processing of Sentinel Imagery for Monitoring of Acacia Forest Degradation in Lake Nakuru Riparian Reserve, ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, 3, pp. 525-532, (2020); Phantom D., RTK User Manual v1. 4, (2018); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-Time object detection with region proposal networks, Advances in Neural Information Processing Systems, 28, pp. 91-99, (2015); Santos A. A. d., Marcato Junior J., Araujo M. S., Di Martini D. R., Tetila E. C., Siqueira H. L., Aoki C., Eltner A., Matsubara E. T., Pistori H., Et al., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVs, Sensors, 19, 16, (2019); She X., Zhang L., Cen Y., Wu T., Huang C., Baig M. H. A., Comparison of the continuity of vegetation indices derived from Landsat 8 OLI and Landsat 7 ETM+ data among different vegetation types, Remote Sensing, 7, 10, pp. 13485-13506, (2015); Swanson F., Franklin J., Promoting the Science of Ecology, Ecological Applications, 2, 3, pp. 262-274, (1992); Thiel C., Mueller M. M., Epple L., Thau C., Hese S., Voltersen M., Henkel A., UAS Imagery-Based Mapping of Coarse Wood Debris in a Natural Deciduous Forest in Central Germany (Hainich National Park), Remote Sensing, 12, 20, (2020); Torres P., Rodes-Blanco M., Viana-Soto A., Nieto H., Garcia M., The Role of Remote Sensing for the Assessment and Monitoring of Forest Health: A Systematic Evidence Synthesis, Forests, 12, 8, (2021); Uijlings J., van de Sande K., Gevers T., Selective Search for Object Recognition, International Journal of Computer Vision, 104, pp. 154-171, (2013); Vareschi E., Jacobs J., The ecology of Lake Nakuru, Oecologia, 65, 3, pp. 412-424, (1985); Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., Detectron2, (2019); Zhang W., Liljedahl A. K., Kanevskiy M., Epstein H. E., Jones B. M., Jorgenson M. T., Kent K., Transferability of the deep learning mask R-CNN model for automated mapping of ice-wedge polygons in highresolution satellite and UAV images, Remote Sensing, 12, 7, (2020)","A.A. Osio; Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; email: osio@univ-ubs.fr","Jiang J.; Shaker A.; Zhang H.","Copernicus GmbH","","2022 24th ISPRS Congress on Imaging Today, Foreseeing Tomorrow, Commission III","6 June 2022 through 11 June 2022","Nice","179841","21949042","","","","English","ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132009450"
"Schmohl S.; Vallejo A.N.; Soergel U.","Schmohl, Stefan (57195935005); Vallejo, Alejandra Narváez (57487040500); Soergel, Uwe (55955024200)","57195935005; 57487040500; 55955024200","Individual Tree Detection in Urban ALS Point Clouds with 3D Convolutional Networks","2022","Remote Sensing","14","6","1317","","","","5","10.3390/rs14061317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126285066&doi=10.3390%2frs14061317&partnerID=40&md5=b32edd827443332c9606b0d9fa0ecdb8","Institute for Photogrammetry, University of Stuttgart, Stuttgart, 70174, Germany; Institute of Landscape Planning and Ecology, University of Stuttgart, Stuttgart, 70174, Germany","Schmohl S., Institute for Photogrammetry, University of Stuttgart, Stuttgart, 70174, Germany; Vallejo A.N., Institute of Landscape Planning and Ecology, University of Stuttgart, Stuttgart, 70174, Germany; Soergel U., Institute for Photogrammetry, University of Stuttgart, Stuttgart, 70174, Germany","Since trees are a vital part of urban green infrastructure, automatic mapping of individual urban trees is becoming increasingly important for city management and planning. Although deep-learning-based object detection networks are the state-of-the-art in computer vision, their adaptation to individual tree detection in urban areas has scarcely been studied. Some existing works have employed 2D object detection networks for this purpose. However, these have used three-dimensional information only in the form of projected feature maps. In contrast, we exploited the full 3D potential of airborne laser scanning (ALS) point clouds by using a 3D neural network for individual tree detection. Specifically, a sparse convolutional network was used for 3D feature extraction, feeding both semantic segmentation and circular object detection outputs, which were combined for further increased accuracy. We demonstrate the capability of our approach on an urban topographic ALS point cloud with 10,864 hand-labeled ground truth trees. Our method achieved an average precision of 83% regarding the common 0.5 intersection over union criterion. Eighty-five percent of the stems were found correctly with a precision of eighty-eight percent, while tree area was covered by the individual tree detections with an F1 accuracy of ninety-two percent. Thereby, we outperformed traditional delineation baselines and recent detection networks. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Airborne laser scanning; Deep learning; Object detection; Sparse convolutional network; Vegetation","Convolution; Convolutional neural networks; Deep learning; Feature extraction; Laser applications; Object recognition; Semantic Segmentation; Semantics; Urban planning; Airborne Laser scanning; Automatic mapping; City management; Convolutional networks; Deep learning; Green infrastructure; Individual tree detections; Laser scanning point clouds; Sparse convolutional network; Urban trees; Object detection","","","","","Baden-Württemberg; Cemeteries and Forest; LGL; Landesamt für Geoinformation und Landentwicklung; Office for Parks; State Office for Geoinformation and Land Development; Stuttgart City Surveying Office; European Space Agency, ESA, (49634)","The authors would like to show their gratitude to the State Office for Geoinformation and Land Development (Landesamt für Geoinformation und Landentwicklung (LGL)), Baden-Württemberg, for providing the ALS point clouds covering the city of Stuttgart. Furthermore, we would like to thank both the Stuttgart City Surveying Office (Stadtmessungsamt) for providing the orthophoto and Stuttgart’s Office for Parks, Cemeteries and Forest (Garten-, Friedhofsund Forstamt) for providing the municipal tree cadaster. AIRBUS Defence and Space Pléiades-1 satellite images were accessed as an ESA Third Party Mission within the ESA TPM project research id 49634.","Nowak D.J., Crane D.E., Stevens J.C., Air pollution removal by urban trees and shrubs in the United States, Urban For. Urban Green, 4, pp. 115-123, (2006); Weng Q., Lu D., Schubring J., Estimation of land surface temperature–vegetation abundance relationship for urban heat island studies, Remote Sens. Environ, 89, pp. 467-483, (2004); Chen X.L., Zhao H.M., Li P.X., Yin Z.Y., Remote sensing image-based analysis of the relationship between urban heat island and land use/cover changes, Remote Sens. Environ, 104, pp. 133-146, (2006); Kurn D.M., Bretz S.E., Huang B., Akbari H., The Potential for Reducing Urban Air Temperatures and Energy Consumption through Vegetative Cooling, (1994); Huang Y.J., Akbari H., Taha H., The wind-shielding and shading effects of trees on residential heating and cooling requirements, Proceedings of the ASHRAE Winter Conference, pp. 11-14, (1990); McPherson E.G., Simpson J.R., Potential energy savings in buildings by an urban tree planting programme in California, Urban For. Urban Green, 2, pp. 73-86, (2003); Pesola L., Cheng X., Sanesi G., Colangelo G., Elia M., Lafortezza R., Linking above-ground biomass and biodiversity to stand development in urban forest areas: A case study in Northern Italy, Landsc. Urban Plan, 157, pp. 90-97, (2017); Nielsen A.B., Ostberg J., Delshammar T., Review of urban tree inventory methods used to collect data at single-tree level, Arboric. Urban For, 40, pp. 96-111, (2014); Bardekjian A., Kenney A.R.M., Trends in Canada’s Urban Forests, Trees Canada—Arbres Canada and Canadian Urban Forest Network—Réseau Canadien de la Floret Urbaine, (2016); Hauer R.J., Peterson W.D., Municipal Tree Care and Management in the United States: A 2014 Urban & Community Forestry Census of Tree Activities, (2016); Ostberg J., Wistrom B., Randrup T.B., The state and use of municipal tree inventories in Swedish municipalities—Results from a national survey, Urban Ecosyst, 21, pp. 467-477, (2018); McPherson E.G., Structure and sustainability of Sacramento’s urban forest, J. Arboric, 24, pp. 174-190, (1998); Cameron R.W., Blanusa T., Taylor J.E., Salisbury A., Halstead A.J., Henricot B., Thompson K., The domestic garden—Its contribution to urban green infrastructure, Urban For. Urban Green, 11, pp. 129-137, (2012); Kelly M., Urban trees and the green infrastructure agenda, Trees, People and the Built Environment, Proceedings of the Urban Trees Research Conference, pp. 166-180, (2011); Li X., Chen W.Y., Sanesi G., Lafortezza R., Remote Sensing in Urban Forestry: Recent Applications and Future Directions, Remote Sens, 11, (2019); Casalegno S., Anderson K., Hancock S., Gaston K.J., Improving models of urban greenspace: From vegetation surface cover to volumetric survey, using waveform laser scanning, Methods Ecol. Evol, 8, pp. 1443-1452, (2017); Strimbu V.F., Strimbu B.M., A graph-based segmentation algorithm for tree crown extraction using airborne LiDAR data, ISPRS J. Photogramm. Remote Sens, 104, pp. 30-43, (2015); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, ISPRS J. Photogramm. Remote Sens, 173, pp. 24-49, (2021); Schmohl S., Kolle M., Frolow R., Soergel U., Towards Urban Tree Recognition in Airborne Point Clouds with Deep 3D Single-Shot Detectors, Pattern Recognition. ICPR International Workshops and Challenges, pp. 521-535, (2021); Kaartinen H., Hyyppa J., Yu X., Vastaranta M., Hyyppa H., Kukko A., Holopainen M., Heipke C., Hirschmugl M., Morsdorf F., Et al., An International Comparison of Individual Tree Detection and Extraction Using Airborne Laser Scanning, Remote Sens, 4, pp. 950-974, (2012); Jakubowski M.K., Li W., Guo Q., Kelly M., Delineating Individual Trees from Lidar Data: A Comparison of Vector-and Raster-based Segmentation Approaches, Remote Sens, 5, pp. 4163-4186, (2013); Eysn L., Hollaus M., Lindberg E., Berger F., Monnet J.M., Dalponte M., Kobal M., Pellegrini M., Lingua E., Mongus D., Et al., A benchmark of lidar-based single tree detection methods using heterogeneous forest data from the alpine space, Forests, 6, pp. 1721-1747, (2015); Wang Y., Hyyppa J., Liang X., Kaartinen H., Yu X., Lindberg E., Holmgren J., Qin Y., Mallet C., Ferraz A., Et al., International Benchmarking of the Individual Tree Detection Methods for Modeling 3-D Canopy Structure for Silviculture and Forest Ecology Using Airborne Laser Scanning, IEEE Trans. Geosci. Remote Sens, 54, pp. 5011-5027, (2016); Hyyppa J., Kelle O., Lehikoinen M., Inkinen M., A segmentation-based method to retrieve stem volume estimates from 3-D tree height models produced by laser scanners, IEEE Trans. Geosci. Remote Sens, 39, pp. 969-975, (2001); Hirschmugl M., Ofner M., Raggam J., Schardt M., Single tree detection in very high resolution remote sensing data, Remote Sens. Environ, 110, pp. 533-544, (2007); Dalponte M., Coomes D.A., Tree-centric mapping of forest carbon density from airborne laser scanning and hyperspectral data, Methods Ecol. Evol, 7, pp. 1236-1245, (2016); Pyysalo U., Hyyppa H., Reconstructing Tree Crowns from Laser Scanner Data for Feature Extraction, Int. Arch. Photogramm. Remote Sens, 34, pp. 218-221, (2002); Koch B., Heyder U., Weinacker H., Detection of Individual Tree Crowns in Airborne Lidar Data, Photogramm. Eng. Remote Sens, 72, pp. 357-363, (2006); Zhao K., Popescu S., Hierarchical Watershed Segmentation of Canopy Height Model for Multi-Scale Forest Inventory, ISPRS Workshop Laser Scanning, 3, pp. 436-441, (2007); Persson A., Holmgren J., Soderman U., Detecting and Measuring Individual Trees Using an Airborne Laser Scanner, Photogramm. Eng. Remote Sens, 68, pp. 925-932, (2002); Reitberger J., Heurich M., Krzystek P., Stilla U., Single tree detection in forest areas with high-density LIDAR data, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 36, pp. 139-144, (2007); Li W., Guo Q., Jakubowski M.K., Kelly M., A New Method for Segmenting Individual Trees from the Lidar Point Cloud, Photogramm. Eng. Remote Sens, 78, pp. 75-84, (2012); Lu X., Guo Q., Li W., Flanagan J., A bottom-up approach to segment individual deciduous trees using leaf-off lidar point cloud data, ISPRS J. Photogramm. Remote Sens, 94, pp. 1-12, (2014); Ferraz A., Bretar F., Jacquemoud S., Goncalves G., Pereira L., Tome M., Soares P., 3-D mapping of a multi-layered Mediterranean forest using ALS data, Remote Sens. Environ, 121, pp. 210-223, (2012); Dai W., Yang B., Dong Z., Shaker A., A new method for 3D individual tree extraction using multispectral airborne LiDAR point clouds, ISPRS J. Photogramm. Remote Sens, 144, pp. 400-411, (2018); Xiao W., Zaforemska A., Smigaj M., Wang Y., Gaulton R., Mean Shift Segmentation Assessment for Individual Forest Tree Delineation from Airborne Lidar Data, Remote Sens, 11, (2019); Pollock R.J., Model-based approach to automatically locating tree crowns in high spatial resolution images, Image and Signal Processing for Remote Sensing, 2315, pp. 526-537, (1994); Tittmann P., Shafii S., Hartsough B.R., Hamann B., Tree Detection and Delineation from LiDAR point clouds using RANSAC, Proceedings of SilviLaser, 11th International Conference on LiDAR Applications for Assessing Forest Ecosystems, (2011); Lindberg E., Eysn L., Hollaus M., Holmgren J., Pfeifer N., Delineation of Tree Crowns and Tree Species Classification From Full-Waveform Airborne Laser Scanning Data Using 3-D Ellipsoidal Clustering, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 7, pp. 3174-3181, (2014); Hadas E., Kolle M., Karpina M., Borkowski A., Identification of Peach Tree Trunks from Laser Scanning Data obtained with small Unmanned Aerial System, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 2, pp. 735-740, (2020); Reitberger J., Schnorr C., Krzystek P., Stilla U., 3D segmentation of single trees exploiting full waveform LIDAR data, ISPRS J. Photogramm. Remote Sens, 64, pp. 561-574, (2009); Wolf B.M., Heipke C., Automatic extraction and delineation of single trees from remote sensing data, Mach. Vis. Appl, 18, pp. 317-330, (2007); Iovan C., Boldo D., Cord M., Detection, Characterization, and Modeling Vegetation in Urban Areas From High-Resolution Aerial Imagery, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 1, pp. 206-213, (2008); Yang L., Wu X., Praun E., Ma X., Tree Detection from Aerial Imagery, GIS ’09, Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 131-137, (2009); Zhang C., Zhou Y., Qiu F., Individual Tree Segmentation from LiDAR Point Clouds for Urban Forest Inventory, Remote Sens, 7, pp. 7892-7913, (2015); Bulatov D., Wayand I., Schilling H., Automatic Tree-Crown Detection in Challenging Scenarios, ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 41, pp. 575-582, (2016); Liew S.C., Huang X., Lin E.S., Shi C., Yee A.T.K., Tandon A., Integration of Tree Database Derived from Satellite Imagery and LiDAR Point Cloud Data, ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, XLII-4, pp. 105-111, (2018); Man Q., Dong P., Yang X., Wu Q., Han R., Automatic Extraction of Grasses and Individual Trees in Urban Areas Based on Airborne Hyperspectral and LiDAR Data, Remote Sens, 12, (2020); Persson A., Extraction of Individual Trees Using Laser Radar Data, (2001); Reitberger J., 3D Segmentierung von Einzelbäumen und Baumartenklasifikation aus Daten Flugzeuggetragener Full Waveform Laserscanner, (2010); Hofle B., Hollaus M., Hagenauer J., Urban vegetation detection using radiometrically calibrated small-footprint full-waveform airborne LiDAR data, ISPRS J. Photogramm. Remote Sens, 67, pp. 134-147, (2012); Weinmann M., Weinmann M., Mallet C., Bredif M., A Classification-Segmentation Framework for the Detection of Individual Trees in Dense MMS Point Cloud Data Acquired in Urban Areas, Remote Sens, 9, (2017); Wu B., Yu B., Yue W., Shu S., Tan W., Hu C., Huang Y., Wu J., Liu H., A voxel-based method for automated identification and morphological parameters estimation of individual street trees from mobile laser scanning data, Remote Sens, 5, pp. 584-611, (2013); Gorte B., Oude Elberink S., Sirmacek B., Wang J., IQPC 2015 Track: Tree Separation and Classification in Mobile Mapping LiDAR Data, ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 40, pp. 607-612, (2015); Lindenbergh R.C., Berthold D., Sirmacek B., Herrero-Huerta M., Wang J., Ebersbach D., Automated large scale parameter extraction of road-side trees sampled by a laser mobile mapping system, ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 40, pp. 589-594, (2015); Li L., Li D., Zhu H., Li Y., A dual growing method for the automatic extraction of individual trees from mobile laser scanning data, ISPRS J. Photogramm. Remote Sens, 120, pp. 37-52, (2016); Monnier F., Vallet B., Soheilian B., Trees Detection from Laser Point Clouds Acquired in Dense Urban Areas by a Mobile Mapping System, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, I-3, pp. 245-250, (2012); Xu S., Xu S., Ye N., Zhu F., Automatic extraction of street trees’ nonphotosynthetic components from MLS data, Int. J. Appl. Earth Obs. Geoinf, 69, pp. 64-77, (2018); Wu J., Yao W., Polewski P., Mapping Individual Tree Species and Vitality along Urban Road Corridors with LiDAR and Imaging Sensors: Point Density versus View Perspective, Remote Sens, 10, (2018); Xu Y., Sun Z., Hoegner L., Stilla U., Yao W., Instance Segmentation of Trees in Urban Areas from MLS Point Clouds Using Supervoxel Contexts and Graph-Based Optimization, Proceedings of the 2018 10th IAPR Workshop on Pattern Recognition in Remote Sensing (PRRS), pp. 1-5, (2018); Hirt P.R., Xu Y., Hoegner L., Stilla U., Change Detection of Urban Trees in MLS Point Clouds Using Occupancy Grids, PFG–J. Photogramm. Remote Sens. Geoinf. Sci, 89, pp. 301-318, (2021); Xie Y., Bao H., Shekhar S., Knight J., A Timber Framework for Mining Urban Tree Inventories Using Remote Sensing Datasets, Proceedings of the 2018 IEEE International Conference on Data Mining (ICDM), pp. 1344-1349, (2018); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens, 9, (2017); Freudenberg M., Nolke N., Agostini A., Urban K., Worgotter F., Kleinn C., Large Scale Palm Tree Detection in High Resolution Satellite Images Using U-Net, Remote Sens, 11, (2019); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S.E., Fu C., Berg A.C., SSD: Single Shot MultiBox Detector (v5), (2016); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6517-6525, (2017); Lin T., Goyal P., Girshick R., He K., Dollar P.; Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks, Remote Sens, 11, (2019); Weinstein B.G., Marconi S., Bohlman S.A., Zare A., White E.P., Cross-site learning in deep learning RGB tree crown detection, Ecol. Inform, 56, (2020); Culman M., Delalieux S., Van Tricht K., Individual Palm Tree Detection Using Deep Learning on RGB Imagery to Support Tree Inventory, Remote Sens, 12, (2020); Windrim L., Bryson M., Detection, Segmentation, and Model Fitting of Individual Tree Stems from Airborne Laser Scanning of Forests Using Deep Learning, Remote Sens, 12, (2020); Plesoianu A.I., Stupariu M.S., Sandric I., Patru-Stupariu I., Dragut L., Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model, Remote Sens, 12, (2020); Branson S., Wegner J.D., Hall D., Lang N., Schindler K., Perona P., From Google Maps to a fine-grained catalog of street trees, ISPRS J. Photogramm. Remote Sens, 135, pp. 13-30, (2018); Lumnitz S., Devisscher T., Mayaud J.R., Radic V., Coops N.C., Griess V.C., Mapping trees along urban street networks with deep learning and street-level imagery, ISPRS J. Photogramm. Remote Sens, 175, pp. 144-157, (2021); Qi C.R., Yi L., Su H., Guibas L.J., PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space, Advances in Neural Information Processing Systems 30, pp. 5105-5114, (2017); Graham B., van der Maaten L., Submanifold Sparse Convolutional Networks, (2017); Yan Y., Mao Y., Li B., SECOND: Sparsely Embedded Convolutional Detection, Sensors, 18, (2018); Choy C., Park J., Koltun V., Fully Convolutional Geometric Features, Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8957-8965, (2019); Li Y., Bu R., Sun M., Wu W., Di X., Chen B., PointCNN: Convolution On X-Transformed Points, Advances in Neural Information Processing Systems 31, pp. 820-830, (2018); Thomas H., Qi C.R., Deschaud J.E., Marcotegui B., Goulette F., Guibas L., KPConv: Flexible and Deformable Convolution for Point Clouds, Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6410-6419, (2019); Schmohl S., Soergel U., Submanifold Sparse Convolutional Networks for Semantic Segmentation of Large-Scale ALS Point Clouds, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 4, pp. 77-84, (2019); Winiwarter L., Mandlburger G., Schmohl S., Pfeifer N., Classification of ALS Point Clouds Using End-to-End Deep Learning, PFG—J. Photogramm. Remote Sens. Geoinf. Sci, 87, pp. 75-90, (2019); Varney N., Asari V.K., Graehling Q.; Lin Y., Vosselman G., Cao Y., Yang M.Y., Active and incremental learning for semantic ALS point cloud segmentation, ISPRS J. Photogramm. Remote Sens, 169, pp. 73-92, (2020); Li N., Kaehler O., Pfeifer N., A Comparison of Deep Learning Methods for Airborne LiDAR Point Clouds Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 6467-6486, (2021); Geiger A., Lenz P., Urtasun R., Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3354-3361, (2012); Caesar H., Bankiti V., Lang A.H., Vora S., Liong V.E., Xu Q., Krishnan A., Pan Y., Baldan G., Beijbom O.; Song S., Lichtenberg S.P., Xiao J., SUN RGB-D: A RGB-D scene understanding benchmark suite, Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 567-576, (2015); Chen X., Ma H., Wan J., Li B., Xia T., Multi-view 3D Object Detection Network for Autonomous Driving, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6526-6534, (2017); Ku J., Mozifian M., Lee J., Harakeh A., Waslander S.L., Joint 3D Proposal Generation and Object Detection from View Aggregation, Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1-8, (2018); Simon M., Milz S., Amende K., Gross H.M., Complex-YOLO: An Euler-Region-Proposal for Real-Time 3D Object Detection on Point Clouds, Proceedings of the Computer Vision—ECCV 2018 Workshops, 11129, pp. 197-209, (2019); Lang A.H., Vora S., Caesar H., Zhou L., Yang J., Beijbom O., PointPillars: Fast Encoders for Object Detection From Point Clouds, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12689-12697, (2019); Li B., 3D fully convolutional network for vehicle detection in point cloud, Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1513-1518, (2017); Zhou Y., Tuzel O., VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection, Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4490-4499, (2018); Hu P., Held D., Ramanan D., Learning to Optimally Segment Point Clouds, IEEE Robot. Autom. Lett, 5, pp. 875-882, (2020); Qi C.R., Litany O., He K., Guibas L., Deep Hough Voting for 3D Object Detection in Point Clouds, Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9276-9285, (2019); Shi S., Wang X., Li H., PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-779, (2019); Yang Z., Sun Y., Liu S., Shen X., Jia J., STD: Sparse-to-Dense 3D Object Detector for Point Cloud, Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1951-1960, (2019); Ren M., Pokrovsky A., Yang B., Urtasun R., SBNet: Sparse Blocks Network for Fast Inference, Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8711-8720, (2018); Shi S., Wang Z., Wang X., Li H., From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Partaggregation Network, (2019); Zhu B., Jiang Z., Zhou X., Li Z., Yu G., Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection, (2019); Wang D.Z., Posner I., Voting for Voting in Online Point Cloud Object Detection, Proceedings of the Robotics: Science and Systems, (2015); Graham B., Engelcke M., van der Maaten L., 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks, Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9224-9232, (2018); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional Networks for Biomedical Image Segmentation, Medical Image Computing and Computer-Assisted Intervention (MICCAI), 9351, pp. 234-241, (2015); Bennett S., openTrees.org [WWW Document], (2021); Weinstein B.G., Graves S.J., Marconi S., Singh A., Zare A., Stewart D., Bohlman S.A., White E.P., A benchmark dataset for canopy crown detection and delineation in co-registered airborne RGB, LiDAR and hyperspectral imagery from the National Ecological Observation Network, PLOS Comput. Biol, 17, (2021); Ioffe S., Szegedy C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Proceedings of the 32nd International Conference on Machine Learning, 37, pp. 448-456, (2015); Girshick R., Fast R-CNN, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1440-1448, (2015); Baumkontrollrichtlinie—Richtlinien für Baumkontrollen zur Überprüfung der Verkehrssicherheit, (2020); Walter V., Kolle M., Yin Y., Evaluation and Optimisation of Crowd-based Collection of Trees from 3D Point Clouds, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 5, pp. 49-56, (2020); Meyer F., Topographic distance and watershed lines, Signal Process, 38, pp. 113-125, (1994); Zevenbergen L.W., Thorne C.R., Quantitative analysis of land surface topography, Earth Surf. Process. Landforms, 12, pp. 47-56, (1987); Roussel J.R., Auty D., Coops N.C., Tompalski P., Goodbody T.R., Meador A.S., Bourdon J.F., de Boissieu F., Achim A., lidR: An R package for analysis of Airborne Laser Scanning (ALS) data, Remote Sens. Environ, 251, (2020); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Deng J., Dong W., Socher R., Li L., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, (2009); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature Pyramid Networks for Object Detection, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 936-944, (2017); Kolle M., Laupheimer D., Schmohl S., Haala N., Rottensteiner F., Wegner J.D., Ledoux H., The Hessigheim 3D (H3D) benchmark on semantic segmentation of high-resolution 3D point clouds and textured meshes from UAV LiDAR and Multi-View-Stereo, ISPRS Open J. Photogramm. Remote Sens, 1, (2021); Lian Y., Feng T., Zhou J., Jia M., Li A., Wu Z., Jiao L., Brown M., Hager G., Yokoya N., Et al., Large-Scale Semantic 3-D Reconstruction: Outcome of the 2019 IEEE GRSS Data Fusion Contest—Part B, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 1158-1170, (2021)","S. Schmohl; Institute for Photogrammetry, University of Stuttgart, Stuttgart, 70174, Germany; email: stefan.schmohl@ifp.uni-stuttgart.de","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85126285066"
"Palle R.R.; Boda R.","Palle, Rajashekar Reddy (57787136900); Boda, Ravi (57192543283)","57787136900; 57192543283","Automated image and video object detection based on hybrid heuristic-based U-net segmentation and faster region-convolutional neural network-enabled learning","2023","Multimedia Tools and Applications","82","3","","3459","3484","25","1","10.1007/s11042-022-13216-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133607800&doi=10.1007%2fs11042-022-13216-0&partnerID=40&md5=d03efdf205ae020ddb8d91e18a729eb2","Department of ECE, Koneru Lakshmaiah Educational Foundation (KLEF), Telangana, Hyderabad, India","Palle R.R., Department of ECE, Koneru Lakshmaiah Educational Foundation (KLEF), Telangana, Hyderabad, India; Boda R., Department of ECE, Koneru Lakshmaiah Educational Foundation (KLEF), Telangana, Hyderabad, India","Object detection is one of the major areas of computer vision, which adopts machine learning approaches in diverse contributions. Nowadays, the machine learning field has been directed through Deep Neural Networks (DNNs) that takes eminent features of progressions in data availability and computing power. In all the cases, the quality of images and videos are biased and noisy, and thus, the distributions of data are also considered as imbalanced and disturbed. Different techniques are developed for solving the abovementioned challenges, which are mostly considered based on deep learning and computer vision. Though, traditional algorithms constantly offer poor detection for dense and small objects and yet fail the detection of objects through random geometric transformations. One of the categories of deep learning called Convolutional Neural Network (CNN) is famous and well-matched method for image-related tasks, in which the network is trained for discovering the numerous features like colour differences, corners, and edges in the images and videos that are combined into more complex shapes. This proposal intends to develop improved object detection in images and videos with the advancements of deep learning models. The three main phases of the proposed object detection model are (a) pre-processing, (b) segmentation, and (c) detection. Once the pre-processing of the image is performed by median filtering approach, the adaptive U-Net segmentation is performed for the object segmentation using the newly proposed Sun Flower-Deer Hunting Optimization Algorithm (SF-DHOA). The maximization of segmentation accuracy and dice coefficient is considered as the main objective of the proposed segmentation. The hybrid meta-heuristic algorithm termed SF-DHOA is proposed with Sun Flower Optimization (SFO) and Deer Hunting Optimization Algorithm (DHOA), which is used for optimally tuning the U-Net by optimizing the encoder depth and the number of epoch. Further, the detection is performed by the modified Faster Region-Convolutional Neural Network (Faster-RCNN), in which the optimization of number of epoch is performed by hybrid SF-DHOA algorithm with the intention of minimizing the error and training loss function. The performance of the proposed algorithm is evaluated, and the proposed algorithm shows high improvement when compared to existing deep learning-based algorithms. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Adaptive U-net segmentation; Convolutional neural network; Deep learning; Modified faster region-convolutional neural network; Object detection; Sun flower-deer hunting optimization algorithm","Computer vision; Convolution; Deep neural networks; Edge detection; Heuristic algorithms; Image enhancement; Image segmentation; Learning algorithms; Learning systems; Object detection; Object recognition; Adaptive U-net segmentation; Convolutional neural network; Deep learning; Deer hunting; Modified fast region-convolutional neural network; Objects detection; Optimization algorithms; Pre-processing; Sun flower-deer hunting optimization algorithm; Convolutional neural networks","","","","","","","Azzam R., Kemouche M.S., Aouf N., Richardson M., Efficient visual object detection with spatially global gaussian mixture models and uncertainties, J Vis Commun Image Represent, 36, pp. 90-106, (2016); Bonyadi M.R., Michalewicz Z., Analysis of stability, local convergence, and transformation sensitivity of a variant of the particle swarm optimization algorithm, IEEE Trans Evol Comput, 20, 3, pp. 370-385, (2016); Bouwmans T., Traditional and recent approaches in background modeling for foreground detection: An overview, Comput Sci Rev, 11-12, pp. 31-66, (2014); Brammya G., Praveena S., Ninu Preetha N.S., Ramya R., Rajakumar B.R., Binu D Deer Hunting Optimization Algorithm: A New Nature-Inspired Meta-Heuristic Paradigm, (2019); Cao W., Yuan J., He Z., Zhang Z., He Z., Fast deep neural networks with knowledge guided training and predicted regions of interests for real-time video object detection, IEEE Access, 6, pp. 8990-8999, (2018); Cuevas C., Yanez E.M., Garcia N., Labeled dataset for integral evaluation of moving object detection algorithms: LASIESTA, Comput Vis Image Understand, 152, pp. 103-117, (2016); Dai J., Li Y., He K., Sun J., R-FCN: Object Detection via Region-Based Fully Convolutional Networks, (2016); Fan M., Li Y., Zheng S., Peng W., Tang W., Li L., Computer-aided detection of mass in digital breast tomosynthesis using a faster region-based convolutional neural network, Methods, 166, pp. 103-111, (2019); Felzenszwalb P.F., Girshick R.B., Mcallester D., Ramanan D., Object detection with discriminatively trained part based models, IEEE Trans Pattern Anal Mach Intell, 32, 9, pp. 1-20, (2009); Gomes G.F., da Cunha S.S., Ancelotti A.C., A sunflower optimization (SFO) algorithm applied to damage identification on laminated composite plates, Eng Comput, 35, pp. 619-626, (2019); Goyette N., Jodoin P.-M., Porikli F., Konrad J., Ishwar P., A novel video dataset for change detection benchmarking, IEEE Trans Image Process, 23, 11, pp. 4663-4679, (2014); Hambarde P., Talbar S., Mahajan A., Chavan S., Thakur M., Sable N., Prostate lesion segmentation in MR images using radiomics based deeply supervised U-net, Biocybern Biomed Eng, 40, 4, pp. 1421-1435, (2020); Han J., Zhang D., Cheng G., Liu N., Xu D., Advanced deep-learning techniques for salient and category-specific object detection: a survey, IEEE Signal Process Mag, 35, 1, pp. 84-100, (2018); Hu W.-C., Chen C.-H., Chen T.-Y., Huang D.-Y., Wu Z.-C., Moving object detection and tracking from video captured by moving camera, J Vis Commun Image Represent, 30, pp. 164-180, (2015); Hu Q., Paisitkriangkrai S., Shen C., van den Hengel A., Porikli F., Fast detection of multiple objects in traffic scenes with a common detection framework, IEEE Trans Intell Transp Syst, 17, 4, pp. 1002-1014, (2016); Hu Z., Yang D., Zhang K., Chen Z., Object tracking in satellite videos based on convolutional regression network with appearance and motion features, IEEE J Sel Top Appl Earth Obs Remote Sens, 13, pp. 783-793, (2020); Huang L., Yan P., Li G., Wang Q., Lin L., Attention embedded Spatio-temporal network for video salient object detection, IEEE Access, 7, pp. 166203-166213, (2019); Kim J.-Y., Ha J.-E., Foreground Objects Detection Using a Fully Convolutional Network With a Background Model Image and Multiple Original Images, IEEE Access, 8, pp. 159864-159878, (2020); Kim J.H., Kim B., Roy P.P., Jeong D., Efficient Facial Expression Recognition Algorithm Based on Hierarchical Deep Neural Network Structure, IEEE Access, 7, pp. 41273-41285, (2019); Manne R., Kantheti S., Kantheti S., Classification of Skin cancer using deep learning,Convolutional Neural Networks -Opportunities and vulnerabilities, Int J Mod Trends Sci Technol, 6, 11, pp. 101-108, (2020); Marsaline Beno M., Valarmathi I.R., Swamy S.M., Rajakumar B.R., Threshold prediction for segmenting tumour from brain MRI scans, Int J Imaging Syst Technol, 24, 2, pp. 129-137, (2014); Murthy M.Y.B., Koteswararao A., Babu M.S., Adaptive fuzzy deformable fusion and optimized CNN with ensemble classification for automated brain tumor diagnosis, Biomed Eng Lett, (2021); Nirmala Sreedharan N.P., Ganesan B., Raveendran R., Sarala P., Dennis B., Boothalingam R., Grey Wolf optimisation-based feature selection and classification for facial emotion recognition, IET Biometrics, 7, 5, pp. 490-499, (2018); Patil P.W., Murala S., MSFgNet: a novel compact end-to-end deep network for moving object detection, IEEE Trans Intell Transp Syst, 20, 11, pp. 4066-4077, (2019); Reddy V., Sanderson C., Lovell B.C., Improved foreground detection via block-based classifier cascade with probabilistic decision integration, IEEE Trans Circuits Syst Video Technol, 23, 1, pp. 83-93, (2013); Rhee P.K., Erdenee E., Kyun S.D., Ahmed M.U., Jin S., Active and semi-supervised learning for object detection with imperfect data, Cogn Syst Res, 45, pp. 109-123, (2017); Rodriguez-Ramos A., Rodriguez-Vazquez J., Sampedro C., Campoy P., Adaptive Inattentional framework for video object detection with reward-conditional training, IEEE Access, 8, pp. 124451-124466, (2020); St-Charles P.-L., Bilodeau G.-A., Bergevin R., SuBSENSE:Auniversal change detection method with local adaptive sensitivity, IEEE Trans Image Process, 24, 1, pp. 359-373, (2015); Tsang S., Kao B., Yip K.Y., Ho W., Lee S.D., Decision trees for uncertain data, IEEE Trans Knowl Data Eng, 23, 1, pp. 64-78, (2011); Ucar A., Demir Y., Guzelis C., Object recognition and detection with deep learning for autonomous driving applications, Simulation, 93, 9, pp. 759-769, (2017); Unnisa N., Tatineni M., Adaptive deep learning strategy with Red Deer algorithm for Sparse Channel estimation and hybrid precoding in millimeter wave massive MIMO-OFDM systems, Wirel Pers Commun, 122, pp. 3019-3051, (2021); Varadarajan S., Miller P., Zhou H., Region-based mixture of gaussians modelling for foreground detection in dynamic scenes, Pattern Recogn, 48, 11, pp. 3488-3503, (2015); Wang S., Pan H., Zhang C., Tian Y., Rgb-d image-based detection of stairs, pedestrian crosswalks and traffic signs, J Vis Commun Image Represent, 25, 2, pp. 263-272, (2014); Wang K., Lin L., Yan X., Chen Z., Zhang D., Zhang L., Cost-Effective Object Detection: Active Sample Mining With Switchable Selection Criteria, IEEE Trans Neural Networks Learn Syst, PP, pp. 1-17, (2018); Wu J., Yang H., Linear regression-based efficient SVM learning for large-scale classification, IEEE Trans Neural Netw Learn Syst, 26, 10, pp. 2357-2369, (2015); Yousif H., Yuan J., Kays R., He Z., Object detection from dynamic scene using joint background modeling and fast deep learning classification, J Vis Commun Image Represent, 55, pp. 802-815, (2018); Yu H., Guo D., Yan Z., Fud L., Simmons J., Przybyla C.P., Wang S., Weakly supervised easy-to-hard learning for object detection in image sequences, Neurocomputing, 398, pp. 71-82, (2020); Zhang C., Kim J., Video object detection with two-path convolutional LSTM pyramid, IEEE Access, 8, pp. 151681-151691, (2020); Zhang Z., He Z., Cao G., Cao W., Animal detection from highly cluttered natural scenes using spatiotemporal object region proposals and patch verification, IEEE Trans Multimed, 18, 10, pp. 2079-2092, (2016); Zhao W., Ma W., Jiao L., Chen P., Yang S., Hou B., Multi-Scale Image Block-Level F-CNN for Remote Sensing Images Object Detection, IEEE Access, 7, pp. 43607-43621, (2019); Zhu Y., Huang C., An improved median filtering algorithm for image noise reduction, Phys Procedia, 25, pp. 609-616, (2012)","R.R. Palle; Department of ECE, Koneru Lakshmaiah Educational Foundation (KLEF), Hyderabad, Telangana, India; email: raju.sheker@gmail.com","","Springer","","","","","","13807501","","MTAPF","","English","Multimedia Tools Appl","Article","Final","","Scopus","2-s2.0-85133607800"
"Takimoto H.; Sato Y.; Nagano A.J.; Shimizu K.K.; Kanagawa A.","Takimoto, Hironori (7006013536); Sato, Yasuhiro (55874083800); Nagano, Atsushi J. (35305206700); Shimizu, Kentaro K. (35775802600); Kanagawa, Akihiro (7003471524)","7006013536; 55874083800; 35305206700; 35775802600; 7003471524","Using a two-stage convolutional neural network to rapidly identify tiny herbivorous beetles in the field","2021","Ecological Informatics","66","","101466","","","","8","10.1016/j.ecoinf.2021.101466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120689786&doi=10.1016%2fj.ecoinf.2021.101466&partnerID=40&md5=b6a338c99e2f519e9dafd4cc929e2c5d","Faculty of Computer Science and Systems Engineering, Okayama Prefectural University, Kuboki 111, Soja, 719-1197, Okayama, Japan; Research Institute for Food and Agriculture, Ryukoku University, Yokotani 1-5, Seta Oe-cho, Otsu, 520-2194, Shiga, Japan; Department of Evolutionary Biology and Environmental Studies, University of Zurich, Winterthurerstrasse 190, Zurich, 8057, Switzerland; Faculty of Agriculture, Ryukoku University, Yokotani 1-5, Seta Oe-cho, Otsu, 520-2194, Shiga, Japan; Institute for Advanced Biosciences, Keio University, Nipponkoku 403-1, Daihouji, Tsuruoka, 997-0017, Yamagata, Japan; Kihara Institute for Biological Research, Yokohama City University, Maioka 641-12, Yokohama, 244-08137, Kanagawa, Japan","Takimoto H., Faculty of Computer Science and Systems Engineering, Okayama Prefectural University, Kuboki 111, Soja, 719-1197, Okayama, Japan; Sato Y., Research Institute for Food and Agriculture, Ryukoku University, Yokotani 1-5, Seta Oe-cho, Otsu, 520-2194, Shiga, Japan, Department of Evolutionary Biology and Environmental Studies, University of Zurich, Winterthurerstrasse 190, Zurich, 8057, Switzerland; Nagano A.J., Faculty of Agriculture, Ryukoku University, Yokotani 1-5, Seta Oe-cho, Otsu, 520-2194, Shiga, Japan, Institute for Advanced Biosciences, Keio University, Nipponkoku 403-1, Daihouji, Tsuruoka, 997-0017, Yamagata, Japan; Shimizu K.K., Department of Evolutionary Biology and Environmental Studies, University of Zurich, Winterthurerstrasse 190, Zurich, 8057, Switzerland, Kihara Institute for Biological Research, Yokohama City University, Maioka 641-12, Yokohama, 244-08137, Kanagawa, Japan; Kanagawa A., Faculty of Computer Science and Systems Engineering, Okayama Prefectural University, Kuboki 111, Soja, 719-1197, Okayama, Japan","Recently, deep convolutional neural networks (CNN) have been adopted to help non-experts identify insect species from field images. However, the application of these methods on the rapid identification of tiny congeneric species moving across heterogeneous background remains difficult. To improve rapid and automatic identification in the field, we customized an existing CNN-based method for a field video involving two Phyllotreta beetles. We first performed data augmentation using transformations, syntheses, and random erasing of the original images. We then proposed a two-stage method for the detection and identification of small insects based on CNN, where YOLOv4 and EfficientNet were used as a detector and a classifier, respectively. Evaluation of the model revealed that one-step object detection by YOLOv4 alone was not precise (Precision = 0.55) when classifying two species of flea beetles and background objects. In contrast, the two-step CNNs improved the precision (Precision = 0.89) with moderate accuracy (F-measure = 0.55) and acceptable speed (ca. 5 frames per second for full HD images) of detection and identification of insect species in the field. Although real-time identification of tiny insects remains a challenge in the field, our method aids in improving small object detection on a heterogeneous background. © 2021 The Authors","Deep learning; Entomology; Fine-grained image classification; Herbivory; Small object detection","Alticini; Coleoptera; Hexapoda; Phyllotreta; accuracy assessment; artificial neural network; beetle; field method; herbivore; identification method","","","","","URPP; University Research Priority Project; Japan Society for the Promotion of Science, KAKEN, (20K15880); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, (31003A_182318); Japan Science and Technology Agency, JST; Core Research for Evolutional Science and Technology, CREST, (JPMJCR15O2, JPMJCR16O3); Universität Zürich, UZH; Precursory Research for Embryonic Science and Technology, PRESTO, (JPMJPR17Q4)","The authors would like to express their sincerest gratitude to H. Kuzuhara and T. Kano for the development of analytical pipelines; K. Takeda for the insect annotation; and to all members of the Shimizu group for helping with the set-up of plants in the field. This study was supported by the Japan Science and Technology Agency ( JST ) through the PRESTO (No. JPMJPR17Q4 to YS) and CREST (No. JPMJCR15O2 and JPMJCR16O3 to AJN and KKS) projects, the Japan Society for the Promotion Science ( JSPS ) KAKENHI (No. 20K15880 to YS), the Swiss National Science Foundation (No. 31003A_182318 to KKS), and the University Research Priority Project ( URPP ) Global Change and Biodiversity of the University of Zurich to KKS.","Ahuja I., Rohloff J., Bones A.M., Defence mechanisms of Brassicaceae: implications for plant-insect interactions and potential for integrated pest management: a review, Agron. Sustain. Dev., 30, pp. 311-348, (2010); Almryad A.S., Kutucu H., Automatic identification for field butterflies by convolutional neural networks, Eng. Sci. Technol. Int. J., 23, pp. 189-195, (2020); Bochkovskiy A., Wang C.Y., Liao H.Y.M., YOLOv4: optimal speed and accuracy of object detection, (2020); Chollet F., Xception: deep learning with depthwise separable convolutions, (2016); Dai J., Li Y., He K., Sun J., R-FCN: object detection via region-based fully convolutional networks, Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 379-387, (2016); Deng L., Wang Y., Han Z., Yu R., Research on insect pest image detection and recognition based on bio-inspired methods, Biosyst. Eng., 169, pp. 139-148, (2018); Hansen O.L., Svenning J.C., Olsen K., Dupont S., Garner B.H., Iosifidis A., Price B.W., Hoye T.T., Species-level image classification with convolutional neural network enables insect identification from habitus images, Ecol. Evol., 10, pp. 737-747, (2020); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, (2015); He Y., Zeng H., Fan Y., Ji S., Wu J., Application of deep learning in integrated pest management: a real-time system for detection and diagnosis of oilseed rape pests, Mobile Inf. Syst., 2019, (2019); Hogeweg L., Zeegers T., Katramados I., Jongejans E., Smart insect cameras, Biodivers. Inf. Sci. Stand., 3, (2019); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., Mobilenets: efficient convolutional neural networks for mobile vision applications, (2017); Hoye T.T., Arje J., Bjerge K., Hansen O.L., Iosifidis A., Leese F., Mann H.M., Meissner K., Melvad C., Raitoharju J., Deep learning and computer vision will transform entomology, Proc. Natl. Acad. Sci. USA, 118, 2, (2021); Hu J., Shen L., Albanie S., Sun G., Wu E., Squeeze-and-Excitation Networks, (2017); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Lim S., Kim S., Kim D., Performance effect analysis for insect classification using convolutional neural network, 2017 7th IEEE International Conference on Control System, Computing and Engineering (ICCSCE), pp. 210-215, (2017); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietika inen M., Deep learning for generic object detection: a survey, Int. J. Comput. Vis., 128, pp. 261-318, (2019); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S.E., Fu C.Y., Berg A.C., SSD: Single shot multibox detector, ECCV (1), pp. 21-37, (2016); Martineau M., Conte D., Raveaux R., Arnault I., Munier D., Venturini G., A survey on image-based insect classification, Pattern Recognit., 65, pp. 273-284, (2017); Matsubara S., Sugiura S., A technique for multi-generational rearing of Phaedon brassicae (Coleoptera: Chrysomelidae), Entomol. News, 129, pp. 431-434, (2021); Mayo M., Watson A.T., Automatic species identification of live moths, Knowl.-Based Syst., 20, pp. 195-202, (2007); Norouzzadeh M.S., Nguyen A., Kosmala M., Swanson A., Palmer M.S., Packer C., Clune J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proc. Natl. Acad. Sci. USA, 115, pp. E5716-E5725, (2018); Ovchinnikova K., James M.A., Mendo T., Dawkins M., Crall J., Boswarva K., Exploring the potential to use low cost imaging and an open source convolutional neural network detector to support stock assessment of the king scallop (Pecten maximus), Ecol. Inf., 62, (2021); Padilla R., Passos W.L., Dias T.L.B., Netto S.L., da Silva E.A.B., A comparative analysis of object detection metrics with a companion open-source toolkit, Electronics, 10, (2021); Park J., Kim D.I., Choi B., Kang W., Kwon H.W., Classification and morphological analysis of vector mosquitoes using deep convolutional neural networks, Sci. Rep., 10, pp. 1-12, (2020); Preti M., Verheggen F., Angeli S., Insect pest monitoring with camera-equipped traps: strengths and limitations, J. Pest Sci., pp. 1-15, (2020); Rani R.U., Amsini P., Pest identification in leaf images using SVM classifier, Int. J. Comput. Intell. Inform., 6, pp. 248-260, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1137-1149, (2017); Rustia D.J.A., Lin C.E., yung Chung J., A real-time multi-class insect pest identification method using cascaded convolutional neural networks, Proceedings of 9th International Symposium on Machinery and Mechatronics for Agriculture and Biosystems Engineering, (2018); Samanta R.K., Ghosh I., Tea insect pests classification based on artificial neural networks, Int. J. Comput. Eng. Sci., 2, pp. 1-13, (2012); Sato Y., Shimizu-Inatsugi R., Yamazaki M., Shimizu K.K., Nagano A.J., Plant trichomes and a single gene GLABRA1 contribute to insect community composition on field-grown Arabidopsis thaliana, BMC Plant Biol., 19, (2019); Shen Y., Zhou H., Li J., Jian F., Jayas D.S., Detection of stored-grain insects using deep learning, Comput. Electron. Agric., 145, pp. 319-325, (2018); Shimizu-Inatsugi R., Milosavljevic S., Shimizu K.K., Schaepman-Strub G., Tanoi K., Sato Y., Metal accumulation and its effect on leaf herbivory in an allopolyploid species Arabidopsis kamchatica inherited from a diploid hyperaccumulator A. halleri, Plant Species Biol., 36, pp. 208-217, (2021); Shorten C., Khoshgoftaar T., A survey on image data augmentation for deep learning, J. Big Data, 6, pp. 1-48, (2019); Silvertown J., Harvey M., Greenwood R., Dodd M., Rosewell J., Rebelo T., Ansine J., McConway K., Crowdsourcing the identification of organisms: a case-study of iSpot, ZooKeys, 125, (2015); Tan M., Le Q., EfficientNet: rethinking model scaling for convolutional neural networks, Proceedings of the 36th International Conference on Machine Learning, pp. 6105-6114, (2019); Tresson P., Carval D., Tixier P., Puech W., Hierarchical classification of very small objects: application to the detection of arthropod species, IEEE Access, 9, pp. 63925-63932, (2021); Tresson P., Tixier P., Puech W., Bagny Beilhe L., Roudine S., Pages C., Carval D., CORIGAN: assessing multiple species and interactions within images, Methods Ecol. Evol., 10, pp. 1888-1893, (2019); Valan M., Makonyi K., Maki A., Vondracek D., Ronquist F., Automated taxonomic identification of insects with expert-level accuracy using effective feature transfer from convolutional networks, Syst. Biol., 68, pp. 876-895, (2019); Van Etten A., You only look twice: rapid multi-scale object detection in satellite imagery, (2018); Van Horn G., Mac Aodha O., Song Y., Cui Y., Sun C., Shepard A., Adam H., Perona P., Belongie S., The iNaturalist species classification and detection dataset, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8769-8778, (2018); Venugoban K., Ramanan A., Image classification of paddy field insect pests using gradient-based features, Int. J. Mach. Learn. Comput., pp. 1-5, (2014); Wang J., Lin C., Ji L., Liang A., A new automatic identification system of insect images at the order level, Knowl.-Based Syst., 33, pp. 102-110, (2012); Willi M., Pitman R.T., Cardoso A.W., Locke C., Swanson A., Boyer A., Veldthuis M., Fortson L., Identifying animal species in camera trap images using deep learning and citizen science, Methods Ecol. Evol., 10, pp. 80-91, (2019); Xia D., Chen P., Wang B., Zhang J., Xie C., Insect detection and classification based on an improved convolutional neural network, Sensors, 18, (2018); Xie C., Wang R., Zhang J., Chen P., Dong W., Li R., Chen T., Chen H., Multi-level learning features for automatic classification of field crop pests, Comput. Electr. Agric., 152, pp. 233-241, (2018); Xie C., Zhang J., Li R., Li J., Hong P., Xia J., Chen P., Automatic classification for field crop insects via multiple-task sparse representation and multiple-kernel learning, Comput. Electron. Agric., 119, pp. 123-132, (2015); Yalcin H., Vision based automatic inspection of insects in pheromone traps, 2015 Fourth International Conference on Agro-Geoinformatics (Agro-geoinformatics), pp. 333-338, (2015); Yamashita R., Nishio M., Do R., Togashi K., Convolutional neural networks: an overview and application in radiology, Insights Imaging, 9, pp. 611-629, (2018); Zhong Z., Zheng L., Kang G., Li S., Yang Y., Random erasing data augmentation, Proceedings of the AAAI Conference on Artificial Intelligence, 34, pp. 13001-13008, (2020)","H. Takimoto; Faculty of Computer Science and Systems Engineering, Okayama Prefectural University, Soja, Kuboki 111, 719-1197, Japan; email: takimoto@c.oka-pu.ac.jp","","Elsevier B.V.","","","","","","15749541","","","","English","Ecol. Informatics","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85120689786"
"Bai J.; Ren J.; Yang Y.; Xiao Z.; Yu W.; Havyarimana V.; Jiao L.","Bai, Jing (57193646200); Ren, Junjie (57326647900); Yang, Yujia (57326085700); Xiao, Zhu (36912987800); Yu, Wentao (57218160020); Havyarimana, Vincent (36666310200); Jiao, Licheng (7102491544)","57193646200; 57326647900; 57326085700; 36912987800; 57218160020; 36666310200; 7102491544","Object Detection in Large-Scale Remote-Sensing Images Based on Time-Frequency Analysis and Feature Optimization","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","8","10.1109/TGRS.2021.3119344","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118675184&doi=10.1109%2fTGRS.2021.3119344&partnerID=40&md5=c060981557ac3d42ecfb90b6008c4c9c","Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Hunan University, College of Computer Science and Electronic Engineering, Changsha, 410082, China; Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Hunan University, College of Computer Science and Electronic Engineering, Changsha, 410082, China; Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China","Bai J., Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Ren J., Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Yang Y., Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Xiao Z., Hunan University, College of Computer Science and Electronic Engineering, Changsha, 410082, China; Yu W., Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China; Havyarimana V., Hunan University, College of Computer Science and Electronic Engineering, Changsha, 410082, China; Jiao L., Xidian University, Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the School of Artificial Intelligence, Xi’an, 710071, China","Recently, optical remote-sensing images have been steadily growing in size, as they contain massive data and complex backgrounds. This trend presents several problems for object detection, for example, increased computation time and memory consumption and more false positives due to the complex backgrounds of large-scale images. Inspired by deep neural networks combined with time-frequency analysis, we propose a time-frequency analysis-based object detection method for large-scale remote-sensing images with complex backgrounds. We utilize wavelet decomposition to carry out a time-frequency transform and then integrate it with deep learning in feature optimization. To effectively capture the time-frequency features, we propose a feature optimization method based on deep reinforcement learning to select the dominant time-frequency channels. Furthermore, we design a discrete wavelet multiscale attention mechanism (DW-MAM), enabling the detector to concentrate on the object area rather than the background. Extensive experiments show that the proposed method of learning from time-frequency channels not only solves the challenges of large-scale and complex backgrounds, but also improves the performance compared to the original state-of-the-art object detection methods. In addition, the proposed method can be used with almost all object detection neural networks, regardless of whether they are anchor-based or anchor-free detectors, horizontal or rotation detectors. 1558-0644 © 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.","Detectors; Feature extraction; Geospatial analysis; Object detection; Reinforcement learning; Remote sensing; Time-frequency analysis","Complex networks; Deep neural networks; Object detection; Object recognition; Reinforcement learning; Remote sensing; Wavelet decomposition; Complex background; Features extraction; Features optimizations; Geo-spatial analysis; Large-scales; Remote sensing images; Remote-sensing; Time frequency features; Time-frequency Analysis; algorithm; detection method; optimization; satellite imagery; wavelet analysis; Feature extraction","","","","","National Natural Science Foundation of China, (10.13039/501100001809); Fundamental Research Funds for the Central Universities, (10.13039/501100012226)","","Hong D., Gao L., Yao J., Zhang B., Plaza A., Chanussot J., Graph convolutional networks for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens., 59, 7, pp. 5966-5978, (2021); Hong D., Yokoya N., Chanussot J., Zhu X.X., An augmented linear mixing model to address spectral variability for hyperspectral unmixing, IEEE Trans. Image Process., 28, 4, pp. 1923-1938, (2019); Hong D., Et al., More diverse means better: Multimodal deep learning meets remote-sensing imagery classification, IEEE Trans. Geosci. Remote Sens., 59, 5, pp. 4340-4354, (2021); Zhang F., Bai J., Zhang J., Xiao Z., Pei C., An optimized training method for GAN-based hyperspectral image classification, IEEE Geosci. Remote Sens. Lett., 18, 10, pp. 1791-1795, (2021); Li K., Cheng G., Bu S., You X., Rotation-insensitive and contextaugmented object detection in remote sensing images, IEEE Trans. Geosci. Remote Sens., 56, 4, pp. 2337-2348, (2018); Wang C., Bai X., Wang S., Zhou J., Ren P., Multiscale visual attention networks for object detection in VHR remote sensing images, IEEE Geosci. Remote Sens. Lett., 16, 2, pp. 310-314, (2019); Yu W., Bai J., Jiao L., Background subtraction based on GAN and domain adaptation for VHR optical remote sensing videos, IEEE Access, 8, pp. 119144-119157, (2020); Li Q., Mou L., Liu Q., Wang Y., Zhu X.X., HSF-Net: Multiscale deep feature embedding for ship detection in optical remote sensing imagery, IEEE Trans. Geosci. Remote Sens., 56, 12, pp. 7147-7161, (2018); Liu W., Ma L., Chen H., Arbitrary-oriented ship detection framework in optical remote-sensing images, IEEE Geosci. Remote Sens. Lett., 15, 6, pp. 937-941, (2018); Yang Y., Zhuang Y., Bi F., Shi H., Xie Y., M-FCN: Effective fully convolutional network-based airplane detection framework, IEEE Geosci. Remote Sens. Lett., 14, 8, pp. 1293-1297, (2017); Cai B., Jiang Z., Zhang H., Yao Y., Nie S., Online exemplar-based fully convolutional network for aircraft detection in remote sensing images, IEEE Geosci. Remote Sens. Lett., 15, 7, pp. 1095-1099, (2018); Benami E., Et al., Uniting remote sensing, crop modelling and economics for agricultural risk management, Nature Rev. Earth Environ., 2, pp. 140-159, (2021); Huang J., Et al., Assimilation of remote sensing into crop growth models: Current status and perspectives, Agricult. Forest Meteorol., 276-277, (2019); Jin X., Et al., A review of data assimilation of remote sensing and crop models, Eur. J. Agron., 92, pp. 141-152, (2018); Fleisher A.J., Et al., Optical sensors and sensing 2019: Introduction to the joint feature issue, Appl. Opt., 59, 7, pp. OSS1-OSS2, (2020); Wang P., Sun X., Diao W., Fu K., FMSSD: Feature-merged single-shot detection for multiscale objects in large-scale remote sensing imagery, IEEE Trans. Geosci. Remote Sens., 58, 5, pp. 3377-3390, (2020); Xu K., Qin M., Sun F., Wang Y., Chen Y.-K., Ren F., Learning in the frequency domain, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1740-1749, (2020); Shleymovich M., Medvedev M., Lyasheva S., Image analysis in unmanned aerial vehicle on-board system for objects detection and recognition with the help of energy characteristics based on wavelet transform, Proc. SPIE, 10342, (2017); Wu X., Hong D., Chanussot J., Xu Y., Tao R., Wang Y., Fourierbased rotation-invariant feature boosting: An efficient framework for geospatial object detection, IEEE Geosci. Remote Sens. Lett., 17, 2, pp. 302-306, (2020); Wu X., Hong D., Tian J., Chanussot J., Li W., Tao R., ORSIm detector: A novel object detection framework in optical remote sensing imagery using spatial-frequency channel features, IEEE Trans. Geosci. Remote Sens., 57, 7, pp. 5146-5158, (2019); Cao X., Yao J., Fu X., Bi H., Hong D., An enhanced 3-D discrete wavelet transform for hyperspectral image classification, IEEE Geosci. Remote Sens. Lett., 18, 6, pp. 1104-1108, (2021); Ehrlich M., Davis L., Deep residual learning in the JPEG transform domain, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 3484-3493, (2019); Gueguen L., Sergeev A., Kadlec B., Liu R., Yosinski J., Faster neural networks straight from JPEG, Proc. Adv. Neural Inf. Proces. Syst. (NIPS), 31, pp. 3933-3944, (2018); Hess-Nielsen N., Wickerhauser M.V., Wavelets and time-frequency analysis, Proc. IEEE, 84, 4, pp. 523-540, (1996); Chen W., Wilson J., Tyree S., Weinberger K.Q., Chen Y., Compressing convolutional neural networks in the frequency domain, Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pp. 1475-1484, (2016); Wang Y., Xu C., Xu C., Tao D., CNNpack: Packing convolutional neural networks in the frequency domain, IEEE Trans. Pattern Anal. Mach. Intell., 41, 10, pp. 2495-2510, (2018); Liu Z., Xu J., Peng X., Xiong R., Frequency-domain dynamic pruning for convolutional neural networks, Proc. 32nd Int. Conf. Neural Inf. Process. Syst., pp. 1051-1061, (2018); Wang H., Wu X., Huang Z., Xing E.P., High-frequency component helps explain the generalization of convolutional neural networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 8684-8694, (2020); Kang E., Chang W., Yoo J., Ye J.C., Deep convolutional framelet denosing for low-dose CT via wavelet residual network, IEEE Trans. Med. Imag., 37, 6, pp. 1358-1369, (2018); Xia G.-S., Et al., DOTA: A large-scale dataset for object detection in aerial images, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3974-3983, (2018); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS J. Photogramm. Remote Sens., 159, pp. 296-307, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 580-587, (2014); Long Y., Gong Y., Xiao Z., Liu Q., Accurate object localization in remote sensing images based on convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 55, 5, pp. 2486-2498, (2017); Cheng G., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens., 54, 12, pp. 7405-7415, (2016); Liu Z., Hu J., Weng L., Yang Y., Rotated region based CNN for ship detection, Proc. Int. Conf. Image Process. (ICIP), pp. 900-904, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Tayara H., Chong K.T., Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network, Sensors, 18, 10, (2018); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 779-788, (2016); Liu W., Et al., SSD: Single shot MultiBox detector, Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, Switzerland: Springer, pp. 21-37, (2016); Du L., Li L., Wei D., Mao J., Saliency-guided single shot multibox detector for target detection in SAR images, IEEE Trans. Geosci. Remote Sens., 58, 5, pp. 3366-3376, (2020); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 2980-2988, (2017); Lei J., Luo X., Fang L., Wang M., Gu Y., Region-enhanced convolutional neural network for object detection in remote sensing images, IEEE Trans. Geosci. Remote Sens., 58, 8, pp. 5693-5702, (2020); Tian Z., Shen C., Chen H., He T., FCOS: Fully convolutional one-stage object detection, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9627-9636, (2019); Zhang X., Wang G., Zhu P., Zhang T., Li C., Jiao L., GRS-Det: An anchor-free rotation ship detector based on Gaussian-mask in remote sensing images, IEEE Trans. Geosci. Remote Sens., 59, 4, pp. 3518-3531, (2021); Zhou L., Wei H., Li H., Zhao W., Zhang Y., Zhang Y., Objects Detection for Remote Sensing Images Based on Polar Coordinates, (2020); Yu Y., Et al., OA-CapsNet: A one-stage anchor-free capsule network for geospatial object detection from remote sensing imagery, Can. J. Remote Sens., 47, pp. 1-14, (2021); Torfason R., Mentzer F., Agustsson E., Tschannen M., Timofte R., Van Gool L., Towards Image Understanding from Deep Compression Without Decoding, (2018); Wu C.-Y., Zaheer M., Hu H., Manmatha R., Smola A.J., Krahenbuhl P., Compressed video action recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 6026-6035, (2018); Mallat S.G., A theory for multiresolution signal decomposition: The wavelet representation, IEEE Trans. Pattern Anal. Mach. Intell., 11, 7, pp. 674-693, (1989); Kaelbling L.P., Littman M.L., Moore A.W., Reinforcement learning: A survey, J. Artif. Intell. Res., 4, 1, pp. 237-285, (1996); Watkins C.J.C.H., Dayan P., Q-learning, Mach. Learn., 8, 3-4, pp. 279-292, (1992); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Wang Q., Wu B., Zhu P., Li P., Zuo W., Hu Q., ECA-Net: Efficient channel attention for deep convolutional neural networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), (2020); Hu J., Shen L., Sun G., Squeeze-and-excitation networks, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7132-7141, (2018); Qin Z., Zhang P., Wu F., Li X., FcaNet: Frequency Channel Attention Networks, (2020); Chen K., Et al., MMDetection: Open MMLab Detection Toolbox and Benchmark, (2019); Yang X., Hou L., Zhou Y., Wang W., Yan J., Dense Label Encoding for Boundary Discontinuity Free Rotation Detection, (2020); Yang X., Yan J., Ming Q., Wang W., Zhang X., Tian Q., Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss, (2021); Yang X., Et al., SCRDet: Towards more robust detection for small, cluttered and rotated objects, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 8232-8241, (2019)","Z. Xiao; Hunan University, College of Computer Science and Electronic Engineering, Changsha, 410082, China; email: zhxiao@hnu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85118675184"
"Liu J.; Xiang J.; Jin Y.; Liu R.; Yan J.; Wang L.","Liu, Jia (56066228900); Xiang, Jianjian (57223242989); Jin, Yongjun (57358093300); Liu, Renhua (57223242856); Yan, Jining (55560283600); Wang, Lizhe (23029267900)","56066228900; 57223242989; 57358093300; 57223242856; 55560283600; 23029267900","Boost precision agriculture with unmanned aerial vehicle remote sensing and edge intelligence: A survey","2021","Remote Sensing","13","21","4387","","","","36","10.3390/rs13214387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120337164&doi=10.3390%2frs13214387&partnerID=40&md5=eb88473d904ad8374bc374d7c4012997","School of Computer Science, China University of Geosciences, Wuhan, 430078, China","Liu J., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Xiang J., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Jin Y., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Liu R., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Yan J., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Wang L., School of Computer Science, China University of Geosciences, Wuhan, 430078, China","In recent years unmanned aerial vehicles (UAVs) have emerged as a popular and costeffective technology to capture high spatial and temporal resolution remote sensing (RS) images for a wide range of precision agriculture applications, which can help reduce costs and environmental impacts by providing detailed agricultural information to optimize field practices. Furthermore, deep learning (DL) has been successfully applied in agricultural applications such as weed detection, crop pest and disease detection, etc. as an intelligent tool. However, most DL-based methods place high computation, memory and network demands on resources. Cloud computing can increase processing efficiency with high scalability and low cost, but results in high latency and great pressure on the network bandwidth. The emerging of edge intelligence, although still in the early stages, provides a promising solution for artificial intelligence (AI) applications on intelligent edge devices at the edge of the network close to data sources. These devices are with built-in processors enabling onboard analytics or AI (e.g., UAVs and Internet of Things gateways). Therefore, in this paper, a comprehensive survey on the latest developments of precision agriculture with UAV RS and edge intelligence is conducted for the first time. The major insights observed are as follows: (a) in terms of UAV systems, small or light, fixed-wing or industrial rotor-wing UAVs are widely used in precision agriculture; (b) sensors on UAVs can provide multi-source datasets, and there are only a few public UAV dataset for intelligent precision agriculture, mainly from RGB sensors and a few from multispectral and hyperspectral sensors; (c) DL-based UAV RS methods can be categorized into classification, object detection and segmentation tasks, and convolutional neural network and recurrent neural network are the mostly common used network architectures; (d) cloud computing is a common solution to UAV RS data processing, while edge computing brings the computing close to data sources; (e) edge intelligence is the convergence of artificial intelligence and edge computing, in which model compression especially parameter pruning and quantization is the most important and widely used technique at present, and typical edge resources include central processing units, graphics processing units and field programmable gate arrays. © 2021 by the authors.","Deep learning; Edge intelligence; High performance; Mobile devices; Model compression; Precision agriculture; Remote sensing; Unmanned aerial vehicles","Aircraft detection; Antennas; Classification (of information); Convolution; Convolutional neural networks; Data handling; Drones; Fixed wings; Graphics processing unit; Network architecture; Object detection; Recurrent neural networks; Remote sensing; Surveys; Cloud-computing; Data-source; Deep learning; Edge computing; Edge intelligence; High performance; Model compression; Performance; Precision Agriculture; Remote-sensing; Edge computing","","","","","National Natural Science Foundation of China, NSFC, (41901376, 42172333); China University of Geosciences, Wuhan, CUG; Fundamental Research Funds for the Central Universities","This research was funded in part by the National Natural Science Foundation of China under Grant No. 41901376 and No. 42172333, and in part by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan).","Precision Ag Definition; Messina G., Modica G., Applications of UAV Thermal Imagery in Precision Agriculture: State of the Art and Future Research Outlook, Remote Sens, 12, (2020); Schimmelpfennig D., Farm profits and adoption of precision agriculture, (2016); Maes W.H., Steppe K., Perspectives for Remote Sensing with Unmanned Aerial Vehicles in Precision Agriculture, Trends Plant Sci, 24, pp. 152-164, (2019); Lillesand T., Kiefer R.W., Chipman J., Remote Sensing and Image Interpretation, (2015); Mulla D.J., Twenty five years of remote sensing in precision agriculture: Key advances and remaining knowledge gaps, Biosyst. Eng, 114, pp. 358-371, (2013); Eskandari R., Mahdianpari M., Mohammadimanesh F., Salehi B., Brisco B., Homayouni S., Meta-Analysis of Unmanned Aerial Vehicle (UAV) Imagery for Agro-Environmental Monitoring Using Machine Learning and Statistical Models, Remote Sens, 12, (2020); Tsouros D.C., Bibi S., Sarigiannidis P.G., A Review on UAV-Based Applications for Precision Agriculture, Information, 10, (2019); Zhang H., Wang L., Tian T., Yin J., A Review of Unmanned Aerial Vehicle Low-Altitude Remote Sensing (UAV-LARS) Use in Agricultural Monitoring in China, Remote Sens, 13, (2021); Jang G., Kim J., Yu J.-K., Kim H.-J., Kim Y., Kim D.-W., Kim K.-H., Lee C.W., Chung Y.S., Review: Cost-Effective Unmanned Aerial Vehicle (UAV) Platform for Field Plant Breeding Application, Remote Sens, 12, (2020); Unmanned Aerial Vehicle; Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens, 146, pp. 124-136, (2018); Christiansen M.P., Laursen M.S., Jorgensen R.N., Skovsen S., Gislum R., Designing and Testing a UAV Mapping System for Agricultural Field Surveying, Sensors, 17, (2017); Popescu D., Stoican F., Stamatescu G., Ichim L., Dragana C., Advanced UAV-WSN System for Intelligent Monitoring in Precision Agriculture, Sensors, 20, (2020); Zhou X., Zheng H., Xu X., He J., Ge X., Yao X., Cheng T., Zhu Y., Cao W., Tian Y., Predicting grain yield in rice using multi-temporal vegetation indices from UAV-based multispectral and digital imagery, ISPRS J. Photogramm. Remote Sens, 130, pp. 246-255, (2017); Yang Q., Shi L., Han J., Zha Y., Zhu P., Deep convolutional neural networks for rice grain yield estimation at the ripening stage using UAV-based remotely sensed images, Field Crops Res, 235, pp. 142-153, (2019); Su J., Liu C., Coombes M., Hu X., Wang C., Xu X., Li Q., Guo L., Chen W.-H., Wheat yellow rust monitoring by learning from multispectral UAV aerial imagery, Comput. Electron. Agric, 155, pp. 157-166, (2018); Guo A., Huang W., Dong Y., Ye H., Ma H., Liu B., Wu W., Ren Y., Ruan C., Geng Y., Wheat Yellow Rust Detection Using UAV-Based Hyperspectral Technology, Remote Sens, 13, (2021); Bajwa A., Mahajan G., Chauhan B., NonconventionalWeed Management Strategies for Modern Agriculture, Weed Sci, 63, pp. 723-747, (2015); Huang Y., Reddy K.N., Fletcher R.S., Pennington D., UAV Low-Altitude Remote Sensing for Precision Weed Management, Weed Technol, 32, pp. 2-6, (2018); Van Klompenburg T., Kassahun A., Catal C., Crop yield prediction using machine learning: A systematic literature review, Comput. Electron. Agric, 177, (2020); Su Y.-X., Xu H., Yan L.-J., Support vector machine-based open crop model (SBOCM): Case of rice production in China, Saudi J. Biol. Sci, 24, pp. 537-547, (2017); Everingham Y., Sexton J., Skocaj D., Inman-Bamber G., Accurate prediction of sugarcane yield using a random forest algorithm, Agron. Sustain. Dev, 36, (2016); Chandra A.L., Desai S.V., Guo W., Balasubramanian V.N., Computer vision with deep learning for plant phenotyping in agriculture: A survey, (2020); Zhou L., Zhang C., Liu F., Qiu Z., He Y., Application of Deep Learning in Food: A Review, Compr. Rev. Food Sci. Food Saf, 18, pp. 1793-1811, (2019); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Bah M.D., Hafiane A., Canals R., Deep Learning with Unsupervised Data Labeling forWeed Detection in Line Crops in UAV Images, Remote Sens, 10, (2018); Kitano B.T., Mendes C.C.T., Geus A.R., Oliveira H.C., Souza J.R., Corn plant counting using deep learning and UAV images, IEEE Geosci. Remote. Sens. Lett, pp. 1-5, (2019); Nowakowski A., Mrziglod J., Spiller D., Bonifacio R., Ferrari I., Mathieu P.P., Garcia-Herranz M., Kim D.-H., Crop type mapping by using transfer learning, Int. J. Appl. Earth Obs. Geoinf, 98, (2021); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnarson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Chen J., Ran X., Deep Learning with Edge Computing: A Review, Proc. IEEE, 107, pp. 1655-1674, (2019); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, Proceedings of the International Conference on Learning Representations, (2015); Liu J., Liu R., Ren K., Li X., Xiang J., Qiu S., High-Performance Object Detection for Optical Remote Sensing Images with Lightweight Convolutional Neural Networks, Proceedings of the 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), pp. 585-592, (2020); Zhou Z., Chen X., Li E., Zeng L., Luo K., Zhang J., Edge Intelligence: Paving the Last Mile of Artificial Intelligence with Edge Computing, Proc. IEEE, 107, pp. 1738-1762, (2019); Pu Q., Ananthanarayanan G., Bodik P., Kandula S., Akella A., Bahl P., Stoica I., Low latency geo-distributed data analytics, ACM SIGCOMM Comp. Com. Rev, 45, pp. 421-434, (2015); Sitton-Candanedo I., Alonso R.S., Rodriguez-Gonzalez S., Coria J.A.G., De La Prieta F., Edge Computing Architectures in Industry 4.0: A General Survey and Comparison, International Workshop on Soft Computing Models in Industrial and Environmental Applications, pp. 121-131, (2019); Plastiras G., Terzi M., Kyrkou C., Theocharidcs T., Edge intelligence: Challenges and opportunities of near-sensor machine learning applications, Proceedings of the 2018 IEEE 29th International Conference on Application-Specific Systems, Architectures and Processors (ASAP), pp. 1-7, (2018); Deng S., Zhao H., Fang W., Yin J., Dustdar S., Zomaya A.Y., Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence, IEEE Internet Things J, 7, pp. 7457-7469, (2020); Boursianis A.D., Papadopoulou M.S., Diamantoulakis P., Liopa-Tsakalidi A., Barouchas P., Salahas G., Karagiannidis G., Wan S., Goudos S.K., Internet of Things (IoT) and Agricultural Unmanned Aerial Vehicles (UAVs) in smart farming: A comprehensive review, Internet Things, (2020); Kim J., Kim S., Ju C., Son H.I., Unmanned Aerial Vehicles in Agriculture: A Review of Perspective of Platform, Control, and Applications, IEEE Access, 7, pp. 105100-105115, (2019); Mogili U.R., Deepak B.B.V.L., Review on Application of Drone Systems in Precision Agriculture, Procedia Comput. Sci, 133, pp. 502-509, (2018); Kamilaris A., Prenafeta-Boldu F.X., A review of the use of convolutional neural networks in agriculture, J. Agric. Sci, 156, pp. 312-322, (2018); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Santos L., Santos F.N., Oliveira P.M., Shinde P., Deep Learning Applications in Agriculture: A Short Review, Iberian Robotics Conference, pp. 139-151, (2019); Interim Regulations on Flight Management of Unmanned Aerial Vehicles, 2021, (2018); Park M., Lee S., Lee S., Dynamic topology reconstruction protocol for uav swarm networking, Symmetry, 12, (2020); Radoglou-Grammatikis P., Sarigiannidis P., Lagkas T., Moscholios I., A compilation of UAV applications for precision agriculture, Comput. Netw, 172, (2020); Hayat S., Yanmaz E., Muzaffar R., Survey on Unmanned Aerial Vehicle Networks for Civil Applications: A Communications Viewpoint, IEEE Commun. Surv. Tutor, 18, pp. 2624-2661, (2016); Xie C., Yang C., A review on plant high-throughput phenotyping traits using UAV-based sensors, Comput. Electron. Agric, 178, (2020); Delavarpour N., Koparan C., Nowatzki J., Bajwa S., Sun X., A Technical Study on UAV Characteristics for Precision Agriculture Applications and Associated Practical Challenges, Remote Sens, 13, (2021); Tsouros D.C., Triantafyllou A., Bibi S., Sarigannidis P.G., Data acquisition and analysis methods in UAV-based applications for Precision Agriculture, Proceedings of the 2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS), pp. 377-384, (2019); Tahir M.N., Lan Y., Zhang Y., Wang Y., Nawaz F., Shah M.A.A., Gulzar A., Qureshi W.S., Naqvi S.M., Naqvi S.Z.A., Real time estimation of leaf area index and groundnut yield using multispectral UAV, Int. J. Precis. Agric. Aviat, (2020); Stroppiana D., Villa P., Sona G., Ronchetti G., Candiani G., Pepe M., Busetto L., Migliazzi M., Boschetti M., Early season weed mapping in rice crops using multi-spectral UAV data, Int. J. Remote Sens, 39, pp. 5432-5452, (2018); Wang H., Mortensen A.K., Mao P., Boelt B., Gislum R., Estimating the nitrogen nutrition index in grass seed crops using a UAV-mounted multispectral camera, Int. J. Remote Sens, 40, pp. 2467-2482, (2019); Ishida T., Kurihara J., Viray F.A., Namuco S.B., Paringit E.C., Perez G.J., Takahashi Y., Marciano J.J., A novel approach for vegetation classification using UAV-based hyperspectral imaging, Comput. Electron. Agric, 144, pp. 80-85, (2018); Ge X., Wang J., Ding J., Cao X., Zhang Z., Liu J., Li X., Combining UAV-based hyperspectral imagery and machine learning algorithms for soil moisture content monitoring, PeerJ, 7, (2019); Zhao X., Yang G., Liu J., Zhang X., Xu B., Wang Y., Zhao C., Gai J., Estimation of soybean breeding yield based on optimization of spatial scale of UAV hyperspectral image, Trans. Chin. Soc. Agric. Eng, 33, pp. 110-116, (2017); Prakash A., Thermal remote sensing: Concepts, issues and applications, Int. Arch. Photogramm. Remote Sens, 33, pp. 239-243, (2000); Weng Q., Thermal infrared remote sensing for urban climate and environmental studies: Methods, applications, and trends, ISPRS J. Photogramm. Remote Sens, 64, pp. 335-344, (2009); Khanal S., Fulton J., Shearer S., An overview of current and potential applications of thermal remote sensing in precision agriculture, Comput. Electron. Agric, 139, pp. 22-32, (2017); Dong P., Chen Q., LiDAR Remote Sensing and Applications, (2017); Zhou L., Gu X., Cheng S., Yang G., Shu M., Sun Q., Analysis of plant height changes of lodged maize using UAV-LiDAR data, Agriculture, 10, (2020); Shendryk Y., Sofonia J., Garrard R., Rist Y., Skocaj D., Thorburn P., Fine-scale prediction of biomass and leaf nitrogen content in sugarcane using UAV LiDAR and multispectral imaging, Int. J. Appl. Earth Obs. Geoinf, 92, (2020); Ndikumana E., Minh D.H.T., Baghdadi N., Courault D., Hossard L., Deep Recurrent Neural Network for Agricultural Classification using multitemporal SAR Sentinel-1 for Camargue, France, Remote Sens, 10, (2018); Lyalin K.S., Biryuk A.A., Sheremet A.Y., Tsvetkov V.K., Prikhodko D.V., UAV synthetic aperture radar system for control of vegetation and soil moisture, Proceedings of the 2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus), pp. 1673-1675, (2018); Liu C.-A., Chen Z.-X., Shao Y., Chen J.-S., Hasi T., Pan H.-Z., Research advances of SAR remote sensing for agriculture applications: A review, J. Integr. Agric, 18, pp. 506-525, (2019); Padua L., Vanko J., Hruska J., Adao T., Sousa J.J., Peres E., Morais R., UAS, sensors, and data processing in agroforestry: A review towards practical applications, Int. J. Remote Sens, 38, pp. 2349-2391, (2017); Allred B., Eash N., Freeland R., Martinez L., Wishart D., Effective and efficient agricultural drainage pipe mapping with UAS thermal infrared imagery: A case study, Agric. Water Manag, 197, pp. 132-137, (2018); Santesteban L.G., Di Gennaro S.F., Herrero-Langreo A., Miranda C., Royo J., Matese A., High-resolution UAV-based thermal imaging to estimate the instantaneous and seasonal variability of plant water status within a vineyard, Agric. Water Manag, 183, pp. 49-59, (2017); Xue J., Su B., Significant Remote Sensing Vegetation Indices: A Review of Developments and Applications, J. Sensors, 2017, pp. 1-17, (2017); Dai B., He Y., Gu F., Yang L., Han J., Xu W., A vision-based autonomous aerial spray system for precision agriculture, Proceedings of the 2017 IEEE International Conference on Robotics and Biomimetics (ROBIO), pp. 507-513, (2017); Faical B.S., Freitas H., Gomes P.H., Mano L., Pessin G., de Carvalho A., Krishnamachari B., Ueyama J., An adaptive approach for UAV-based pesticide spraying in dynamic environments, Comput. Electron. Agric, 138, pp. 210-223, (2017); Faical B.S., Pessin G., Filho G.P.R., Carvalho A.C.P.L.F., Gomes P.H., Ueyama J., Fine-Tuning of UAV Control Rules for Spraying Pesticides on Crop Fields: An Approach for Dynamic Environments, Int. J. Artif. Intell. Tools, 25, (2016); Esposito M., Crimaldi M., Cirillo V., Sarghini F., Maggio A., Drone and sensor technology for sustainable weed management: A review, Chem. Biol. Technol. Agric, 8, (2021); Bah M.D., Dericquebourg E., Hafiane A., Canals R., Deep Learning based Classification System for Identifying Weeds using High-Resolution UAV Imagery, Science and Information Conference, pp. 176-187, (2018); Huang H., Deng J., Lan Y., Yang A., Deng X., Zhang L., A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery, PLoS ONE, 13, (2018); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning, Sci. Rep. UK, 9, pp. 1-12, (2019); Sa I., Popovic M., Khanna R., Chen Z., Lottes P., Liebisch F., Nieto J., Stachniss C., Walter A., Siegwart R., WeedMap: A Large-Scale SemanticWeed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming, Remote Sens, 10, (2018); Scherrer B., Sheppard J., Jha P., Shaw J.A., Hyperspectral imaging and neural networks to classify herbicide-resistant weeds, J. Appl. Remote Sens, 13, (2019); Huang H., Lan Y., Yang A., Zhang Y., Wen S., Deng J., Deep learning versus Object-based Image Analysis (OBIA) in weed mapping of UAV imagery, Int. J. Remote Sens, 41, pp. 3446-3479, (2020); Hasan R.I., Yusuf S.M., Alzubaidi L., Review of the State of the Art of Deep Learning for Plant Diseases: A Broad Analysis and Discussion, Plants, 9, (2020); Abdulridha J., Batuman O., Ampatzidis Y., UAV-Based Remote Sensing Technique to Detect Citrus Canker Disease Utilizing Hyperspectral Imaging and Machine Learning, Remote Sens, 11, (2019); Tetila E.C., Machado B.B., Astolfi G., Belete N.A.D.S., Amorim W.P., Roel A.R., Pistori H., Detection and classification of soybean pests using deep learning with UAV images, Comput. Electron. Agric, 179, (2020); Zhang X., Han L., Dong Y., Shi Y., Huang W., Han L., Gonzalez-Moreno P., Ma H., Ye H., Sobeih T., A Deep Learning-Based Approach for Automated Yellow Rust Disease Detection from High-Resolution Hyperspectral UAV Images, Remote Sens, 11, (2019); Hu G., Yin C., Wan M., Zhang Y., Fang Y., Recognition of diseased Pinus trees in UAV images using deep learning and AdaBoost classifier, Biosyst. Eng, 194, pp. 138-151, (2020); Tetila E.C., Machado B.B., Menezes G.K., Oliveira A.D.S., Alvarez M., Amorim W.P., Belete N.A.D.S., Da Silva G.G., Pistori H., Automatic Recognition of Soybean Leaf Diseases Using UAV Images and Deep Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett, 17, pp. 903-907, (2019); Wiesner-Hanks T., Wu H., Stewart E., DeChant C., Kaczmar N., Lipson H., Gore M.A., Nelson R.J., Millimeter-Level Plant Disease Detection from Aerial Photographs via Deep Learning and Crowdsourced Data, Front. Plant Sci, 10, (2019); Albetis J., Jacquin A., Goulard M., Poilve H., Rousseau J., Clenet H., Dedieu G., Duthoit S., On the Potentiality of UAV Multispectral Imagery to Detect Flavescence dorée and Grapevine Trunk Diseases, Remote Sens, 11, (2018); Kerkech M., Hafiane A., Canals R., Vine disease detection in UAV multispectral images using optimized image registration and deep learning segmentation approach, Comput. Electron. Agric, 174, (2020); Bendig J., Willkomm M., Tilly N., Gnyp M.L., Bennertz S., Qiang C., Miao Y., Lenz-Wiedemann V.I.S., Bareth G., Very high resolution crop surface models (CSMs) from UAV-based stereo images for rice growth monitoring In Northeast China, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 40, pp. 45-50, (2013); Ni J., Yao L., Zhang J., Cao W., Zhu Y., Tai X., Development of an Unmanned Aerial Vehicle-Borne Crop-Growth Monitoring System, Sensors, 17, (2017); Fu Z., Jiang J., Gao Y., Krienke B., Wang M., Zhong K., Cao Q., Tian Y., Zhu Y., Cao W., Et al., Wheat Growth Monitoring and Yield Estimation based on Multi-Rotor Unmanned Aerial Vehicle, Remote Sens, 12, (2020); Zhao J., Zhang X., Gao C., Qiu X., Tian Y., Zhu Y., Cao W., Rapid Mosaicking of Unmanned Aerial Vehicle (UAV) Images for Crop Growth Monitoring Using the SIFT Algorithm, Remote Sens, 11, (2019); Li B., Xu X., Zhang L., Han J., Bian C., Li G., Liu J., Jin L., Above-ground biomass estimation and yield prediction in potato by using UAV-based RGB and hyperspectral imaging, ISPRS J. Photogramm. Remote Sens, 162, pp. 161-172, (2020); Maimaitijiang M., Sagan V., Sidike P., Hartling S., Esposito F., Fritschi F.B., Soybean yield prediction from UAV using multimodal data fusion and deep learning, Remote Sens. Environ, 237, (2020); Nebiker S., Lack N., Abacherli M., Laderach S., Light-weight multispectral UAV sensors and their capabilities for predicting grain yield and detecting plant diseases, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 41, (2016); Stroppiana D., Migliazzi M., Chiarabini V., Crema A., Musanti M., Franchino C., Villa P., Rice yield estimation using multispectral data from UAV: A preliminary experiment in northern Italy, Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 4467-4664, (2015); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geosci. Remote Sens. Lett, 14, pp. 778-782, (2017); Teimouri N., Dyrmann M., Jorgensen R.N., A Novel Spatio-Temporal FCN-LSTM Network for Recognizing Various Crop Types Using Multi-Temporal Radar Images, Remote Sens, 11, (2019); Wang S., Di Tommaso S., Faulkner J., Friedel T., Kennepohl A., Strey R., Lobell D., Mapping Crop Types in Southeast India with Smartphone Crowdsourcing and Deep Learning, Remote Sens, 12, (2020); Rebetez J., Satizabal H.F., Mota M., Noll D., Buchi L., Wendling M., Cannelle B., Perez-Uribe A., Burgos S., Augmenting a Convolutional Neural Network with Local Histograms-A Case Study in Crop Classification from High-Resolution UAV Imagery, (2016); Zhao L., Shi Y., Liu B., Hovis C., Duan Y., Shi Z., Finer Classification of Crops by Fusing UAV Images and Sentinel-2A Data, Remote Sens, 11, (2019); Hinton G.E., Salakhutdinov R.R., Reducing the Dimensionality of Data with Neural Networks, Science, 313, pp. 504-507, (2006); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Adv. Neural Inf. Process. Syst, 25, pp. 1097-1105, (2012); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., Generative adversarial nets, Adv. Neural Inf. Process. Syst, 27, (2014); Reyes M.F., Auer S., Merkle N.M., Henry C., Schmitt M., SAR-to-Optical Image Translation Based on Conditional Generative Adversarial Networks - Optimization, Opportunities and Limits, Remote Sens, 11, (2019); Wang X., Yan H., Huo C., Yu J., Pant C., Enhancing Pix2Pix for Remote Sensing Image Classification, Proceedings of the International Conference on Pattern Recognition, pp. 2332-2336, (2018); Lv N., Ma H., Chen C., Pei Q., Zhou Y., Xiao F., Li J., Remote Sensing Data Augmentation Through Adversarial Training, Int. Geosci. Remote Sens. Symp, pp. 2511-2514, (2020); Ren C.X., Ziemann A., Theiler J., Durieux A.M.S., Deep snow: Synthesizing remote sensing imagery with generative adversarial nets, Proceedings of the 2020 Algorithms, Technologies, and Applications for Multispectral and Hyperspectral Imagery XXVI, pp. 196-205; Everingham M., Eslami S.M.A., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes Challenge: A Retrospective, Int. J. Comput. Vis, 111, pp. 98-136, (2015); Ha J.G., Moon H., Kwak J.T., Hassan S.I., Dang M., Lee O.N., Park H.Y., Deep convolutional neural network for classifying Fusarium wilt of radish from unmanned aerial vehicles, J. Appl. Remote Sens, (2017); Huang H., Deng J., Lan Y., Yang A., Zhang L., Wen S., Zhang H., Zhang Y., Deng Y., Detection of Helminthosporium Leaf Blotch Disease Based on UAV Imagery, Appl. Sci, 9, (2019); De Camargo T., Schirrmann M., Landwehr N., Dammer K.-H., Pflanz M., Optimized Deep Learning Model as a Basis for Fast UAV Mapping ofWeed Species in Winter Wheat Crops, Remote Sens, 13, (2021); Ukaegbu U., Tartibu L., Okwu M., Olayode I., Development of a Light-Weight Unmanned Aerial Vehicle for Precision Agriculture, Sensors, 21, (2021); Onishi M., Ise T., Automatic classification of trees using a UAV onboard camera and deep learning, (2018); Zhao J., Zhong Y., Hu X., Wei L., Zhang L., A robust spectral-spatial approach to identifying heterogeneous crops using remote sensing imagery with high spectral and spatial resolutions, Remote Sens. Environ, 239, (2020); Chen C.-J., Huang Y.-Y., Li Y.-S., Chen Y.-C., Chang C.-Y., Huang Y.-M., Identification of Fruit Tree Pests with Deep Learning on Embedded Drone to Achieve Accurate Pesticide Spraying, IEEE Access, 9, pp. 21986-21997, (2021); Li F., Liu Z., Shen W., Wang Y., Wang Y., Ge C., Sun F., Lan P., A Remote Sensing and Airborne Edge-Computing Based Detection System for Pine Wilt Disease, IEEE Access, 9, pp. 66346-66360, (2021); Valente J., Doldersum M., Roers C., Kooistra L., Detecting rumex obtusifolius weed plants in grasslands from UAV RGB imagery using deep learning, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 4, pp. 179-185, (2019); Veeranampalayam Sivakumar A.N., Li J., Scott S., Psota E., Jhala A.J., Luck J.D., Shi Y., Comparison of object detection and patch-based classification deep learning models on mid-to late-season weed detection in UAV imagery, Remote Sens, 12, (2020); Apolo-Apolo O., Martinez-Guanter J., Egea G., Raja P., Perez-Ruiz M., Deep learning techniques for estimation of the yield and size of citrus fruits using a UAV, Eur. J. Agron, 115, (2020); Chen Y., Lee W.S., Gan H., Peres N., Fraisse C., Zhang Y., He Y., Strawberry Yield Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoimages, Remote Sens, 11, (2019); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Zhang Z., Flores P., Igathinathane C., Naik D.L., Kiran R., Ransom J.K., Wheat Lodging Detection from UAS Imagery Using Machine Learning Algorithms, Remote Sens, 12, (2020); Stewart E.L., Wiesner-Hanks T., Kaczmar N., DeChant C., Wu H., Lipson H., Nelson R.J., Gore M.A., Quantitative Phenotyping of Northern Leaf Blight in UAV Images Using Deep Learning, Remote Sens, 11, (2019); Kerkech M., Hafiane A., Canals R., VddNet: Vine Disease Detection Network Based on Multispectral Images and Depth Map, Remote Sens, 12, (2020); Zou K., Chen X., Zhang F., Zhou H., Zhang C., A FieldWeed Density Evaluation Method Based on UAV Imaging and Modified U-Net, Remote Sens, 13, (2021); Osco L.P., Nogueira K., Ramos A.P.M., Pinheiro M.M.F., Furuya D.E.G., Goncalves W.N., Jorge L.A.D.C., Junior J.M., dos Santos J.A., Semantic segmentation of citrus-orchard using deep neural networks and multispectral UAV-based imagery, Precis. Agric, 22, pp. 1-18, (2021); Zhang J., Xie T., Yang C., Song H., Jiang Z., Zhou G., Zhang D., Feng H., Xie J., Segmenting Purple Rapeseed Leaves in the Field from UAV RGB Imagery Using Deep Learning as an Auxiliary Means for Nitrogen Stress Detection, Remote Sens, 12, (2020); Xu W., Yang W., Chen S., Wu C., Chen P., Lan Y., Establishing a model to predict the single boll weight of cotton in northern Xinjiang by using high resolution UAV remote sensing data, Comput. Electron. Agric, 179, (2020); Champ J., Mora-Fallas A., Goeau H., Mata-Montero E., Bonnet P., Joly A., Instance segmentation for the fine detection of crop and weed plants by precision agricultural robots, Appl. Plant Sci, 8, (2020); Mora-Fallas A., Goeau H., Joly A., Bonnet P., Mata-Montero E., Instance segmentation for automated weeds and crops detection in farmlands, A first approach to Acoustic Characterization of Costa Rican Children’s Speech, (2020); Toda Y., Okura F., Ito J., Okada S., Kinoshita T., Tsuji H., Saisho D., Training instance segmentation neural network with synthetic datasets for crop seed phenotyping, Commun. Biol, 3, (2020); Khan S., Tufail M., Khan M.T., Khan Z.A., Iqbal J., Wasim A., Real-time recognition of spraying area for UAV sprayers using a deep learning approach, PLoS ONE, 16, (2021); Deng J., Zhong Z., Huang H., Lan Y., Han Y., Zhang Y., Lightweight Semantic Segmentation Network for Real-TimeWeed Mapping Using Unmanned Aerial Vehicles, Appl. Sci, 10, (2020); Liu C., Li H., Su A., Chen S., Li W., Identification and Grading of Maize Drought on RGB Images of UAV Based on Improved U-Net, IEEE Geosci. Remote Sens. Lett, 18, pp. 198-202, (2020); Tri N.C., Duong H.N., Van Hoai T., Van Hoa T., Nguyen V.H., Toan N.T., Snasel V., A novel approach based on deep learning techniques and UAVs to yield assessment of paddy fields, Proceedings of the 2017 9th International Conference on Knowledge and Systems Engineering (KSE), pp. 257-262, (2017); Osco L.P., Arruda M.D.S.D., Goncalves D.N., Dias A., Batistoti J., de Souza M., Gomes F.D.G., Ramos A.P.M., Jorge L.A.D.C., Liesenberg V., Et al., A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery, ISPRS J. Photogramm. Remote Sens, 174, pp. 1-17, (2021); Osco L.P., Arruda M.D.S.D., Junior J.M., da Silva N.B., Ramos A.P.M., Moryia A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens, 160, pp. 97-106, (2020); Zheng J., Fu H., Li W., Wu W., Yu L., Yuan S., Tao W.Y.W., Pang T.K., Kanniah K.D., Growing status observation for oil palm trees using Unmanned Aerial Vehicle (UAV) images, ISPRS J. Photogramm. Remote Sens, 173, pp. 95-121, (2021); Ampatzidis Y., Partel V., Costa L., Agroview: Cloud-based application to process, analyze and visualize UAV-collected data for precision agriculture applications utilizing artificial intelligence, Comput. Electron. Agric, 174, (2020); Pang Y., Shi Y., Gao S., Jiang F., Veeranampalayam-Sivakumar A.-N., Thompson L., Luck J., Liu C., Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery, Comput. Electron. Agric, 178, (2020); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic Counting of in situ Rice Seedlings from UAV Images Based on a Deep Fully Convolutional Neural Network, Remote Sens, 11, (2019); Yang M.-D., Tseng H.-H., Hsu Y.-C., Yang C.-Y., Lai M.-H., Wu D.-H., A UAV Open Dataset of Rice Paddies for Deep Learning Practice, Remote Sens, 13, (2021); Zhao W., Yamada W., Li T., Digman M., Runge T., Augmenting Crop Detection for Precision Agriculture with Deep Visual Transfer Learning-A Case Study of Bale Detection, Remote Sens, 13, (2020); Ampatzidis Y., Partel V., UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence, Remote Sens, 11, (2019); Aeberli A., Johansen K., Robson A., Lamb D., Phinn S., Detection of Banana Plants Using Multi-Temporal Multispectral UAV Imagery, Remote Sens, 13, (2021); Fan Z., Lu J., Gong M., Xie H., Goodman E.D., Automatic Tobacco Plant Detection in UAV Images via Deep Neural Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 11, pp. 876-887, (2018); Zan X., Zhang X., Xing Z., Liu W., Zhang X., Su W., Liu Z., Zhao Y., Li S., Automatic Detection of Maize Tassels from UAV Images by Combining Random Forest Classifier and VGG16, Remote Sens, 12, (2020); Liu Y., Cen C., Che Y., Ke R., Ma Y., Ma Y., Detection of Maize Tassels from UAV RGB Imagery with Faster R-CNN, Remote Sens, 12, (2020); Yuan W., Choi D., UAV-Based Heating Requirement Determination for Frost Management in Apple Orchard, Remote Sens, 13, (2021); Dyson J., Mancini A., Frontoni E., Zingaretti P., Deep Learning for Soil and Crop Segmentation from Remotely Sensed Data, Remote Sens, 11, (2019); Feng Q., Yang J., Liu Y., Ou C., Zhu D., Niu B., Liu J., Li B., Multi-Temporal Unmanned Aerial Vehicle Remote Sensing for Vegetable Mapping Using an Attention-Based Recurrent Convolutional Neural Network, Remote Sens, 12, (2020); Der Yang M., Tseng H.H., Hsu Y.C., Tseng W.C., Real-time Crop Classification Using Edge Computing and Deep Learning, Proceedings of the 2020 IEEE 17th Annual Consumer Communications & Networking Conference, pp. 1-4, (2020); Yang M.-D., Boubin J.G., Tsai H.P., Tseng H.-H., Hsu Y.-C., Stewart C.C., Adaptive autonomous UAV scouting for rice lodging assessment using edge computing with deep learning EDANet, Comput. Electron. Agric, 179, (2020); Zhang Q., Liu Y., Gong C., Chen Y., Yu H., Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review, Sensors, 20, (2020); Zhong Y., Hu X., Luo C., Wang X., Zhao J., Zhang L., WHU-Hi: UAV-borne hyperspdectral with high spatial resolution (H2) benchmark datasets and classifier for precise crop identification based on deep convolutional neural network with CRF, Remote Sens. Environ, 250, (2020); Wiesner-Hanks T., Stewart E.L., Kaczmar N., DeChant C., Wu H., Nelson R.J., Lipson H., Gore M.A., Image set for deep learning: Field images of maize annotated with disease symptoms, BMC Res. Notes, 11, (2018); Daudt R.C., Le Saux B., Boulch A., Gousseau Y., Multitask learning for large-scale semantic change detection, Comput. Vis. Image Underst, 187, (2019); Zhang Y., CSIF. figshare. Dataset, (2018); Oldoni L.V., Sanches I.D., Picoli M.C.A., Covre R.M., Fronza J.G., LEM+ dataset: For agricultural remote sensing applications, Data Brief, 33, (2020); Ferreira A., Felipussi S.C., Pires R., Avila S., Santos G., Lambert J., Huang J., Rocha A., Eyes in the Skies: A Data-Driven Fusion Approach to Identifying Drug Crops from Remote Sensing Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 12, pp. 4773-4786, (2019); Russwurm M., Pelletier C., Zollner M., Lefevre S., Korner M., BreizhCrops: A time series dataset for crop type mapping, (2019); Rustowicz R., Cheong R., Wang L., Ermon S., Burke M., Lobell D., Semantic Segmentation of Crop Type in Ghana Dataset; Rustowicz R., Cheong R., Wang L., Ermon S., Burke M., Lobell D., Semantic Segmentation of Crop Type in South Sudan Dataset; Torre M., Remeseiro B., Radeva P., Martinez F., DeepNEM: Deep Network Energy-Minimization for Agricultural Field Segmentation, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 726-737, (2020); EarthExplorer; Copernicus Open Access Hub; Weikmann G., Paris C., Bruzzone L., TimeSen2Crop: A Million Labeled Samples Dataset of Sentinel 2 Image Time Series for Crop-Type Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 4699-4708, (2021); Khan W.Z., Ahmed E., Hakak S., Yaqoob I., Ahmed A., Edge computing: A survey, Future Gener. Comput. Syst, 97, pp. 219-235, (2019); Liu J., Xue Y., Ren K., Song J., Windmill C., Merritt P., High-Performance Time-Series Quantitative Retrieval from Satellite Images on a GPU Cluster, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 12, pp. 2810-2821, (2019)","J. Liu; School of Computer Science, China University of Geosciences, Wuhan, 430078, China; email: liujia@cug.edu.cn","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120337164"
"Liu J.; Wang Y.","Liu, Jiahang (23995402200); Wang, Yue (58395401700)","23995402200; 58395401700","Water Body Extraction in Remote Sensing Imagery Using Domain Adaptation-Based Network Embedding Selective Self-Attention and Multi-Scale Feature Fusion","2022","Remote Sensing","14","15","3538","","","","2","10.3390/rs14153538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137121953&doi=10.3390%2frs14153538&partnerID=40&md5=6e3401ade6df4d866a4f4ecd7f6d41e8","College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China","Liu J., College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China; Wang Y., College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China","A water body is a common object in remote sensing images and high-quality water body extraction is important for some further applications. With the development of deep learning (DL) in recent years, semantic segmentation technology based on deep convolution neural network (DCNN) brings a new way for automatic and high-quality body extraction from remote sensing images. Although several methods have been proposed, there exist two major problems in water body extraction, especially for high resolution remote sensing images. One is that it is difficult to effectively detect both large and small water bodies simultaneously and accurately predict the edge position of water bodies with DCNN-based methods, and the other is that DL methods need a large number of labeled samples which are often insufficient in practical application. In this paper, a novel SFnet-DA network based on the domain adaptation (DA) embedding selective self-attention (SSA) mechanism and multi-scale feature fusion (MFF) module is proposed to deal with these problems. Specially, the SSA mechanism is used to increase or decrease the space detail and semantic information, respectively, in the bottom-up branches of the network by selective feature enhancement, thus it can improve the detection capability of water bodies with drastic scale change and can prevent the prediction from being affected by other factors, such as roads and green algae. Furthermore, the MFF module is used to accurately acquire edge information by changing the number of the channel of advanced feature branches with a unique fusion method. To skip the labeling work, SFnet-DA reduces the difference in feature distribution between labeled and unlabeled datasets by building an adversarial relationship between the feature extractor and the domain classifier, so that the trained parameters of the labeled datasets can be directly used to predict the unlabeled images. Experimental results demonstrate that the proposed SFnet-DA has better performance on water body segmentation than state-of-the-art methods. © 2022 by the authors.","domain adaptation; multi-scale feature fusion module; remote sensing images; selective self-attention module; water body extraction","Algae; Classification (of information); Deep learning; Embeddings; Forecasting; Remote sensing; Semantic Segmentation; Semantics; Convolution neural network; Domain adaptation; Features fusions; Fusion modules; Multi-scale feature fusion module; Multi-scale features; Remote sensing images; Selective self-attention module; Water body extraction; Waterbodies; Extraction","","","","","Innovative Talent Program of Jiangsu, (41-Y30F07-9001-20/22, JSSCR2021501); Nanjing University of Aeronautics and Astronautics, NUAA","This work was supported in part by the Innovative Talent Program of Jiangsu under Grant JSSCR2021501, by the China High-Resolution Earth Observation System Program under Grant 41-Y30F07-9001-20/22, and by the High-Level Talent Plan of NUAA, China.","Kundzewicz Z.W., Water resources for sustainable development, Hydrol. Sci. J, 42, pp. 467-480, (1997); Yang X., Zhao S., Qin X., Mapping of Urban Surface Water Bodies from Sentinel-2 MSI Imagery at 10 m Resolution via NDWI-Based Image Sharpening, Remote Sens, 9, (2017); Puttinaovarat S., Khaimook K., Polnigongit W., Horkaew P., Robust water body extraction from landsat imagery by using gradual assignment of water index and DSM, Proceedings of the IEEE International Conference on Signal & Image Processing Applications, pp. 122-126; Yang H., Wang Z., Zhao H., Yu G., Water Body Extraction Methods Study Based on RS and GIS, Procedia Environ. Sci, 10, pp. 2619-2624, (2011); Zhang Z., Lu M., Ji S., Yu H., Nie C., Rich CNN Features for Water-Body Segmentation from Very High Resolution Aerial and Satellite Imagery, Remote Sens, 13, (2021); Wu W., Li Q., Zhang Y., Du X., Wang H., Two-Step Urban Water Index (TSUWI): A New Technique for High-Resolution Mapping of Urban Surface Water, Remote Sens, 10, (2018); Yao F., Wang C., Dong D., Luo J., Shen Z., Yang K., High-Resolution Mapping of Urban Surface Water Using ZY-3 Multi-Spectral Imagery, Remote Sens, 7, pp. 12336-12355, (2015); McFeeters S.K., The use of the Normalized Difference Water Index (NDWI) in the delineation of open water features, Int. J. Remote Sens, 17, pp. 1425-1432, (1996); Guo H., He G., Jiang W., Yin R., Leng W., A Multi-Scale Water Extraction Convolutional Neural Network (MWEN) Method for GaoFen-1 Remote Sensing Images, ISPRS Int. J. Geo-Inf, 9, (2020); Feyisa G.L., Meilby H., Fensholt R., Proud S.R., Automated Water Extraction Index: A new technique for surface water mapping using Landsat imagery, Remote Sens. Environ, 140, pp. 23-35, (2014); Fisher A., Flood N., Danaher T., Comparing Landsat water index methods for automated water classification in eastern Australia, Remote Sens. Environ, 175, pp. 167-182, (2016); Mishra V.K., Pant T., Open surface water index: A novel approach for surface water mapping and extraction using multispectral and multisensory data, Remote Sens. Lett, 11, pp. 973-982, (2020); Khandelwal A., Karpatne A., Marlier M.E., Kim J., Lettenmaier D.P., Kumar V., An approach for global monitoring of surface water extent variations in reservoirs using MODIS data, Remote Sens. Environ, 202, pp. 113-128, (2017); Acharya T.D., Subedi A., Dong H.L., Evaluation of Machine Learning Algorithms for Surface Water Extraction in a Landsat 8 Scene of Nepal, Sensors, 19, (2019); Li L., Yan Z., Shen Q., Cheng G., Zhang B., Water Body Extraction from Very High Spatial Resolution Remote Sensing Data Based on Fully Convolutional Networks, Remote Sens, 11, (2019); Acharya T.D., Lee D.H., Yang I.T., Lee J.K., Identification of Water Bodies in a Landsat 8 OLI Image Using a J48 Decision Tree, Sensors, 16, (2016); Tao Y., Xu M., Zhang F., Du B., Zhang L., Unsupervised-Restricted Deconvolutional Neural Network for Very High Resolution Remote-Sensing Image Classification, IEEE Trans. Geosci. Remote Sens, 55, pp. 6805-6823, (2017); Yuan S.Y., Zhao Y., Xie T., SegNet-based first-break picking via seismic waveform classification directly from shot gathers with sparsely distributed traces, Pet. Sci, 19, pp. 162-179, (2021); Yu L., Wang Z., Tian S., Ye F., Ding J., Kong J., Convolutional Neural Networks for Water Body Extraction From Landsat Imagery, Int. J. Comput. Intell. Appl, 16, (2017); Rezaee M., Mahdianpari M., Zhang Y., Salehi B., Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 11, pp. 3030-3039, (2018); Long J., Shelhamer E., Darrell T., Fully Convolutional Networks for Semantic Segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 640-651, (2015); Isikdogan F., Bovik A.C., Passalacqua P., Surface Water Mapping by Deep Learning, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 10, pp. 4909-4918, (2017); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional Networks for Biomedical Image Segmentation, Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241; Feng W., Sui H., Huang W., Xu C., An K., Water Body Extraction From Very High-Resolution Remote Sensing Imagery Using Deep U-Net and a Superpixel-Based Conditional Random Field Model, IEEE Geosci. Remote Sens. Lett, 16, pp. 618-622, (2018); Duan L., Hu X., Multiscale Refinement Network for Water-Body Segmentation in High-Resolution Satellite Imagery, IEEE Geosci. Remote Sens. Lett, 17, pp. 686-690, (2019); Shamsolmoali P., Zareapoor M., Wang R., Zhou H., Yang J., A novel Deep Structure U-Net for Sea-Land Segmentation in Remote Sensing Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 12, pp. 3219-3232, (2020); Sun K., Xiao B., Liu D., Wang J., Deep High-Resolution Representation Learning for Human Pose Estimation, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Dang B., Li Y., MSResNet: Multiscale Residual Network via Self-Supervised Learning for Water-Body Detection in Remote Sensing Imagery, Remote Sens, 13, (2021); Chen L.C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell, 40, pp. 834-848, (2018); Ganin Y., Ustinova E., Ajakan H., Germain P., Larochelle H., Laviolette F., Marchand M., Lempitsky V., Domain-Adversarial Training of Neural Networks, J. Mach. Learn. Res, 17, pp. 2030-2096, (2016); Demir I., Koperski K., Lindenbaum D., Pang G., Huang J., Basu S., Hughes F., Tuia D., Raskar R., DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images, Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW); Sun X., Shi A., Huang H., Mayer H., BAS44Net: Boundary-Aware Semi-Supervised Semantic Segmentation Network for Very High Resolution Remote Sensing Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 5398-5413, (2020); Shao Z., Zhou W., Deng X., Zhang M., Cheng Q., Multilabel Remote Sensing Image Retrieval Based on Fully Convolutional Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 318-328, (2020); Shao Z., Yang K., Zhou W., Performance Evaluation of Single-Label and Multi-Label Remote Sensing Image Retrieval Using a Dense Labeling Dataset, Remote Sens, 10, (2018); Chen L.C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation, Proceedings of the Computer Vision—ECCV, pp. 833-851; Li H., Xiong P., Fan H., Sun J., DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9522-9531; Yu C., Wang J., Peng C., Gao C., Yu G., Sang N., BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation, Proceedings of the Computer Vision—ECCV, pp. 334-349; Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Muller A., Nothman J., Louppe G., Scikit-learn: Machine Learning in Python, J. Mach. Learn, 12, pp. 2825-2830, (2012); Kim W., Kanezaki A., Tanaka M., Unsupervised Learning of Image Segmentation Based on Differentiable Feature Clustering, IEEE Trans. Image Process, 29, pp. 8055-8068, (2020); Ouali Y., Hudelot C., Tami M., Autoregressive Unsupervised Image Segmentation, Proceedings of the European Conference on Computer Vision, pp. 142-158; Ball G.H., Hall D.J., ISODATA, a Novel Method of Data Analysis and Pattern Classification, (1965); Kanezaki A., Unsupervised Image Segmentation by Backpropagation, Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1543-1547","J. Liu; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China; email: jhliu003@gmail.com","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137121953"
"Hayes K.; Rajput A.","Hayes, Kimberley (58243500500); Rajput, Amit (58243992100)","58243500500; 58243992100","NDE 4.0: Image and Sound Recognition","2022","Handbook of Nondestructive Evaluation 4.0","","","","403","422","19","0","10.1007/978-3-030-73206-6_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159026109&doi=10.1007%2f978-3-030-73206-6_26&partnerID=40&md5=90563deb626f6cbabe382d6a3d02b21d","Valkim Technologies, LLC, San Antonio, TX, United States; XaasLabs Inc, Sunnyvale, CA, United States","Hayes K., Valkim Technologies, LLC, San Antonio, TX, United States; Rajput A., XaasLabs Inc, Sunnyvale, CA, United States","Advances powered by Artificial Intelligence (AI) centric technologies have enveloped nearly every aspect of our lives. Of the many aspects of AI, seven patterns have been classified, with the most common being the recognition pattern (Walch, Kathleen-Contributor, Cognitive Word-Contributor Group. September 17, 2019. The seven patterns of AI. https://www.forbes.com/sites/ cognitiveworld/2019/09/17/the-seven-patterns-of-ai/?sh=71056b2b12d0). This chapter focuses on pattern recognition with a subset emphasis on image and sound as it may relate to NDE 4.0. Optical character recognition (OCR) leveraged image recognition for the past decade in document conversion and computer-assisted check deposit may present precedence for AI-assisted flaw detection systems for radiographic images. Computer vision (CV) is the base building block for extraction of data from an image and can recognize objects using algorithms and machine learning concepts (Brownlee, Jason. May 22, 2019 (updated January 27, 2021). Deep Learning for Computer Vision. A Gentle Introduction to Object Recognition with Deep Learning. https://machinelearningmastery.com/object-recognition-with-deeplearning/). Computer vision has been integral in detection, segmentation, classification, monitoring, and prediction of radiographs in the medical community and has applicability in visual and radiographic inspection in industry and the NDE community. Sound recognition has a large portion of defect formations and flaw mechanical movements release energy in the method of elastic waves with the broad frequency spectrum. Typically, these signals are digitized and converted into amplitude time series. Regardless of their frequency content, these digital acoustics, or sound, signals can be analyzed and classified by any method that applies to time series data, including those developed specifically for audible sound signals such as deep learning algorithms. © Springer Nature Switzerland AG 2022.","Artificial intelligence; Computer vision; Convolutional neural network; Deep learning; Image recognition; Machine learning; Pattern recognition; Sound recognition","","","","","","","","Grady D., The vision thing: Mainly in the brain, Discover Magazine, (1993); Klinger A., Data structures and pattern recognition, Advances in information systems science, pp. 273-3100, (1978); Artificial Intelligence for Decision Making, (2006); Phillips-Wien G., Lakhmi J., Artificial intelligence for decision making, (2006); Schmelzer R., Data science vs machine learning vs. AI: Wow they work together, (2021); Klotzbucher M., Mazeika L., Samaitis V., Ashwin P., ENIQ publication. Qualification of an Artificial Intelligence/Machine Learning Non-destructive Testing System; Dietterich T.G., Kong E.B., Machine learning bias, statistical bias, and statistical variance of decision tree algorithms; Srivastav B., The inventive: All about machine learning for non-IT persons; Haynes S.D., Stone J., Cheung P.Y.K., Luk W., Video image processing with the sonic architecture, Computer, 33, 4, pp. 50-57, (2000); Tucker P., US Navy Turns to Drones, AI to Monitor Rust, (2020); Lecun Y., Bottou L., Bengio Y., Heffner P., Gradient-based learning applied to document recognition; Popescu D., Anania F.D., Cotet C.E., Amza C.G., Fully-automated liquid penetrant inspection line simulation model for increasing productivity, (2013); Beyeler M., Rokem A., Boynton G.M., Fine I., Learning to see again: Biological constraints on cortical plasticity and the implications for sight restoration technologies, J Neural Eng, 14, 5, (2017); The seven patterns of AI, (2019); Huang M., Wu D., Yu C.H., Fang Z., Interlandi M., Condie T., Cong J., Programming and runtime support to blaze FPGA accelerator deployment at datacenter scale, (2016); Ranjan R.K., Gulati T., Condition assessment of metallic objects using edge detection, Int J Adv Res Comput Sci Softw Eng, 4, 5, pp. 253-258, (2014); Barchiesi, Et al., Acoustic scene classification: Classifying environments from the sounds they produce; Vanderbrug G.J., Nagel R.N., Image pattern recognition in industrial inspection [*NBSIR 79-1764*]; Ji G., Zhu Y., Zhang Y., The corroded defect rating system of coating material based on computer vision, pp. 210-220; Bondadaa V., Kumar D., Cheruvu P., Kumara S., Detection and quantitative assessment of corrosion on pipelines through image analysis, ScienceDirect, 133, pp. 804-811, (2018); Motamedi, Et al., Dynamic Analysis of fixed cracks in composites by the extended finite element method; Lohade D.M., Chopade P.B., Metal inspection for surface defect detection by image thresholding; Choi K.Y., Kim S.S., Morphological analysis and classification of types of surface corrosion damage by digital image processing, Corros Sci, 47, 1, pp. 1-15, (2005); Itzhak D., Dinstein I., Zilberberg T., Pitting corrosion evaluation by computer image processing, Corros Sci, 21, 1, pp. 17-22, (1981); Nash W.T., Powell C.J., Drummond T., Birbilis N., Automated corrosion detection using crowdsourced training for deep learning, Corr J Sci Eng, (2019); Zeghidour N., Et al., LEAF: A learnable frontend for audio classification","K. Hayes; Valkim Technologies, LLC, San Antonio, United States; email: khayes712017@gmail.com","","Springer International Publishing","","","","","","","978-303073206-6; 978-303073205-9","","","English","Handb. of Nondestructive Evaluation 4.0","Book chapter","Final","","Scopus","2-s2.0-85159026109"
"Spasov A.; Petrova-Antonova D.","Spasov, A. (57279305200); Petrova-Antonova, D. (35932460300)","57279305200; 35932460300","TRANSFERABILITY ASSESSMENT of OPEN-SOURCE DEEP LEARNING MODEL for BUILDING DETECTION on SATELLITE DATA","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","46","4/W4-2021","","107","110","3","0","10.5194/isprs-archives-XLVI-4-W4-2021-107-2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118304763&doi=10.5194%2fisprs-archives-XLVI-4-W4-2021-107-2021&partnerID=40&md5=feed808f6b788229a44ffafaf18622b1","GATE Institute, Sofia University, Sofia, Bulgaria; Faculty of Mathematics and Informatics, Sofia University, Sofia, Bulgaria","Spasov A., GATE Institute, Sofia University, Sofia, Bulgaria; Petrova-Antonova D., GATE Institute, Sofia University, Sofia, Bulgaria, Faculty of Mathematics and Informatics, Sofia University, Sofia, Bulgaria","A great number of studies for identification and localization of buildings based on remote sensing data has been conducted over the past few decades. The majority of the more recent models make use of neural networks, which show high performance in semantic segmentation for the purpose of building detection even in complex regions like the city landscape. However, they could require a substantial amount of labelled training data depending on the diversity of objects targeted, which could be expensive and time consuming to acquire. Transfer Learning is a technique that could be used to reduce the amount of data and resources needed by applying knowledge obtained solving one problem to another one. In addition, if open-source data and models are used, this process is much more affordable. In this paper, the Transfer Learning challenges and issues are explored by utilizing an open-sourced pre-trained deep learning model on satellite data for building detection. © 2021 A. Spasov.","Computer vision; Convolutional neural networks; Image segmentation; Remote sensing; Transfer learning","Computer vision; Convolutional neural networks; Deep learning; Remote sensing; Semantics; Transfer learning; Building detection; Convolutional neural network; Images segmentations; Learning models; Localisation; Neural-networks; Open-source; Remote sensing data; Remote-sensing; Transfer learning; Image segmentation","","","","","Horizon 2020 WIDESPREAD-2018-2020; Horizon 2020 Framework Programme, H2020, (857155); Bulgarian National Science Fund, BNSF, (DN12/9)","This research work has been supported by Big Data for Smart Society (GATE) project, funded by Operational Programme Science and Education for Smart Growth under Grant Agreement No. BG05M2OP001-1.003-0002-C01 and funded by the Horizon 2020 WIDESPREAD-2018-2020 TEAMING Phase 2 programme under grant agreement no. 857155, and by Big4Smart project, funded by the Bulgarian National Science fund under agreement no. DN12/9.","Works CosmiQ, An open source machine learning pipeline for geospatial imagery, (2021); Li Q., Shi Y., Auer S., Roschlaub R., Most K., Schmitt M., Zhu X. X., Detection of Undocumented Buildings Using Convolutional Neural Network and Official Geodata, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 5, 2, pp. 517-524, (2020); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark, IEEE Int. Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229, (2017); Digital Globe Repository, (2021); 2021 QGIS User Guide; Ronneberger O., Fischer P., Brox T., U-net: Convolutional net-works for biomedical image segmentation, Int. Conf. on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z, Karpathy A., Khosla A., Berstein M., Berg C. A., Fei-Fei L., ImageNet Large Scale Visual Recognition Challenge, Int. Journal Computer Vision, 115, pp. 211-252, (2015); Accelerating Geospatial Machine Learning, (2021); Marathon Match - Solution Description, (2017); Vaiopoulos D., PanFusion: Image Fusion - Pansharp App, (2021); Geo Vekom, (2019); Zhao W., Ivanov I., Persello C., Building outline delineation: From very high resolution remote sensing imagery to polygons with an improved end-to-end learning framework, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 731-735, (2020); Zhao K., Kamran M., Sohn G., Boundary Regularized Building Footprint Extraction From Satellite Images Using Deep Neural Network, (2020); Zhu X. X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geoscience and Remote Sensing Magazine, 5, pp. 8-36, (2017)","A. Spasov; GATE Institute, Sofia University, Sofia, Bulgaria; email: angel.spasov@gate-ai.eu","Truong-Hong L.; Che E.; Jia F.; Emamgholian S.; Laefer D.; Vo A.V.","International Society for Photogrammetry and Remote Sensing","","16th 3D GeoInfo Conference 2021","11 October 2021 through 14 October 2021","New York City","172993","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118304763"
"Sharma R.; Sharma H.; Meena T.; Khandnor P.; Bansal P.; Sharma P.","Sharma, Rahul (58266130800); Sharma, Harshit (58435319900); Meena, Tamanna (57821867700); Khandnor, Padmavati (56857620100); Bansal, Palak (57201770911); Sharma, Paras (57217448934)","58266130800; 58435319900; 57821867700; 56857620100; 57201770911; 57217448934","Performance Evaluation of Deep Learning Models for Ship Detection","2022","Communications in Computer and Information Science","1568 CCIS","","","273","287","14","1","10.1007/978-3-031-11349-9_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135065300&doi=10.1007%2f978-3-031-11349-9_24&partnerID=40&md5=7388e0c5b8c059a3b29b9639a1e06ecc","Punjab Engineering College, Chandigarh, 160012, India","Sharma R., Punjab Engineering College, Chandigarh, 160012, India; Sharma H., Punjab Engineering College, Chandigarh, 160012, India; Meena T., Punjab Engineering College, Chandigarh, 160012, India; Khandnor P., Punjab Engineering College, Chandigarh, 160012, India; Bansal P., Punjab Engineering College, Chandigarh, 160012, India; Sharma P., Punjab Engineering College, Chandigarh, 160012, India","Although enormous success is achieved in the field of object detection, yet ship detection in high-resolution images is still an crucial task. Ship detection from optical remote sensing images plays a significant role in military and civil applications. For maritime surveillance, monitoring and traffic supervision ship detection deserves optimal solutions to identify objects accurately with faster speed. In this work, various object detection methods such as You Only Look Once (YOLO) v3, YOLO v4, RetinaNet152, EfficientDet-D2 and Faster-RCNN have been implemented to improve efficiency, speed and accuracy. Numerous experiments were conducted to evaluate the efficiency of object detection methods. The YOLO v4 with custom selection of anchor boxes using K-means++ clustering algorithm outperformed as compared to other detection methods in terms of accuracy, which is evaluated using COCO metrics, training and detection time. All the experiments are performed on the Airbus detection dataset from https://www.kaggle.com/c/airbus-ship-detection. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Convolution neural network; Satellite imagery; Ship detection; YOLO","Deep learning; K-means clustering; Military applications; Military photography; Object detection; Object recognition; Optical remote sensing; Satellite imagery; Ships; Vehicle performance; Convolution neural network; High-resolution images; Learning models; Object detection method; Objects detection; Optical remote sensing; Performances evaluation; Remote sensing images; Ship detection; You only look once; Efficiency","","","","","","","Airbus Ship Detection Challenge, (2021); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Adv. Neural. Inf. Process. Syst., 25, pp. 1097-1105, (2012); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Huang G., Liu Z., van Der Maaten L., Weinberger K.Q., Densely connected convolutional networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708, (2017); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117-2125, (2017); Nie M., Zhang J., Zhang X., Ship segmentation and orientation estimation using keypoints detection and voting mechanism in remote sensing images, International Symposium on Neural Networks, pp. 402-413, (2019); Stepec D., Martincic T., Skocaj D., Automated system for ship detection from medium resolution satellite optical imagery, Oceans 2019 MTS/IEEE, Seattle, Pp. 1–10, IEEE, (2019); Nie X., Duan M., Ding H., Hu B., Wong E.K., Attention mask R-CNN for ship detection and segmentation from remote sensing images, IEEE Access, 8, pp. 9325-9334, (2020); Li X., Cai K., Method research on ship detection in remote sensing image based on Yolo algorithm, 2020 International Conference on Information Science, Parallel and Distributed Systems (ISPDS), Pp. 104–108, IEEE, (2020); Li L., Zhou Z., Wang B., Miao L., An Z., Xiao X., Domain adaptive ship detection in optical remote sensing images, Remote Sens, 13, 16, (2021); Zhang Z.X., Et al., CCNet: A high-speed cascaded convolutional neural network for ship detection with multispectral images, Infrared. Millim. Waves, 38, 3, pp. 290-295, (2019); Huang Z., Sun S., Li R., Fast single-shot ship instance segmentation based on polar template mask in remote sensing images, IGARSS 2020–2020 IEEE International Geoscience and Remote Sensing Symposium, Pp. 1236–1239, IEEE, (2020); Polat M., Mohammed H.M.A., Oral E.A., Ship Detection in Satellite Images. In: ISASE2018, (2018); Xia X., Lu Q., Gu X., Exploring an easy way for imbalanced data sets in semantic image segmentation, J. Phys. Conf. Ser., 1213, 2, (2019); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Hordiiuk D., Oliinyk I., Hnatushenko V., Maksymov K., Semantic segmentation for ships detection from satellite imagery, 2019 IEEE 39Th International Conference on Electronics and Nanotechnology (ELNANO), pp. 454-457, (2019); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258, (2017); de Vieilleville F., May S., Lagrange A., Dupuis A., Ruiloba R., Simplification of deep neural networks for image analysis at the edge, Actes De La Conférence CAID 2020, (2020); Smith B., Chester S., Coady Y., Ship detection in satellite optical imagery, 2020 3Rd Artificial Intelligence and Cloud Computing Conference, pp. 11-18, (2020); Talon P., Perez-Villar J.I.B., Hadland A., Wyniawskyj N.S., Petit D., Wilson M., Ship detection on single-band grayscale imagery using deep learning and AIS signal matching using non-rigid transformations, IGARSS 2020–2020 IEEE International Geoscience and Remote Sensing Symposium, Pp. 248–251, IEEE, (2020); Tan M., Le Q., Efficientnet: Rethinking model scaling for convolutional neural networks, International Conference on Machine Learning, pp. 6105-6114, (2019); Karki S., Kulkarni S., Ship detection and segmentation using Unet, 2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT), Pp. 1–7, IEEE, (2021); Howard J., Gugger S., Fastai: A layered API for deep learning, Information, 11, 2, (2020); Szegedy C., Et al., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Chen J., Xie F., Lu Y., Jiang Z., Finding arbitrary-oriented ships from remote sensing images using corner detection, IEEE Geosci. Remote Sens. Lett., 17, 10, pp. 1712-1716, (2019); Rogers C., Et al., Adversarial artificial intelligence for overhead imagery classification models, 2019 Systems and Information Engineering Design Symposium (SIEDS), Pp. 1–6, IEEE, (2019); Hu J., Zhi X., Zhang W., Ren L., Bruzzone L., Salient ship detection via background prior and foreground constraint in remote sensing images, Remote Sens, 12, 20, (2020); Xu W., Zhang C., Wu M., Multi-scale deep residual network for satellite image super-resolution reconstruction, Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pp. 332-340, (2019); Duan Y., Li Z., Tao X., Li Q., Hu S., Lu J., EEG-based maritime object detection for iot-driven surveillance systems in smart ocean, IEEE Internet Things J, 7, 10, pp. 9678-9687, (2020); Haas L.F., Hans berger (1873–1941), richard caton (1842–1926), and electroencephalogra-phy, J. Neurol. Neurosurg. Psychiatry, 74, 1, (2003); Ashton K., That ‘internet of things’ thing, RFID J, 22, 7, pp. 97-114, (2009); Zhong Z., Li Y., Han Z., Yang Z., Ship target detection based on Lightgbm algorithm, 2020 International Conference on Computer Information and Big Data Applications (CIBDA), pp. 425-429, (2020); Ojala T., Pietikainen M., Harwood D., A comparative study of texture measures with classification based on featured distributions, Pattern Recogn, 29, 1, pp. 51-59, (1996); Ke G., Et al., Lightgbm: A highly efficient gradient boosting decision tree, Adv. Neural. Inf. Process. Syst., 30, pp. 3146-3154, (2017); Zhou X., Wang D., Krahenbuhl P., Objects as Points, (2019); Wang J., Yang W., Guo H., Zhang R., Xia G.S., Tiny object detection in aerial images, 2020 25Th International Conference on Pattern Recognition (ICPR), pp. 3791-3798, (2021); Yu F., Wang D., Shelhamer E., Darrell T., Deep layer aggregation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2403-2412, (2018); Cordova A.W.A., Quispe W.C., Inca R.J.C., Choquehuayta W.N., Gutierrez E.C., New approaches and tools for ship detection in optical satellite imagery, J. Phys. Conf. Ser., 1642, 1, (2020); van Etten A., You Only Look Twice: Rapid Multi-Scale Object Detection in Satellite Imagery. Arxiv Preprint Arxiv, 1805, (2018); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., Yolov4: Optimal Speed and Accuracy of Object Detection, (2020); Redmon J., Farhadi A., Yolov3: An Incremental Improvement, (2018); Mohamed E., Shaker A., Rashed H., El-Sallab A., Hadhoud M., INSTA-YOLO: Real-Time Instance Segmentation, (2021); Arthur D., Vassilvitskii S., K-Means++: The Advantages of Careful Seeding, (2006); Jin X., Han J., K-means clustering, Encyclopedia of Machine Learning, pp. 563-564, (2010); Tan M., Pang R., Le Q.V., Efficientdet: Scalable and efficient object detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10781-10790, (2020); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision (ICCV), Pp. 2980–, 2988, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2016); Ng A.Y., Jordan M.I., Weiss Y., On spectral clustering: Analysis and an algorithm, Advances in Neural Information Processing Systems, pp. 849-856, (2002); Zepeda-Mendoza M.L., Resendis-Antonio O., Hierarchical agglomerative clustering, Ency. Syst. Biol., 43, 1, pp. 886-887, (2013); Lin T.-Y., Et al., Detection Evaluation, (2021)","T. Meena; Punjab Engineering College, Chandigarh, 160012, India; email: tamanna.meena95@gmail.com","Raman B.; Murala S.; Chowdhury A.; Dhall A.; Goyal P.","Springer Science and Business Media Deutschland GmbH","","6th International Conference on Computer Vision and Image Processing, CVIP 2021","3 December 2021 through 5 December 2021","Virtual, Online","281229","18650929","978-303111348-2","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85135065300"
"Chen Y.; Ming D.; Yu J.; Xu L.; Ma Y.; Li Y.; Ling X.; Zhu Y.","Chen, Yangyang (57203919663); Ming, Dongping (55236226300); Yu, Junchuan (35182162900); Xu, Lu (57202712306); Ma, Yanni (57216813008); Li, Yan (58449091600); Ling, Xiao (57205597826); Zhu, Yueqin (36702385700)","57203919663; 55236226300; 35182162900; 57202712306; 57216813008; 58449091600; 57205597826; 36702385700","Susceptibility-Guided Landslide Detection Using Fully Convolutional Neural Network","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","998","1018","20","0","10.1109/JSTARS.2022.3233043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146224539&doi=10.1109%2fJSTARS.2022.3233043&partnerID=40&md5=7e10ab36cae296598d8060319c4455f7","China Aero Geophysical Survey and Remote Sensing Center for Natural Resources, Beijing, 100083, China; China University of Geosciences, School of Information Engineering, Beijing, 100083, China; Ministry of Natural Resources of China, Polytechnic Center for Natural Resources Big-data, Beijing, 100036, China; Ministry of Emergency Management, National Institute of Natural Hazards, Beijing, 100085, China","Chen Y., China Aero Geophysical Survey and Remote Sensing Center for Natural Resources, Beijing, 100083, China; Ming D., China University of Geosciences, School of Information Engineering, Beijing, 100083, China, Ministry of Natural Resources of China, Polytechnic Center for Natural Resources Big-data, Beijing, 100036, China; Yu J., China Aero Geophysical Survey and Remote Sensing Center for Natural Resources, Beijing, 100083, China; Xu L., China University of Geosciences, School of Information Engineering, Beijing, 100083, China; Ma Y., China Aero Geophysical Survey and Remote Sensing Center for Natural Resources, Beijing, 100083, China, China University of Geosciences, School of Information Engineering, Beijing, 100083, China; Li Y., China University of Geosciences, School of Information Engineering, Beijing, 100083, China; Ling X., China University of Geosciences, School of Information Engineering, Beijing, 100083, China; Zhu Y., Ministry of Emergency Management, National Institute of Natural Hazards, Beijing, 100085, China","Automatic landslide detection based on very high spatial resolution remote sensing images is crucial for disaster prevention and mitigation applications. With the rapid development of deep-learning techniques, state-of-The-Art semantic segmentation methods based on fully convolutional network (FCNN) have achieved outstanding performance in the landslide detection task. However, most of the existing articles only utilize visual features. Even if the advanced FCNN models are applied, there is still a certain amount of falsely detected and miss detected landslides. In this article, we innovatively introduce landslide susceptibility as prior knowledge and propose an innovative susceptibility-guided landslide detection method based on FCNN (SG-FCNN) to detect landslides from single temporal images. In addition, an unsupervised change detection method based on the mean changing magnitude of objects (MCMO) is further proposed and integrated with the SG-FCNN to detect newly occurred landslides from bitemporal images. The effectiveness of the proposed SG-FCNN and MCMO has been tested in Lantau Island, Hong Kong. The experimental results show that the SG-FCNN can significantly reduce the amount of falsely detected and miss detected landslides compared with the FCNN. It can conclude that applying landslide susceptibility as prior knowledge is much more effective than using visual features only, which introduces a new methodology of landslide detection and lifts the detection performance to a new level.  © 2008-2012 IEEE.","Convolutional neural network (CNN); landslide detection; landslide susceptibility mapping; Lantau Island; remote sensing","China; Hong Kong; Lantau Island; Convolution; Deep learning; Disaster prevention; Landslides; Neural networks; Remote sensing; Semantic Segmentation; Semantics; Convolutional neural network; Features extraction; Landslide detection; Landslide susceptibility mapping; Lantau island; Remote-sensing; Support vectors machine; Terrain factors; artificial neural network; detection method; disaster management; efficiency measurement; hazard assessment; landslide; mitigation; performance assessment; remote sensing; satellite data; slope stability; spatial resolution; unsupervised classification; Feature extraction","","","","","","","Deng X., Xu D., Zeng M., Qi Y., Landslides and cropland abandonment in China's mountainous areas: Spatial distribution, empirical analysis and policy implications, Sustainability, 10, 11, (2018); Lam C.L.H., Lau J.W.C., Chan H.W., Factual Report on Hong Kong Rainfall and Landslides in 2008, (2009); Chae B.-G., Park H.-J., Catani F., Simoni A., Berti M., Landslide prediction, monitoring and early warning: A concise review of state-ofthe-Art, Geosci. J, 21, 6, pp. 1033-1070, (2017); Galli M., Ardizzone F., Cardinali M., Guzzetti F., Reichenbach P., Comparing landslide inventory maps, Geomorphology, 94, 3-4, pp. 268-289, (2008); Van Westen C.J., Castellanos E., Kuriakose S.L., Spatial data for landslide susceptibility, hazard, and vulnerability assessment: An overview, Eng. Geol, 102, 3-4, pp. 112-131, (2008); Guzzetti F., Mondini A.C., Cardinali M., Fiorucci F., Santangelo M., Chang K.-T., Landslide inventory maps: New tools for an old problem, Earth-Sci. Rev, 112, 1-2, pp. 42-66, (2012); Van Westen C., Van Asch T.W., Soeters R., Landslide hazard and risk zonation-Why is it still so difficult?, Bull. Eng. Geol. Environ, 65, 2, pp. 167-184, (2006); Brunsden D., Landslide types, mechanisms, recognition, identification, Landslides in the South Wales Coalfield, pp. 1-3, (1985); Xing A., Et al., Dynamic analysis and field investigation of a fluidized landslide in Guanling, Guizhou, China, Eng. Geol, 181, pp. 1-14, (2014); Metternicht G., Hurni L., Gogu R., Remote sensing of landslides: An analysis of the potential contribution to geo-spatial systems for hazard assessment inmountainous environments, Remote Sens. Environ, 98, 2-3, pp. 284-303, (2005); Nichol J.E., Shaker A., Wong M.-S., Application of high-resolution stereo satellite images to detailed landslide hazard assessment, Geomorphology, 76, 1-2, pp. 68-75, (2006); Zhu X.X., Et al., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geosci. Remote Sens. Mag, 5, 4, pp. 8-36; Catani F., Landslide detection by deep learning of non-nadiral and crowdsourced optical images, Landslides, 18, 3, pp. 1025-1044, (2021); Li Z., Shi W., Lu P., Yan L., Wang Q., Miao Z., Landslide mapping from aerial photographs using change detection-based Markov random field, Remote Sens. Environ, 187, pp. 76-90, (2016); Speight J.G., Landform pattern description from aerial photographs, Photogrammetria, 32, 5, pp. 161-182, (1977); Cai H., Chen T., Niu R., Plaza A., Landslide detection using densely connected convolutional networks and environmental conditions, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 5235-5247; Danneels G., Pirard E., Havenith H.-B., Automatic landslide detection from remote sensing images using supervised classification methods, Proc IEEE Int. Geosci. Remote Sens. Symp, pp. 3014-3017, (2007); Moosavi V., Talebi A., Shirmohammadi B., Producing a landslide inventory map using pixel-based and object-oriented approaches optimized by Taguchi method, Geomorphology, 204, pp. 646-656, (2014); Bui D.T., Et al., Landslide detection and susceptibility mapping by airsar data using support vector machine and index of entropy models in Cameron highlands, Malaysia, Remote Sens, 10, 10, (2018); Stumpf A., Kerle N., Object-oriented mapping of landslides using random forests, Remote Sens. Environ, 115, 10, pp. 2564-2577, (2011); Zhao W., Du S., Learning multiscale and deep representations for classifying remotely sensed imagery, ISPRS J. Photogramm. Remote Sens, 113, pp. 155-165, (2016); Zhong C., Et al., Landslide mapping with remote sensing: Challenges and opportunities, Int. J. Remote Sens, 41, 4, pp. 1555-1581, (2020); Wan L., Chen T., Plaza A., Cai H., Hyperspectral unmixing based on spectral and sparse deep convolutional neural networks, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 11669-11682, (2021); Larochelle H., Bengio Y., Louradour J., Lamblin P., Exploring strategies for training deep neural networks, J. Mach. Learn. Res, 10, pp. 1-40, (2009); Zhou W., Shao Z., Cheng Q., Deep feature representations for highresolution remote sensing scene classification, Proc. 4th Int.Workshop Earth Observ. Remote Sens. Appl, pp. 338-342, (2016); Chen Y., Ming D., Lv X., Superpixel based land cover classification of VHR satellite image combining multi-scale CNN and scale parameter estimation, Earth Sci. Inform, 12, 3, pp. 341-363, (2019); He D., Shi Q., Liu X., Zhong Y., Zhang X., Deep subpixel mapping based on semantic information modulated network for urban land use mapping, IEEE Trans. Geosci. Remote Sens, 59, 12, pp. 10628-10646; Li X., He M., Li H., Shen H., A combined loss-based multiscale fully convolutional network for high-resolution remote sensing image change detection, IEEE Geosci. Remote Sens. Lett, 19; Shi Q., Liu M., Li S., Liu X., Wang F., Zhang L., A deeply supervised attentionmetric-based network and an open aerial image dataset for remote sensing change detection, IEEE Trans. Geosci. Remote Sens, 60; Wu W., Et al., Application of local fully convolutional neural network combined with YOLO v5 algorithm in small target detection of remote sensing image, PLoS One, 16, 10; Chen Y., Ming D., Ling X., Lv X., Zhou C., Landslide susceptibility mapping using feature fusion-based CPCNN-ML in Lantau Island, Hong Kong, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 3625-3639, (2021); Lv L., Chen T., Dou J., Plaza A., A hybrid ensemble-based deeplearning framework for landslide susceptibility mapping, Int. J. Appl. Earth Observ. Geo-Inf, 108, (2022); Ma Z., Mei G., Prezioso E., Zhang Z., Xu N., A deep learning approach using graph convolutional networks for slope deformation prediction based on time-series displacement data, Neural Comput. Appl, 33, 21, pp. 14441-14457, (2021); Meng Q., Wang H., He M., Gu J., Qi J., Yang L., Displacement prediction of water-induced landslides using a recurrent deep learning model, Eur. J. Environ. Civil Eng, pp. 1-15, (2020); Ghorbanzadeh O., Crivellari A., Ghamisi P., Shahabi H., Blaschke T., A comprehensive transferability evaluation of U-Net and ResU-Net for landslide detection from sentinel-2 data (case study areas from Taiwan, China, and Japan), Sci. Rep, 11, 1; Yu H., Ma Y., Wang L., Zhai Y., Wang X., A landslide intelligent detection method based on CNN and RSG-R, Proc IEEE Int. Conf. Mechatron. Automat, pp. 40-44, (2017); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of different machine learning methods and deeplearning convolutional neural networks for landslide detection, Remote Sens, 11, 2, (2019); Sameen M.I., Pradhan B., Landslide detection using residual networks and the fusion of spectral and topographic information, IEEE Access, 7, pp. 114363-114373, (2019); Ji S., Yu D., Shen C., Li W., Xu Q., Landslide detection from an open satellite imagery and digital elevation model dataset using attention boosted convolutional neural networks, Landslides, 17, 6, pp. 1337-1352, (2020); Bragagnolo L., Rezende L.R., Da Silva R.V., Grzybowski J.M.V., Convolutional neural networks applied to semantic segmentation of landslide scars, Catena, 201; Meena S.R., Et al., Landslide detection in the Himalayas using machine learning algorithms andU-Net, Landslides, 19, 5, pp. 1209-1229, (2022); Prakash N., Manconi A., Loew S., Mapping landslides on EO data: Performance of deep learning models vs.Traditional machine learning models, Remote Sens, 12, 3, (2020); Liu P., Wei Y., Wang Q., Chen Y., Xie J., Research on post-earthquake landslide extraction algorithm based on improved u-net model, Remote Sens, 12, 5, (2020); Ghorbanzadeh O., Gholamnia K., Ghamisi P., The application of ResU-Net and OBIA for landslide detection frommulti-Temporal sentinel-2 images, Big Earth Data, pp. 1-26, (2022); Liu P., Et al., A research on landslides automatic extraction model based on the improved mask R-CNN, ISPRS Int. J. Geo-Inf, 10, 3, (2021); Ghorbanzadeh O., Meena S.R., Abadi H.S.S., Piralilou S.T., Zhiyong L., Blaschke T., Landslide mapping using two main deep-learning convolution neural network streams combined by the Dempster-Shafer model, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 452-463, (2021); Lei T., Zhang Y., Lv Z., Li S., Liu S., Nandi A.K., Landslide inventory mapping from bitemporal images using deep convolutional neural networks, IEEE Geosci. Remote Sens. Lett, 16, 6, pp. 982-986; Gao X., Chen T., Niu R., Plaza A., Recognition and mapping of landslide using a fully convolutional DenseNet and influencing factors, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 7881-7894, (2021); Liu T., Chen T., Niu R., Plaza A., Landslide detection mapping employing CNN, ResNet, and DenseNet in the Three Gorges Reservoir, China, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 14, pp. 11417-11428, (2021); Ghorbanzadeh O., Shahabi H., Crivellari A., Homayouni S., Blaschke T., Ghamisi P., Landslide detection using deep learning and object-based image analysis, Landslides, 19, 4, pp. 929-939, (2022); Shi W., Zhang M., Ke H., Fang X., Zhan Z., Chen S., Landslide recognition by deep convolutional neural network and change detection, IEEE Trans. Geosci. Remote Sens, 59, 6, pp. 4654-4672; Cheng K.S., Wei C., Chang S., Locating landslides using multitemporal satellite images, Adv. Space Res, 33, 3, pp. 296-301, (2004); Wen D., Et al., Change detection from very-high-spatial-resolution optical remote sensing images: Methods, applications, and future directions, IEEE Geosci. Remote Sens. Mag, 9, 4, pp. 68-101; Reichenbach P., Rossi M., Malamud B.D., Mihir M., Guzzetti F., A review of statistically-based landslide susceptibility models, Earth-Sci. Rev, 180, pp. 60-91, (2018); Guzzetti F., Landslide Hazard and Risk Assessment, (2006); Langford R., James J., Shaw R., Campbell S., Kirk P., Sewell R., Geology of Lantau District, 6, (1995); Climate Change and Extreme Landslide Events, (2021); Su Z., Chow J.K., Tan P.S., Wu J., Ho Y.K., Wang Y.-H., Deep convolutional neural network-based pixel-wise landslide inventory mapping, Landslides, 18, 4, pp. 1421-1443, (2021); Venture M.-F.J., Final Report on Compilation of the Enhanced Natural Terrain Landslide Inventory (ENTLI), (2007); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention, pp. 234-241, (2015); Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.-C., Mobilenetv2: Inverted residuals and linear bottlenecks, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 4510-4520, (2018); Odena A., Dumoulin V., Olah C., Deconvolution and Checkerboard Artifacts, (2016); Milletari F., Navab N., Ahmadi S.-A., V-net: Fully convolutional neural networks for volumetric medical image segmentation, Proc. 4th Int. Conf. 3D Vis, pp. 565-571, (2016); Ho Y., Wookey S., The real-world-weight cross-entropy loss function: Modeling the costs of mislabeling, IEEE Access, 8, pp. 4806-4813, (2020); Weisstein E.W., Moore neighborhood, From MathWorld-A Wolfram Web Resource, (2005); Brabb E.E., Innovative approaches to landslide hazard and risk mapping, Proc. Int. Landslide Symp, pp. 17-22, (1985); Pardeshi S.D., Autade S.E., Pardeshi S.S., Landslide hazard assessment: Recent trends and techniques, Springer Plus, 2, 1, pp. 1-11, (2013); Wang Y., Fang Z., Hong H., Comparison of convolutional neural networks for landslide susceptibility mapping in Yanshan County, China, Sci. Total Environ, 666, pp. 975-993, (2019); Fang Z., Wang Y., Peng L., Hong H., Integration of convolutional neural network and conventionalmachine learning classifiers for landslide susceptibility mapping, Comput.Geosci, 139; Wang H., Zhang L., Luo H., He J., Cheung R., AI-powered landslide susceptibility assessment in Hong Kong, Eng. Geol, 288; Krahenbohl P., Koltun V., Efficient inference in fully connected CRFs with Gaussian edge potentials, Adv. Neural Inf. Process. Syst, 24, pp. 109-117, (2011); Shotton J., Winn J., Rother C., Criminisi A., Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context, Int. J. Comput. Vis, 81, 1, pp. 2-23, (2009); Soille P., Morphological Image Analysis: Principles and Applications, (1999); Li Z., Shi W., Myint S.W., Lu P., Wang Q., Semi-Automated landslide inventory mapping from bitemporal aerial photographs using change detection and level set method, Remote Sens. Environ, 175, pp. 215-230, (2016); Lv Z.Y., Shi W., Zhang X., Benediktsson J.A., Landslide inventory mapping from bitemporal high-resolution remote sensing images using change detection and multiscale segmentation, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens, 11, 5, pp. 1520-1532; Gong P., Et al., Finer resolution observation and monitoring of global land cover: First mapping results with Landsat TM and ETM+ data, Int. J. Remote Sens, 34, 7, pp. 2607-2654, (2013)","D. Ming; China University of Geosciences, School of Information Engineering, Beijing, 100083, China; email: mingdp@cugb.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","19391404","","","","English","IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146224539"
"Carranza-García M.; Lara-Benítez P.; García-Gutiérrez J.; Riquelme J.C.","Carranza-García, Manuel (57205743857); Lara-Benítez, Pedro (57215861341); García-Gutiérrez, Jorge (57209423602); Riquelme, José C. (57219716128)","57205743857; 57215861341; 57209423602; 57219716128","Enhancing object detection for autonomous driving by optimizing anchor generation and addressing class imbalance","2021","Neurocomputing","449","","","229","244","15","19","10.1016/j.neucom.2021.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104482306&doi=10.1016%2fj.neucom.2021.04.001&partnerID=40&md5=30661692e9442238a28a7633d5c8ea49","Division of Computer Science, University of Sevilla, Seville, ES-41012, Spain","Carranza-García M., Division of Computer Science, University of Sevilla, Seville, ES-41012, Spain; Lara-Benítez P., Division of Computer Science, University of Sevilla, Seville, ES-41012, Spain; García-Gutiérrez J., Division of Computer Science, University of Sevilla, Seville, ES-41012, Spain; Riquelme J.C., Division of Computer Science, University of Sevilla, Seville, ES-41012, Spain","Object detection has been one of the most active topics in computer vision for the past years. Recent works have mainly focused on pushing the state-of-the-art in the general-purpose COCO benchmark. However, the use of such detection frameworks in specific applications such as autonomous driving is yet an area to be addressed. This study presents an enhanced 2D object detector based on Faster R-CNN that is better suited for the context of autonomous vehicles. Two main aspects are improved: the anchor generation procedure and the performance drop in minority classes. The default uniform anchor configuration is not suitable in this scenario due to the perspective projection of the vehicle cameras. Therefore, we propose a perspective-aware methodology that divides the image into key regions via clustering and uses evolutionary algorithms to optimize the base anchors for each of them. Furthermore, we add a module that enhances the precision of the second-stage header network by including the spatial information of the candidate regions proposed in the first stage. We also explore different re-weighting strategies to address the foreground-foreground class imbalance, showing that the use of a reduced version of focal loss can significantly improve the detection of difficult and underrepresented objects in two-stage detectors. Finally, we design an ensemble model to combine the strengths of the different learning strategies. Our proposal is evaluated with the Waymo Open Dataset, which is the most extensive and diverse up to date. The results demonstrate an average accuracy improvement of 6.13% mAP when using the best single model, and of 9.69% mAP with the ensemble. The proposed modifications over the Faster R-CNN do not increase computational cost and can easily be extended to optimize other anchor-based detection frameworks. © 2021 Elsevier B.V.","Anchor optimization; Autonomous vehicles; Class imbalance; Convolutional neural networks; Deep learning; Object detection","Autonomous vehicles; Clustering algorithms; Deep neural networks; Object recognition; 2D objects; Anchor optimization; Autonomous driving; Autonomous Vehicles; Class imbalance; Convolutional neural network; Deep learning; Detection framework; Objects detection; State of the art; Object detection","","","","","Andalusian Regional Government, (P18-RT-2778, US-1263341); Nvidia; Ministerio de Ciencia, Innovación y Universidades, MCIU; European Regional Development Fund, ERDF; Agencia Estatal de Investigación, AEI, (TIN2017-88209-C2)","Funding text 1: This research has been funded by FEDER/Ministerio de Ciencia, Innovación y Universidades - Agencia Estatal de Investigación/Proyecto TIN2017-88209-C2 and by the Andalusian Regional Government under the projects: BIDASGRI: Big Data technologies for Smart Grids (US-1263341), Adaptive hybrid models to predict solar and wind renewable energy production (P18-RT-2778). ; Funding text 2: We are grateful to NVIDIA for their GPU Grant Program that has provided us the high-quality GPU devices for carrying out the study.","Alvaro Arcos-Garcia, Alvarez Garcia J.A., Soria-Morillo L.M., Evaluation of deep neural networks for traffic sign detection systems, Neurocomputing, 316, pp. 332-344, (2018); Yin J., Wang W., Meng Q., Yang R., Shen J., A Unified Object Motion and Affinity Model for Online Multi-Object Tracking, in, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 6767-6776, (2020); Hoseini P., Blankenburg J., Nicolescu M., Nicolescu M., Feil-Seifer D., Active eye-in-hand data management to improve the robotic object detection performance, Computers, 8, 4, (2019); Salazar Gonzalez J.L., Zaccaro C., Alvarez Garcia J.A., Soria Morillo L.M., Sancho Caparrini F., Real-time gun detection in CCTV: An open problem, Neural Networks, 132, pp. 297-308, (2020); Carranza-Garcia M., Garcia-Gutierrez J., Riquelme J.C., A framework for evaluating land use and land cover classification using convolutional neural networks, Remote Sensing, 11, 3, (2019); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Hassaballah M., Awad A.I., Deep Learning in Computer Vision: Principles and Applications, CRC Press, (2020); Litman T., (2020); Liu F., Zhao F., Liu Z., Hao H., Can autonomous vehicle reduce greenhouse gas emissions? A country-level evaluation, Energy Policy, 132, pp. 462-473, (2019); Liu L., Ouyang W., Wang X., Fieguth P.W., Chen J., Liu X., Pietikainen M., Deep learning for generic object detection: A survey, International Journal of Computer Vision, 128, pp. 261-318, (2019); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common Objects in Context, Computer Vision – ECCV 2014, pp. 740-755, (2014); Zhao Z., Zheng P., Xu S., Wu X., Object detection with deep learning: a review, IEEE Transactions on Neural Networks and Learning Systems, 30, 11, pp. 3212-3232, (2019); Pang J., Chen K., Shi J., Feng H., Ouyang W., Lin D., Libra R-CNN: towards balanced learning for object detection, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 821-830, (2019); Sun P., Kretzschmar H., Dotiwalla X., Chouard A., Patnaik V., Tsui P., Guo J., Zhou Y., Chai Y., Caine B., Vasudevan V., Han W., Ngiam J., Zhao H., Timofeev A., Ettinger S., Krivokon M., Gao A., Joshi A., Zhao S., Cheng S., Zhang Y., Shlens J., Chen Z., Anguelov D., Scalability in perception for autonomous driving, Waymo Open Dataset, (2019); Geiger A., Lenz P., Urtasun R., Are we ready for autonomous driving? The KITTI vision benchmark suite, in, IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354-3361, (2012); Liu Y., Wang Y., Wang S., Liang T., Zhao Q., Tang Z., Ling H., CBNet: A novel composite backbone network architecture for object detection, pp. 11653-11660, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S.E., Fu C., Berg A.C., SSD: Single shot multibox detector, 9905, pp. 21-37, (2016); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 580-587, (2014); Dai J., Li Y., He K., Sun J., R-FCN: object detection via region-based fully convolutional networks, Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 379-387, (2016); Lin T., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 936-944, (2017); Cheng G., Han J., Zhou P., Xu D., Learning rotation-invariant and fisher discriminative convolutional neural networks for object detection, IEEE Transactions on Image Processing, 28, 1, pp. 265-278, (2019); Cheng G., Yang J., Gao D., Guo L., Han J., High-quality proposals for weakly supervised object detection, IEEE Transactions on Image Processing, 29, pp. 5794-5804, (2020); Cai Z., Vasconcelos N., Cascade R-CNN: delving into high quality object detection, IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 6154-6162, (2018); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 779-788, (2016); Lin T., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, IEEE Transactions on Pattern Analysis and Machine Intelligence, 42, 2, pp. 318-327, (2020); Tian Z., Shen C., Chen H., He T., FCOS: fully convolutional one-stage object detection, IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9626-9635, (2019); Zhou X., Wang D., Krahenbuhl P., Objects as points, CoRR abs/1904.07850, (2019); Zhou X., Zhuo J., Krahenbuhl P., Bottom-up object detection by grouping extreme and center points, vol. 2019-June, IEEE Computer Society, pp. 850-859, (2019); Law H., Deng J., CornerNet: detecting objects as paired keypoints, International Journal of Computer Vision, 128, 3, pp. 642-656, (2020); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, 3rd International Conference on Learning Representations ICLR, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778, (2016); He K., Gkioxari G., Dollar P., Girshick R., Mask R.-C.N.N.; Xie S., Girshick R., Dollar P., Tu Z., He K., Aggregated residual transformations for deep neural networks, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5987-5995, (2017); Gao S., Cheng M., Zhao K., Zhang X., Yang M., Torr P.H.S., Res2Net: a new multi-scale backbone architecture, IEEE Transactions on Pattern Analysis and Machine Intelligence, (2019); Wang J., Sun K., Cheng T., Jiang B., Deng C., Zhao Y., Liu D., Mu Y., Tan M., Wang X., Et al., Deep high-resolution representation learning for visual recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, (2020); Redmon J., Farhadi A., YOLOv3: an incremental improvement, CoRR abs/1804.02767, (2018); Ghiasi G., Lin T., Le Q.V., NAS-FPN: learning scalable feature pyramid architecture for object detection, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7029-7038, (2019); Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L., MobileNetV2: inverted residuals and linear bottlenecks, IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 4510-4520, (2018); Jiao L., Zhang F., Liu F., Yang S., Li L., Feng Z., Qu R., A survey of deep learning-based object detection, CoRR abs/1907.09408, (2019); Caesar H., Bankiti V., Lang A.H., Vora S., Liong V.E., Xu Q., Krishnan A., Pan Y., Baldan G., Beijbom O., nuscenes: A multimodal dataset for autonomous driving, CoRR abs/1903.11027, (2019); Scale H., (2019); Feng D., Haase-Schuetz C., Rosenbaum L., Hertlein H., Duffhauss F., Glaser C., Wiesbeck W., Dietmayer K., Deep multi-modal object detection and semantic segmentation for autonomous driving, datasets, methods, and challenges, CoRR abs/1902.07830, (2019); Carranza-Garcia M., Torres-Mateo J., Lara-Benitez P., Garcia-Gutierrez J., On the performance of one-stage and two-stage object detectors in autonomous vehicles using camera data, Remote Sensing, 13, 1, (2021); Rajaram R.N., Ohn-Bar E., Trivedi M.M., RefineNet: refining object detectors for autonomous driving, IEEE Transactions on Intelligent Vehicles, 1, 4, pp. 358-368, (2016); Wang Y., Liu Z., Deng W., Anchor generation optimization and region of interest assignment for vehicle detection, Sensors, 19, (2019); Hassaballah M., Kenk M.A., Muhammad K., Minaee S., Vehicle detection and tracking in adverse weather using a deep learning framework, IEEE Transactions on Intelligent Transportation Systems, pp. 1-13, (2020); Yin J., Shen J., Guan C., Zhou D., Yang R., LiDAR-based online 3D video object detection with graph-based message passing and spatiotemporal transformer attention, IEEE Computer Society, pp. 11492-11501, (2020); Meng Q., Wang W., Zhou T., Shen J., (2020); Liang Z., Shen J., Local semantic siamese networks for fast tracking, IEEE Transactions on Image Processing, 29, pp. 3351-3364, (2020); Dong X., Shen J., Wu D., Guo K., Jin X., Porikli F., Quadruplet network with one-shot learning for fast visual object tracking, IEEE Transactions on Image Processing, 28, 7, pp. 3516-3527, (2019); Carranza-Garcia M., Lara-Benitez P., (2020); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I., Wojna Z., Song Y., Guadarrama S., Murphy K., Speed/accuracy trade-offs for modern convolutional object detectors, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 3296-3297, (2017); Ahmad M., Abdullah M., Han D., Small Object Detection in Aerial Imagery using RetinaNet with Anchor Optimization, in: 2020 International Conference on Electronics, Information, and Communication (ICEIC), pp. 1-3, (2020); Zlocha M., Dou Q., Glocker B., Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels, CoRR abs/1906.02283, (2019); Oksuz K., Cam B.C., Kalkan S., Akbas E., Imbalance problems in object detection: a review, IEEE Transactions on Pattern Analysis and Machine Intelligence, (2020); Cui Y., Jia M., Lin T., Song Y., Belongie S., Class-balanced loss based on effective number of samples, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 9260-9269, (2019); Huang C., Li Y., Loy C.C., Tang X., Learning deep representation for imbalanced classification, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5375-5384, (2016); Shrivastava A., Gupta A., Girshick R., Training region-based object detectors with online hard example mining, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 761-769, (2016); Sergievskiy N., Ponamarev A., Reduced focal loss: 1st place solution to xview object detection in satellite imagery, CoRR abs/1903.01347, (2019); Xu J., Wang W., Wang H., Guo J., Multi-model ensemble with rich spatial information for object detection, Pattern Recognition, 99, (2020); Hosang J., Benenson R., Schiele B., Learning non-maximum suppression, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6469-6477, (2017); Casado-Garcia A., Heras J., Ensemble methods for object detection, ECAI, (2020); Huang J., Rathod V., Sun C., (2020); Zou Z., Shi Z., Guo Y., Ye J., Object detection in 20 years: a survey, CoRR, (2019)","M. Carranza-García; Division of Computer Science, University of Sevilla, Seville, ES-41012, Spain; email: mcarranzag@us.es","","Elsevier B.V.","","","","","","09252312","","NRCGE","","English","Neurocomputing","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104482306"
"Kanerva H.; Honkavaara E.; Näsi R.; Hakala T.; Junttila S.; Karila K.; Koivumäki N.; Alves Oliveira R.; Pelto-Arvo M.; Pölönen I.; Tuviala J.; Östersund M.; Lyytikäinen-Saarenmaa P.","Kanerva, Heini (58031947400); Honkavaara, Eija (14325124400); Näsi, Roope (57006485700); Hakala, Teemu (26325493600); Junttila, Samuli (56938630100); Karila, Kirsi (8358578000); Koivumäki, Niko (57219447873); Alves Oliveira, Raquel (57737573600); Pelto-Arvo, Mikko (58031811100); Pölönen, Ilkka (55257235200); Tuviala, Johanna (58031844200); Östersund, Madeleine (58031980400); Lyytikäinen-Saarenmaa, Päivi (6602484195)","58031947400; 14325124400; 57006485700; 26325493600; 56938630100; 8358578000; 57219447873; 57737573600; 58031811100; 55257235200; 58031844200; 58031980400; 6602484195","Estimating Tree Health Decline Caused by Ips typographus L. from UAS RGB Images Using a Deep One-Stage Object Detection Neural Network †","2022","Remote Sensing","14","24","6257","","","","0","10.3390/rs14246257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144816277&doi=10.3390%2frs14246257&partnerID=40&md5=ca543d69c8d141f545a4bd663c604be7","Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; School of Forest Sciences, University of Eastern Finland, P.O. Box 111, Joensuu, 80100, Finland; Department of Forest Sciences, University of Helsinki, P.O. Box 27, Helsinki, 00014, Finland; Faculty of Information Technology, University of Jyväskylä, Jyväskylä, 40100, Finland","Kanerva H., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Honkavaara E., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Näsi R., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Hakala T., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Junttila S., School of Forest Sciences, University of Eastern Finland, P.O. Box 111, Joensuu, 80100, Finland; Karila K., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Koivumäki N., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Alves Oliveira R., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Pelto-Arvo M., Department of Forest Sciences, University of Helsinki, P.O. Box 27, Helsinki, 00014, Finland; Pölönen I., Faculty of Information Technology, University of Jyväskylä, Jyväskylä, 40100, Finland; Tuviala J., Department of Forest Sciences, University of Helsinki, P.O. Box 27, Helsinki, 00014, Finland; Östersund M., Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; Lyytikäinen-Saarenmaa P., School of Forest Sciences, University of Eastern Finland, P.O. Box 111, Joensuu, 80100, Finland","Various biotic and abiotic stresses are causing decline in forest health globally. Presently, one of the major biotic stress agents in Europe is the European spruce bark beetle (Ips typographus L.) which is increasingly causing widespread tree mortality in northern latitudes as a consequence of the warming climate. Remote sensing using unoccupied aerial systems (UAS) together with evolving machine learning techniques provide a powerful tool for fast-response monitoring of forest health. The aim of this study was to investigate the performance of a deep one-stage object detection neural network in the detection of damage by I. typographus in Norway spruce trees using UAS RGB images. A Scaled-YOLOv4 (You Only Look Once) network was implemented and trained for tree health analysis. Datasets for model training were collected during 2013–2020 from three different areas, using four different RGB cameras, and under varying weather conditions. Different model training options were evaluated, including two different symptom rules, different partitions of the dataset, fine-tuning, and hyperparameter optimization. Our study showed that the network was able to detect and classify spruce trees that had visually separable crown symptoms, but it failed to separate spruce trees with stem symptoms and a green crown from healthy spruce trees. For the best model, the overall F-score was 89%, and the F-scores for the healthy, infested, and dead trees were 90%, 79%, and 98%, respectively. The method adapted well to the diverse dataset, and the processing results with different options were consistent. The results indicated that the proposed method could enable implementation of low-cost tools for management of I. typographus outbreaks. © 2022 by the authors.","bark beetle; deep learning; drone; object detection; remote sensing; tree health","Aircraft detection; Antennas; Damage detection; Deep neural networks; Drones; Forestry; Learning systems; Object detection; Object recognition; Aerial systems; Bark beetle; Deep learning; Ips typographus; Neural-networks; Objects detection; Remote-sensing; RGB images; Spruce trees; Tree health; Remote sensing","","","","","Ministry of Agriculture and Forestry of Finland, (647/03.02.06.00/2018, VN/3482/2021, VN/5292/2021); Academy of Finland, AKA, (327861, 327862, 330422); Maj ja Tor Nesslingin Säätiö, (2014462, 337127)","This research was funded by the Academy of Finland under grants 327861, 327862 and 330422, by the Ministry of Agriculture and Forestry of Finland with the projects MONITUHO (no. 647/03.02.06.00/2018), SPRUCERISK (no. VN/5292/2021) and MMM_UNITE (no. VN/3482/2021) and by Maj and Tor Nessling Foundation with IPSCAR project (no. 2014462). This study has been performed with affiliation to the Academy of Finland Flagship Forest–Human–Machine Interplay—Building Resilience, Redefining Value Networks and Enabling Meaningful Experiences (UNITE) (decision no. 337127).","Chinellato F., Faccoli M., Marini L., Battisti A., Distribution of Norway Spruce Bark and Wood-Boring Beetles along Alpine Elevational Gradients: Norway Spruce Bark and Wood Beetles along Altitude, Agr. Forest Entomol, 16, pp. 111-118, (2014); Hlasny T., Konig L., Krokene P., Lindner M., Montagne-Huck C., Muller J., Qin H., Raffa K.F., Schelhaas M.-J., Svoboda M., Et al., Bark Beetle Outbreaks in Europe: State of Knowledge and Ways Forward for Management, Curr. For. Rep, 7, pp. 138-165, (2021); Biedermann P.H.W., Muller J., Gregoire J.-C., Gruppe A., Hagge J., Hammerbacher A., Hofstetter R.W., Kandasamy D., Kolarik M., Kostovcik M., Et al., Bark Beetle Population Dynamics in the Anthropocene: Challenges and Solutions, Trends Ecol. Evol, 34, pp. 914-924, (2019); Barta V., Lukes P., Homolova L., Early Detection of Bark Beetle Infestation in Norway Spruce Forests of Central Europe Using Sentinel-2, Int. J. Appl. Earth Obs. Geoinf, 100, (2021); Nasi R., Honkavaara E., Lyytikainen-Saarenmaa P., Blomqvist M., Litkey P., Hakala T., Viljanen N., Kantola T., Tanhuanpaa T., Holopainen M., Using UAV-Based Photogrammetry and Hyperspectral Imaging for Mapping Bark Beetle Damage at Tree-Level, Remote Sens, 7, pp. 15467-15493, (2015); Fernandez-Carrillo A., Patocka Z., Dobrovolny L., Franco-Nieto A., Revilla-Romero B., Monitoring Bark Beetle Forest Damage in Central Europe. A Remote Sensing Approach Validated with Field Data, Remote Sens, 12, (2020); Huo L., Persson H.J., Lindberg E., Early Detection of Forest Stress from European Spruce Bark Beetle Attack, and a New Vegetation Index: Normalized Distance Red & SWIR (NDRS), Remote Sens. Environ, 255, (2021); Duarte A., Borralho N., Cabral P., Caetano M., Recent Advances in Forest Insect Pests and Diseases Monitoring Using UAV-Based Data: A Systematic Review, Forests, 13, (2022); Nasi R., Honkavaara E., Blomqvist M., Lyytikainen-Saarenmaa P., Hakala T., Viljanen N., Kantola T., Holopainen M., Remote Sensing of Bark Beetle Damage in Urban Forests at Individual Tree Level Using a Novel Hyperspectral Camera from UAV and Aircraft, Urban For. Urban Green, 30, pp. 72-83, (2018); Kloucek T., Komarek J., Surovy P., Hrach K., Janata P., Vasicek B., The Use of UAV Mounted Sensors for Precise Detection of Bark Beetle Infestation, Remote Sens, 11, (2019); Junttila S., Nasi R., Koivumaki N., Imangholiloo M., Saarinen N., Raisio J., Holopainen M., Hyyppa H., Hyyppa J., Lyytikainen-Saarenmaa P., Et al., Multispectral Imagery Provides Benefits for Mapping Spruce Tree Decline Due to Bark Beetle Infestation When Acquired Late in the Season, Remote Sens, 14, (2022); Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788; Safonova A., Hamad Y., Alekhina A., Kaplun D., Detection of Norway Spruce Trees (Picea Abies) Infested by Bark Beetle in UAV Images Using YOLOs Architectures, IEEE Access, 10, pp. 10384-10392, (2022); Minarik R., Langhammer J., Lendzioch T., Detection of Bark Beetle Disturbance at Tree Level Using UAS Multispectral Imagery and Deep Learning, Remote Sens, 13, (2021); Wu B., Liang A., Zhang H., Zhu T., Zou Z., Yang D., Tang W., Li J., Su J., Application of Conventional UAV-Based High-Throughput Object Detection to the Early Diagnosis of Pine Wilt Disease by Deep Learning, For. Ecol. Manag, 486, (2021); Lim W., Choi K., Cho W., Chang B., Ko D.W., Efficient Dead Pine Tree Detecting Method in the Forest Damaged by Pine Wood Nematode (Bursaphelenchus Xylophilus) through Utilizing Unmanned Aerial Vehicles and Deep Learning-Based Object Detection Techniques, For. Sci. Technol, 18, pp. 36-43, (2022); Li F., Liu Z., Shen W., Wang Y., Wang Y., Ge C., Sun F., Lan P., A Remote Sensing and Airborne Edge-Computing Based Detection System for Pine Wilt Disease, IEEE Access, 9, pp. 66346-66360, (2021); Sun Z., Ibrayim M., Hamdulla A., Detection of Pine Wilt Nematode from Drone Images Using UAV, Sensors, 22, (2022); Blomqvist M., Kosunen M., Starr M., Kantola T., Holopainen M., Lyytikainen-Saarenmaa P., Modelling the Predisposition of Norway Spruce to Ips Typographus L. Infestation by Means of Environmental Factors in Southern Finland, Eur. J. Forest Res, 137, pp. 675-691, (2018); Viljanen N., Honkavaara E., Nasi R., Hakala T., Niemelainen O., Kaivosoja J., A Novel Machine Learning Method for Estimating Biomass of Grass Swards Using a Photogrammetric Canopy Height Model, Images and Vegetation Indices Captured by a Drone, Agriculture, 8, (2018); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, arXiv, (2016); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement 2018, arXiv, (2018); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection 2020, arXiv, (2020); Wang C.-Y., Bochkovskiy A., Liao H.-Y.M., Scaled-YOLOv4: Scaling Cross Stage Partial Network, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Zheng Z., Wang P., Liu W., Li J., Ye R., Ren D., Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression, AAAI, 34, pp. 12993-13000, (2020); Nesterov Y.E., A method of solving a convex programming problem with convergence rate O(1/k^2), Dokl. Akad. Nauk SSSR, 269, pp. 543-547, (1983); Sutskever I., Martens J., Dahl G., Hinton G., On the importance of initialization and momentum in deep learning, Proceedings of the 30th International Conference on Machine Learning, PMLR, 28, pp. 1139-1147; Lin T.-Y., Maire M., Belongie S., Bourdev L., Girshick R., Hays J., Perona P., Ramanan D., Zitnick C.L., Dollar P., Microsoft COCO: Common Objects in Context, European Conference on Computer Vision, (2014); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2999-3007; Zhang H., Cisse M., Dauphin Y.N., Lopez-Paz D., Mixup: Beyond Empirical Risk Minimization, arXiv, (2017); Mitchell M., An Introduction to Genetic Algorithms, (1998); Padilla R., Passos W.L., Dias T.L.B., Netto S.L., da Silva E.A.B., A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit, Electronics, 10, (2021)","E. Honkavaara; Finnish Geospatial Research Institute (FGI), National Land Survey of Finland, Espoo, 02150, Finland; email: eija.honkavaara@nls.fi","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144816277"
"Zheng Y.; Wu G.","Zheng, Yueyuan (57373314100); Wu, Gang (57217100260)","57373314100; 57217100260","YOLOv4-Lite–Based Urban Plantation Tree Detection and Positioning With High-Resolution Remote Sensing Imagery","2022","Frontiers in Environmental Science","9","","756227","","","","4","10.3389/fenvs.2021.756227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123489429&doi=10.3389%2ffenvs.2021.756227&partnerID=40&md5=c3ec47190fd59d1b6c15c4531239ff57","College of Information, Beijing Forestry University, Beijing, China","Zheng Y., College of Information, Beijing Forestry University, Beijing, China; Wu G., College of Information, Beijing Forestry University, Beijing, China","Automatic tree identification and position using high-resolution remote sensing images are critical for ecological garden planning, management, and large-scale environmental quality detection. However, existing single-tree detection methods have a high rate of misdetection in forests not only due to the similarity of background and crown colors but also because light and shadow caused abnormal crown shapes, resulting in a high rate of misdetections and missed detection. This article uses urban plantations as the primary research sample. In conjunction with the most recent deep learning method for object detection, a single-tree detection method based on the lite fourth edition of you only look once (YOLOv4-Lite) was proposed. YOLOv4’s object detection framework has been simplified, and the MobileNetv3 convolutional neural network is used as the primary feature extractor to reduce the number of parameters. Data enhancement is performed for categories with fewer single-tree samples, and the loss function is optimized using focal loss. The YOLOv4-Lite method is used to detect single trees on campus, in an orchard, and an economic plantation. Not only is the YOLOv4-Lite method compared to traditional methods such as the local maximum value method and the watershed method, where it outperforms them by nearly 46.1%, but also to novel methods such as the Chan-Vese model and the template matching method, where it outperforms them by nearly 26.4%. The experimental results for single-tree detection demonstrate that the YOLOv4-Lite method improves accuracy and robustness by nearly 36.2%. Our work establishes a reference for the application of YOLOv4-Lite in additional agricultural and plantation products. Copyright © 2022 Zheng and Wu.","high-resolution remote sensing image; tree detection; tree position; urban plantation; YOLOv4-Lite","","","","","","General; Systems Development Foundation; Office of Naval Research, ONR, (N00014-86-K-0685); Lincoln Laboratory, Massachusetts Institute of Technology; Advanced Research Projects Agency, ARPA, (N00014-85-K-0124)","This report describes work done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for this research is provided in part by the University Research Initiative under Office of Naval Research contract N00014-86-K-0685, in part by a grant from the Systems Development Foundation, and in part by the Advanced Research Projects Agency under Office of Naval Research contract N00014-85-K-0124. Mr. Connell also received support from a General Motors graduate fellowship. This report is based on a portion of a thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Department of Electrical Engineering and Computer Science at the Massachusetts Institute of Technology in September 1989.","Artes-hernandez F., Formica-Oliveira A.C., Artes F., Martinez-Hernandez G.B., Improved Quality of a Vitamin B12-Fortified 'ready to Blend' Fresh-Cut Mix Salad with Chitosan, Food Sci. Technol. Int, 23, pp. 513-528, (2017); Bouvier M., Durrieu S., Fournier R.A., Renaud J.-P., Generalizing Predictive Models of forest Inventory Attributes Using an Area-Based Approach with Airborne LiDAR Data, Remote Sensing Environ, 156, pp. 322-334, (2015); Chaitanya, Sarath S., Malavika, Prasanna, Karthik, Human Emotions Recognition from Thermal Images Using Yolo Algorithm, 2020 International Conference on Communication and Signal Processing (ICCSP), (2020); Chollet F., Xception: Deep Learning with Depthwise Separable Convolutions, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2017); Dimitrios P., Azadeh A., Accuracy Assessment of Total Stem Volume Using Close-Range Sensing: Advances in Precision Forestry, Forests, 12, 6, (2021); Dimitrios P., Azadeh A., Martin S., Assessment of Stem Volume on Plots Using Terrestrial Laser Scanner: A Precision Forestry Application, Sensors, 21, 1, (2021); Dong T., Shen Y., Zhang J., Ye Y., Fan J., Progressive Cascaded Convolutional Neural Networks for Single Tree Detection with Google Earth Imagery, Remote Sensing, 11, 15, (2019); Dong T.Y., Zhou Q.Z., Single Tree Detection in Remote Sensing Images Based on Morphological Snake Model, ISPRS, Int. J. Geo-Inf, (2018); Dong Y., Ren Z., Fu Y., Miao Z., Yang R., Sun Y., Et al., Recording Urban Land Dynamic and its Effects during 2000-2019 at 15-m Resolution by Cloud Computing with Landsat Series, REMOTE SENSING, 12, 15, (2020); Garcia-ortiz L.B., Sanchez-Perez G., Hernandez-Suarez A., Olivares-Mercado J., Portillo-Portillo J., A Fast-RCNN Implementation for Human Silhouette Detection in Video Sequences, Knowledge Innovation through Intelligent Software Methodologies, Tools and Techniques, (2020); Gomes M.F., Maillard P., Deng H., Individual Tree crown Detection in Sub-meter Satellite Imagery Using Marked Point Processes and a Geometrical-Optical Model, Remote Sensing Environ, pp. 211184-211195, (2018); Gougeon F.A., FranOis A., A crown-following Approach to the Automatic Delineation of Individual Tree Crowns in High Spatial Resolution Aerial Images, Can. J. Remote Sensing, 21, 3, pp. 274-284, (2014); Hashim S.A., Daliman S., Rodi I., Aziz N.A., Rak A.E., Analysis of Oil Palm Tree Recognition Using Drone-Based Remote Sensing Images, IOP Conf. Ser. Earth Environ. Sci, (2020); Hofmann H., Exploring Categorical Data: Interactive Mosaic Plots, Metrika, 51, 1, pp. 11-26, (2000); Huiling L., Xiaoli Z., Ying Z., Yunfeng Z., Hui L., Longyang W., Review on Individual Tree Detection Based on Airborne LiDAR, [J]. Prog. Laser Optoelectronics, 55, 8, (2018); Jiang R., Wang C., Shen L., Wang P., Koirala A., Walsh K., Et al., A Method for Lichee's Tree-crown Information Extraction Based on High Spatial Resolution Image, Transactions of the Chinese Society for Agricultural MachineryDeep Learning for Real-Time Fruit Detection and Orchard Fruit Load Estimation: Benchmarking of 'MangoYOLO, 20, pp. 1107-1135, (2016); Larsen M., Eriksson M., Descombes X., Perrin G., Brandtberg T., Gougeon F.A., Comparison of Six Individual Tree crown Detection Algorithms Evaluated under Varying forest Conditions, Int. J. Remote Sensing, 32, 20, pp. 5827-5852, (2011); Liang Y., Liao J.C., Pan J.H., Mesh-Based Scale-Invariant Feature Transform-like Method for Three-Dimensional Face Recognition under Expressions and Missing Data, J. Electron. Imaging, 29, 5, (2020); Liu H., Zhang X., Zhang Y., Advances in Single Wood Identification of Airborne Lidar, J. Laser Optoelectron. Prog, 55, 8, pp. 39-47, (2018); Ma K., Cheng Y., Ge W., Zhao Y., Qi Z., Identification, Extraction and Three-Dimensional Building Model Reconstruction Though Faster R-CNN of Architectural Plans, (2020); Meneghetti D., Homem T., Oliveira J., Silva I., Bianchi R., Detecting Soccer Balls with Reduced Neural Networks, J. Intell. Robotic Syst, 101, 3, (2021); Nasor M., Obaid W., Segmentation of Osteosarcoma in MRI Images by K-Means Clustering, Chan-Vese Segmentation, and Iterative Gaussian Filtering, IET Image Process, 15, (2021); Olli N., Eija H., Sakari T., Niko V., Teemu H., Yu X., Et al., Individual Tree Detection and Classification with UAV-Based Photogrammetric Point Clouds and Hyperspectral Imaging, Remote Sensing, (2017); Peng L., Hui Z., Eom K.B., Active Deep Learning for Classification of Hyperspectral Images, IEEE J. Selected Top. Appl. Earth Observations Remote Sensing, 10, 2, pp. 712-724, (2017); Picos J., Bastos G., Miguez D., Martinez L.A., Armesto J., Individual Tree Detection in a Eucalyptus Plantation Using Unmanned Aerial Vehicle (UAV)-LiDAR, Remote Sensing, 12, 5, (2020); Rau A., Kim S., Yang S., Reisert M., Egger K., SVM-based Normal Pressure Hydrocephalus Detection, 17, (2021); Richey B., Shirvaikar M.V., Deep Learning Based Real-Time Detection of Northern Corn Leaf Blight Crop Disease Using YoloV4, Real-Time Image Processing and Deep Learning 2021, (2021); Ruihuan hou X.Y.Z.W., A Real-Time Detection Methods for forest Pests Based on YOLO V4-TIA Algorithm, Computer Eng, pp. 1-8, (2021); Syaputra R., Syamsuar D., Negara E.S., Multiple Smile Detection Using Histogram of Oriented Gradient and Support Vector Machine Methods, IOP Conf. Ser. Mater. Sci. Eng, 1071, 1, (2021); Toth C., Jozkow G., Remote Sensing Platforms and Sensors: A Survey, ISPRS J. Photogrammetry Remote Sensing, 115, may, pp. 22-36, (2016); Wang L., Gong P., Biging G.S., Individual Tree-Crown Delineation and Treetop Detection in High-Spatial-Resolution Aerial Imagery, Photogramm Eng. Remote Sensing, 70, 3, pp. 351-357, (2004); Wang M., Multi-level Assessment of Mangrove Ecosystem Based on Remote Sensing Technology, (2019); Wang X., Wang S., Cao J., Wang Y., Data-driven Based Tiny-YOLOv3 Method for Front Vehicle Detection Inducing SPP-Net, 99, (2020); Wulder M., Niemann K.O., Goodenough D.G., Local Maximum Filtering for the Extraction of Tree Locations and Basal Area from High Spatial Resolution Imagery, Remote Sensing Environ, 73, 1, pp. 103-114, (2000); Xiao Y., Wang D., Wu Y., Investigation and Protection of Ancient and Famous Trees Resources of Daxiong Mountain, IOP Conf. Ser. Earth Environ. Sci, 692, 37pp, (2021); Yinghai K., Xiaojuan L., Huili G., Application of Remote Sensing Technology in Automatic forest Inventory, (2015); Yuz S.Y.S.C., A Real-Time Detection Approach for Bridge Cracks Based on YOLOv4-FPM, Automation in Construction, 122, (2021); Zhang L., Liu P., Zhao L., Wang G., Liu J., Air Quality Predictions with a Semi-supervised Bidirectional LSTM Neural Network, Atmos. Pollut. Res, 12, 1, (2020); Zhang N., Zhang X., Ye L., Tree crown Extraction Based on Segmentation of High-Resolution Remote Sensing Image Improved Peak-Climbing Algorithm, Trans. Chin. Soc. Agric. Machinery, 45, 12, pp. 294-300, (2014); Zhang S., Hu X., Jie X., Li S., Wang Z., Zhao Z., Et al., Research on Image Detection Method for Assembly Failure of Monomer thermal Battery, (2019); Zhao L., Zeng Y., Liu P., Su X., Band Selection with the Explanatory Gradient Saliency Maps of Convolutional Neural Networks, IEEE Geosci. Remote Sensing Lett, 99, pp. 1-5, (2020); Zheng X., Wang R., Jin M., Cas, Extraction of High-Resolution Images of Single Tree Crown Based on Watershed Algorithm with Morphological Threshold Mark, (2017); Zhibin R., Hongbo Z., Yao F., Lu X., Yulin D., Effects of Urban Street Trees on Human thermal comfort and Physiological Indices: A Case Study in Changchun City, China, (2020)","G. Wu; College of Information, Beijing Forestry University, Beijing, China; email: wugang@bjfu.edu.cn","","Frontiers Media S.A.","","","","","","2296665X","","","","English","Front. Environ. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123489429"
"Arshad M.A.; Khan S.H.; Qamar S.; Khan M.W.; Murtza I.; Gwak J.; Khan A.","Arshad, Muhammad Arif (57212308371); Khan, Saddam Hussain (57219645789); Qamar, Suleman (57462119000); Khan, Muhammad Waleed (57211373770); Murtza, Iqbal (56046240600); Gwak, Jeonghwan (36620985200); Khan, Asifullah (8510710900)","57212308371; 57219645789; 57462119000; 57211373770; 56046240600; 36620985200; 8510710900","Drone Navigation Using Region and Edge Exploitation-Based Deep CNN","2022","IEEE Access","10","","","95441","95450","9","2","10.1109/ACCESS.2022.3204876","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137869465&doi=10.1109%2fACCESS.2022.3204876&partnerID=40&md5=6a9a0c13aa87f3f06259f3e939283940","Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan; Centres of Excellence in Science and Applied Technologies, Islamabad, 44000, Pakistan; Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan; University of Engineering and Applied Sciences, Department of Computer Systems Engineering, Swat, 19060, Pakistan; Department of Mechanical and Aerospace Engineering, Columbus, 43210, OH, United States; The Ohio State University, Center for Automotive Research, Columbus, 43212, OH, United States; Air University, Faculty of Computing and Ai, Department of Creative Technologies, Islamabad, 44230, Pakistan; Department of Software, Korea National University of Transportation, Chungju, 27469, South Korea; Korea National University of Transportation, Department of Biomedical Engineering, Chungju, 27469, South Korea; Korea National University of Transportation, Department of Ai Robotics Engineering, Chungju, 27469, South Korea; Korea National University of Transportation, Department of It and Energy Convergence (BK21 FOUR), Chungju, 27469, South Korea; Pakistan Institute of Engineering and Applied Sciences, Deep Learning Laboratory, Center for Mathematical Sciences, Nilore, Islamabad, 45650, Pakistan","Arshad M.A., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Centres of Excellence in Science and Applied Technologies, Islamabad, 44000, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan; Khan S.H., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, University of Engineering and Applied Sciences, Department of Computer Systems Engineering, Swat, 19060, Pakistan; Qamar S., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan; Khan M.W., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan, Department of Mechanical and Aerospace Engineering, Columbus, 43210, OH, United States, The Ohio State University, Center for Automotive Research, Columbus, 43212, OH, United States; Murtza I., Air University, Faculty of Computing and Ai, Department of Creative Technologies, Islamabad, 44230, Pakistan; Gwak J., Department of Software, Korea National University of Transportation, Chungju, 27469, South Korea, Korea National University of Transportation, Department of Biomedical Engineering, Chungju, 27469, South Korea, Korea National University of Transportation, Department of Ai Robotics Engineering, Chungju, 27469, South Korea, Korea National University of Transportation, Department of It and Energy Convergence (BK21 FOUR), Chungju, 27469, South Korea; Khan A., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Deep Learning Laboratory, Center for Mathematical Sciences, Nilore, Islamabad, 45650, Pakistan","Drones are unmanned aerial vehicles (UAV) utilized for a broad range of functions, including delivery, aerial surveillance, traffic monitoring, architecture monitoring, and even War-field. Drones confront significant obstacles while navigating independently in complex and highly dynamic environments. Moreover, the targeted objects within a dynamic environment have irregular morphology, occlusion, and minor contrast variation with the background. In this regard, a novel deep Convolutional Neural Network(CNN) based data-driven strategy is proposed for drone navigation in the complex and dynamic environment. The proposed Drone Split-Transform-and-Merge Region-and-Edge (Drone-STM-RENet) CNN is comprised of convolutional blocks where each block methodically implements region and edge operations to preserve a diverse set of targeted properties at multi-levels, especially in the congested environment. In each block, the systematic implementation of the average and max-pooling operations can deal with the region homogeneity and edge properties. Additionally, these convolutional blocks are merged at a multi-level to learn texture variation that efficiently discriminates the target from the background and helps obstacle avoidance. Finally, the Drone-STM-RENet generates steering angle and collision probability for each input image to control the drone moving while avoiding hindrances and allowing the UAV to spot risky situations and respond quickly, respectively. The proposed Drone-STM-RENet has been validated on two urban cars and bicycles datasets: udacity and collision-sequence, and achieved considerable performance in terms of explained variance (0.99), recall (95.47%), accuracy (96.26%), and F-score (91.95%). The promising performance of Drone-STM-RENet on urban road datasets suggests that the proposed model is generalizable and can be deployed for real-time autonomous drones navigation and real-world flights.  © 2013 IEEE.","convolutional neural network; drone; drone split transform merge; perception and autonomy; Residual network","Air navigation; Aircraft detection; Antennas; Complex networks; Convolution; Deep neural networks; Drones; Object recognition; Aerial vehicle; Convolutional neural network; Drone split transform merge; Dynamic environments; Multilevels; Objects recognition; Perception and autonomy; Performance; Residual network; Robots","","","","","","","Mushtaq A., Haq I.U., Imtiaz M.U., Khan A., Shafiq O., Traffic flow management of autonomous vehicles using deep reinforcement learning and smart rerouting, IEEE Access, 9, pp. 51005-51019, (2021); Scherer S., Rehder J., Achar S., Cover H., Chambers A., Nuske S., Singh S., River mapping from a flying robot: State estimation, river detec- tion, and obstacle mapping, Auto. Robots, 33, 1, pp. 189-214, (2012); Faessler M., Fontana F., Forster C., Mueggler E., Pizzoli M., Scaramuzza D., Autonomous, vision-based flight and live dense 3D map- ping with a quadrotor micro aerial vehicle, J. Field Robot., 33, 4, pp. 431-450, (2016); Qamar S., Khan S.H., Arshad M.A., Qamar M., Khan A., Autonomous drone swarm navigation and multi-target tracking in 3D environments with dynamic obstacles, (2022); Shen S., Mulgaonkar Y., Michael N., Kumar V., Multi-sensor fusion for robust autonomous flight in indoor and outdoor environments with a rotorcraft MAV, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 4974-4981, (2014); Lee T., McKeever S., Courtney J., Flying free: A research overview of deep learning in drone navigation autonomy, Drones, 5, 2, (2021); Khan A., Shamsi M.H., Choi T.-S., Correlating dynamical mechan- ical properties with temperature and clay composition of polymer-clay nanocomposites, Comput. Mater. Sci., 45, 2, pp. 257-265, (2009); Tahir M., Khan A., Majid A., Protein subcellular localization offluorescence imagery using spatial and transform domain features, Bioin- formatics, 28, 1, pp. 91-97, (2012); Khan S.H., Sohail A., Khan A., Hassan M., Lee Y.S., Alam J., Basit A., Zubair S., COVID-19 detection in chest X-ray images using deep boosted hybrid learning, Comput. Biol. Med., 137, (2021); Giusti A., Guzzi J., Ciresan D.C., He F.L., Rodriguez J.P., Fontana F., Faessler M., Forster C., Schmidhuber J., Caro G.D., Scaramuzza D., A machine learning approach to visual perception of forest trails for mobile robots, IEEE Robot. Automat. Lett., 1, 2, pp. 661-667, (2015); Ross S., Melik-Barkhudarov N., Shankar K.S., Wendel A., Dey D., Bagnell J.A., Hebert M., Learning monocular reactive UAV control in cluttered natural environments, Proc. IEEE Int. Conf. Robot. Autom., pp. 1765-1772, (2013); Khan A., Sohail A., Ali A., A new channel boosted convolutional neural network using transfer learning, (2018); Lillicrap T.P., Hunt J.J., Pritzel A., Heess N., Erez T., Tassa Y., Silver D., Wierstra D., Continuous control with deep reinforcement learning, (2015); Khan S.H., Sohail A., Zafar M.M., Khan A., Coronavirus dis- ease analysis using chest X-ray images and a novel deep convolutional neural network, Photodiagnosis Photodyn. Therapy, 35, (2021); Khan A., Khan S.H., Saif M., Batool A., Sohail A., Khan M.W., A survey of deep learning techniques for the analysis of COVID-19 and their usability for detecting omicron, (2022); Long J., Et al., A novel self-training semi-supervised deep learning approach for machinery fault diagnosis, Int. J. Prod. Res., pp. 1-14, (2022); Asam M., Hussain S.J., Mohatram M., Khan S.H., Jamal T., Zafar A., Khan A., Ali M.U., Zahoora U., Detection of exceptional malware variants using deep boosted feature spaces and machine learning, Appl. Sci., 11, 21, (2021); Asam M., Khan S.H., Jamal T., Zahoora U., Khan A., Malware classification using deep boosted learning, (2021); Chamlawi R., Khan A., Idris A., Wavelet based image authentication and recovery, J. Comput. Sci. Technol., 22, 6, pp. 795-804, (2007); Asam M., Khan S.H., Jamal T., Khan A., IoT malware detection architecture using a novel channel boosted and squeezed CNN, (2022); Gandhi D., Pinto L., Gupta A., Learning to fly by crashing, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 3948-3955, (2017); Yang Z., Long J., Zi Y., Zhang S., Li C., Incremental novelty identification from initially one-class learning to unknown abnormality classification, IEEE Trans. Ind. Electron., 69, 7, pp. 7394-7404, (2022); Loquercio A., Maqueda A.I., Blanco C.R.D., Scaramuzza D., DroNet: Learning to fly by driving, IEEE Robot. Autom. Lett., 3, 2, pp. 1088-1095, (2018); Javadi S., Dahl M., Pettersson M.I., Vehicle detection in aerial images based on 3D depth maps and deep neural networks, IEEE Access, 9, pp. 8381-8391, (2021); Lin H.-Y., Peng X.-Z., Autonomous quadrotor navigation with vision based obstacle avoidance and path planning, IEEE Access, 9, pp. 102450-102459, (2021); Lynen S., Sattler T., Bosse M., Hesch J.A., Pollefeys M., Siegwart R., Get out of my lab: Large-scale, real-time visual-inertial localization, Robotics: Science and Systems, 1, (2015); Kouris A., Bouganis C.-S., Learning to fly by MySelf: A self supervised CNN-based approach for autonomous navigation, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1-9, (2018); Taha B., Shoufan A., Machine learning-based drone detection and classification: State-of-The-art in research, IEEE Access, 7, pp. 138669-138682, (2019); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Butt M.Q., Rahman A.U., Audiovisual saliency prediction in uncategorized video sequences based on audio-video correlation, (2021); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 779-788, (2016); Girshick R., Fast r-cnn, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 1440-1448, (2015); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., LeCun Y., OverFeat: Integrated recognition, localization and detection using convo lutional networks, (2013); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Com- put. Comput.-Assist. Intervent. Cham, Switzerland: Springer, pp. 234-241, (2015); Wang X., Cheng P., Liu X., Uzochukwu B., Fast and accurate, convolutional neural network based approach for object detection from UAV, Proc. 44th Annu. Conf. IEEE Ind. Electron. Soc. (IECON), pp. 3171-3175, (2018); Saripalli S., Montgomery J.F., Sukhatme G.S., Vision-based autonomous landing of an unmanned aerial vehicle, Proc. IEEE Int. Conf. Robot. Autom., pp. 2799-2804, (2002); Scaramuzza D., Achtelik M.C., Doitsidis L., Friedrich F., Kosmatopoulos E., Martinelli A., Achtelik M.W., Chli M., Chatzichristofis S., Kneip L., Gurdan D., Vision-controlled micro flying robots: From system design to autonomous navigation and mapping in GPS-denied environments, IEEE Robot. Automat. Mag., 21, 3, pp. 26-40, (2014); Meier L., Tanskanen P., Heng L., Lee G.H., Fraundorfer F., Pollefeys M., PIXHAWK: A micro aerial vehicle design for autonomous flight using onboard computer vision, Auto. Robots, 33, 1-2, pp. 21-39, (2012); Kendoul F., Fantoni I., Nonami K., Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles, Robot. Auto. Syst., 57, 6-7, pp. 591-602, (2009); Ashraf M.W., Sultani W., Shah M., Dogfight: Detecting drones from drones videos, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7067-7076, (2021); Xie S., Girshick R., Dollar P., Tu Z., He K., Aggregated residual transformations for deep neural networks, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1492-1500, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Jiang P., Chen Y., Liu B., He D., Liang C., Real-time detection of apple leaf diseases using deep learning approach based on improved convolutional neural networks, IEEE Access, 7, pp. 59069-59080, (2019); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1-9, (2015); Pan Z., Yu W., Yi X., Khan A., Yuan F., Zheng Y., Recent progress on generative adversarial networks (GANs): A survey, IEEE Access, 7, pp. 36322-36333, (2019); Khan S.H., Sohail A., Khan A., Lee Y.-S., COVID-19 detection in chest X-ray images using a new channel boosted CNN, Diagnostics, 12, 2, (2022); Khan S.H., Shah N.S., Nuzhat R., Majid A., Alquhayz H., Khan A., Malaria parasite classification framework using a novel channel squeezed and boosted CNN, Microscopy, (2022); Tian G., Sun Y., Liu Y., Zeng X., Wang M., Liu Y., Zhang J., Chen J., Adding before pruning: Sparse filter fusion for deep convolutional neural networks via auxiliary attention, IEEE Trans. Neural Netw. Learn. Syst., (2021); Christodoulidis S., Anthimopoulos M., Ebner L., Christe A., Mougiakakou S., Multisource transfer learning with convolutional neural networks for lung pattern analysis, IEEE J. Biomed. Health Inform., 21, 1, pp. 76-84, (2017); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Mancini M., Costante G., Valigi P., Ciarfuglia T.A., Delmerico J., Scaramuzza D., Toward domain independence for learning-based monocular depth estimation, IEEE Robot. Autom. Lett., 2, 3, pp. 1778-1785, (2017); Chew R., Rineer J., Beach R., O'Neil M., Ujeneza N., Lapidus D., Miano T., Hegarty-Craver M., Polly J., Temple D.S., Deep neural net- works and transfer learning for food crop identification in UAV images, Drones, 4, 1, (2020); Back S., Cho G., Oh J., Tran X.-T., Oh H., Autonomous UAV trail navigation with obstacle avoidance using deep neural networks, J. Intell. Robotic Syst., 100, 3-4, pp. 1195-1211, (2020); Kazakova A., Parallelization of autonomous driving tasks for safe high-speed vehicle control; Smolyanskiy N., Kamenev A., Smith J., Birchfield S., Toward low- flying autonomous MAV trail navigation using deep neural networks for environmental awareness, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 4241-4247, (2017); Yang S., Konam S., Ma C., Rosenthal S., Veloso M., Scherer S., Obstacle avoidance through deep networks based intermediate percep- tion, (2017); Pfeiffer M., Schaeuble M., Nieto J., Siegwart R., Cadena C., From perception to decision: A data-driven approach to end-to-end motion plan- ning for autonomous ground robots, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 1527-1533, (2017); Gao W., Hsu D., Lee W.S., Shen S., Subramanian K., Intention- Net: Integrating planning and deep learning for goal-directed autonomous navigation, Proc. Conf. Robot Learn., pp. 185-194, (2017); Richter C., Roy N., Safe visual navigation via deep learning and novelty detection, Proc. Robot., Sci. Syst. XIII, (2017)","A. Khan; Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Nilore, 45650, Pakistan; email: asif@pieas.edu.pk; J. Gwak; Department of Software, Korea National University of Transportation, Chungju, 27469, South Korea; email: jgwak@ut.ac.kr","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137869465"
"Xie S.; Hu C.; Bagavathiannan M.; Song D.","Xie, Shuangyu (57225173089); Hu, Chengsong (57224534070); Bagavathiannan, Muthukumar (24179346400); Song, Dezhen (16550861000)","57225173089; 57224534070; 24179346400; 16550861000","Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass Turf Using Inaccurate and Insufficient Training Data","2021","IEEE Robotics and Automation Letters","6","4","9492056","7365","7372","7","8","10.1109/LRA.2021.3098012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111010116&doi=10.1109%2fLRA.2021.3098012&partnerID=40&md5=ac3f6e2c3dd7dfc422b979d772d129f8","Computer Science and Engineering Department, Texas AandM University, College Station, 77843-3112, TX, United States; Department of Soil and Crop Sciences, Texas AandM University, College Station, 77843, TX, United States","Xie S., Computer Science and Engineering Department, Texas AandM University, College Station, 77843-3112, TX, United States; Hu C., Department of Soil and Crop Sciences, Texas AandM University, College Station, 77843, TX, United States; Bagavathiannan M., Department of Soil and Crop Sciences, Texas AandM University, College Station, 77843, TX, United States; Song D., Computer Science and Engineering Department, Texas AandM University, College Station, 77843-3112, TX, United States","To enable robotic weed control, we develop algorithms to detect nutsedge weed from bermudagrass turf. Due to the similarity between the weed and the background turf, manual data labeling is expensive and error-prone. Consequently, directly applying deep learning methods for object detection cannot generate satisfactory results. Building on an instance detection approach (i.e. Mask R-CNN), we combine synthetic data with raw data to train the network. We propose an algorithm to generate high fidelity synthetic data, adopting different levels of annotations to reduce labeling cost. Moreover, we construct a nutsedge skeleton-based probabilistic map (NSPM) as the neural network input to reduce the reliance on pixel-wise precise labeling. We also modify loss function from cross entropy to Kullback-Leibler divergence which accommodates uncertainty in the labeling process. We implement the proposed algorithm and compare it with both Faster R-CNN and Mask R-CNN. The results show that our design can effectively overcome the impact of imprecise and insufficient training sample issues and significantly outperform the Faster R-CNN counterpart with a false negative rate of only 0.4%. In particular, our approach also reduces labeling time by 95% while achieving better performance if comparing with the original Mask R-CNN approach.  © 2016 IEEE.","deep Learning; precision agriculture; robotic weed control; Weed detection","Convolutional neural networks; Deep learning; Educational robots; Learning systems; Object detection; Robotics; Weed control; Bermudagrasses; Detection approach; False negative rate; Kullback Leibler divergence; Learning methods; Loss functions; Synthetic data; Training sample; Agricultural robots","","","","","National Science Foundation, NSF, (NRI-1925037); Texas A and M University, TAMU","Manuscript received February 21, 2021; accepted July 10, 2021. Date of publication July 20, 2021; date of current version August 2, 2021. This letter was recommended for publication by Associate Editors H. Ryu and S. Manzoor and Editor Y. Choi upon evaluation of the reviewers’ comments. This work was supported in part by a T3 grant at TAMU and in part by NSF under NRI-1925037. (S. Xie and C. Hu are co-first authors of this paper.) (Corresponding author: Dezhen Song.) Shuangyu Xie and Dezhen Song are with the Computer Science and Engineering Department, Texas A&M University, College Station, TX 77843-3112, USA (e-mail: sy.xie@tamu.edu; dzsong@cs.tamu.edu).","He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proc. IEEE Conf. Comput. Vis., pp. 2961-2969, (2017); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detectionwith region proposal networks, IEEE Trans. PatternAnal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Thayer T.C., Vougioukas S., Goldberg K., Carpin S., Multirobot routing algorithms for robots operating in vineyards, IEEE Trans. Autom. Sci. Eng., 17, 3, pp. 1184-1194, (2020); You A., Sukkar F., Fitch R., Karkee M., Davidson J.R., An efficient planning and control framework for pruning fruit trees, Proc. IEEE Int. Conf. Robot. Autom., pp. 3930-3936, (2020); Slaughter D., Giles D., Downey D., Autonomous robotic weed control systems: A review, Comput. Electron. Agric., 61, 1, pp. 63-78, (2008); Abidine A., Heidman B., Upadhyaya S., Hills D., Autoguidance system operated at high speed causes almost no tomato damage, Calif. Agric., 58, 1, pp. 44-47, (2004); English A., Ross P., Ball D., Corke P., Vision based guidance for robot navigation in agriculture, Proc. IEEE Int. Conf. Robot. Autom., pp. 1693-1698, (2014); McCool C., Et al., Efficacy of mechanical weeding tools: A study into alternative weed management strategies enabled by robotics, IEEE Robot. Autom. Lett., 3, 2, pp. 1184-1190, (2018); Michaels A., Haug S., Albert A., Vision-based high-speed manipulation for robotic ultra-precise weed control, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 5498-5505, (2015); Melita C., Muscato G., Poncelet M., A simulation environment for an augmented global navigation satellite system assisted autonomous robotic lawn-mower, J. Intell. Robot. Syst., 71, pp. 127-142, (2013); Lee W.S., Slaughter D., Giles D., Robotic weed control system for tomatos, Precis. Agric., 1, pp. 95-113, (1999); Bawden O., Et al., Robot for weed species plant-specific management, J. Field Robot., 34, pp. 1179-1199, (2017); Burks T., Shearer S., Payne F., Classification of weed species using color texture features and discriminant analysis, Trans. Am Soc. Agric. Eng., 43, 2, pp. 441-448, (2000); Herrera P., Dorado J., Ribeiro A., A novel approach for weed type classification based on shape descriptors and a fuzzy decisionmaking method, IEEE Sensors J., 14, 8, pp. 15304-15324, (2014); Ferreira A., Freitas D., Silva G., Pistori H., Folhes M., Weed detection in soybean crops using convnets, Comput. Electron. Agric., 143, pp. 314-324, (2017); Dyrmann M., Skovsen S., Laursen M., Jorgensen R., Using a fully convolutional neural network for detecting locations of weeds in images from cereal fields, Proc. Int. Conf. Precis. Agric., pp. 1-7, (2018); Yu J., Schumann A., Cao Z., Sharpe S., Boyd N., Weed detection in perennial ryegrass with deep learning convolutional neural network, Front. Plant Sci., 10, pp. 1-9, (2019); Asad M.H., Bais A., Weed detection in canola fields using maximum likelihood classification and deep convolutional neural network, Inf. Process. Agric., 7, 4, pp. 535-545, (2019); Barrero O., Rojas D., Gonzalez C., Perdomo S., Weed detection in rice fields using aerial images and neural networks, Proc. Symp. Signal Process., Image Artif. Vis., pp. 1-4, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 779-788, (2016); Liu W., Et al., SSD: Single shot multibox detector, Proc. Eur. Conf. Comput. Vis., 9905, pp. 21-37, (2016); Chen L.-C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoderdecoder with atrous separable convolution for semantic image segmentation, Proc. Eur. Conf. Comput. Vis., pp. 801-818, (2018); Wang Y., Xu Y., Tsogkas S., Bai X., Dickinson S.J., Siddiqi K., Deepflux for skeletons in the wild, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 5287-5296, (2019); Zhao K., Shen W., Gao S., Li D., Cheng M.-M., Hi-fi: Hierarchical feature integration for skeleton detection, Proc. Int. Joint Conf. Artif. Intell., pp. 1191-1197, (2018); Shrivastava A., Pfister T., Tuzel O., Susskind J., Wang W., Webb R., Learning from simulated and unsupervised images through adversarial training, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2107-2116, (2017); Gao J., French A., Pound M., He L., Pridmore T., Pieters J., Deep convolutional neural networks for image-based convolvulus sepium detection in sugar beet fields, Plant Methods, 16, 29, pp. 1-12, (2020); Toda Y., Et al., Training instance segmentation neural network with synthetic datasets for crop seed phenotyping, Commun. Biol., 3, pp. 1-12, (2020); Bergado J.R., Persello C., Gevaert C., A deep learning approach to the classification of sub-decimetre resolution aerial images, Proc. IEEE Int. Geosci. Remote Sens. Symp., pp. 1516-1519, (2016); Caesar H., Uijlings J., Ferrari V., Coco-stuff: Thing and stuff classes in context, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1209-1218, (2018); Ashikhmin M., Synthesizing natural textures, Proc. ACM Symp. Interact. 3D Graph., pp. 217-226, (2001); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2117-2125, (2017); Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., Detectron 2, (2019); Xie S., Hu C., Bagavathiannan M., Song D., Toward Robotic Weed Control: Detection Ofnutsedge Weed in Bermudagrass Turf Usinginaccurate and Insufficient Training Data: Nutsedge Dataset, (2021); Wada K., Labelme: Image Polygonal Annotation with Python, (2016)","D. Song; Computer Science and Engineering Department, Texas AandM University, College Station, 77843-3112, United States; email: dzsong@cs.tamu.edu","","Institute of Electrical and Electronics Engineers Inc.","","","","","","23773766","","","","English","IEEE Robot. Autom.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111010116"
"Puliti S.; Astrup R.","Puliti, Stefano (56784209400); Astrup, Rasmus (13410099800)","56784209400; 13410099800","Automatic detection of snow breakage at single tree level using YOLOv5 applied to UAV imagery","2022","International Journal of Applied Earth Observation and Geoinformation","112","","102946","","","","8","10.1016/j.jag.2022.102946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135935078&doi=10.1016%2fj.jag.2022.102946&partnerID=40&md5=04212798a113ebbd3315aa7c47b8dd6d","Norwegian Institute for Bioeconomy Research (NIBIO), Division of Forest and Forest Resources, Høgskoleveien 8, Ås, 1433, Norway","Puliti S., Norwegian Institute for Bioeconomy Research (NIBIO), Division of Forest and Forest Resources, Høgskoleveien 8, Ås, 1433, Norway; Astrup R., Norwegian Institute for Bioeconomy Research (NIBIO), Division of Forest and Forest Resources, Høgskoleveien 8, Ås, 1433, Norway","The assessment of forest abiotic damages such as snow breakage is important to ensure compensation to forest owners. Currently, information on the extent of snow breakage is gathered through time-consuming and potentially biased field surveys. In such situations where field surveys are still common practice, unmanned aerial vehicles (UAVs) are increasingly being used to provide a more cost-efficient and objective methods to answer forest information needs. Further, the advent of sophisticated computer vision techniques such as convolutional neural networks (CNNs) offers new ways to analyze image data more efficiently and accurately. We proposed an object detection method to automatically identify trees and classify them according to the damage by snow based on a YOLO CNN architecture. UAV imagery collected across 89 study areas and over the course of the entire year were manually annotated into a total of >55 K single trees classified as healthy, damaged, or dead. The annotated trees, along with the corresponding UAV imagery were used to train a YOLOv5 object detection model. Furthermore, we tested the effect of seasonality, and varying atmospheric and lighting conditions on the model's performance. Based on an independent test set of data we found that the general model including all of the data (i.e. any seasons, atmospheric conditions, and time of the day) outperformed all other tested scenarios (i.e. precision = 62 %; recall = 61 %). Furthermore, we found that despite the fact that the snow damaged trees represented a minority class (i.e. 16 % of the annotated trees), they were detected with the largest precision (76 %) and recall (78 %). Finally, the general model transferred well across the variation in seasons, atmospheric and illumination conditions, making it suitable for usage for any new UAV image acquisition. © 2022 The Authors","Convolutional neural network; Deep-learning; Drones; Forest damage; Object detection","artificial neural network; forest; imagery; snow; tree; unmanned vehicle","","","","","","","(2021); Astrup R., Rahlf J., Bjorkelo K., Debella-Gilo M., Gjertsen A.-K., Breidenbach J., Forest information at multiple scales: development, evaluation and application of the Norwegian forest resources map SR16, Scandinavian Journal of Forest Research, 34, 6, pp. 484-496, (2019); Culvenor D.S., Extracting individual tree information. Remote Sensing of Forest Environments, Remote Sensing of Forest Environments, pp. 255-277, (2003); Dash J.P., Watt M.S., Pearse G.D., Heaphy M., Dungey H.S., Assessing very high resolution UAV imagery for monitoring forest health during a simulated disease outbreak, ISPRS Journal of Photogrammetry and Remote Sensing, 131, pp. 1-14, (2017); Diaz-Yanez O., Mola-Yudego B., Gonzalez-Olabarria J.R., Pukkala T., How does forest composition and structure affect the stability against wind and snow?, Forest Ecology and Management, 401, pp. 215-222, (2017); Diaz-Yanez O., Mola-Yudego B., Gonzalez-Olabarria J.R., Modelling damage occurrence by snow and wind in forest ecosystems, Ecological Modelling, 408, (2019); (2019); Du J., Understanding of Object Detection Based on CNN Family and YOLO, Journal of Physics: Conference Series, 1004, (2018); Fang Y., Guo X., Chen K., Zhou Z., Ye Q., Accurate and Automated Detection of Surface Knots on Sawn Timbers Using YOLO-V5 Model, BioResources, 16, 3, pp. 5390-5406, (2021); Fromm M., Schubert M., Castilla G., Linke J., McDermid G., Automated Detection of Conifer Seedlings in Drone Imagery Using Convolutional Neural Networks, Remote Sensing, 11, 21, (2019); Inoue T., Nagai S., Yamashita S., Fadaei H., Ishii R., Okabe K., Taki H., Honda Y., Kajiwara K., Suzuki R., Quattrochi D.A., Unmanned Aerial Survey of Fallen Trees in a Deciduous Broadleaved Forest in Eastern Japan, PLOS ONE, 9, 10, (2014); Japkowicz N., Stephen S., The class imbalance problem: A systematic study, Intelligent data analysis, 6, pp. 429-449, (2002); Jintasuttisak T., Edirisinghe E., Elbattay A., Deep neural network based date palm tree detection in drone imagery, Computers and Electronics in Agriculture, 192, (2022); Jocher G., YOLO v5 - Tips for Best Training Results, (2021); Jocher G., Nishimura K., Mineeva T., Vilarino R., YOLOv5, Available at, (2020); Kattenborn T., Schiefer F., Frey J., Feilhauer H., Mahecha M.D., Dormann C.F., Spatially autocorrelated training and validation samples inflate performance assessment of convolutional neural networks, ISPRS Open Journal of Photogrammetry and Remote Sensing, 5, (2022); Krisanski S., Taskhiri M.S., Gonzalez Aracil S., Herries D., Muneri A., Gurung M.B., Montgomery J., Turner P., Forest Structural Complexity Tool—An Open Source, Fully-Automated Tool for Measuring Forest Point Clouds, Remote Sensing, 13, 22, (2021); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., (2014); Liu S., Qi L., Shi J., pp. 8759-8768, (2018); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988, (2017); Lobo Torres D., Queiroz Feitosa R., Nigri Happ P., Elena Cue La Rosa L., Marcato Junior J., Martins J., Ola Bressan P., Goncalves W.N., Liesenberg V., Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery, Sensors, 20, 2, (2020); Longadge R., (2013); Miyoshi G.T., Arruda M.D.S., Osco L.P., Marcato Junior J., Goncalves D.N., Imai N.N., Tommaselli A.M.G., Honkavaara E., Goncalves W.N., A Novel Deep Learning Method to Identify Single Tree Species in UAV-Based Hyperspectral Images, Remote Sensing, 12, 8, (2020); Nagai S., Taku M.S., Kajiwara K., Yoshitake S., (2018); Nasi R., Honkavaara E., Lyytikainen-Saarenmaa P., Blomqvist M., Litkey P., Hakala T., Viljanen N., Kantola T., Tanhuanpaa T., Holopainen M., Using UAV-Based Photogrammetry and Hyperspectral Imaging for Mapping Bark Beetle Damage at Tree-Level, Remote Sensing, 7, 11, pp. 15467-15493, (2015); Nuijten R., Coops N., Goodbody T., Pelletier G., Examining the Multi-Seasonal Consistency of Individual Tree Segmentation on Deciduous Stands Using Digital Aerial Photogrammetry (DAP) and Unmanned Aerial Systems (UAS), Remote Sensing, 11, 7, (2019); Nykanen M.-L., Broadgate M., Kellomaki S., Peltola H., (1997); Pearse G.D., Tan A.Y.S., Watt M.S., Franz M.O., Dash J.P., Detecting and mapping tree seedlings in UAV imagery using convolutional neural networks and field-verified data, ISPRS Journal of Photogrammetry and Remote Sensing, 168, pp. 156-169, (2020); Redmon J., Divvala S., Girshick R., (2016); (2016); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in neural information processing systems, 28, pp. 91-99, (2015); Santos A.A.D., Marcato Junior J., Araujo M.S., Di Martini D.R., Tetila E.C., Siqueira H.L., Aoki C., Eltner A., Matsubara E.T., Pistori H., Feitosa R.Q., Liesenberg V., Goncalves W.N., Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs, Sensors, 19, 16, (2019); Sylvain J.-D., Drolet G., Brown N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography, ISPRS Journal of Photogrammetry and Remote Sensing, 156, pp. 14-26, (2019); Tao H., Li C., Zhao D., Deng S., Hu H., Xu X., Jing W., Deep learning-based dead pine tree detection from unmanned aerial vehicle images, International Journal of Remote Sensing, 41, 21, pp. 8238-8255, (2020); Thieurmel B., Elmarhraoui A., Package ‘suncalc’: CRAN, Available at, (2019); Tomppo E., Antropov O., Praks J., Boreal Forest Snow Damage Mapping Using Multi-Temporal Sentinel-1 Data, Remote Sensing, 11, 4, (2019); Vastaranta M., Korpela I., Uotila A., Hovi A., Holopainen M., Mapping of snow-damaged trees based on bitemporal airborne LiDAR data, European Journal of Forest Research, 131, 4, pp. 1217-1228, (2012); Vauhkonen J., Ene L., Gupta S., Heinzel J., Holmgren J., Pitkanen J., Solberg S., Wang Y., Weinacker H., Hauglin K.M., Lien V., Packalen P., Gobakken T., Koch B., Naesset E., Tokola T., Maltamo M., Comparative testing of single-tree detection algorithms under different types of forest, Forestry: An International Journal of Forest Research, 85, 1, pp. 27-40, (2012); Wang C.Y., Liao H.Y.M., Wu Y.H., Chen P.Y., Hsieh Y.I.-H., CSPNet: A new backbone that can enhance learning capability of CNNC, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops 2020, pp. 390-391, (2020); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks, Remote Sensing, 11, 11, (2019); Windrim L., Bryson M., Detection, Segmentation, and Model Fitting of Individual Tree Stems from Airborne Laser Scanning of Forests Using Deep Learning, Remote Sensing, 12, 9, (2020); Xu B., Wang N., Chen T., (2015); Xu R., Lin H., Lu K., Cao L., Liu Y., A Forest Fire Detection System Based on Ensemble Learning, Forests, 12, 2, (2021); Zamboni P., Junior J.M., Silva J.D.A., Miyoshi G.T., Matsubara E.T., Nogueira K., Goncalves W.N., Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images, Remote Sensing, 13, 13, (2021); Zhu J., Li X., Liu Z., Cao W., Gonda Y., (2006)","S. Puliti; Norwegian Institute for Bioeconomy Research (NIBIO), Division of Forest and Forest Resources, Høgskoleveien 8, Ås, 1433, Norway; email: stefano.puliti@nibio.no","","Elsevier B.V.","","","","","","15698432","","","","English","Int. J. Appl. Earth Obs. Geoinformation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135935078"
"Hu G.; Wang T.; Wan M.; Bao W.; Zeng W.","Hu, Gensheng (8644853100); Wang, Tongxiang (57485668600); Wan, Mingzhu (57208056149); Bao, Wenxia (35193489800); Zeng, Weihui (56149611600)","8644853100; 57485668600; 57208056149; 35193489800; 56149611600","UAV remote sensing monitoring of pine forest diseases based on improved Mask R-CNN","2022","International Journal of Remote Sensing","43","4","","1274","1305","31","10","10.1080/01431161.2022.2032455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126181901&doi=10.1080%2f01431161.2022.2032455&partnerID=40&md5=a205102a588f1fb02630de381033e644","National Engineering Research Center for Agro, Ecological Big Data Analysis Application, Anhui University, Hefei, China; School of Information Science and Technology, Fudan University, Shanghai, China","Hu G., National Engineering Research Center for Agro, Ecological Big Data Analysis Application, Anhui University, Hefei, China; Wang T., National Engineering Research Center for Agro, Ecological Big Data Analysis Application, Anhui University, Hefei, China; Wan M., School of Information Science and Technology, Fudan University, Shanghai, China; Bao W., National Engineering Research Center for Agro, Ecological Big Data Analysis Application, Anhui University, Hefei, China; Zeng W., National Engineering Research Center for Agro, Ecological Big Data Analysis Application, Anhui University, Hefei, China","Timely and accurate monitoring of pine forest diseases is greatly important to pine forest disease prevention and forest protection. A method for unmanned aerial vehicle (UAV) remote sensing monitoring of pine forest diseases is proposed based on improved Mask region-based convolutional neural network (R-CNN) to solve the problem of low accuracy of the existing methods due to complex background and nonevident characteristics of pine forest diseases. The Mask R-CNN with multitask capabilities is utilized as backbone network to build the monitoring model. An improved multiscale receptive field block module is added to the Mask R-CNN to extract the detailed features of pine forest diseases and reduce the missed detection of pine forest diseases. Meanwhile, a multilevel fusion residual pyramid structure is adopted to integrate low-level and high-level features to obtain distinguishable features of pine forest diseases to reduce the misdetection and misidentification of complex backgrounds. In addition, the proposed method uses cutting mixing splicing method to construct training data set to increase the number of background objects in the training images and reduce the impact of complex backgrounds on the monitoring results. Experimental results show that the proposed method can monitor pine forest diseases more accurately than methods of SSD, Faster R-CNN, MaskScoring R-CNN, and the original Mask R-CNN. Compared with the method of backbone network, the precision of the proposed method has increased by 22.4%, the recall has increased by 3.5%, and the F1-score has increased by 14.4%. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","deep learning; disease monitoring; Mask R-CNN; Pine forest disease; UAV remote sensing","Antennas; Complex networks; Convolutional neural networks; Deep learning; Forestry; Monitoring; Remote sensing; Convolutional neural network; Deep learning; Disease monitoring; Forest disease; Mask region-based convolutional neural network; Pine forest; Pine forest disease; Region-based; Remote-sensing; Unmanned aerial vehicle remote sensing; accuracy assessment; artificial neural network; coniferous forest; disease incidence; experimental study; image analysis; monitoring; precision; remote sensing; unmanned vehicle; Unmanned aerial vehicles (UAV)","","","","","Major Natural Science Research Projects in Colleges and Universities of Anhui Province, (KJ2020ZD03); Natural Science Research Projects in Colleges and Universities of Anhui Province; Anhui University, (AE201902)","Funding text 1: The authors thank the Major Natural Science Research Projects in Colleges and Universities of Anhui Province under Grant KJ2020ZD03 and the Open Research Fund of National Engineering Research Center for Agro-Ecological Big Data Analysis & Application of Anhui University under Grant (AE201902) for their support. ; Funding text 2: The work was supported by the?Major Natural Science Research Projects in Colleges and Universities of Anhui Province, China [KJ2020ZD03]; Open Research Fund of National Engineering Research Center for Agro-Ecological Big Data Analysis & Application of Anhui University [AE201902]. The authors thank the Major Natural Science Research Projects in Colleges and Universities of Anhui Province under Grant KJ2020ZD03 and the Open Research Fund of National Engineering Research Center for Agro-Ecological Big Data Analysis & Application of Anhui University under Grant (AE201902) for their support.","Bochkovskiy A., Wang C.Y., Liao H., Yolov4: Optimal Speed and Accuracy of Object Detection, Conference on Computer Vision and Pattern Recognition (CVPR) 2020, (2020); Calvao T., Duarte C.M., Pimentel C.S., Climate and Landscape Patterns of Pine Forest Decline After Invasion by the Pinewood Nematode, Forest Ecology and Management, 433, (2019); Choi W.I., Nam Y., Lee C.Y., Choi B.K., Shin Y.J., Lim J.-H., Koh S.-H., Park Y.-S., Changes in Major Insect Pests of Pine Forests in Korea Over the Last 50 Years, Forests, 10, 8, (2019); Fan D.P., Ji G., Sun G., Cheng M., Shao L., Camouflaged Object Detection, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2020); Fuente B., Saura S., Pieter B., Predicting the Spread of an Invasive Tree Pest: The Pine Wood Nematode in Southern Europe, The Journal of Applied Ecology, 55, 5, (2018); He Y., Chen G., Potter C., Meentemeyer R.K., Integrating Multi-Sensor Remote Sensing and Species Distribution Modeling to Map the Spread of Emerging Forest Disease and Tree Mortality, Remote Sensing of Environment, 231, (2019); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, 2017 IEEE International Conference on Computer Vision (ICCV), (2017); He K., Zhang X., Ren S., Sun J., ”Deep Residual Learning for Image Recognition,”, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Hu G.S., Yin C., Wan M., Zhang Y., Fang Y., Recognition of Diseased Pinus Trees in UAV Images Using Deep Learning and AdaBoost Classifier, Biosystems Engineering, 194, (2020); Hu G.S., Zhu Y., Wan M., Bao W., Zhang Y., Detection of Diseased Pine Trees in Unmanned Aerial Vehicle Images by Using Deep Convolutional Neural Networks, Geocarto International, (2021); Huang Z., Huang L., Gong Y., Huang C., Wang X., Mask Scoring R-CNN, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019); Ikegami M., Jenkins T.A.R., Estimate Global Risks of a Forest Disease Under Current and Future Climates Using Species Distribution Model and Simple Thermal Model-Pine Wilt Disease as a Model Case, Forest Ecology and Management, 409, (2018); Immitzer M., Bock S., Einzmann K., Vuolo F., Pinnel N., Wallner A., Atzberger C., Fractional Cover Mapping of Spruce and Pine at 1ha Resolution Combining Very High and Medium Spatial Resolution Satellite Imagery, Remote Sensing of Environment, 204, (2018); Iordache M.-D., Mantas V., Baltazar E., Pauly K., Lewyckyj N., A Machine Learning Approach to Detecting Pine Wilt Disease Using Airborne Spectral Imagery, Remote Sensing, 12, 14, (2020); Kichas N.E., Trowbridge A.M., Raffa K.F., Malone S.C., Hood S.M., Everett R.G., McWethy D.B., Pederson G.T., Whitebark Pine (Pinus Albicaulis) Growth and Defense in Response to Mountain Pine Beetle Outbreaks, Forest Ecology and Management, 457, (2020); Liang L., Hawbaker T.J., Chen Y., Zhu Z., Gong P., Characterizing Recent and Projecting Future Potential Patterns of Mountain Pine Beetle Outbreaks in the Southern Rocky Mountains, Applied Geography, 55, (2014); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single Shot MultiBox Detector, European Conference on Computer Vision (ECCV) 2016, Lecture Notes in Computer Science, (2016); Lossou E., Nat O., Agyemang G., Monitoring Land Cover Changes in the Tropical High Forests Using Multi-Temporal Remote Sensing and Spatial Analysis Techniques, Remote Sensing Applications: Society and Environment, 16, (2019); Luo W., Li Y., Urtasun R., Zemel R., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS 4898-4906, (2017); Matsuhashi S., Hirata A., Akiba M., Nakamura K., Oguro M., Takano K., Nakao K., Hijioka Y., Matsui T., Developing a Point Process Model for Ecological Risk Assessment of Pine Wilt Disease at Multiple Scales, Forest Ecology and Management, 463, (2020); Nasi R., Honkavaara E., Blomqvist M., Saarenmaa P., Hakala T., Viljanen N., Kantola T., Holopainen M., Remote Sensing of Bark Beetle Damage in Urban Forests at Individual Tree Level Using a Novel Hyperspectral Camera from UAV and Aircraft, Urban Forestry & Urban Greening, 30, (2018); Park H.G., Yun J.P., Kim M.Y., Jeong S.H., Multichannel Object Detection for Detecting Suspected Trees with Pine Wilt Disease Using Multispectral Drone Imagery, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14, (2021); Ren S.Q., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, (2017); Sambaraju K.R., Carroll A.L., Aukema B.H., Multiyear Weather Anomalies Associated with Range Shifts by the Mountain Pine Beetle Preceding Large Epidemics, Forest Ecology and Management, 438, (2019); Shang T., Dai Q., Zhu S., Yang T., Guo Y., Perceptual Extreme Super Resolution Network with Receptive Field Block, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), (2020); Suzuki K., PATHOLOGY | Pine Wilt and the Pine Wood Nematode.”|“pathology | Pine Wilt and the Pine Wood Nematode, Encyclopedia of Forest Sciences, 555, (2004); Szegedy C., Liu W., Jia Y., Sermanet P., Rabinovich A., Going Deeper with Convolutions, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2015); Tao H., Li C., Zhao D., Deng S., Jing W., Deep Learning-Based Dead Pine Trees Detection from Unmanned Aerial Vehicle Images, International Journal of Remote Sensing, 41, pp. 8238-8255, (2020); Wu B.Z., Liang A., Zhang H., Zhu T., Zou Z., Yang D., Tang W., Li J., Su J., Application of Conventional UAV-Based High-Throughput Object Detection to the Early Diagnosis of Pine Wilt Disease by Deep Learning, Forest Ecology and Management, 486, (2021); Yu F., Koltun V., Multi-Scale Context Aggregation by Dilated Convolutions, 4th International Conference on Learning Representations.{ICLR}, (2016); Yu R., Luo Y., Zhou Q., Zhang X., Wu D., Ren L., Early Detection of Pine Wilt Disease Using Deep Learning Algorithms and UAV-Based Multispectral Imagery, Forest Ecology and Management, 497, (2021); Zhan P., Zhu W., Li N., An Automated Rice Mapping Method Based on Flooding Signals in Synthetic Aperture Radar Time Series, Remote Sensing of Environment, 252, (2021); Zhang N., Zhang X., Yang G., Zhu C., Huo L., Feng H., Assessment of Defoliation During the Dendrolimus Tabulaeformis Tsai Et Liu Disaster Outbreak Using UAV-Based Hyperspectral Images, Remote Sensing of Environment, 217, (2018); Zhao L., Mota M., Vieira P., Butcher R., Sun J., Interspecific Communication Between Pinewood Nematode, Its Insect Vector, and Associated Microbes, Trends in Parasitology, 30, 6, pp. 299-308, (2014); Zhao T., Wu X., Pyramid Feature Attention Network for Saliency Detection, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), (2019); Zhao J., Zhong Y., Hu X., Wei L., Zhang L., A Robust Spectral-Spatial Approach to Identifying Heterogeneous Crops Using Remote Sensing Imagery with High Spectral and Spatial Resolutions, Remote Sensing of Environment, 239, (2020); Zheng J.P., Fu H., Li W., Wu W., Yu L., Yuan S., Tao W., Pang T.K., Kanniah K.D., Growing Status Observation for Oil Palm Trees Using Unmanned Aerial Vehicle (UAV) Images, ISPRS Journal of Photogrammetry and Remote Sensing, 173, (2021); Zhu Z.J., Li M., Hu Y., Li J., Liu D., Li J., Indoor Scene Segmentation Algorithm Based on Full Convolutional Neural Network, Neural Computing & Applications, 33, pp. 8261-8273, (2021)","W. Bao; School of Electronic and Information Engineering, Anhui University, Hefei, 230601, China; email: bwxia@ahu.edu.cn","","Taylor and Francis Ltd.","","","","","","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Final","","Scopus","2-s2.0-85126181901"
"Liu X.; Ghazali K.H.; Han F.; Mohamed I.I.","Liu, Xinni (57205122751); Ghazali, Kamarul Hawari (24070207000); Han, Fengrong (57206894004); Mohamed, Izzeldin Ibrahim (58105205300)","57205122751; 24070207000; 57206894004; 58105205300","Review of CNN in aerial image processing","2023","Imaging Science Journal","71","1","","1","13","12","0","10.1080/13682199.2023.2174651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148207110&doi=10.1080%2f13682199.2023.2174651&partnerID=40&md5=70b8dc315c35914f0bb929b9edf0b451","School of Information, Xi’an University of Finance and Economics, Xi’an, China; Faculty of Electrical and Electronics Engineering, University Malaysia Pahang, Pekan, Malaysia; School of Computer, Baoji University of Arts and Sciences, Baoji, China","Liu X., School of Information, Xi’an University of Finance and Economics, Xi’an, China; Ghazali K.H., Faculty of Electrical and Electronics Engineering, University Malaysia Pahang, Pekan, Malaysia; Han F., School of Computer, Baoji University of Arts and Sciences, Baoji, China; Mohamed I.I., Faculty of Electrical and Electronics Engineering, University Malaysia Pahang, Pekan, Malaysia","In recent years, deep learning algorithm has been used in many applications mainly in image processing of object detection and classification. The use of image processing techniques is becoming more interesting with the existence of drone technology with the employ of deep learning in aerial view image processing because of the high resolution and heaps of images taken. This paper aims to review neural networks specifically on the aerial view image by drones and to discuss the work principles and classic architectures of convolutional neural networks, its latest research trend and typical models along with target detection in object detection, image classification and semantic segmentation. In addition, this study also provided a specific application in the aerial image. Finally, the limitations of the convolutional network and expected future development trends were also discussed. Based on the findings, the deep learning algorithm was observed to provide high accuracy, it outperformed other generally image processing-based techniques. © 2023 The Royal Photographic Society.","Aerial image; convolutional neural networks; deep learning algorithm; drone technology; image classification; image processing; object detection; review","Aircraft detection; Antennas; Convolution; Convolutional neural networks; Deep neural networks; Image classification; Learning algorithms; Object detection; Object recognition; Semantic Segmentation; Semantics; Aerial image processing; Aerial images; Convolutional neural network; Deep learning algorithm; Drone technology; Image processing technique; Images classification; Images processing; Object classification; Objects detection; Drones","","","","","Social Science Foundation Program of Shaanxi of China, (2022M005); Xi’an University of Finance and Economics of China, (22FCJH008); Xi'an University of Finance and Economics, XAUFE; Universiti Malaysia Pahang, UMP, (RDU1703228, UIC191502); Social Science Foundation of Shaanxi Province","Funding text 1: This work was supported by Xi’an University of Finance and Economics [grant number 22FCJH008]; Universiti Malaysia Pahang [grant number UIC191502; RDU1703228]; Social Science Foundation Program of Shaanxi [grant number 2022M005]. This work was supported by the Research Support Program of Xi’an University of Finance and Economics of China (22FCJH008), and the Social Science Foundation Program of Shaanxi of China (2022M005).; Funding text 2: This work was supported by the Research Support Program of Xi’an University of Finance and Economics of China (22FCJH008), and the Social Science Foundation Program of Shaanxi of China (2022M005). ","Wuttichai B., Yumin T., Yinghua Y., Et al., A deep learning approach on building detection from unmanned aerial vehicle-based images in riverbank monitoring, Sensors, 18, 11, pp. 3921-3933, (2018); Zhengxia Z., Zhenwei S., Random access memories: a new paradigm for target detection in high resolution aerial remote sensing images, IEEE Trans Image Proc, 27, 3, pp. 1100-1111, (2018); Griffiths D., Boehm J., Rapid object detection systems, utilising deep learning and unmanned aerial systems (UAS) for civil engineering applications, ISPRS TC II Mid-term Sym, Riva del Garda, Italy, Proceedings of ISPRS 2018, pp. 391-398; Kasthurirangan G., Hoda G., Akash V., Et al., Crack damage detection in unmanned aerial vehicle images of civil infrastructure using pre-trained deep learning model, Int J Traffic Trans Eng, 8, 1, pp. 1-14, (2018); Diogo D., Francesco N., Norman K., Et al., Multi-resolution feature fusion for image classification of building damages with convolutional neural networks, Rem Sens., 10, 10, pp. 1636-1661, (2018); Dimitrios M., Mihai D., Thomas E., Et al., Deep learning earth observation classification using ImageNet pretrained networks, IEEE Geosci Rem Sens Let., 13, 1, pp. 105-109, (2016); Xiangtao Z., Yuan Y., Lu X., A deep scene representation for aerial scene classification, IEEE Trans: Geosci Rem Sens, 57, 7, pp. 4799-4809, (2019); Lillesand T.M., Kiefer R.W., Chipman J., Remote sensing and image interpretation, (2008); Shaw G., Manolakis D., Signal processing for hyperspectral image exploitation, IEEE Sig Proc Mag, 19, 1, pp. 12-16, (2002); Ishimwe R., Abutaleb K., Ahmed F., Applications of thermal imaging in agriculture–A review, Adv in Rem Sens, 3, 2014, pp. 128-140, (2014); Fedra T., Andres F., Carlos S., Et al., Corn classification using deep learning with UAV imagery. An operational proof of concept, IEEE 1st ColCACI; Mohammad R., Yun Z., Rakesh M., Et al., Using a VGG-16 network for individual tree species detection with an object-based approach, 10th IAPR Work. Pat. Rec. in Rem. Sensg (PRRS); Calhoon S., Conwell W Y.; Saxena L., Armstrong L., A survey of image processing techniques for agriculture, Proceedings of AFITA, pp. 405-418; Arti S., Baskar G., Et al., Machine learning for high-throughput stress phenotyping in plants, Tre Plant Sci, 21, 2, pp. 110-124, (2016); Alfatni M.S.M., Shariff A.R.M., Shafri H.Z.M., Et al., Oil palm fruit bunch grading system using red, green and blue digital number, J App Sci., 8, 8, pp. 1444-1452, (2008); Chen C., Baochang Z., Et al., Land-use scene classification using multi-scale completed local binary patterns, Sig ima vid Proc, 10, 4, pp. 745-752, (2016); Kamusoko C., Importance of remote sensing and land change modeling for urbanization studies, Urban development in Asia and Africa, pp. 3-10, (2017); Kerkech M., Hafiane A., Canals R., Deep leaning approach with colorimetric spaces and vegetation indices for vine diseases detection in UAV images, Comp Electr Agric, 155, pp. 237-243, (2018); Nair V., Hinton G.E., Rectified linear units improve restricted Boltzmann machines, Proceedings of International Conference on Mac. Lear (ICML-10), pp. 807-814; White A.R., Human expertise in the interpretation of remote sensing data: A cognitive task analysis of forest disturbance attribution, Int J App Ear Obser Geoinformation, 74, pp. 37-44, (2019); Hassan M., Golam R A.M., Uddin M Z., Et al., Human emotion recognition using deep belief network architecture, Infor Fusion, 51, pp. 10-18, (2019); Rig D., Emanuela P., Emanuele M., Et al., Convolutional neural network for finger-vein-based biometric identification, IEEE Trans Infor Forensic Sec, 14, 2, pp. 360-373, (2019); Calvo-Zaragoza J., Gallego A., A selectional auto-encoder approach for document image binarization, Com Ver Pattern Rec, 86, pp. 37-47, (2019); He K., Xiangyu Z., Shaoqing R., Et al., Deep residual learning for image recognition, IEEE Conference on Computer Vision Pattern Recognition (CVPR); Simonyan K., Zisserman A.; Wenlu Z., Rongjian L., Tao Z., Et al., Deep model based transfer and multi-task learning for biological image analysis, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1475–1484, (2015); Weixun Z., Zhenfeng S., Chunyuan D., Et al., High-resolution remote-sensing imagery retrieval using sparse features by auto-encoder, Rem. Sens Lett, 6, 10, pp. 775-783, (2015); Agostinelli F., Hoffman M., Sadowski P., Et al.; Metin N.G., Heang-Ping C., Optimal neural network architecture selection: improvement in computerized detection of microcalcifications, Acad Radiol, 9, 4, pp. 420-429, (2002); Krizhevsky A., Sutskever I., Hinton G., Imagenet classification with deep convolutional neural networks, Advances in Neural information Processing System, pp. 1097-1105; Kaiming H., Xiangyu Z., Shaoqing R., Et al., Delving deep into rectifiers: surpassing human-level performance on ImageNet classification, Proceedings of IEEE International Conference on Computer Vision, pp. 1026-1034; Hilal E., Yusuf C A., Mustafa S., Et al., Early and late level fusion of deep convolutional neural networks for visual concept recognition, Int J Sema Comp, 10, 3, pp. 347-363, (2016); Lecun Y., Bottou L., Bengio Y., Et al., Gradient-based learning applied to document recognition, Proc IEEE, 86, 11, pp. 2278-2324, (1998); Deng J., Dong W., Socher R., Imagenet: a large-scale hierarchical image database, IEEE Conference on Computer Vision Pattern Recognition, pp. 248-255; Bengio Y., Simard P., Frasconi P., Learning long-term dependencies with gradient descent is difficult, IEEE Trans Neu Net, 5, 2, pp. 157-166, (1994); Szegedy C., Wei L., Yangqing J., Et al., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Min L., Chen Q., Yan S.; Tayara H., Chong K.T., Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network, Sens., 18, 10, pp. 3341-3358, (2018); Gong C., Ceyuan Y., Xiwen Y., Et al., When deep learning meets metric learning: remote sensing image scene classification via learning discriminative CNNs, IEEE Trans Geosci Rem Sens, 56, 5, pp. 2811-2821, (2018); Marco C., Giovanni P., Carlo S., Et al.; Guofeng S., Wen Y., Tao X., Et al., High-resolution satellite scene classification using a sparse coding based multiple feature combination, Int J Rem Sens, 33, 8, pp. 2395-2412, (2012); Gong C., Junwei H., Xiaoqiang L., Remote sensing image scene classification: benchmark and state of the art, Proc IEEE, 105, 10, pp. 1865-1883, (2017); GuiSong X., Jingwen H., Et al., AID: a benchmark data set for performance evaluation of aerial scene classification, IEEE Trans Geosci Rem Sens Let., 55, 7, pp. 3965-3981, (2017); Gong C., Junwei H., Peicheng Z., Et al., Multi-class geospatial object detection and geographic image classification based on collection of part detectors, ISPRS J Photog Rem Sens Let, 98, pp. 119-132, (2014); Razakarivonyb S., Jurie F., Vehicle detection in aerial imagery: a small target detection benchmark, J Vis Commu Ima Rep, 34, pp. 187-203, (2016); Franklin T., Brian C., Craig P., Et al., Overhead imagery research data set–an annotated data library & tools to aid in the development of computer vision algorithms, AIPR 2009; Rottensteiner F., Sohn G., Jung J., Et al., The ISPRS benchmark on urban object classification and 3D building reconstruction, ISPRS Anna Photog, Rem Sens Spa Infor Sci, 1, 3, pp. 293-298, (2012); Kang L., Gellert M., Fast multiclass vehicle detection on aerial images, IEEE Geosci Rem Sens Let, 12, 9, pp. 1938-1942, (2015); Mnih V., Machine learning for aerial image labeling, (2013); Kemker R., Salvaggio C., Kanan C., Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning, ISPRS J Photog Re Sens Let, 145, pp. 60-77, (2018); Volpi M., Tuia D., Deep multi-task learning for a geographically-regularized semantic segmentation of aerial images, ISPRS J Photog Rem Sens Let., 144, pp. 48-60, (2018); Jinwang W., Wei G., Pan T., Et al., Bottle detection in the wild using low-altitude unmanned aerial vehicles, International Confer. Infor. Fusion (FUSION); Jiandan Z., Tao L., Guangle Y., Robust vehicle detection in aerial images based on cascaded convolutional neural networks, Sens., 17, 12, pp. 2720-2737, (2017); Ying S., Xinchang Z., Silvano G., Et al., Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data, ISPRS J Photog Rem Sens Let., 143, pp. 3-14, (2018); Marmanis D., Wegner J.D., Et al., Semantic segmentation of aerial images with an ensemble of CNNs, ISPRS Ann photog. Rem Sens Spa Infor Sci., 3, pp. 473-480, (2016); Qi C., Lei W., Yifan W., Et al., Aerial imagery for roof segmentation: a large-scale dataset towards automatic mapping of buildings, ISPRS J Photog Rem Sens Let., 147, pp. 42-55, (2019); Xiang L., Xiaojing Y., Yi F., Building-a-nets: robust building extraction from high-resolution remote sensing images with adversarial networks, IEEE J Sel Top App Earth Obs Rem Sens Let., 11, 10, pp. 3680-3687, (2018); Lichao M., Xiaoxiang Z., Vehicle instance segmentation from aerial image and video using a multi-task learning residual fully convolutional network, IEEE Trans Geosci Rem Sens, 56, 11, pp. 6699-6711, (2018); Hani A., Eduard T., Et al., Learning to match aerial images with deep attentive architectures, IEEE Conference Computer Vision Pattern Recognition (CVPR), pp. 3539-3547; Ruihua W., Xionwu X., Bingxuan G., Et al., An effective image denoising method for UAV images via improved generative adversarial networks, Sens., 18, 7, pp. 1985-2007, (2018); Benjamin K., Diego M., Devis T., Detecting mammals in UAV images: best practices to address a substantially imbalanced dataset with deep learning, Rem Sens Enviro., 216, pp. 139-153, (2018); Yang L., Peng S., Highsmith M R., Et al., Performance comparison of deep learning techniques for recognizing birds in aerial images, IEEE Third International Conference on DSC, pp. 317-324; Amin F., Mohammad E., Babak M., A deep residual neural network for low altitude remote sensing image classification, Ira. Joi. CFIS; Wei H., Ruyi F., Lizhe W., Et al., A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification, ISPRS J: Photog Rem Sens Let, 145, pp. 23-43, (2018); Otavio A.P., Keiller N., Jefersson A., Et al., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains, IEEE Conference Computer Vision Pattern Recognition (CVPR) Workshops, pp. 44-51; Fan H., GuiSong X., Jingwen H., Et al., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Rem Sens, 7, 11, pp. 14680-14707, (2015); Barak O., Annie H., Et al.; Vysakh S.M., Sowmya V., Soman K.P., Deep neural networks as feature extractors for classification of vehicles in aerial imagery, International Conference on SPIN, pp. 105-110; Makantasis K., Karantzalos K., Anastasios D., Et al., Deep supervised learning for hyperspectral data classification through convolutional neural networks, IEEE IGARSS, pp. 4959-4962; Yuhao C., Javier R., Edward J.D., Estimating plant centers using a deep binary classifier, IEEE SSIAI; Rui C., Jiasong Z., Wei T., Et al., Integrating aerial and street view images for urban land use classification, Rem Sens., 10, 10, pp. 1553-1575, (2018); Adriana R., Carlo G., Gustau C.V., Unsupervised deep feature extraction for remote sensing image classification, IEEE Trans Geosci Rem Sens, 54, 3, pp. 1349-1362, (2016); Jasper R U., Koen E.A., Van D.S., Et al., Selective search for object recognition, Int J Comp Vis, 104, 2, pp. 154-171, (2013); Joseph R., Santosh D., Ross G., Et al., You only look once: unified, real-time object detection, Proceedings of IEEE Conference on CVPR, pp. 779-788; Shaoqing R., Kaiming H., Ross G., Et al., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans Pat Anal & Mac Int, 39, 6, pp. 1137-1149, (2017); Mahyar N., Pouya S., Rama C., Et al., Ssh: single stage headless face detector, IEEE ICCV, pp. 4875-4884; Xiaohong W., Abhinav S., Abhinav G., A-fast-rcnn: hard positive generation via adversary for object detection, Proceedings of IEEE Conference on Computer Vision Pattern Recognition, pp. 2606-2615","X. Liu; School of Information, Xi’an University of Finance and Economics, Xi’an, China; email: lxinni@163.com","","Taylor and Francis Ltd.","","","","","","13682199","","","","English","Imag. Sci. J.","Review","Final","","Scopus","2-s2.0-85148207110"
"Hoeser T.; Feuerstein S.; Kuenzer C.","Hoeser, Thorsten (57216967331); Feuerstein, Stefanie (57205419456); Kuenzer, Claudia (55927784300)","57216967331; 57205419456; 55927784300","DeepOWT: A global offshore wind turbine data set derived with deep learning from Sentinel-1 data","2022","Earth System Science Data","14","9","","4251","4270","19","0","10.5194/essd-14-4251-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140395043&doi=10.5194%2fessd-14-4251-2022&partnerID=40&md5=84a3e1cf6415a2b7fae930f60636fa42","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; Department of Remote Sensing, Institute of Geography and Geology, University of Würzburg, Würzburg, 97074, Germany","Hoeser T., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; Feuerstein S., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; Kuenzer C., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany, Department of Remote Sensing, Institute of Geography and Geology, University of Würzburg, Würzburg, 97074, Germany","Offshore wind energy is at the advent of a massive global expansion. To investigate the development of the offshore wind energy sector, optimal offshore wind farm locations, or the impact of offshore wind farm projects, a freely accessible spatiotemporal data set of offshore wind energy infrastructure is necessary. With free and direct access to such data, it is more likely that all stakeholders who operate in marine and coastal environments will become involved in the upcoming massive expansion of offshore wind farms. To that end, we introduce the DeepOWT (Deep-learning-derived Offshore Wind Turbines) data set (available at 10.5281/zenodo.5933967, ), which provides 9941 offshore wind energy infrastructure locations along with their deployment stages on a global scale. DeepOWT is based on freely accessible Earth observation data from the Sentinel-1 radar mission. The offshore wind energy infrastructure locations were derived by applying deep-learning-based object detection with two cascading convolutional neural networks (CNNs) to search the entire Sentinel-1 archive on a global scale. The two successive CNNs have previously been optimised solely on synthetic training examples to detect the offshore wind energy infrastructures in real-world imagery. With subsequent temporal analysis of the radar signal at the detected locations, the DeepOWT data set reports the deployment stages of each infrastructure with a quarterly frequency from July 2016 until June 2021. The spatiotemporal information is compiled in a ready-to-use geographic information system (GIS) format to make the usability of the data set as accessible as possible.  © 2022 Thorsten Hoeser et al.","","","","","","","","","4C Offshore: 4C Offshorewind, (2021); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Kudlur M., Levenberg J., Monga R., Moore S., Murray D. G., Steiner B., Tucker P., Vasudevan V., Warden P., Wicke M., Yu Y., Zheng X., TensorFlow: A System for Large-Scale Machine Learning, 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265-283, (2016); Aschbacher J., ESA's earth observation strategy and Copernicus, Satellite earth observations and their impact on society and policy, pp. 81-86, (2017); Bailey H., Brookes K. L., Thompson P. M., Assessing environmental impacts of offshore wind farms: Lessons learned and recommendations for the future, Aquatic Biosystems, 10, (2014); Baumhoer C. A., Dietz A. J., Kneisel C., Kuenzer C., Automated Extraction of Antarctic Glacier and Ice Shelf Fronts from Sentinel-1 Imagery Using Deep Learning, Remote Sens, 11, (2019); Bazzi H., Ienco D., Baghdadi N., Zribi M., Demarez V., Distilling Before Refine: Spatio-Temporal Transfer Learning for Mapping Irrigated Areas Using Sentinel-1 Time Series, IEEE Geosci. Remote S, 17, pp. 1909-1913, (2020); Belenguer-Plomer M. A., Tanase M. A., Chuvieco E., Bovolo F., CNN-based burned area mapping using radar and optical data, Remote Sens. Environ, 260, (2021); Bergstr'm L., Kautsky L., Malm T., Rosenberg R., Wahlberg M., Capetillo N. A., Wilhelmsson D., Effects of offshore wind farms on marine wildlife-a generalized impact assessment, Environ. Res. Lett, 9, (2014); Boeck M., Voinov S., Keim S., Volkmann R., Langbein M., MA1/4hlbauer M., Frontend Libraries for DLR UKIS (Map) Applications, (2022); Cavazzi S., Dutton A., An Offshore Wind Energy Geographic Information System (OWE-GIS) for assessment of the UK's offshore wind energy potential, Renew. Energ, 87, pp. 212-228, (2016); COP26: Global coal to clean power transition statement, (2021); Cu La Rosa L. E., Happ P. N., Feitosa R. Q., Dense Fully Convolutional Networks for Crop Recognition from Multitemporal SAR Image Sequences, IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 7460-7463, (2018); Dirscherl M., Dietz A. J., Kneisel C., Kuenzer C., A Novel Method for Automated Supraglacial Lake Mapping in Antarctica Using Sentinel-1 SAR Imagery and Deep Learning, Remote Sens, 13, (2021); Drewitt A. L., Langston R. H. W., Assessing the impacts of wind farms on birds, Ibis, 148, pp. 29-42, (2006); Esteban M. D., Diez J. J., LApez J. S., Negro V., Why offshore wind energy?, Renew. Energ, 36, pp. 444-450, (2011); European Commission: An EU Strategy to harness the potential of offshore renewable energy for a climate neutral future, (2020); Fox A., Desholm M., Kahlert J., Christensen T. K., Krag Petersen I., Information needs to support environmental impact assessment of the effects of European marine offshore wind farms on birds, Ibis, 148, pp. 129-144, (2006); Gorelick N., Hancher M., Dixon M., Ilyushchenko S., Thau D., Moore R., Google Earth Engine: Planetary-scale geospatial analysis for everyone, Remote Sens. Environ, 202, pp. 18-27, (2017); Gusatu L. F., Yamu C., Zuidema C., Faaij A., A spatial analysis of the potentials for offshore wind farm locations in the North Sea region: Challenges and opportunities, ISPRS Int. J. Geo-Inf, 9, (2020); Gus, atu L., Menegon S., Depellegrin D., Zuidema C., Faaij A., Yamu C., Spatial and temporal analysis of cumulative environmental effects of offshore wind farms in the North Sea basin, Sci. Rep, 11, (2021); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Henderson A. R., Morgan C., Smith B., SArensen H. C., Barthelmie R. J., Boesmans B., Offshore Wind Energy in Europe-A Review of the State-of-the-Art, Wind Energy, 6, pp. 35-52, (2003); Hoeser T., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part I: Evolution and Recent Trends, Remote Sens, 12, (2020); Hoeser T., Kuenzer C., SyntEO: Synthetic dataset generation for earth observation and deep learning-Demonstrated for offshore wind farm detection, ISPRS J. Photogramm, 189, pp. 163-184, (2022); Hoeser T., Kuenzer C., DeepOWT: A global offshore wind turbine data set, Zenodo [data set], (2022); Hoeser T., Bachofer F., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part II: Applications, Remote Sens, 12, (2020); Johnson A. F., Dawson C. L., Conners M. G., Locke C. C., Maxwell S. M., Offshore renewables need an experimental mindset, Science, 376, pp. 361-361, (2022); Kang M., Ji K., Leng X., Lin Z., Contextual RegionBased Convolutional Neural Network with Multilayer Fusion for SAR Ship Detection, Remote Sens, 9, (2017); Krizhevsky A., Sutskever I., Hinton G. E., ImageNet Classification with Deep Convolutional Neural Networks, pp. 1097-1105, (2012); Krizhevsky A., Sutskever I., Hinton G. E., ImageNet Classification with Deep Convolutional Neural Networks, Commun. ACM, 60, pp. 84-90, (2017); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Loshchilov I., Hutter F., SGDR: Stochastic Gradient Descent with Warm Restarts, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B. A., Deep learning in remote sensing applications: A metaanalysis and review, ISPRS J. Photogramm, 152, pp. 166-177, (2019); Majidi Nezhad M., Groppi D., Marzialetti P., Fusilli L., Laneve G., Cumo F., Garcia D. A., Wind energy potential analysis using Sentinel-1 satellite: A review and a case study on Mediterranean islands, Renew. Sust. Energ. Rev, 109, pp. 499-513, (2019); Mullissa A. G., Persello C., Tolpekin V., Fully Convolutional Networks for Multi-Temporal SAR Image Classification, IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6635-6638, (2018); Opitz J., Burst S., Macro F1 and Macro F1, (2019); Padilla R., Passos W. L., Dias T. L. B., Netto S. L., da Silva E. A. B., A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit, Electronics, 10, (2021); Reichstein M., Camps-Valls G., Stevens B., Jung M., Denzler J., Carvalhais N., Prabhat, Deep learning and process understanding for data-driven Earth system science, Nature, 566, pp. 195-204, (2019); Ren S., He K., Girshick R. B., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE T. Pattern Anal, 39, pp. 1137-1149, (2015); Rodrigues S., Restrepo C., Kontos E., Teixeira Pinto R., Bauer P., Trends of offshore wind projects, Renew. Sust. Energ. Rev, 49, pp. 1114-1135, (2015); Slavik K., Lemmen C., Zhang W., Kerimoglu O., Klingbeil K., Wirtz K. W., The large-scale impact of offshore wind farm structures on pelagic primary productivity in the southern North Sea, Hydrobiologia, 845, pp. 35-53, (2019); Torres R., Snoeij P., Geudtner D., Bibby D., Davidson M., Attema E., Potin P., Rommen B., Floury N., Brown M., Traver I. N., Deghaye P., Duesmann B., Rosich B., Miranda N., Bruno C., L'Abbate M., Croci R., Pietropaolo A., Huchler M., Rostan F., GMES Sentinel-1 mission, Remote Sens. Environ, 120, pp. 9-24, (2012); Virtanen E., Lappalainen J., Nurmi M., Viitasalo M., TikanmA¤ki M., Heinonen J., Atlaskin E., Kallasvuo M., Tikkanen H., Moilanen A., Balancing profitability of energy production, societal impacts and biodiversity in offshore wind farm design, Renew. Sust. Energ. Rev, 158, (2022); Virtanen P., Gommers R., Oliphant T. E., Haberland M., Reddy T., Cournapeau D., Burovski E., Peterson P., Weckesser W., Bright J., van der Walt S. J., Brett M., Wilson J., Millman K. J., Mayorov N., Nelson A. R. J., Jones E., Kern R., Larson E., Carey C. J., Polat E, Feng Y., Moore E. W., VanderPlas J., Laxalde D., Perktold J., Cimrman R., Henriksen I., Quintero E. A., Harris C. R., Archibald A. M., Ribeiro A. H., Pedregosa F., van Mulbregt P., SciPy 1.0 Contributors: SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python, Nature Methods, 17, pp. 261-272, (2020); Wever L., Krause G., Buck B. H., Lessons from stakeholder dialogues on marine aquaculture in offshore wind farms: Perceived potentials, constraints and research gaps, Mar. Policy, 51, pp. 251-259, (2015); Wilson J. C., Elliott M., The habitat-creation potential of offshore wind farms, Wind Energy, 12, pp. 203-212, (2009); Wong B. A., Thomas C., Halpin P., Automating offshore infrastructure extractions using synthetic aperture radar and Google Earth Engine, Remote Sens. Environ, 233, (2019); Xu W., Liu Y., Wu W., Dong Y., Lu W., Liu Y., Zhao B., Li H., Yang R., Proliferation of offshore wind farms in the North Sea and surrounding waters revealed by satellite image time series, Renew. Sust. Energ. Rev, 133, (2020); Zhang J., Wang Q., Su F., Automatic extraction of offshore platforms in single SAR images based on a dual-step-modified model, Sensors, 19, (2019); Zhang T., Tian B., Sengupta D., Zhang L., Si Y., Global offshore wind turbine dataset, Sci. Data, 8, (2021); Zhu X. X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote S, 5, pp. 8-36, (2017); Zhu X. X., Montazeri S., Ali M., Hua Y., Wang Y., Mou L., Shi Y., Xu F., Bamler R., Deep Learning Meets SAR: Concepts, models, pitfalls, and perspectives, IEEE Geosci. Remote S, 9, pp. 143-172, (2021)","T. Hoeser; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; email: thorsten.hoeser@dlr.de","","Copernicus Publications","","","","","","18663508","","","","English","Earth Sys. Sci. Data","Data paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85140395043"
"Aldahoul N.; Karim H.A.; Sabri A.Q.M.; Tan M.J.T.; Momo M.A.; Fermin J.L.","Aldahoul, Nouar (56656478800); Karim, Hezerul Abdul (24492028400); Sabri, Aznul Qalid Md. (56150450300); Tan, Myles Joshua Toledo (57221311738); Momo, Mhd. Adel (57441948400); Fermin, Jamie Ledesma (57221304029)","56656478800; 24492028400; 56150450300; 57221311738; 57441948400; 57221304029","A Comparison between Various Human Detectors and CNN-Based Feature Extractors for Human Activity Recognition via Aerial Captured Video Sequences","2022","IEEE Access","10","","","63532","63553","21","5","10.1109/ACCESS.2022.3182315","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132780659&doi=10.1109%2fACCESS.2022.3182315&partnerID=40&md5=9580a8e2ad48a7692875bb324012ecba","Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia; Faculty of Engineering, Multimedia University, Cyberjaya, 63100, Malaysia; College of Engineering and Technology, University of St. la Salle, Bacolod, 6100, Philippines","Aldahoul N., Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia, Faculty of Engineering, Multimedia University, Cyberjaya, 63100, Malaysia; Karim H.A., Faculty of Engineering, Multimedia University, Cyberjaya, 63100, Malaysia; Sabri A.Q.M., Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia; Tan M.J.T., College of Engineering and Technology, University of St. la Salle, Bacolod, 6100, Philippines; Momo M.A., Faculty of Engineering, Multimedia University, Cyberjaya, 63100, Malaysia; Fermin J.L., College of Engineering and Technology, University of St. la Salle, Bacolod, 6100, Philippines","Human detection and activity recognition (HDAR) in videos plays an important role in various real-life applications. Recently, object detection methods have been used to detect humans in videos for subsequent decision-making applications. This paper aims to address the problem of human detection in aerial captured video sequences using a moving camera attached to an aerial platform with dynamical events such as varied altitudes, illumination changes, camera jitter, and variations in viewpoints, object sizes and colors. Unlike traditional datasets that have frames captured by a static ground camera with medium or large regions of humans in these frames, the UCF-ARG aerial dataset is more challenging because it contains videos with large distances between the humans in the frames and the camera. The performance of human detection methods that have been described in the literature are often degraded when input video frames are distorted by noise, blur, illumination changes, and the like. To address these limitations, the object detection methods used in this study were trained on the COCO dataset and evaluated on the publicly available UCF-ARG dataset. The comparison between these detectors was done in terms of detection accuracy. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that EfficientDetD7 was able to outperform other detectors with 92.9% average accuracy in detecting all activities and various conditions including blurring, addition of Gaussian noise, lightening, and darkening. Additionally, deep pre-trained convolutional neural networks (CNNs) such as ResNet and EfficientNet were used to extract highly informative features from the detected and cropped human patches. The extracted spatial features were utilized by Long Short-Term Memory (LSTM) to consider temporal relations between features for human activity recognition (HAR). Experimental results found that the EfficientNetB7-LSTM was able to outperform existing HAR methods in terms of average accuracy (80%), and average F1 score (80%). The outcome is a robust HAR system which combines EfficientDetD7, EfficientNetB7, and LSTM for human detection and activity classification.  © 2022 IEEE.","Aerial captured video; convolutional neural network; human activity recognition; human detection; long short-term memory; transfer learning","Antennas; Brain; Convolution; Data mining; Decision making; Gaussian noise (electronic); Learning systems; Long short-term memory; Object detection; Object recognition; Self organizing maps; Speech recognition; Video cameras; Video recording; Activity recognition; Aerial captured video; Convolutional neural network; Features extraction; Human activities; Human activity recognition; Human detection; Self-organizing feature map; Transfer learning; Video sequences; Feature extraction","","","","","","","Paul M., Haque S.M.E., Chakraborty S., Human detection in surveillance videos and its applications A review, EURASIP J. Adv. Signal Process., 2013, 1, pp. 1-16, (2013); Awad F., Shamroukh R., Human detection by robotic urban search and rescue using image processing and neural networks, Int. J. Intell. Sci., 4, 2, pp. 39-53, (2014); Nourbakhsh I., Sycara K., Koes M., Yong M., Lewis M., Burion S., Human robot teaming for search and rescue, IEEE Pervasive Comput., 4, 1, pp. 72-78, (2005); Doherty P., Rudol P., A UAV search and rescue scenario with human body detection and geolocalization, AI 2007: Advances in Artificial Intelligence., pp. 1-13, (2007); Uddin Z., Islam M., Search and rescue system for alive human detection by semi-autonomous mobile rescue robot, Proc. Int. Conf. Innov. Sci., Eng. Technol. (ICISET), pp. 1-5, (2016); Lygouras E., Santavas N., Taitzoglou A., Tarchanidis K., Mitropoulos A., Gasteratos A., Unsupervised human detection with an embedded vision system on a fully autonomous UAV for search and rescue operations, Sensors, 19, 16, (2019); Mishra B., Garg D., Narang P., Mishra V., Drone-surveillance for search and rescue in natural disaster, Comput. Commun., 156, pp. 1-10, (2020); AlDahoul N., Sabri A.Q.M., Mansoor A.M., Real-time human detection for aerial captured video sequences via deep models, Comput. Intell. Neurosci., 2018, pp. 1-14, (2018); Engberts B., Gillissen E., Policing from above: Drone use by the police, the Future of Drone Use: Opportunities and Threats from Ethical and Legal Perspectives, pp. 93-113, (2016); Stone R., Schnieders T.M., Push K.A., Terry S., Truong M., Seshie I., Socha K., Human robot interaction with drones and drone swarms in law enforcement clearing operations, Proc. Hum. Factors Ergonom. Soc. Annu. Meeting, 63, 1, pp. 1213-1217; Prabhakaran A., Sharma R., Autonomous intelligent UAV system for criminal pursuit A proof of concept, Indian Police J., 68, 1, pp. 1-20, (2021); Srivastava D., Shaikh S., Shah P., Automatic traffic surveillance system utilizing object detection and image processing, Proc. Int. Conf. Comput. Commun. Informat. (ICCCI), pp. 1-5, (2021); Gawande U., Hajari K., Golhar Y., Pedestrian detection and tracking in video surveillance system: Issues, comprehensive review, and challenges, Recent Trends in Computational Intelligence., (2020); Kumar K., Mishra R.K., A heuristic SVM based pedestrian detection approach employing shape and texture descriptors, Multimedia Tools Appl., 79, 29-30, pp. 21389-21408, (2020); Singh Gupta A.P.M., Video based vehicle and pedestrian detection, Ann. Romanian Soc. Cell Biol., 25, 6, pp. 14653-14658, (2021); Zadobrischi E., Negru M., Pedestrian detection based on TensorFlow YOLOv3 embedded in a portable system adaptable to vehicles, Proc. Int. Conf. Develop. Appl. Syst. (DAS), pp. 21-26, (2020); Pourhomayoun M., Automatic Traffic Monitoring and Management for Pedestrian and Cyclist Safety Using Deep Learning and Artificial Intelligence, (2020); Sun P., Boukerche A., Challenges and potential solutions for designing a practical pedestrian detection framework for supporting autonomous driving, Proc. 18th ACM Symp. Mobility Manage. Wireless Access, pp. 75-82, (2020); Balani K., Deshpande S., Nair R., Rane V., Human detection for autonomous vehicles, Proc. IEEE Int. Transp. Elec-trific. Conf. (ITEC), pp. 1-5, (2015); Nizam Y., Mohd M.N.H., Jamil M.M.A., Human fall detection from depth images using position and velocity of subject, Proc. Comput. Sci., 105, pp. 131-137, (2017); Ali S.F., Fatima A., Nazar N., Muaz M., Idrees F., Human fall detection, Proc. INMIC, pp. 101-105, (2013); Zhang J., Wu C., Wang Y., Human fall detection based on body posture spatio-temporal evolution, Sensors, 20, 3, (2020); Wang X., Ellul J., Azzopardi G., Elderly fall detection systems: A literature survey, Frontiers Robot. AI; Ezatzadeh S., Keyvanpour M.R., Shojaedini S.V., A human fall detection framework based on multi-camera fusion, J. Exp. Theor. Artif. Intell., pp. 1-20, (2021); Asif U., Mashford B., Von Cavallar S., Yohanandan S., Roy S., Tang J., Harrer S., Privacy preserving human fall detection using video data, Proc. Mach. Learn. Health NeurIPS Workshop, 116, pp. 39-51, (2020); Li T., Liu J., Zhang W., Ni Y., Wang W., Li Z., UAV-human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles, (2021); Laroze M., Courtrai L., Lefevre S., Human detection from aerial imagery for automatic counting of shellfish gatherers, Proc. 11th Joint Conf. Comput. Vis., Imag. Comput. Graph. Theory Appl. (VISIGRAPP), pp. 664-671, (2016); Safadinho D., Ramos J., Ribeiro R., Filipe V., Barroso J., Pereira A., UAV landing using computer vision techniques for human detection, Sensors, 20, 3, (2020); Perera A.G., Al-Naji A., Law Y.W., Chahl J., Human detection and motion analysis from a quadrotor UAV, Proc. IOP Conf., Mater. Sci. Eng., 405, (2018); Mueller M., Smith N., Ghanem B., A benchmark and simulator for UAV tracking, Computer Vision ECCV 2016., pp. 445-461, (2016); Nex F., Remondino F., UAVfor 3D mapping applications:Areview, Appl. Geomatics, 6, 1, pp. 1-15, (2014); Torresan C., Berton A., Carotenuto F., Di Gennaro S.F., Gioli B., Matese A., Miglietta F., Vagnoli C., Zaldei A., Wallace L., Forestry applications of UAVs in Europe: A review, Int. J. Remote Sens., 38, 8-10, pp. 2427-2447, (2017); Bertinetto L., Valmadre J., Golodetz S., Miksik O., Torr P.H.S., Staple: Complementary learners for real-time tracking, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1401-1409, (2016); Zhang S., Zhuo L., Zhang H., Li J., Object tracking in unmanned aerial vehicle videos via multifeature discrimination and instance-aware attention network, Remote Sens., 12, 16, (2020); Jocher G., Ultralytics/YOLOv5: V5.0_YOLOv5-P6 1280 Models, AWS, Supervise.ly and YouTube Integrations, (2021); Girshick R., Donahue J., Darrell T., Malik J., Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, (2014); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, (2018); Wu B., Wan A., Iandola F., Jin P.H., Keutzer K., SqueezeDet: Uni-fied, small, low power fully convolutional neural networks for real-time object detection for autonomous driving, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 446-454, (2017); Tan M., Pang R., Le Q.V., EfficientDet: Scalable and Efficient Object Detection, (2020); Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.-C., MobileNetV2: Inverted residuals and linear bottlenecks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4510-4520, (2018); Wang C., Zhao S., Zhang R., Occlusion-aware discriminative networks for visual object tracking, Proc. 2nd Int. Conf. Robot., Intell. Control Artif. Intell., pp. 320-326, (2020); Ouyang W., Wang X., Joint deep learning for pedestrian detection, Proc. IEEE Int. Conf. Comput. Vis., pp. 2056-2063, (2013); Shao Z., Cheng G., Ma J., Wang Z., Wang J., Li D., Real-time and accurate UAV pedestrian detection for social distancing monitoring in COVID-19 pandemic, IEEE Trans. Multimedia, 24, pp. 2069-2083, (2022); Li W., Li H., Wu Q., Meng F., Xu L., Ngan K.N., HeadNet: An end-to-end adaptive relational network for head detection, IEEE Trans. Circuits Syst. Video Technol., 30, 2, pp. 482-494, (2020); Vora A., Chilaka V., FCHD: Fast and Accurate Head Detection in Crowded Scenes, (2019); Wang Y., Yin Y., Wu W., Sun S., Wang X., Robust person head detection based on multi-scale representation fusion of deep convolution neural network, Proc. IEEE Int. Conf. Robot. Biomimetics (ROBIO), pp. 296-301, (2017); Wang S., Zhang J., Miao Z., A new edge feature for headshoulder detection, Proc. IEEE Int. Conf. Image Process., pp. 2822-2826, (2013); Zeng C., Ma H., Robust head-shoulder detection by PCAbased multilevel HOG-LBP detector for people counting, Proc. 20th Int. Conf. Pattern Recognit., pp. 2069-2072, (2010); Ahmad M., Ahmed I., Khan F.A., Qayum F., Aljuaid H., Convolutional neural network based person tracking using overhead views, Int. J. Distrib. Sensor Netw., 16, 6, (2020); Haq E.U., Jianjun H., Li K., Haq H.U., Human detection and tracking with deep convolutional neural networks under the constrained of noise and occluded scenes, Multimedia Tools Appl., 79, 41-42, pp. 30685-30708, (2020); Liu T., Stathaki T., Faster R-CNN for robust pedestrian detection using semantic segmentation network, Frontiers Neurorobot., 12, (2018); Nikouei S.Y., Chen Y., Song S., Xu R., Choi B.-Y., Faughnan T.R., Real-time Human Detection As An Edge Service Enabled by a Lightweight CNN, (2018); Vasic M.K., Papic V., Multimodel deep learning for person detection in aerial images, Electronics, 9, 9, (2020); Fereidoonian F., Firouzi F., Farahani B., Human activity recognition: From sensors to applications, Proc. Int. Conf. Omni-Layer Intell. Syst. (COINS), pp. 1-8, (2020); Jia C., Kong Y., Ding Z., Fu Y., RGB-D action recognition, Human Activity Recognition and Prediction 1st Ed., pp. 87-106, (2016); Yang W., Liu X., Zhang L., Yang L.T., Big data real-time processing based on storm, Proc. 12th IEEE Int. Conf. Trust Secur. Pri-vacy Comput. Commun., pp. 1784-1787, (2013); Ashraf I., Zikria Y.B., Hur S., Bashir A.K., Alhussain T., Park Y., Localizing pedestrians in indoor environments using magnetic field data with term frequency paradigm and deep neural networks, Int. J. Mach. Learn. Cybern., 12, 11, pp. 3203-3219, (2021); Gharaee Z., Gardenfors P., Johnsson M., First and second order dynamics in a hierarchical SOM system for action recognition, Appl. Soft Comput., 59, pp. 574-585, (2017); Vemulapalli R., Arrate F., Chellappa R., Human action recognition by representing 3D skeletons as points in a lie group, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 588-595, (2014); Ke Q., Bennamoun M., An S., Sohel F., Boussaid F., A new representation of skeleton sequences for 3D action recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 4570-4579, (2017); Xia L., Chen C.-C., Aggarwal J.K., View invariant human action recognition using histograms of 3D joints, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 20-27, (2012); Zhu Y., Chen W., Guo G., Fusing spatiotemporal features and joints for 3D action recognition, Proc. IEEE Conf. Com-put. Vis. Pattern Recognit. Workshops, pp. 486-491, (2013); Zanr M., Leordeanu M., Sminchisescu C., The moving pose: An efficient 3D kinematics descriptor for low-latency action recognition and detection, Proc. IEEE Int. Conf. Comput. Vis., pp. 2752-2759, (2013); Yang Y., Deng C., Gao S., Liu W., Tao D., Gao X., Discriminative multi-instance multitask learning for 3D action recognition, IEEE Trans. Multimedia, 19, 3, pp. 519-529, (2017); Du Y., Wang W., Wang L., Hierarchical recurrent neural network for skeleton based action recognition, Proc. IEEE Conf. Com-put. Vis. Pattern Recognit. (CVPR), pp. 1110-1118, (2015); Wang H., Wang L., Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-stream Recurrent Neural Networks, (2017); Zhu W., Lan C., Xing J., Zeng W., Li Y., Shen L., Xie X., Co-occurrence Feature Learning for Skeleton Based Action Recognition Using Regularized Deep LSTM Networks, (2016); Liu J., Shahroudy A., Xu D., Wang G., Spatio-temporal LSTM with Trust Gates for 3D Human Action Recognition, (2016); Song S., Lan C., Xing J., Zeng W., Liu J., An end-to-end spatiotemporal attention model for human action recognition from skeleton data, Proc. AAAI Conf. Artif. Intell., 31, 1, (2017); Liu J., Akhtar N., Mian A., Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition, (2018); Ren J., Reyes N.H., Barczak A.L.C., Scogings C., Liu M., An investigation of skeleton-based optical flow-guided features for 3D action recognition using a multi-stream CNN model, Proc. IEEE 3rd Int. Conf. Image, Vis. Comput. (ICIVC), pp. 199-203, (2018); Li B., He M., Cheng X., Chen Y., Dai Y., Skeleton Based Action Recognition Using Translation-scale Invariant Image Mapping and Multi-scale Deep CNN, (2017); Li C., Hou Y., Wang P., Li W., Joint distance maps based action recognition with convolutional neural networks, IEEE Sig-nal Process. Lett., 24, 5, pp. 624-628, (2017); Tasnim N., Islam M.K., Baek J.-H., Deep learning based human activity recognition using spatio-temporal image formation of skeleton joints, Appl. Sci., 11, 6, (2021); Mliki H., Bouhlel F., Hammami M., Human activity recognition from UAV-captured video sequences, Pattern Recognit., 100, (2020); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, (2018); Zhang X., Zhou X., Lin M., Sun J., ShufeNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices, (2017); Wang R.J., Li X., Ling C.X., Pelee: A Real-time Object Detection System on Mobile Devices, (2019); AlDahoul N., Akmeliawati R., Zaw Z., Feature fusion: H-ELM based learned features and hand-crafted features for human activity recognition, Int. J. Adv. Comput. Sci. Appl., 10, 7, pp. 509-514, (2019); Yadav S.K., Tiwari K., Pandey H.M., Akbar S.A., A review of multimodal human activity recognition with special emphasis on classification, applications, challenges and future directions, Knowl.-Based Syst., 223, (2021); Van Eekeren A.W.M., Dijk J., Burghouts G., Detection and tracking of humans from an airborne platform, Electro-Optical and Infrared Systems: Technology and Applications XI, 9249, pp. 249-255, (2014); Burghouts G.J., Van Eekeren A.W.M., Dijk J., Focus-of-attention for human activity recognition from UAVs, Proc. SPIE, 9249, pp. 256-267, (2014); Peng H., Razi A., Fully autonomous UAV-based action recognition system using aerial imagery, Advances in Visual Computing., pp. 276-290, (2020); Redmon J., Divvala S., Girshick R., Farhadi A., You only Look Once: Unified, Real-time Object Detection, (2016); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, (2016); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020); Wang C.-Y., Liao H.-Y.M., Yeh I.-H., Wu Y.-H., Chen P.-Y., Hsieh J.-W., CSPNet: A New Backbone That Can Enhance Learning Capability of CNN, (2019); Woo S., Park J., Lee J.-Y., Kweon I.S., CBAM: Convolutional Block Attention Module, (2018); He K., Zhang X., Ren S., Sun J., Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, (2014); Liu S., Qi L., Qin H., Shi J., Jia J., Path Aggregation Network for Instance Segmentation, (2018); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Zheng Z., Wang P., Liu W., Li J., Ye R., Ren D., Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression; Yao Z., Cao Y., Zheng S., Huang G., Lin S., Cross-iteration Batch Normalization; Ghiasi G., Lin T.-Y., Le Q.V., DropBlock: A Regularization Method for Convolutional Networks, (2018); Loshchilov I., Hutter F., SGDR: Stochastic Gradient Descent with Warm Restarts, (2017); Misra D., Mish: A Self Regularized Non-monotonic Activation Function, (2020); Yun S., Han D., Joon Oh S., Chun S., Choe J., Yoo Y., CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, (2019); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the Inception Architecture for Computer Vision, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-time Object Detection with Region Proposal Networks, (2016); Chorowski J., Bahdanau D., Serdyuk D., Cho K., Bengio Y., Attention-based Models for Speech Recognition, (2015); Tan M., Le Q.V., EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, (2020); Hochreiter S., Schmidhuber J., Long short-term memory, Neural Comput., 9, 8, pp. 1735-1780, (1997); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, (2015); Chevalier G., LARNN: Linear Attention Recurrent Neural Network, (2018); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 248-255, (2009); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common objects in context, Computer Vision ECCV 2014., pp. 740-755, (2014); New System Performs Persistent Wide-Area Aerial Surveil-lance; CRCV | Center for Research in Computer Vision at the University of Central Florida.; AlDahoul N., Karim H.A., Abdullah M.H.L., Fauzi M.F.A., Wazir A.S.B., Mansor S., See J., Transfer detection of YOLO to focus CNN's attention on nude regions for adult content detection, Symmetry, 13, 1, (2020); Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N., An Image Is Worth 16-16 Words: Transformers for Image Recognition at Scale, (2021); Dollar P., Belongie S., Perona P., The fastest pedestrian detector in the west, Proc. Brit. Mach. Vis. Conf., pp. 681-6811, (2010); Patel C., Bhatt D., Sharma U., Patel R., Pandya S., Modi K., Cholli N., Patel A., Bhatt U., Khan M.A., Majumdar S., Zuhair M., Patel K., Shah S.A., Ghayvat H., DBGC: Dimension-based generic convolution block for object recognition, Sensors, 22, 5, (2022)","N. Aldahoul; Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, 50603, Malaysia; email: nouar.aldahoul@live.iium.edu.my","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Short survey","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132780659"
"Xi J.; Ersoy O.K.; Cong M.; Zhao C.; Qu W.; Wu T.","Xi, Jiangbo (57027046000); Ersoy, Okan K. (7005742903); Cong, Ming (55995722800); Zhao, Chaoying (55476833100); Qu, Wei (29567486900); Wu, Tianjun (57193855591)","57027046000; 7005742903; 55995722800; 55476833100; 29567486900; 57193855591","Wide and Deep Fourier Neural Network for Hyperspectral Remote Sensing Image Classification","2022","Remote Sensing","14","12","2931","","","","5","10.3390/rs14122931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132800194&doi=10.3390%2frs14122931&partnerID=40&md5=3d4139588ae5d471f452c81c23b3ff47","College of Geological Engineering and Geomatics, Chang’an University, Xi’an, 710054, China; Key Laboratory of Western China’s Mineral Resources and Geological Engineering, Ministry of Education, Xi’an, 710054, China; Big Data Center for Geosciences and Satellites (BDCGS), Chang’an University, Xi’an, 710054, China; Key Laboratory of Ecological Geology and Disaster Prevention, Ministry of Natural Resources, Xi’an, 710054, China; School of Electrical and Computer Engineering, Purdue University, West Lafayette, 47907, IN, United States; Department of Mathematics and Information Science, College of Science, Chang’an University, Xi’an, 710064, China","Xi J., College of Geological Engineering and Geomatics, Chang’an University, Xi’an, 710054, China, Key Laboratory of Western China’s Mineral Resources and Geological Engineering, Ministry of Education, Xi’an, 710054, China, Big Data Center for Geosciences and Satellites (BDCGS), Chang’an University, Xi’an, 710054, China, Key Laboratory of Ecological Geology and Disaster Prevention, Ministry of Natural Resources, Xi’an, 710054, China; Ersoy O.K., School of Electrical and Computer Engineering, Purdue University, West Lafayette, 47907, IN, United States; Cong M., College of Geological Engineering and Geomatics, Chang’an University, Xi’an, 710054, China, Key Laboratory of Western China’s Mineral Resources and Geological Engineering, Ministry of Education, Xi’an, 710054, China; Zhao C., College of Geological Engineering and Geomatics, Chang’an University, Xi’an, 710054, China, Key Laboratory of Western China’s Mineral Resources and Geological Engineering, Ministry of Education, Xi’an, 710054, China; Qu W., College of Geological Engineering and Geomatics, Chang’an University, Xi’an, 710054, China, Key Laboratory of Western China’s Mineral Resources and Geological Engineering, Ministry of Education, Xi’an, 710054, China; Wu T., Department of Mathematics and Information Science, College of Science, Chang’an University, Xi’an, 710064, China","Hyperspectral remote sensing image (HSI) classification is very useful in different applications, and recently, deep learning has been applied for HSI classification successfully. However, the number of training samples is usually limited, causing difficulty in use of very deep learning models. We propose a wide and deep Fourier network to learn features efficiently by using pruned features extracted in the frequency domain. It is composed of multiple wide Fourier layers to extract hierarchical features layer-by-layer efficiently. Each wide Fourier layer includes a large number of Fourier transforms to extract features in the frequency domain from a local spatial area using sliding windows with given strides.These extracted features are pruned to retain important features and reduce computations. The weights in the final fully connected layers are computed using least squares. The transform amplitudes are used for nonlinear processing with pruned features. The proposed method was evaluated with HSI datasets including Pavia University, KSC, and Salinas datasets. The overall accuracies (OAs) of the proposed method can reach 99.77%, 99.97%, and 99.95%, respectively. The average accuracies (AAs) can achieve 99.55%, 99.95%, and 99.95%, respectively. The Kappa coefficients are as high as 99.69%, 99.96%, and 99.94%, respectively. The experimental results show that the proposed method achieved excellent performance among other compared methods. The proposed method can be used for applications including classification, and image segmentation tasks, and has the ability to be implemented with lightweight embedded computing platforms. The future work is to improve the method to make it available for use in applications including object detection, time serial data prediction, and fast implementation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","convolutional neural network; deep fourier neural network; deep learning; frequency domain learning; hyperspectral image classification","Convolution; Convolutional neural networks; Deep neural networks; Fourier transforms; Frequency domain analysis; Image classification; Image segmentation; Remote sensing; Spectroscopy; Convolutional neural network; Deep fourier neural network; Deep learning; Domain learning; Fourier neural networks; Frequency domain learning; Frequency domains; Hyperspectral image classification; Hyperspectral Remote Sensing Image; Remote sensing image classification; Object detection","","","","","Shaanxi Forestry Science and Technology Innovation Program, (SXLK2021-0225); National Natural Science Foundation of China, NSFC, (41874005, 41929001, 41941019, 42090055, 42171348, 42174006); Government of Inner Mongolia Autonomous Region, NMG, (2021SZD0036); Chang'an University, CHD, (300102120201, 300102260301, 300102262202, 300102262902); Fundamental Research Funds for the Central Universities; Shanxi Provincial Key Research and Development Project, (2021NY-170); Science Fund for Distinguished Young Scholars of Sichuan Province, (2022JC-18)","Funding: This work was supported in part by the National Natural Science Foundation of China under Grants 42171348, 41874005, 41929001, 41941019, 42174006, and 42090055; in part by the Fundamental Research Funds for the Central Universities, CHD under Grants 300102262202, 300102260301, 300102120201, and 300102262902; in part by Science Fund for Distinguished Young Scholars of Shaanxi Province (2022JC-18); in part by Key Research and Development Program of Shaanxi (Grant No. 2021NY-170); in part by Major Science and Technology Project of Inner Mongolia Autonomous Region under Grant 2021SZD0036; in part by Shaanxi Forestry Science and Technology Innovation Program, NO. SXLK2021-0225.","Safavian S.R., Landgrebe D., A survey of decision tree classifier methodology, IEEE Trans. Syst. Man Cybern, 21, pp. 660-674, (1991); Tarabalka Y., Fauvel M., Chanussot J., Benediktsson J.A., SVM-and MRF-based method for accurate classification of hyperspectral images, IEEE Geosci. Remote Sens. Lett, 7, pp. 736-740, (2010); Xi J., Ersoy O.K., Fang J., Wu T., Wei X., Zhao C., Parallel Multistage Wide Neural Network, IEEE Trans. Neural Netw. Learn. Syst, pp. 1-14, (2021); Li J., Bioucas-Dias J.M., Plaza A., Semisupervised hyperspectral image classification using soft sparse multinomial logistic regression, IEEE Geosci. Remote Sens. Lett, 10, pp. 318-322, (2012); Lee H., Kwon H., Going deeper with contextual CNN for hyperspectral image classification, IEEE Trans. Image Process, 26, pp. 4843-4855, (2017); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag, 4, pp. 22-40, (2016); Zhang M., Li W., Du Q., Diverse region-based CNN for hyperspectral image classification, IEEE Trans. Image Process, 27, pp. 2623-2634, (2018); Wang W., Dou S., Jiang Z., Sun L., A fast dense spectral–spatial convolution network framework for hyperspectral images classification, Remote Sens, 10, (2018); Gong Z., Zhong P., Yu Y., Hu W., Li S., A CNN with Multiscale Convolution and Diversified Metric for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 57, pp. 3599-3618, (2019); Gao Q., Lim S., Jia X., Hyperspectral image classification using convolutional neural networks and multiple feature learning, Remote Sens, 10, (2018); Cheng G., Li Z., Han J., Yao X., Guo L., Exploring hierarchical convolutional features for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens, 56, pp. 6712-6722, (2018); Paoletti M., Haut J., Plaza J., Plaza A., A new deep convolutional neural network for fast hyperspectral image classification, ISPRS J. Photogramm. Remote Sens, 145, pp. 120-147, (2018); Roy S.K., Krishna G., Dubey S.R., Chaudhuri B.B., HybridSN: Exploring 3-D–2-D CNN feature hierarchy for hyperspectral image classification, IEEE Geosci. Remote Sens. Lett, 17, pp. 277-281, (2019); Zheng J., Feng Y., Bai C., Zhang J., Hyperspectral Image Classification Using Mixed Convolutions and Covariance Pooling, IEEE Trans. Geosci. Remote Sens, 59, pp. 552-534, (2020); Shi C., Liao D., Zhang T., Wang L., Hyperspectral Image Classification Based on Expansion Convolution Network, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-16, (2022); Haut J.M., Paoletti M.E., Plaza J., Li J., Plaza A., Active Learning With Convolutional Neural Networks for Hyperspectral Image Classification Using a New Bayesian Approach, IEEE Trans. Geosci. Remote Sens, 56, pp. 6440-6461, (2018); Tang X., Meng F., Zhang X., Cheung Y., Ma J., Liu F., Jiao L., Hyperspectral Image Classification Based on 3-D Octave Convolution With Spatial-Spectral Attention Network, IEEE Trans. Geosci. Remote Sens, 59, pp. 2430-2447, (2020); Cao X., Yao J., Xu Z., Meng D., Hyperspectral image classification with convolutional neural network and active learning, IEEE Trans. Geosci. Remote Sens, 58, pp. 4604-4616, (2020); Xie F., Gao Q., Jin C., Zhao F., Hyperspectral image classification based on superpixel pooling convolutional neural network with transfer learning, Remote Sens, 13, (2021); Masarczyk W., Glomb P., Grabowski B., Ostaszewski M., Effective Training of Deep Convolutional Neural Networks for Hyperspectral Image Classification through Artificial Labeling, Remote Sens, 12, (2020); Mou L., Ghamisi P., Zhu X.X., Deep recurrent neural networks for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens, 55, pp. 3639-3655, (2017); Hang R., Liu Q., Hong D., Ghamisi P., Cascaded recurrent neural networks for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens, 57, pp. 5384-5394, (2019); Liu Q., Zhou F., Hang R., Yuan X., Bidirectional-convolutional LSTM based spectral-spatial feature learning for hyperspectral image classification, Remote Sens, 9, (2017); Mei S., Li X., Liu X., Cai H., Du Q., Hyperspectral Image Classification Using Attention-Based Bidirectional Long Short-Term Memory Network, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-12, (2021); Song T., Wang Y., Gao C., Chen H., Li J., MSLAN: A Two-Branch Multidirectional Spectral–Spatial LSTM Attention Network for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-14, (2022); He X., Chen Y., Lin Z., Spatial-Spectral Transformer for Hyperspectral Image Classification, Remote Sens, 13, (2021); Qing Y., Liu W., Feng L., Gao W., Improved Transformer Net for Hyperspectral Image Classification, Remote Sens, 13, (2021); Zhou F., Hang R., Li J., Zhang X., Xu C., Spectral-Spatial Correlation Exploration for Hyperspectral Image Classification via Self-Mutual Attention Network, IEEE Geosci. Remote Sens. Lett, 19, pp. 1-5, (2022); Lin J., Ma L., Yao Y., A Fourier domain acceleration framework for convolutional neural networks, Neurocomputing, 364, pp. 254-268, (2019); Ayat S.O., Khalil-Hani M., Ab Rahman A.A.H., Abdellatef H., Spectral-based convolutional neural network without multiple spatial-frequency domain switchings, Neurocomputing, 364, pp. 152-167, (2019); Khan S.H., Hayat M., Porikli F., Regularization of deep neural networks with spectral dropout, Neural Netw, 110, pp. 82-90, (2019); Uteuliyeva M., Zhumekenov A., Takhanov R., Assylbekov Z., Castro A.J., Kabdolov O., Fourier neural networks: A comparative study, Intell. Data Anal, 24, pp. 1107-1120, (2020); Silvescu A., Fourier neural networks, Proceedings of the IJCNN’99 International Joint Conference on Neural Networks, 1, pp. 488-491, (1999); Li Z., Kovachki N.B., Azizzadenesheli K., Bhattacharya K., Stuart A., Anandkumar A., Fourier Neural Operator for Parametric Partial Differential Equations, Proceedings of the International Conference on Learning Representations, (2020); Rao Y., Zhao W., Zhu Z., Lu J., Zhou J., Global filter networks for image classification, Adv. Neural Inf. Process. Syst, 34, pp. 980-993, (2021); Worrall D.E., Garbin S.J., Turmukhambetov D., Brostow G.J., Harmonic networks: Deep translation and rotation equivariance, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5028-5037, (2017); Liu C., Li J., He L., Plaza A., Li S., Li B., Naive Gabor Networks for Hyperspectral Image Classification, IEEE Trans. Neural Netw. Learn. Syst, 32, pp. 376-390, (2020); Okwuashi O., Ndehedehe C.E., Deep support vector machine for hyperspectral image classification, Pattern Recognit, 103, (2020); Roy S.K., Haut J.M., Paoletti M.E., Dubey S.R., Plaza A., Generative Adversarial Minority Oversampling for Spectral-Spatial Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-15, (2021); Liang H., Bao W., Shen X., Zhang X., Spectral-Spatial Attention Feature Extraction for Hyperspectral Image Classification Based on Generative Adversarial Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 10017-10032, (2021); Parisi G.I., Kemker R., Part J.L., Kanan C., Wermter S., Continual lifelong learning with neural networks: A review, Neural Netw, 113, pp. 54-71, (2019); Venkataramani S., Raghunathan A., Liu J., Shoaib M., Scalable-effort classifiers for energy-efficient machine learning, Proceedings of the 52nd Annual Design Automation Conference, (2015); Panda P., Venkataramani S., Sengupta A., Raghunathan A., Roy K., Energy-Efficient Object Detection Using Semantic Decomposition, IEEE Trans. Very Large Scale Integr. (VLSI) Syst, 25, pp. 2673-2677, (2017); Kirkpatrick J., Pascanu R., Rabinowitz N., Veness J., Desjardins G., Rusu A.A., Milan K., Quan J., Ramalho T., GrabskaBarwinska A., Overcoming catastrophic forgetting in neural networks, Proc. Natl. Acad. Sci. USA, 114, pp. 3521-3526, (2017); Lee S.W., Kim J.H., Jun J., Ha J.W., Zhang B.T., Overcoming catastrophic forgetting by incremental moment matching, Adv. Neural Inf. Process. Syst, 30, pp. 4652-4662, (2017); Ersoy O.K., Hong D., Parallel, self-organizing, hierarchical neural networks, IEEE Trans. Neural Netw, 1, pp. 167-178, (1990); Benediktsson J.A., Sveinsson J.R., Ersoy O.K., Swain P.H., Parallel consensual neural networks, IEEE Trans. Neural Netw, 8, pp. 54-64, (1997); Neyshabur B., Li Z., Bhojanapalli S., LeCun Y., Srebro N., The role of over-parametrization in generalization of neural networks, Proceedings of the International Conference on Learning Representations, (2019); Lee J., Xiao L., Schoenholz S.S., Bahri Y., Sohl-Dickstein J., Pennington J., Wide neural networks of any depth evolve as linear models under gradient descent, (2019); Cheng H.T., Koc L., Harmsen J., Shaked T., Chandra T., Aradhye H., Anderson G., Corrado G., Chai W., Ispir M., Wide & deep learning for recommender systems, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, pp. 7-10, (2016); Liu W., Nie X., Zhang B., Sun X., Incremental Learning with Open-Set Recognition for Remote Sensing Image Scene Classification, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-16, (2022); Xi J., Ersoy O.K., Fang J., Cong M., Wei X., Wu T., Scalable Wide Neural Network: A Parallel, Incremental Learning Model Using Splitting Iterative Least Squares, IEEE Access, 9, pp. 50767-50781, (2021); Xi J., Ersoy O.K., Fang J., Cong M., Wu T., Zhao C., Li Z., Wide Sliding Window and Subsampling Network for Hyperspectral Image Classification, Remote Sens, 13, (2021); Xi J., Cong M., Ersoy O.K., Zou W., Zhao C., Li Z., Gu J., Wu T., Dynamic Wide and Deep Neural Network for Hyperspectral Image Classification, Remote Sens, 13, (2021); Azar S.G., Meshgini S., Rezaii T.Y., Beheshti S., Hyperspectral image classification based on sparse modeling of spectral blocks, Neurocomputing, 407, pp. 12-23, (2020); Cheng C., Li H., Peng J., Cui W., Zhang L., Deep High Order Tensor Convolutional Sparse Coding for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-11, (2022); Cheng C., Li H., Peng J., Cui W., Zhang L., Hyperspectral Image Classification Via Spectral-Spatial Random Patches Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 4753-4764, (2021); Li Z., Wang T., Li W., Du Q., Wang C., Liu C., Shi X., Deep Multilayer Fusion Dense Network for Hyperspectral Image Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 1258-1270, (2020)","M. Cong; College of Geological Engineering and Geomatics, Chang’an University, Xi’an, 710054, China; email: mingc@chd.edu.cn","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132800194"
"Gibril M.B.A.; Shafri H.Z.M.; Shanableh A.; Al-Ruzouq R.; Wayayok A.; Hashim S.J.B.; Sachit M.S.","Gibril, Mohamed Barakat A. (57188810669); Shafri, Helmi Zulhaidi Mohd (24072139200); Shanableh, Abdallah (7003825668); Al-Ruzouq, Rami (8520465800); Wayayok, Aimrun (57211373258); Hashim, Shaiful Jahari bin (55444812700); Sachit, Mourtadha Sarhan (57960732100)","57188810669; 24072139200; 7003825668; 8520465800; 57211373258; 55444812700; 57960732100","Deep convolutional neural networks and Swin transformer-based frameworks for individual date palm tree detection and mapping from large-scale UAV images","2022","Geocarto International","37","27","","18569","18599","30","1","10.1080/10106049.2022.2142966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141665201&doi=10.1080%2f10106049.2022.2142966&partnerID=40&md5=2f109d35910b62c9a9db243c5774737f","Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Department of Civil and Environmental Engineering, Faculty of Engineering, University of Sharjah, Sharjah, United Arab Emirates; GIS and Remote Sensing Center, Research Institute of Sciences and Engineering, University of Sharjah, Sharjah, United Arab Emirates; Department of Biological and Agricultural Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Department of Computer and Communication Systems Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia","Gibril M.B.A., Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Shafri H.Z.M., Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Shanableh A., Department of Civil and Environmental Engineering, Faculty of Engineering, University of Sharjah, Sharjah, United Arab Emirates, GIS and Remote Sensing Center, Research Institute of Sciences and Engineering, University of Sharjah, Sharjah, United Arab Emirates; Al-Ruzouq R., Department of Civil and Environmental Engineering, Faculty of Engineering, University of Sharjah, Sharjah, United Arab Emirates, GIS and Remote Sensing Center, Research Institute of Sciences and Engineering, University of Sharjah, Sharjah, United Arab Emirates; Wayayok A., Department of Biological and Agricultural Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Hashim S.J.B., Department of Computer and Communication Systems Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Sachit M.S., Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia","Timely and reliable mapping of individual date palm trees is essential for their monitoring, health and risk assessment, pest control, and sustainable management of the date palm industry. This study presents an instance segmentation framework for large-scale detection and mapping of date palm trees using unmanned aerial vehicle (UAV)-based images. First, a data conversion framework is created to convert UAV image tiles and ground-truth vector data into annotation format of Common Objects in Context. Second, this study examines the efficacy of various instance segmentation models, namely, mask region convolutional neural network (Mask R-CNN), Mask Scoring R-CNN, You Only Look At CoefficientTs, Point-based Rendering, Segmenting Objects by Locations (SOLO), and SOLOv2) with varying residual learning networks (ResNets) in detecting and delineating individual date palm trees. Furthermore, the performance of two variants of Swin Transformer networks with a feature pyramid network (FPN) (Swin-small-FPN and Swin-tiny-FPN) as Mask R-CNN network backbones was also evaluated. Third, we assess the generalizability of the evaluated instance segmentation models and backbones on different testing datasets with varying spatial resolutions. Results show that Mask R-CNN models based on Swin Transformers backbones outperform those with ResNets in the detection and segmentation of date palm trees with mAP50 of 92% and 91% and F-measures of 94% and 93%. Moreover, the Mask scoring R-CNN-based ResNet-50 and Mask R-CNN with a Swin-small-FPN backbone outperform the evaluated models and demonstrate great generalizability in different datasets with diverse spatial resolutions. The proposed instance segmentation framework provides an efficient tool for date palm tree mapping from multi-scale UAV-based images and is valuable and suitable for individual tree crown delineations and other earth-related applications. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","individual tree crown delineation; Instance segmentation; mask R-CNN; mask scoring R-CNN; PointRend; SOLOv2; Swin transformer; YOLACT","aerial survey; agroindustry; alternative agriculture; angiosperm; artificial neural network; canopy architecture; crop plant; data set; detection method; developing world; fruit production; image analysis; performance assessment; reliability analysis; segmentation; spatial resolution; tree; unmanned vehicle; vegetation cover; vegetation mapping","","","","","University of Sharjah, UOS","The authors would like to acknowledge the Municipality of Ajman for supplying remotely sensed data of the study area and the University of Sharjah for providing the facilities used in this research.","Al-Khayri J.M., Naik P.M., Jain S.M., Johnson D.V., Advances in Date Palm (Phoenix dactylifera L.) breeding, Advances in plant breeding strategies: Fruits, 3, pp. 727-771, (2018); Al-Ruzouq R., Shanableh A., Barakat A. Gibril M., Al-Mansoori S., Image segmentation parameter selection and ant colony optimization for date palm tree detection and mapping from very-high-spatial-resolution aerial imagery, Remote Sens, 10, 9, (2018); Al-Saad M., Aburaed N., Mansoori S.A., Ahmad H.A., Autonomous palm tree detection from remote sensing images–UAE dataset, IGARSS 2022–2022 IEEE Int Geosci Remote Sens Symp, pp. 2191-2194, (2022); Ammar A., Koubaa A., Benjdira B., Deep-learning-based automated palm tree counting and geolocation in large farms from aerial geotagged images, Agronomy, 11, 8, (2021); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sens, 11, 4, (2019); Anagnostis A., Tagarakis A.C., Kateris D., Moysiadis V., Sorensen C.G., Pearson S., Bochtis D., Orchard mapping with deep learning semantic segmentation, Sensors, 21, 11, (2021); Bolya D., Zhou C., Xiao F., Lee Y.J., YOLACT: real-time instance segmentation, 2019 IEEE/CVF Int Conf Comput Vis, pp. 9156-9165, (2019); Braga J.R.G., Peripato V., Dalagnol R., Ferreira M.P., Tarabalka Y., Aragao L.E.O.C., de Campos Velho H.F., Shiguemori E.H., Wagner F.H., Tree crown delineation algorithm based on a convolutional neural network, Remote Sens, 12, 8, pp. 1-27, (2020); Briechle S., Krzystek P., Vosselman G., Silvi-Net–A dual-CNN approach for combined classification of tree species and standing dead trees from remote sensing data, Int J Appl Earth Obs Geoinf, 98, (2021); Cai L., Long T., Dai Y., Huang Y., Mask R-CNN-based detection and segmentation for pulmonary nodule 3D visualization diagnosis, IEEE Access, 8, pp. 44400-44409, (2020); Chao C.T., Krueger R.R., The Date Palm (Phoenix dactylifera L.): overview of biology, uses, and cultivation, horts, 42, 5, pp. 1077-1082, (2007); Chen K., Jiaqi W., Pang J., Cao Y., Xiong Y., Li X., Sun S., Feng W., Liu Z., Xu J., Et al., MMDetection: Open MMLab detection toolbox and benchmark, (2019); Cheng G., Han J., Lu X., Remote sensing image scene classification: benchmark and State of the Art, Proc IEEE, 105, 10, pp. 1865-1883, (2017); Cheng Z., Qi L., Cheng Y., Cherry tree crown extraction from natural Orchard images with complex backgrounds, Agriculture, 11, 5, (2021); Chen X., Jiang K., Zhu Y., Wang X., Yun T., Individual tree crown segmentation directly from uav-borne lidar data using the pointnet of deep learning, Forests, 12, 2, pp. 131-122, (2021); Choi K., Lim W., Chang B., Jeong J., Kim I., Park C.R., Ko D.W., An automatic approach for tree species detection and profile estimation of urban street trees using deep learning and Google street view images, ISPRS J Photogramm Remote Sens, 190, pp. 165-180, (2022); Chowdhury P.N., Shivakumara P., Nandanwar L., Samiron F., Pal U., Lu T., Oil palm tree counting in drone images, Pattern Recognit Lett, 153, pp. 1-9, (2022); Cubes M., A high resolution 3d surface construction algorithm/william e, Proc 14th Annu Conf Comput Graph Interact Tech, pp. 163-169, (1987); Culman M., Delalieux S., Van Tricht K., Individual palm tree detection using deep learning on RGB imagery to support tree inventory, Remote Sens, 12, 21, pp. 1-31, (2020); Dahy B., Issa S., Saleous N., Detecting and mapping mature, medium, and young age date palms in the arid lands of Abu Dhabi, using hierarchical integrated approach (HIA), Remote Sens Appl Soc Environ, 23, June, (2021); Dong Y., Zhang Y., Hou Y., Tong X., Wu Q., Zhou Z., Cao Y., Damage recognition of road auxiliary facilities based on deep convolution network for segmentation and image region correction. Ye Z, editor, Adv Civ Eng, 2022, pp. 1-10, (2022); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The pascal visual object classes (VOC) challenge, Int J Comput Vis, 88, 2, pp. 303-338, (2010); Food and Agriculture Organization. FAOSTAT, (2021); Ferreira M.P., Almeida D.D., Papa D., Minervino J.B.S., Veras H.F.P., Formighieri A., Santos C.A.N., Ferreira M.A.D., Figueiredo E.O., Ferreira E.J.L., Individual tree detection and species classification of Amazonian palms using UAV images and deep learning, For Ecol Manage, 475, April, (2020); Gibril M.B.A., Shafri H.Z.M., Shanableh A., Al-Ruzouq R., Wayayok A., Hashim S.J., Deep convolutional neural network for large-scale date palm tree mapping from uav-based images, Remote Sens, 13, 14, pp. 1-24, (2021); Gonzalez F., Mcfadyen A., Puig E., Advances in unmanned aerial systems and payload technologies for precision agriculture, In: Chen G, editor. Adv Agric Mach Technol, pp. 133-155, (2018); Gu W., Bai S., Kong L., Review article A review on 2D instance segmentation based on deep neural networks, Image Vis Comput, 120, (2022); Han P., Ma C., Chen J., Chen L., Bu S., Xu S., Zhao Y., Zhang C., Hagino T., Fast tree detection and counting on UAVs for sequential aerial images with generating orthophoto mosaicing, Remote Sens, 14, 16, (2022); Hao Z., Lin L., Post C.J., Mikhailova E.A., Li M., Chen Y., Yu K., Liu J., Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN), ISPRS J Photogramm Remote Sens, 178, June, pp. 112-123, (2021); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning, Sensors (Switzerland), 19, 6, pp. 1284-1223, (2019); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, 2017 IEEE Int Conf Comput Vis. Vol. 2017-Octob, pp. 2980-2988, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Vol. 2016-Decem, pp. 770-778, (2016); Hoang T.M., Nam G.P., Cho J., Kim I.J., DEFace: deep efficient face network for small scale variations, IEEE Access, 8, pp. 142423-142433, (2020); Huang Z., Huang L., Gong Y., Huang C., Wang X., Mask scoring R-CNN, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Vol. 2019-June, pp. 6402-6411, (2019); Huang H., Li X., Chen C., Individual tree crown detection and delineation from very-high-resolution UAV images based on bias field and marker-controlled watershed segmentation algorithms, IEEE J Sel Top Appl Earth Observations Remote Sens, 11, 7, pp. 2253-2262, (2018); Hu G., Wang T., Wan M., Bao W., Zeng W., UAV remote sensing monitoring of pine forest diseases based on improved Mask R-CNN, Int J Remote Sens, 43, 4, pp. 1274-1305, (2022); Hu G., Yin C., Wan M., Zhang Y., Fang Y., Recognition of diseased Pinus trees in UAV images using deep learning and AdaBoost classifier, Biosyst Eng, 194, pp. 138-151, (2020); Hu G., Zhu Y., Wan M., Bao W., Zhang Y., Liang D., Yin C., Detection of diseased pine trees in unmanned aerial vehicle images by using deep convolutional neural networks, Geocarto Int, 37, 12, pp. 3520-3539, (2022); Jintasuttisak T., Edirisinghe E., Elbattay A., Deep neural network based date palm tree detection in drone imagery, Comput Electron Agric, 192, (2022); Kagan D., Fuhrmann Alpert G., Fire M., Automatic large scale detection of red palm weevil infestation using street view images, ISPRS J Photogramm Remote Sens, 182, pp. 122-133, (2021); Kentsch S., Karatsiolis S., Kamilaris A., Tomhave L., Caceres M.L.L., Identification of tree species in Japanese forests based on aerial photography and deep learning, Adv New Trends Environ Informatics, pp. 255-270, (2021); Khan N., Kamaruddin M.A., Sheikh U.U., Yusup Y., Bakht M.P., Oil palm and machine learning: reviewing one decade of ideas, innovations, applications, and gaps, Agric, 11, 9, pp. 1-26, (2021); Kirillov A., Wu Y., He K., Girshick R., Pointrend: image segmentation as rendering, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, pp. 9796-9805, (2020); Kolanuvada S.R., Ilango K.K., Automatic extraction of tree crown for the estimation of biomass from UAV imagery using neural networks, J Indian Soc Remote Sens, 49, 3, pp. 651-658, (2021); Kurup S.S., Hedar Y.S., Al Dhaheri M.A., El-Heawiety A.Y., Aly M.A.M., Alhadrami G., Morpho-physiological evaluation and RAPD markers-assisted characterization of date palm (Phoenix dactylifera L.) varieties for salinity tolerance, J Food Agric Environ, 7, 3-4, pp. 503-507, (2009); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks, Remote Sens, 11, 1, (2018); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc IEEE Conf Comput Vis Pattern Recognit, pp. 2117-2125, (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: common objects in context, Eur Conf Comput Vis, pp. 740-755, (2014); Liu X., Ghazali K.H., Han F., Mohamed I.I., Automatic detection of oil palm tree from UAV images based on the deep learning method, Appl Artif Intell, 35, 1, pp. 13-24, (2021); Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo B., Swin transformer: hierarchical vision transformer using shifted windows, 2021 IEEE/CVF Int Conf Comput Vis, pp. 9992-10002, (2021); Liu Y., Zhang Y., Wang Y., Hou F., Yuan J., Tian J., Zhang Y., Shi Z., Fan J., He Z., A survey of visual transformers, pp. 1-21, (2021); Li Y., Wang H., Dang L.M., Piran J., Moon H., A robust instance segmentation framework for underground sewer defect detection, Measurement, 190, January, (2022); Luo Y., Han J., Liu Z., Wang M., Xia G., An elliptic centerness for object instance segmentation in aerial images, J Remote Sens, 2022, pp. 1-14, (2022); Mao Z., Huang X., Gong Y., Xiang H., Zhang F., A dataset and ensemble model for glass façade segmentation in oblique aerial images, IEEE Geosci Remote Sensing Lett, 19, pp. 1-5, (2022); Marin W., Mondragon I.F., Colorado J.D., Aerial identification of amazonian palms in high-density forest using deep learning, Forests, 13, 5, (2022); Martins G.B., La Rosa L.E.C., Happ P.N., Filho L.C.T.C., Santos C.J.F., Feitosa R.Q., Ferreira M.P., Deep learning-based tree species mapping in a highly diverse tropical urban setting, Urban Urban Green, 64, March, (2021); Martins J.A.C., Nogueira K., Osco L.P., Gomes F.D.G., Furuya D.E.G., Goncalves W.N., Sant'ana D.A., Ramos A.P.M., Liesenberg V., Dos Santos J.A., Et al., Semantic segmentation of tree-canopy in urban environment with pixel-wise deep learning, Remote Sens, 13, 16, (2021); Mazloumzadeh S.M., Shamsi M., Nezamabadi-Pour H., Fuzzy logic to classify date palm trees based on some physical properties related to precision agriculture, Precision Agric, 11, 3, pp. 258-273, (2010); Mihi A., Nacer T., Chenchouni H., Monitoring dynamics of date palm plantations from 1984 to 2013 using Landsat time-series in Sahara desert oases of Algeria, Adv Sci Technol Innov, pp. 225-228, (2019); Miraki M., Sohrabi H., Fatehi P., Kneubuehler M., Individual tree crown delineation from high-resolution UAV images in broadleaf forest, Ecol Inform, 61, (2021); Mo J., Lan Y., Yang D., Wen F., Qiu H., Chen X., Deng X., Deep learning-based instance segmentation method of Litchi canopy from UAV-acquired images, Remote Sens, 13, 19, (2021); Mohan M., Silva C.A., Klauberg C., Jat P., Catts G., Cardil A., Hudak A.T., Dia M., Individual tree detection from unmanned aerial vehicle (UAV) derived canopy height model in an open canopy mixed conifer forest, Forests, 8, 9, pp. 340-317, (2017); Morales G., Kemper G., Sevillano G., Arteaga D., Ortega I., Telles J., Automatic segmentation of Mauritia flexuosa in unmanned aerial vehicle (UAV) imagery using deep learning, Forests, 9, 12, (2018); Moura M.M., de Oliveira L.E.S., Sanquetta C.R., Bastos A., Mohan M., Corte A.P.D., Towards Amazon forest restoration: automatic detection of species from UAV imagery, Remote Sens, 13, 13, (2021); Mulley M., Kooistra L., Bierens L., High-resolution multisensor remote sensing to support date palm farm management, Agric, 9, 2, (2019); Nasi R., Honkavaara E., Blomqvist M., Lyytikainen-Saarenmaa P., Hakala T., Viljanen N., Kantola T., Holopainen M., Remote sensing of bark beetle damage in urban forests at individual tree level using a novel hyperspectral camera from UAV and aircraft, Urban for Urban Green, 30, pp. 72-83, (2018); Nguyen H.T., Caceres M.L.L., Moritake K., Kentsch S., Shu H., Diez Y., Correction: nguyen et al. Individual sick fir tree (Abies mariesii) identification in insect infested forests by means of uav images and deep learning, Remote Sens, 13, 11, pp. 1-24, (2021); Ocer N.E., Kaplan G., Erdem F., Kucuk Matci D., Avdan U., Tree extraction from multi-scale UAV images using Mask R-CNN with FPN, Remote Sens Lett, 11, 9, pp. 847-856, (2020); Osco L.P., Arruda M., Marcato Junior J., da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J Photogramm Remote Sens, 160, pp. 97-106, (2020); Ozdarici-Ok A., Ok A.O., Zeybek M., Atesoglu A., Automated extraction and validation of Stone Pine (Pinus pinea L.) trees from UAV-based digital surface models, Geo-Spatial Inf Sci, 25, 3, pp. 1-21, (2022); Pearse G.D., Watt M.S., Soewarto J., Tan A.Y.S., Deep learning and phenology enhance large-scale tree species classification in aerial imagery during a biosecurity response, Remote Sens, 13, 9, (2021); Piyathilaka L., Preethichandra D.M.G., Izhar U., Kahandawa G., pp. 1-7, (2020); Qin J., Wang B., Wu Y., Lu Q., Zhu H., Identifying pine wood nematode disease using uav images and deep learning algorithms, Remote Sens, 13, 2, pp. 1-14, (2021); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans Pattern Anal Mach Intell, 39, 6, pp. 1137-1149, (2017); Safonova A., Guirado E., Maglinets Y., Alcaraz-Segura D., Tabik S., Olive tree biovolume from uav multi-resolution image segmentation with mask R-CNN, Sensors, 21, 5, (2021); Safonova A., Hamad Y., Dmitriev E., Georgiev G., Trenkin V., Georgieva M., Dimitrov S., Iliev M., Individual tree crown delineation for the species classification and assessment of vital status of forest stands from UAV images, Drones, 5, 3, (2021); Schiefer F., Kattenborn T., Frick A., Frey J., Schall P., Koch B., Schmidtlein S., Mapping forest tree species in high resolution UAV-based RGB-imagery by means of convolutional neural networks, ISPRS J Photogramm Remote Sens, 170, vember, pp. 205-215, (2020); eMotion 3 user manual, senseFly Parrot Gr, March, pp. 73-75, (2018); Shareef M.A., Estimation and mapping of dates palm using Landsat-8 Images: a case study in Baghdad City, 2018 Int Conf Adv Sci Eng, pp. 425-430, (2018); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Spennemann D.H.R., Review of the vertebrate-mediated dispersal of the Date Palm, Phoenix dactylifera, Zool Middle East, 64, 4, pp. 283-296, (2018); Stateras D., Kalivas D., Assessment of olive tree canopy characteristics and yield forecast model using high resolution UAV imagery, Agric, 10, 9, pp. 1-13, (2020); Sun Y., Li Z., He H., Guo L., Zhang X., Xin Q., Counting trees in a subtropical mega city using the instance segmentation method, Int J Appl Earth Obs Geoinf, 106, (2022); Torresan C., Carotenuto F., Chiavetta U., Miglietta F., Zaldei A., Gioli B., Individual tree crown segmentation in two-layered dense mixed forests from uav lidar data, Drones, 4, 2, pp. 10-20, (2020); Wang Z., Fan C., Xian M., Application and evaluation of a deep learning architecture to urban tree canopy mapping, Remote Sens, 13, 9, pp. 1-14, (2021); Wang X., Kong T., Shen C., Jiang Y., Li L., SOLO: segmenting objects by locations, Eur Conf Comput Vis. Vol. 12363 LNCS, pp. 649-665, (2020); Wang X., Zhang R., Kong T., Li L., Shen C., SOLOv2: dynamic and fast instance segmentation, Proc 34th Conf Neural Inf Process Syst (NeurIPS 2020). Vol. 2020-Decem., (2020); Wang X., Zhang R., Shen C., Kong T., Li L., SOLO: a simple framework for instance segmentation, IEEE Trans Pattern Anal Mach Intell, (2021); Weinstein B.G., Marconi S., Aubry-Kientz M., Vincent G., Senyondo H., White E.P., DeepForest: a Python package for RGB deep learning tree crown delineation, Methods Ecol Evol, 11, 12, pp. 1743-1751, (2020); Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., (2019); Xia K., Wang H., Yang Y., Du X., Feng H., Automatic detection and parameter estimation of Ginkgo biloba in urban environment based on RGB images, J Sensors, 2021, pp. 1-12, (2021); Xia L., Zhang R., Chen L., Li L., Yi T., Wen Y., Ding C., Xie C., Evaluation of deep learning segmentation models for detection of pine wilt disease in unmanned aerial vehicle images, Remote Sens, 13, 18, pp. 1-15, (2021); Xi X., Xia K., Yang Y., Du X., Feng H., Evaluation of dimensionality reduction methods for individual tree crown delineation using instance segmentation network and UAV multispectral imagery in urban forest, Comput Electron Agric, 191, October, (2021); Yang M., Mou Y., Liu S., Meng Y., Liu Z., Li P., Xiang W., Zhou X., Peng C., Detecting and mapping tree crowns based on convolutional neural network and Google Earth images, Int J Appl Earth Obs Geoinf, 108, (2022); Yarak K., Witayangkurn A., Kritiyutanont K., Arunplod C., Shibasaki R., Oil palm tree detection and health classification on high‐resolution imagery using deep learning, Agric, 11, 2, pp. 1-17, (2021); Yu K., Hao Z., Post C.J., Mikhailova E.A., Lin L., Zhao G., Tian S., Liu J., Comparison of classical methods and mask R-CNN for automatic tree detection and mapping using UAV imagery, Remote Sens, 14, 2, (2022); Yu R., Luo Y., Zhou Q., Zhang X., Wu D., Ren L., Early detection of pine wilt disease using deep learning algorithms and UAV-based multispectral imagery, For Ecol Manage, 497, (2021); Zamboni P., Junior J.M., de Silva J.A., Miyoshi G.T., Matsubara E.T., Nogueira K., Goncalves W.N., Benchmarking anchor-based and anchor-free state-of-the-art deep learning methods for individual tree detection in rgb high-resolution images, Remote Sens, 13, 13, (2021); Zhang Z., Huang S., Liu X., Zhang B., Dong D., Adversarial attacks on YOLACT instance segmentation, Comput Secur, 116, (2022); Zhang C., Wei S., Ji S., Detecting large-scale urban land cover changes from very high resolution remote sensing images using CNN-based classification, ISPRS Int J Geo-Inf, 8, 4, (2019); Zhang C., Xia K., Feng H., Yang Y., Du X., Tree species classification using deep learning and RGB optical images obtained by an unmanned aerial vehicle, J Res, 32, 5, pp. 1879-1888, (2020); Zhang C., Zhou J., Wang H., Tan T., Cui M., Huang Z., Wang P., Zhang L., Multi‐species individual tree segmentation and identification based on improved Mask R‐CNN and UAV imagery in mixed forests, Remote Sens, 14, 4, (2022); Zheng J., Fu H., Li W., Wu W., Yu L., Yuan S., Tao W.Y.W., Pang T.K., Kanniah K.D., Growing status observation for oil palm trees using Unmanned Aerial Vehicle (UAV) images, ISPRS J Photogramm Remote Sens, 173, pp. 95-121, (2021); Zheng J., Fu H., Li W., Wu W., Zhao Y., Dong R., Yu L., Cross-regional oil palm tree counting and detection via a multi-level attention domain adaptation network, ISPRS J Photogramm Remote Sens, 167, pp. 154-177, (2020); Zheng J., Wu W., Yuan S., Fu H., Li W., Yu L., Multisource-domain generalization-based oil palm, IEEE Geosci Remote Sensing Lett, 19, pp. 1-5, (2022); Zhong Z., Xiong J., Zheng Z., Liu B., Liao S., Huo Z., Yang Z., Original papers A method for litchi picking points calculation in natural environment based on main fruit bearing branch detection, Comput Electron Agric, 189, August, (2021); Zou Z., Shi Z., Guo Y., Ye J., Object detection in 20 years: a survey, (2019)","H.Z.M. Shafri; Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Serdang, Selangor, Malaysia; email: helmi@upm.edu.my","","Taylor and Francis Ltd.","","","","","","10106049","","","","English","Geocarto Int.","Article","Final","","Scopus","2-s2.0-85141665201"
"Ghaffarian S.; Valente J.; Van Der Voort M.; Tekinerdogan B.","Ghaffarian, Saman (56237751400); Valente, João (42062501000); Van Der Voort, Mariska (52264587200); Tekinerdogan, Bedir (15761578600)","56237751400; 42062501000; 52264587200; 15761578600","Effect of attention mechanism in deep learning-based remote sensing image processing: A systematic literature review","2021","Remote Sensing","13","15","2965","","","","49","10.3390/rs13152965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112132537&doi=10.3390%2frs13152965&partnerID=40&md5=8f2a05c9a18ecbe4fda9be23753b592d","Information Technology Group, Wageningen University & Research, Wageningen, 6707 KN, Netherlands; Business Economics Group, Wageningen University & Research, Wageningen, 6700 EW, Netherlands","Ghaffarian S., Information Technology Group, Wageningen University & Research, Wageningen, 6707 KN, Netherlands, Business Economics Group, Wageningen University & Research, Wageningen, 6700 EW, Netherlands; Valente J., Information Technology Group, Wageningen University & Research, Wageningen, 6707 KN, Netherlands; Van Der Voort M., Business Economics Group, Wageningen University & Research, Wageningen, 6700 EW, Netherlands; Tekinerdogan B., Information Technology Group, Wageningen University & Research, Wageningen, 6707 KN, Netherlands","Machine learning, particularly deep learning (DL), has become a central and state-of-the-art method for several computer vision applications and remote sensing (RS) image processing. Researchers are continually trying to improve the performance of the DL methods by developing new architectural designs of the networks and/or developing new techniques, such as attention mechanisms. Since the attention mechanism has been proposed, regardless of its type, it has been increasingly used for diverse RS applications to improve the performances of the existing DL methods. However, these methods are scattered over different studies impeding the selection and application of the feasible approaches. This study provides an overview of the developed attention mechanisms and how to integrate them with different deep learning neural network architectures. In addition, it aims to investigate the effect of the attention mechanism on deep learning-based RS image processing. We identified and analyzed the advances in the corresponding attention mechanism-based deep learning (At-DL) methods. A systematic literature review was performed to identify the trends in publications, publishers, improved DL methods, data types used, attention types used, overall accuracies achieved using At-DL methods, and extracted the current research directions, weaknesses, and open problems to provide insights and recommendations for future studies. For this, five main research questions were formulated to extract the required data and information from the literature. Furthermore, we categorized the papers regarding the addressed RS image processing tasks (e.g., image classification, object detection, and change detection) and discussed the results within each group. In total, 270 papers were retrieved, of which 176 papers were selected according to the defined exclusion criteria for further analysis and detailed review. The results reveal that most of the papers reported an increase in overall accuracy when using the attention mechanism within the DL methods for image classification, image segmentation, change detection, and object detection using remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Attention mechanism; Channel attention; Deep; Image processing; Remote sensing; Spatial attention","Data mining; Deep neural networks; Image classification; Image segmentation; Learning systems; Network architecture; Object detection; Object recognition; Paper; Remote sensing; Attention mechanisms; Computer vision applications; Data and information; Learning neural networks; Remote sensing image processing; Remote sensing images; State-of-the-art methods; Systematic literature review; Deep learning","","","","","","","Weiss M., Jacob F., Duveiller G., Remote sensing for agricultural applications: A meta-review, Remote Sens. Environ, 236, (2020); Ghaffarian S., Turker M., An improved cluster-based snake model for automatic agricultural field boundary extraction from high spatial resolution imagery, Int. J. Remote Sens, 40, pp. 1217-1247, (2019); Valente J., Sari B., Kooistra L., Kramer H., Mucher S., Automated crop plant counting from very high-resolution aerial imagery, Precis. Agric, 21, pp. 1366-1384, (2020); Zhang C., Valente J., Kooistra L., Guo L., Wang W., Orchard management with small unmanned aerial vehicles: A survey of sensing and analysis approaches, Precis. Agric, (2021); Nielsen M.M., Remote sensing for urban planning and management: The use of window-independent context segmentation to extract urban features in Stockholm, Comput. Environ. Urban Syst, 52, pp. 1-9, (2015); Kadhim N., Mourshed M., Bray M., Advances in remote sensing applications for urban sustainability, Euro-Mediterr. J. Environ. Integr, 1, (2016); Ghaffarian S., Ghaffarian S., Automatic building detection based on Purposive FastICA (PFICA) algorithm using monocular high resolution Google Earth images, ISPRS J. Photogramm. Remote Sens, 97, pp. 152-159, (2014); Ghaffarian S., Kerle N., Filatova T., Remote Sensing-Based Proxies for Urban Disaster Risk Management and Resilience: A Review, Remote Sens, 10, (2018); Ghaffarian S., Rezaie Farhadabad A., Kerle N., Post-Disaster Recovery Monitoring with Google Earth Engine, Appl. Sci, 10, (2020); Ghaffarian S., Emtehani S., Monitoring Urban Deprived Areas with Remote Sensing and Machine Learning in Case of Disaster Recovery, Climate, 9, (2021); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Belgiu M., Dragut L., Random forest in remote sensing: A review of applications and future directions, ISPRS J. Photogramm. Remote Sens, 114, pp. 24-31, (2016); Sheykhmousa M., Mahdianpari M., Ghanbari H., Mohammadimanesh F., Ghamisi P., Homayouni S., Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 6308-6325, (2020); Zhang L., Zhang L., Du B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art, IEEE Geosci. Remote Sens. Mag, 4, pp. 22-40, (2016); Li Y., Zhang H., Xue X., Jiang Y., Shen Q., Deep learning for remote sensing image classification: A survey, WIREs Data Min. Knowl. Discov, 8, (2018); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, ISPRS J. Photogramm. Remote Sens, 173, pp. 24-49, (2021); Ghanbari H., Mahdianpari M., Homayouni S., Mohammadimanesh F., A Meta-Analysis of Convolutional Neural Networks for Remote Sensing Applications, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 3602-3613, (2021); Liu X., Wang Y., Liu Q., Psgan: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening, Proceedings of the 2018 25th IEEE International Conference on Image Processing (ICIP), pp. 873-877; Yan X., Ai T., Yang M., Yin H., A graph convolutional neural network for classification of building patterns using spatial vector data, ISPRS J. Photogramm. Remote Sens, 150, pp. 259-273, (2019); Bahdanau D., Cho K., Bengio Y., Neural machine translation by jointly learning to align and translate, (2014); Niu Z., Zhong G., Yu H., A Review on the Attention Mechanism of Deep Learning, Neurocomputing, (2021); Zhang J., Zhou Q., Wu J., Wang Y.C., Wang H., Li Y.S., Chai Y.Z., Liu Y., A Cloud Detection Method Using Convolutional Neural Network Based on Gabor Transform and Attention Mechanism with Dark Channel Subnet for Remote Sensing Image, Remote Sens, 12, (2020); Zeng Y.L., Ritz C., Zhao J.H., Lan J.H., Attention-Based Residual Network with Scattering Transform Features for Hyperspectral Unmixing with Limited Training Samples, Remote Sens, 12, (2020); Yu Y., Li X., Liu F., Attention GANs: Unsupervised Deep Feature Learning for Aerial Scene Classification, IEEE Trans. Geosci. Remote Sens, 58, pp. 519-531, (2020); Gao F., He Y.S., Wang J., Hussain A., Zhou H.Y., Anchor-free Convolutional Network with Dense Attention Feature Aggregation for Ship Detection in SAR Images, Remote Sens, 12, (2020); Li F., Feng R., Han W., Wang L., High-Resolution Remote Sensing Image Scene Classification via Key Filter Bank Based on Convolutional Neural Network, IEEE Trans. Geosci. Remote Sens, 58, pp. 8077-8092, (2020); Yang H., Wu P.H., Yao X.D., Wu Y.L., Wang B., Xu Y.Y., Building Extraction in Very High Resolution Imagery by Dense-Attention Networks, Remote Sens, 10, (2018); Zhu X.X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote Sens. Mag, 5, pp. 8-36, (2017); Li S., Kang X., Fang L., Hu J., Yin H., Pixel-level image fusion: A survey of the state of the art, Inf. Fusion, 33, pp. 100-112, (2017); Galassi A., Lippi M., Torroni P., Attention in Natural Language Processing, IEEE Trans. Neural Netw. Learn. Syst, pp. 1-18, (2020); Koscevic K., Subasic M., Loncaric S., Attention-based Convolutional Neural Network for Computer Vision Color Constancy, Proceedings of the 2019 11th International Symposium on Image and Signal Processing and Analysis (ISPA), pp. 372-377; Li W., Liu K., Zhang L., Cheng F., Object detection based on an adaptive attention mechanism, Sci. Rep, 10, (2020); Cui W., Wang F., He X., Zhang D.Y., Xu X.X., Yao M., Wang Z.W., Huang J.J., Multi-Scale Semantic Segmentation and Spatial Relationship Recognition of Remote Sensing Images Based on an Attention Model, Remote Sens, 11, (2019); Alshehri A., Bazi Y., Ammour N., Almubarak H., Alajlan N., Deep Attention Neural Network for Multi-Label Classification in Unmanned Aerial Vehicle Imagery, IEEE Access, 7, pp. 119873-119880, (2019); Sun H., Zheng X., Lu X., Wu S., Spectral-Spatial Attention Network for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 58, pp. 3232-3245, (2020); Feng J., Wu X., Shang R., Sui C., Li J., Jiao L., Zhang X., Attention Multibranch Convolutional Neural Network for Hyperspectral Image Classification Based on Adaptive Region Search, IEEE Trans. Geosci. Remote Sens, (2020); Zhao Z., Wang H., Yu X., Spectral-Spatial Graph Attention Network for Semisupervised Hyperspectral Image Classification, IEEE Geosci. Remote Sens. Lett, (2021); Censi A.M., Ienco D., Gbodjo Y.J.E., Pensa R.G., Interdonato R., Gaetano R., Attentive Spatial Temporal Graph CNN for Land Cover Mapping from Multi Temporal Remote Sensing Data, IEEE Access, 9, pp. 23070-23082, (2021); Hu J., Shen L., Sun G., Squeeze-and-Excitation Networks, Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7132-7141; Xu K., Ba J., Kiros R., Cho K., Courville A., Salakhudinov R., Zemel R., Bengio Y., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research, pp. 2048-2057; Guo Y., Ji J., Lu X., Huo H., Fang T., Li D., Global-Local Attention Network for Aerial Scene Classification, IEEE Access, 7, pp. 67200-67212, (2019); Ma J., Ma Q., Tang X., Zhang X., Zhu C., Peng Q., Jiao L., Remote Sensing Scene Classification Based on Global and Local Consistent Network, Proceedings of the International Geoscience and Remote Sensing Symposium (IGARSS), pp. 537-540; Hu J., Shen L., Albanie S., Sun G., Wu E., Squeeze-and-Excitation Networks, IEEE Trans. Pattern Anal. Mach. Intell, 42, pp. 2011-2023, (2020); Wang L., Peng J.T., Sun W.W., Spatial-Spectral Squeeze-and-Excitation Residual Network for Hyperspectral Image Classification, Remote Sens, 11, (2019); Alswayed A.S., Alhichri H.S., Bazi Y., SqueezeNet with Attention for Remote Sensing Scene Classification, Proceedings of the ICCAIS 2020—3rd International Conference on Computer Applications and Information Security; Li C.Y., Luo B., Hong H.L., Su X., Wang Y.J., Liu J., Wang C.J., Zhang J., Wei L.H., Object Detection Based on Global-Local Saliency Constraint in Aerial Images, Remote Sens, 12, (2020); Zhou M., Zou Z., Shi Z., Zeng W.J., Gui J., Local Attention Networks for Occluded Airplane Detection in Remote Sensing Images, IEEE Geosci. Remote Sens. Lett, 17, pp. 381-385, (2020); Ding L., Tang H., Bruzzone L., LANet: Local Attention Embedding to Improve the Semantic Segmentation of Remote Sensing Images, IEEE Trans. Geosci. Remote Sens, 59, pp. 426-435, (2021); Chaudhari S., Mithal V., Polatkan G., Ramanath R., An attentive survey of attention models, (2019); Lu J., Yang J., Batra D., Parikh D., Hierarchical question-image co-attention for visual question answering, Proceedings of the NIPS, pp. 289-297; Jiang H.W., Hu X.Y., Li K., Zhang J.M., Gong J.Q., Zhang M., PGA-SiamNet: Pyramid Feature-Based Attention-Guided Siamese Network for Remote Sensing Orthoimagery Building Change Detection, Remote Sens, 12, (2020); He N., Fang L., Li Y., Plaza A., High-Order Self-Attention Network for Remote Sensing Scene Classification, Proceedings of the International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3013-3016; Wu Z.C., Li J., Wang Y.S., Hu Z.W., Molinier M., Self-Attentive Generative Adversarial Network for Cloud Detection in High Resolution Remote Sensing Images, IEEE Geosci. Remote Sens. Lett, 17, pp. 1792-1796, (2020); Cao R., Fang L., Lu T., He N., Self-Attention-Based Deep Feature Fusion for Remote Sensing Scene Classification, IEEE Geosci. Remote Sens. Lett, 18, pp. 43-47, (2021); Wu H.L., Zhao S.Z., Li L., Lu C.Q., Chen W., Self-Attention Network With Joint Loss for Remote Sensing Image Scene Classification, IEEE Access, 8, pp. 210347-210359, (2020); Xiao T., Xu Y., Yang K., Zhang J., Peng Y., Zhang Z., The application of two-level attention models in deep convolutional neural network for fine-grained image classification, Proceedings of the CVPR, pp. 842-850; Sumbul G., Cinbis R.G., Aksoy S., Multisource Region Attention Network for Fine-Grained Object Recognition in Remote Sensing Imagery, IEEE Trans. Geosci. Remote Sens, 57, pp. 4929-4937, (2019); Li J., Tu Z., Yang B., Lyu M.R., Zhang T., Multi-Head Attention with Disagreement Regularization, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2897-2903; Zhang S.Y., Li C.R., Qiu S., Gao C.X., Zhang F., Du Z.H., Liu R.Y., EMMCNN: An ETPS-Based Multi-Scale and Multi-Feature Method Using CNN for High Spatial Resolution Image Land-Cover Classification, Remote Sens, 12, (2020); Cheng B., Li Z.Z., Xu B.T., Yao X., Ding Z.Q., Qin T.Q., Structured Object-Level Relational Reasoning CNN-Based Target Detection Algorithm in a Remote Sensing Image, Remote Sens, 13, (2021); Wu Z., Hou B., Jiao L., Multiscale CNN with Autoencoder Regularization Joint Contextual Attention Network for SAR Image Classification, IEEE Trans. Geosci. Remote Sens, 59, pp. 1200-1213, (2021); Shen T., Zhou T., Long G., Jiang J., Pan S., Zhang C., DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding, Proceedings of the AAAI, (2018); Du J., Han J., Way A., Wan D., Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction, Proceedings of the EMNLP, (2018); Carrasco M., Visual attention: The past 25 years, Vis. Res, 51, pp. 1484-1525, (2011); Beuth F., Hamker F.H., A mechanistic cortical microcircuit of attention for amplification, normalization and suppression, Vis. Res, 116, pp. 241-257, (2015); Itti L., Koch C., Niebur E., A model of saliency-based visual attention for rapid scene analysis, IEEE Trans. Pattern Anal. Mach. Intell, 20, pp. 1254-1259, (1998); Ma W.P., Zhao J.L., Zhu H., Shen J.C., Jiao L.C., Wu Y., Hou B.A., A Spatial-Channel Collaborative Attention Network for Enhancement of Multiresolution Classification, Remote Sens, 13, (2021); Tong W., Chen W., Han W., Li X., Wang L., Channel-Attention-Based DenseNet Network for Remote Sensing Image Scene Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 4121-4132, (2020); Guo D., Xia Y., Luo X., Scene Classification of Remote Sensing Images Based on Saliency Dual Attention Residual Network, IEEE Access, 8, pp. 6344-6357, (2020); Hang R.L., Li Z., Liu Q.S., Ghamisi P., Bhattacharyya S.S., Hyperspectral Image Classification With Attention-Aided CNNs, IEEE Trans. Geosci. Remote Sens, 59, pp. 2281-2293, (2021); Zhu M., Jiao L., Liu F., Yang S., Wang J., Residual Spectral-Spatial Attention Network for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 59, pp. 449-462, (2021); Ren Y.F., Yu Y.T., Guan H.Y., DA-CapsUNet: A Dual-Attention Capsule U-Net for Road Extraction from Remote Sensing Imagery, Remote Sens, 12, (2020); Ren Y., Li X., Yang X., Xu H., Development of a Dual-Attention U-Net Model for Sea Ice and Open Water Classification on SAR Images, IEEE Geosci. Remote Sens. Lett, (2021); He N., Fang L., Plaza A., Hybrid first and second order attention Unet for building segmentation in remote sensing images, Sci. China Inf. Sci, 63, (2020); Pan X., Yang F., Gao L., Chen Z., Zhang B., Fan H., Ren J., Building extraction from high-resolution aerial imagery using a generative adversarial network with spatial and channel attention mechanisms, Remote Sens, 11, (2019); Liu R.C., Cheng Z.H., Zhang L.L., Li J.X., Remote Sensing Image Change Detection Based on Information Transmission and Attention Mechanism, IEEE Access, 7, pp. 156349-156359, (2019); Wang Q., Liu S.T., Chanussot J., Li X.L., Scene Classification With Recurrent Attention of VHR Remote Sensing Images, IEEE Trans. Geosci. Remote Sens, 57, pp. 1155-1167, (2019); Li Z.T., Chen G.K., Zhang T.X., Temporal Attention Networks for Multitemporal Multisensor Crop Classification, IEEE Access, 7, pp. 134677-134690, (2019); Mei X.G., Pan E.T., Ma Y., Dai X.B., Huang J., Fan F., Du Q.L., Zheng H., Ma J.Y., Spectral-Spatial Attention Networks for Hyperspectral Image Classification, Remote Sens, 11, (2019); Ma F., Gao F., Sun J.P., Zhou H.Y., Hussain A., Attention Graph Convolution Network for Image Segmentation in Big SAR Imagery Data, Remote Sens, 11, (2019); Luo X., Li X., Wu Y., Hou W., Wang M., Jin Y., Xu W., Research on Change Detection Method of High-Resolution Remote Sensing Images Based on Subpixel Convolution, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 1447-1457, (2021); Li Y., Chen R., Zhang Y., Zhang M., Chen L., Multi-label remote sensing image scene classification by combining a convolutional neural network and a graph neural network, Remote Sens, 12, (2020); Kitchenham B., Pearl Brereton O., Budgen D., Turner M., Bailey J., Linkman S., Systematic literature reviews in software engineering—A systematic literature review, Inf. Softw. Technol, 51, pp. 7-15, (2009); Chen L.F., Zhang P., Xing J., Li Z.H., Xing X.M., Yuan Z.H., A Multi-Scale Deep Neural Network for Water Detection from SAR Images in the Mountainous Areas, Remote Sens, 12, (2020); Yang Q., Wang C., Zeng T., A method of water change monitoring in remote image time series based on long short time memory, Remote Sens. Lett, 12, pp. 67-76, (2021); Zhang Y.D., Chen G., Vukomanovic J., Singh K.K., Liu Y., Holden S., Meentemeyer R.K., Recurrent Shadow Attention Model (RSAM) for shadow removal in high-resolution urban land-cover mapping, Remote Sens. Environ, 247, (2020); Ma L., Li M., Ma X., Cheng L., Du P., Liu Y., A review of supervised object-based land-cover image classification, ISPRS J. Photogramm. Remote Sens, 130, pp. 277-293, (2017); Cheng G., Han J., Lu X., Remote Sensing Image Scene Classification: Benchmark and State of the Art, Proc. IEEE, 105, pp. 1865-1883, (2017); Li M., Zang S., Zhang B., Li S., Wu C., A Review of Remote Sensing Image Classification Techniques: The Role of Spatio-contextual Information, Eur. J. Remote Sens, 47, pp. 389-411, (2014); Alem A., Kumar S., Deep Learning Methods for Land Cover and Land Use Classification in Remote Sensing: A Review, Proceedings of the 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), pp. 903-908; Sang Q., Zhuang Y., Dong S., Wang G., Chen H., FRF-Net: Land Cover Classification from Large-Scale VHR Optical Remote Sensing Images, IEEE Geosci. Remote Sens. Lett, 17, pp. 1057-1061, (2020); Ienco D., Gbodjo Y.J.E., Gaetano R., Interdonato R., Weakly Supervised Learning for Land Cover Mapping of Satellite Image Time Series via Attention-Based CNN, IEEE Access, 8, pp. 179547-179560, (2020); Tang X., Meng F., Zhang X., Cheung Y.M., Ma J., Liu F., Jiao L., Hyperspectral Image Classification Based on 3-D Octave Convolution with Spatial-Spectral Attention Network, IEEE Trans. Geosci. Remote Sens, 59, pp. 2430-2447, (2021); Feng Q.L., Yang J.Y., Liu Y.M., Ou C., Zhu D.H., Niu B.W., Liu J.T., Li B.G., Multi-Temporal Unmanned Aerial Vehicle Remote Sensing for Vegetable Mapping Using an Attention-Based Recurrent Convolutional Neural Network, Remote Sens, 12, (2020); Li Y.Y., Huang Q., Pei X., Jiao L.C., Shang R.H., RADet: Refine Feature Pyramid Network and Multi-Layer Attention Network for Arbitrary-Oriented Object Detection of Remote Sensing Images, Remote Sens, 12, (2020); Zhou D., Wang G., He G., Long T., Yin R., Zhang Z., Chen S., Luo B., Robust building extraction for high spatial resolution remote sensing images with self-attention network, Sensors, 20, (2020); Zhao Y., Zhao L., Xiong B., Kuang G., Attention receptive pyramid network for ship detection in SAR images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 2738-2756, (2020); Fu J., Sun X., Wang Z., Fu K., An Anchor-Free Method Based on Feature Balancing and Refinement Network for Multiscale Ship Detection in SAR Images, IEEE Trans. Geosci. Remote Sens, 59, pp. 1331-1344, (2021); Ji S.P., Yu D.W., Shen C.Y., Li W.L., Xu Q., Landslide detection from an open satellite imagery and digital elevation model dataset using attention boosted convolutional neural networks, Landslides, 17, pp. 1337-1352, (2020); Yao Z., Jia J., Qian Y., McNet: Multi-scale feature extraction and content-aware reassembly cloud detection model for remote sensing images, Symmetry, 13, (2021); Tan S.Y., Chen L.F., Pan Z.H., Xing J., Li Z.H., Yuan Z.H., Geospatial Contextual Attention Mechanism for Automatic and Fast Airport Detection in SAR Imagery, IEEE Access, 8, pp. 173627-173640, (2020); Zheng J., Fu H., Li W., Wu W., Zhao Y., Dong R., Yu L., Cross-regional oil palm tree counting and detection via a multi-level attention domain adaptation network, ISPRS J. Photogramm. Remote Sens, 167, pp. 154-177, (2020); Qi X., Li K., Liu P., Zhou X., Sun M., Deep Attention and Multi-Scale Networks for Accurate Remote Sensing Image Segmentation, IEEE Access, 8, pp. 146627-146639, (2020); Xiao D., Wang Z., Wu Y., Gao X., Sun X., Terrain Segmentation in Polarimetric SAR Images Using Dual-Attention Fusion Network, IEEE Geosci. Remote Sens. Lett, (2020); Li J.L., Xiu J.P., Yang Z.Q., Liu C., Dual Path Attention Net for Remote Sensing Semantic Image Segmentation, Isprs Int. J. Geo-Inf, 9, (2020); Dong X., Sun X., Jia X., Xi Z., Gao L., Zhang B., Remote Sensing Image Super-Resolution Using Novel Dense-Sampling Networks, IEEE Trans. Geosci. Remote Sens, 59, pp. 1618-1633, (2021); Wang H., Hu Q., Wu C.D., Chi J.N., Yu X.S., Non-Locally up-Down Convolutional Attention Network for Remote Sensing Image Super-Resolution, IEEE Access, 8, pp. 166304-166319, (2020); Li X., Xu F., Lyu X., Tong Y., Chen Z., Li S., Liu D., A Remote-Sensing Image Pan-Sharpening Method Based on Multi-Scale Channel Attention Residual Network, IEEE Access, 8, pp. 27163-27177, (2020); Li J.J., Cui R.X., Li B., Song R., Li Y.S., Du Q., Hyperspectral Image Super-Resolution with 1D-2D Attentional Convolutional Neural Network, Remote Sens, 11, (2019); Chen L., Zhang D.Z., Li P., Lv P., Change Detection of Remote Sensing Images Based on Attention Mechanism, Comput. Intell. Neurosci, 2020, (2020); Chen J., Yuan Z., Peng J., Chen L., Huang H., Zhu J., Liu Y., Li H., DASNet: Dual Attentive Fully Convolutional Siamese Networks for Change Detection in High-Resolution Satellite Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 1194-1206, (2021); Zhang Y., Zhang S.Z., Li Y., Zhang Y.N., Coarse-to-Fine Satellite Images Change Detection Framework via Boundary-Aware Attentive Network, Sensors, 20, (2020); Gu Z.Q., Zhan Z.Q., Yuan Q.Q., Yan L., Single Remote Sensing Image Dehazing Using a Prior-Based Dense Attentive Network, Remote Sens, 11, (2019); Gavriil K., Muntingh G., Barrowclough O.J.D., Void Filling of Digital Elevation Models with Deep Generative Models, IEEE Geosci. Remote Sens. Lett, 16, pp. 1645-1649, (2019); Shen H., Zhou C., Li J., Yuan Q., SAR Image Despeckling Employing a Recursive Deep CNN Prior, IEEE Trans. Geosci. Remote Sens, 59, pp. 273-286, (2021); Li J., Lin D.Y., Wang Y., Xu G.L., Zhang Y.Y., Ding C.B., Zhou Y.H., Deep Discriminative Representation Learning with Attention Map for Scene Classification, Remote Sens, 12, (2020); Bahri A., Majelan S.G., Mohammadi S., Noori M., Mohammadi K., Remote Sensing Image Classification via Improved Cross-Entropy Loss and Transfer Learning Strategy Based on Deep Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett, 17, pp. 1087-1091, (2020); Zhang C., Yue J., Qin Q., Global prototypical network for few-shot hyperspectral image classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 4748-4759, (2020); Lei P.C., Liu C., Inception residual attention network for remote sensing image super-resolution, Int. J. Remote Sens, 41, pp. 9565-9587, (2020); Cheng W.S., Yang W., Wang M., Wang G., Chen J.Y., Context Aggregation Network for Semantic Labeling in Aerial Images, Remote Sens, 11, (2019); Gbodjo Y.J.E., Ienco D., Leroux L., Interdonato R., Gaetano R., Ndao B., Object-based multi-temporal and multi-source land cover mapping leveraging hierarchical class relationships, Remote Sens, 12, (2020); Liang L., Wang G., Efficient recurrent attention network for remote sensing scene classification, IET Image Process, (2021); Wang Z.S., Zou C., Cai W.W., Small Sample Classification of Hyperspectral Remote Sensing Images Based on Sequential Joint Deeping Learning Model, IEEE Access, 8, pp. 71353-71363, (2020); You H., Tian S., Yu L., Lv Y., Pixel-Level Remote Sensing Image Recognition Based on Bidirectional Word Vectors, IEEE Trans. Geosci. Remote Sens, 58, pp. 1281-1293, (2020); Zhang X.K., Pun M.O., Liu M., Semi-Supervised Multi-Temporal Deep Representation Fusion Network for Landslide Mapping from Aerial Orthophotos, Remote Sens, 13, (2021); Wong R., Zhang Z.J., Wang Y.M., Chen F.S., Zeng D., HSI-IPNet: Hyperspectral Imagery Inpainting by Deep Learning With Adaptive Spectral Extraction, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 4369-4380, (2020); Xu R.D., Tao Y.T., Lu Z.Y., Zhong Y.F., Attention-Mechanism-Containing Neural Networks for High-Resolution Remote Sensing Image Classification, Remote Sens, 10, (2018); Gao H., Cao L., Yu D., Xiong X., Cao M., Semantic Segmentation of Marine Remote Sensing Based on a Cross Direction Attention Mechanism, IEEE Access, 8, pp. 142483-142494, (2020); Zheng J., Feng Y., Bai C., Zhang J., Hyperspectral Image Classification Using Mixed Convolutions and Covariance Pooling, IEEE Trans. Geosci. Remote Sens, 59, pp. 522-534, (2021); Zhao L., Yi J., Li X., Hu W., Wu J., Zhang G., Compact Band Weighting Module Based on Attention-Driven for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, (2021); He X., Chen Y., Ghamisi P., Heterogeneous Transfer Learning for Hyperspectral Image Classification Based on Convolutional Neural Network, IEEE Trans. Geosci. Remote Sens, 58, pp. 3246-3263, (2020); Chen H., Chen R., Li N.N., Attentive generative adversarial network for removing thin cloud from a single remote sensing image, IET Image Process, 15, pp. 856-867, (2021); Wang J., Xiao H., Chen L., Xing J., Pan Z., Luo R., Cai X., Integrating weighted feature fusion and the spatial attention module with convolutional neural networks for automatic aircraft detection from sar images, Remote Sens, 13, (2021); Zhang H., Ma J., Chen C., Tian X., NDVI-Net: A fusion network for generating high-resolution normalized difference vegetation index in remote sensing, ISPRS J. Photogramm. Remote Sens, 168, pp. 182-196, (2020); Haut J.M., Fernandez-Beltran R., Paoletti M.E., Plaza J., Plaza A., Remote Sensing Image Superresolution Using Deep Residual Channel Attention, IEEE Trans. Geosci. Remote Sens, 57, pp. 9277-9289, (2019); Dong X., Xi Z., Sun X., Yang L., Remote Sensing Image Super-Resolution via Enhanced Back-Projection Networks, Proceedings of the International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1480-1483; Guo H., Liu J., Yang J., Xiao Z., Wu Z., Deep Collaborative Attention Network for Hyperspectral Image Classification by Combining 2-D CNN and 3-D CNN, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 4789-4802, (2020); Li R., Zheng S.Y., Duan C.X., Yang Y., Wang X.Q., Classification of Hyperspectral Image Based on Double-Branch Dual-Attention Mechanism Network, Remote Sens, 12, (2020); Wang J., Huang R., Guo S., Li L., Zhu M., Yang S., Jiao L., NAS-Guided Lightweight Multiscale Attention Fusion Network for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, (2021); Chen S., Zhan R., Wang W., Zhang J., Learning Slimming SAR Ship Object Detector through Network Pruning and Knowledge Distillation, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 1267-1282, (2021); Li R., Wang X., Wang J., Song Y., Lei L., SAR Target Recognition Based on Efficient Fully Convolutional Attention Block CNN, IEEE Geosci. Remote Sens. Lett, (2020); Qin J., Wang B., Wu Y.L., Lu Q., Zhu H.C., Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms, Remote Sens, 13, (2021); de Alwis Pitts D.A., So E., Enhanced change detection index for disaster response, recovery assessment and monitoring of accessibility and open spaces (camp sites), Int. J. Appl. Earth Obs. Geoinf, 57, pp. 49-60, (2017); Ghaffarian S., Kerle N., Pasolli E., Jokar Arsanjani J., Post-Disaster Building Database Updating Using Automated Deep Learning: An Integration of Pre-Disaster OpenStreetMap and Multi-Temporal Satellite Data, Remote Sens, 11, (2019); Kumar S., Anouncia M., Johnson S., Agarwal A., Dwivedi P., Agriculture change detection model using remote sensing images and GIS: Study area Vellore, Proceedings of the 2012 International Conference on Radar, Communication and Computing (ICRCC), pp. 54-57; Liu Q., Zhou H., Xu Q., Liu X., Wang Y., PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening, IEEE Trans. Geosci. Remote Sens, (2020); Web of Science; Choi Y., Uh Y., Yoo J., Ha J.W., StarGAN v2: Diverse Image Synthesis for Multiple Domains, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8188-8197; Zhang H., Goodfellow I., Metaxas D., Odena A., Self-Attention Generative Adversarial Networks, Proceedings of the 36th International Conference on Machine Learning, pp. 7354-7363","S. Ghaffarian; Information Technology Group, Wageningen University & Research, Wageningen, 6707 KN, Netherlands; email: saman.ghaffarian@wur.nl","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112132537"
"Atik I.","Atik, Ipek (56534061700)","56534061700","Parallel Convolutional Neural Networks and Transfer Learning for Classifying Landforms in Satellite Images","2023","Information Technology and Control","52","1","","228","244","16","0","10.5755/j01.itc.52.1.31779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152662829&doi=10.5755%2fj01.itc.52.1.31779&partnerID=40&md5=4cf58bf577ad6acba8950045690f9a29","Department of Electrical and Electronics Engineering, Gaziantep Islam Science and Technology University, Gaziantep, 27000, Turkey","Atik I., Department of Electrical and Electronics Engineering, Gaziantep Islam Science and Technology University, Gaziantep, 27000, Turkey","The use of remote sensing has great potential for detecting many natural differences, such as disasters, climate changes, and urban changes. Due to technological advances in imaging, remote sensing has become an increas-ingly popular topic. One of the significant benefits of technological advancement has been the ease with which remote sensing data is now accessible. Physical and spatial information is detected by remote sensing, which can be described as the process of identifying distinctive characteristics of an environment. Resolution is one of the most important factors influencing the success of the detection processes. As a result of the resolution be-ing below the necessary level, features of the objects to be differentiated become incomprehensible and there-fore constitute a significant barrier to differentiation. The use of deep learning methods for classifying remote sensing data has become prevalent and successful in recent years. This study classified Satellite images using deep learning and machine learning methods. Based on the transfer learning strategy, a parallel convolutional neural network (CNN) was designed in the study. To improve the feature mapping of an image, convolutional branches use pre-trained knowledge of the transmitted network. Using the offline augmentation method, the raw data set was balanced to overcome its unbalanced class distribution and increased network performance. A total of 35 classes of landforms have been studied in the experiments. The accuracy value of the developed model in the classification study of landforms was 97.84%. According to experimental results, the proposed method provides high classification accuracy in detecting landforms and outperforms existing studies. © 2023, Kauno Technologijos Universitetas. All rights reserved.","Classification; Machine Learning; Remote Sensing; Satellite Imagery; Transfer Learning","","","","","","","","Atik I., Classification of Electronic Components Based on Convolutional Neural Network Architecture, En-ergies, 15, 7, (2022); Atik I., Performance Comparison of Pre-Trained Con-volutional Neural Networks in Flower Image Classifi-cation, European Journal of Science and Technology, 35, pp. 315-321, (2022); Atik I., A New CNN-Based Method for Short-Term Forecasting of Electrical Energy Consumption in the Covid-19 Period: The Case of Turkey, IEEE Access, 10, pp. 22586-22598, (2022); Aydemir C., Classification of Satellite Images with Deep Learning and Machine Learning, (2022); Banerjee B., Bovolo F., Bhattacharya A., Bruzzone L., Chaudhuri S., Mohan B. K., A New Self-Training-Based Unsupervised Satellite Image Classification Technique Using Cluster Ensemble Strategy, IEEE Geoscience and Remote Sensing Letters, 12, 4, pp. 741-745, (2015); Castelluccio M., Poggi G., Sansone C., Verdoliva L., Land Use Classification in Remote Sensing Images by Convolutional Neural Networks; Charou E., Felekis G., Stavroulopoulou D. B., Kout-soukou M., Panagiotopoulou A., Voutos Y., Bratsolis E., Mylonas P., Likforman-Sulem L., Deep Learning for Agricultural Land Detection in Insular Areas, 10th International Conference on Information, Intelligence, Systems and Applications (IISA), pp. 1-4, (2019); Dai D., Yang W., Satellite Image Classification Via Two-layer Sparse Coding with Biased Image Rep-resentation, IEEE Geoscience and Remote Sensing Letters, 8, 1, pp. 173-176, (2010); Deng L., Yu D., Deep Learning: Methods and Applica-tions, Found Trends Signal Process, 7, 3-4, pp. 197-387, (2014); Dogan F., Turkoglu I., Classification of Satellite Images by Deep Learning, (2021); Duarte D., Nex F., Kerle N., Vosselman G., Satellite Image Classification of Building Damages Using Air-borne and Sa℡Lite Image Samples in a Deep Learning Approach, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, IV-2, pp. 89-96, (2018); Fassnacht F. E., Neumann C., Forster M., Budden-baum H., Ghosh A., Clasen A., Joshi P. K., Koch B., Comparison of Feature Reduction Algorithms for Classifying Tree Species with Hyperspectral Data on Three Central European Test Sites, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 7, 6, pp. 2547-2561, (2014); Fausett L., Fundamentals of Neural Networks: Archi-tectures, Algorithms, and Applications, (1994); Guyet T., Nicolas H., Long Term Analysis of Time Se-ries of Satellite Images, Pattern Recognition Letters, 70, (2015); Hinton G., Osindero S., Teh Y.-W., A Fast Learning Algorithm for Deep Belief Nets, Neural Compu-tation, 18, pp. 1527-1554, (2006); Hussain M., Bird J. J., Faria D. R., A Study on CNN Transfer Learning for Image Classification, Advances in Computational Intelligence Systems, Cham, pp. 191-202, (2019); Kadhim M., Abed M., Convolutional Neural Network for Satellite Image Classification, Studies in Computational Intelligence, pp. 165-178, (2020); Krizhevsky A., Sutskever I., Hinton G. E., Imagenet Classification with Deep Convolutional Neural Net-works, Advances in Neural Information Processing Systems, 25, pp. 1097-1105, (2012); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Liu Q., Basu S., Ganguly S., Mukhopadhyay S., DiBi-ano R., Karki M., Nemani R. R., DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification, CoRR, (2019); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Recur-rent Neural Networks to Enhance Satellite Image Classification Maps, CoRR, (2016); Matworks, Pretrained Deep Neural Networks; Michie D., Spiegelhalter D., Taylor C., Machine Learn-ing, Neural and Statistical Classification, Technomet-rics, (1999); Moorthi S. M., Misra I., Kaur R., Darji N. P., Ra-makrishnan R., Kernel Based Learning Approach for Satellite Image Classification Using Support Vector Machine, 2011 IEEE Recent Advances in Intelligent Computational Systems, pp. 107-110, (2011); Nair M., Supervised Techniques and Approaches for Satellite Image Classification, International Journal of Computer Applications, 134, pp. 1-6, (2016); Pritt M., Chern G., Satellite Image Classification with Deep Learning, pp. 1-7, (2017); Rai A. K., Mandal N., Singh A., Singh K. K., Landsat 8 OLI Satellite Image Classification Using Convolutional Neural Network, Procedia Computer Science, 167, pp. 987-993, (2020); Romero A., Gatta C., Camps Valls G., Unsupervised Deep Feature Extraction for Remote Sensing Image Classification, IEEE Transactions on Geoscience and Remote Sensing, 54, pp. 1349-1362, (2016); Schmidhuber J., Deep Learning in Neural Networks: An Overview, CoRR, (2014); Scott G. J., England M., Starms W. A., Marcum R. A., Davis C. H., Training Deep Convolutional Neural Networks for Lan. Cover Classification of High-Res-olution Imagery, I IEEE Transactions on Geoscience and Remote Sensing, 14, pp. 549-553, (2017); Tammina S., Transfer Learning Using VGG-16 with Deep Convolutional Neural Network for Classifying Im-ages, International Journal of Scientific and Research Publication, 9, (2019); Tonyaloglu E. E., Erdogan N., Cavdar B., Kurtsan K., Nurlu E., Comparison of Pixel and Object Based Classification Methods on Rapideye Satellite Image, Turkish Journal of Medical Sciences, 5, 1, pp. 1-11, (2021); Tout K., Yassine H., Jaber M., Improving LULC Classification from Satellite Imagery Using Deep Learning-Eurosat Dataset, (2021); Ucar F., Alcin O., Dandil B., Ata F., A Novel Classification Method Based on One Dimensional Local Binary Patterns and Discrete Wavelet Transform for Power Quality Events, Kahramanmaras Sutcu Imam University Journal of Engineering Sciences, pp. 7-13, (2016); Unnikrishnan A., Deep AlexNet with Reduced Number of Trainable Parameters for Satellite Image Classification, Procedia Computer Sci-ence, 143, pp. 931-938, (2018); Verde N., Mallinis G., Tsakiri-Strati M., Georgiadis C., Patias P., Assessment of Radiometric Resolution Impact on Remote Sensing Data Classification Ac-curacy, Remote Sens, 10, 8, (2018); Voigt S., Kemper T., Riedlinger T., Kiefl R., Scholte K., Mehl H., Satellite Image Analysis for Disaster and Cri-sis-management Support, IEEE Transactions on Geo-science and Remote Sensing, 45, 6, pp. 1520-1528, (2007); Xia G.-S., Hu J., Hu F., Shi B., Bai X., Zhong Y., Zhang L., Lu X., AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification, IEEE Geo-science and Remote Sensing Letters, 55, 7, pp. 3965-3981, (2017); Yamashkin S. A., Yamashkin A. A., Zanozin V. V., Ra-dovanovic M. M., Barmin A. N., Improving the Effi-ciency of Deep Learning Methods in Remote Sensing Data Analysis: Geosystem Approach, IEEE Access, 8, pp. 179516-179529, (2020)","I. Atik; Department of Electrical and Electronics Engineering, Gaziantep Islam Science and Technology University, Gaziantep, 27000, Turkey; email: ipek.atik@gibtu.edu.tr","","Kauno Technologijos Universitetas","","","","","","1392124X","","","","English","Inf. Technol. Control","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85152662829"
"Galodha A.; Vashisht R.; Nidamanuri R.R.; Ramiya A.M.","Galodha, A. (57327277000); Vashisht, R. (57194868813); Nidamanuri, R.R. (35743520500); Ramiya, A.M. (55856164800)","57327277000; 57194868813; 35743520500; 55856164800","DEEP CONVOLUTION NEURAL NETWORKS WITH RESNET ARCHITECTURE FOR SPECTRAL-SPATIAL CLASSIFICATION OF DRONE BORNE AND GROUND BASED HIGH RESOLUTION HYPERSPECTRAL IMAGERY","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B2-2022","","577","584","7","0","10.5194/isprs-archives-XLIII-B2-2022-577-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132045208&doi=10.5194%2fisprs-archives-XLIII-B2-2022-577-2022&partnerID=40&md5=92f7a5750a314ed55b9da8f1bca36410","Department of Earth and Space Sciences, Indian Institute of Space Science and Technology (IIST), Department of Space, DoS, Isro, Trivandrum, India","Galodha A., Department of Earth and Space Sciences, Indian Institute of Space Science and Technology (IIST), Department of Space, DoS, Isro, Trivandrum, India; Vashisht R., Department of Earth and Space Sciences, Indian Institute of Space Science and Technology (IIST), Department of Space, DoS, Isro, Trivandrum, India; Nidamanuri R.R., Department of Earth and Space Sciences, Indian Institute of Space Science and Technology (IIST), Department of Space, DoS, Isro, Trivandrum, India; Ramiya A.M., Department of Earth and Space Sciences, Indian Institute of Space Science and Technology (IIST), Department of Space, DoS, Isro, Trivandrum, India","Drones have been of vital importance in the fields of surveillance, mapping, and infrastructure inspection. Drones have played a vital role in acquiring high-resolution images and with the present need for precision farming, drones have helped in crop classification and monitoring various crop patterns. With the recent advancement in computational power and development of robust algorithms to carry out deep feature learning and neural network, based learning such techniques have regained prominence in contemporary research areas such as classification of common 2-D and 3-D images, object detection, etc. In our research, we propose a deep convolutional neural network architecture (CNN) for the classification of aerial images captured by drones and high-resolution Terrestrial Hyperspectral (THS or HSI) which includes 6-layers and with weights optimized along with the input layer, the convolutional layer, the max-pooling layer, the fully connected layer, softmax probability classifier, and the output layer. We have acquired THS (using Cubert-GmbH data) and drone agricultural data of seasonal crops sowed during the months of March-June for the year 2017. Crop patterns include Cabbage, Eggplant, and Tomato with varying nitrogen concentrations in the region of Bangalore, Southern India. To study the influence and impact of CNN, the ResNets model has been applied. ResNets model and architecture are combined with a deep learning network followed by a recurrent neural learning network model (RCNN). The HSI input layer with corresponding ground truth data for the region is fed into the ResNets model with a spectral and spatial residual network for the 7∗7∗139 input Hyperspectral Imagery (HSI) volume. The network includes two spectral and two spatial residual blocks. An average pooling layer and a fully connected layer transform into a 5∗5∗24 spectral-spatial feature volume further to a single output feature vector. At present we use an RMSProp optimizer for error loss minimization which when applied to the drone data was able to achieve an overall accuracy of 97.16%. Similarly, for cabbage, eggplant and tomato acquired through the same method we achieved overall accuracy at 87.619%, 89.25%, and 80.566% respectively in comparison to ground truth labels. Drones and ground-based datasets equipped with good computational techniques have become promising tools for improving the quality and efficiency of precision agriculture today.  © 2022. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved. ","Convolution Neural Networks (CNN); crop classification; Deep residual networks (ResNet); Hyperspectral image classification (HSI); Precision Agriculture; Unmanned Aerial Vehicle (UAV)","Antennas; Convolution; Convolutional neural networks; Deep neural networks; Drones; Fruits; Hyperspectral imaging; Image classification; Network architecture; Object detection; Precision agriculture; Recurrent neural networks; Remote sensing; Spectroscopy; Convolution neural network; Crop classification; Deep residual network; Hyper-spectral imageries; Hyperspectral image classification; Hyperspectral image classification (hyperspectral imagery); Learning network; Precision Agriculture; Unmanned aerial vehicle; Crops","","","","","","","Acquarelli J., Marchiori E., Buydens L. M., Tran T., van Laarhoven T., Convolutional neural networks and data augmentation for spectral-spatial classification of hyperspectral images, Networks, 16, pp. 21-40, (2017); Bruzzone L., Serpico S. B., A technique for feature selection in multiclass problems, International Journal of Remote Sensing, 21, 3, pp. 549-563, (2000); Chang C. I., Hyperspectral imaging: techniques for spectral detection and classification, 1, (2003); Chen Y., Lin Z., Zhao X., Wang G., Gu Y., Deep learning-based classification of hyperspectral data, IEEE Journal of Selected topics in applied earth observations and remote sensing, 7, 6, pp. 2094-2107, (2014); Ding H., Xu L., Wu Y., Shi W., Classification of hyperspectral images by deep learning of spectral-spatial features, Arabian Journal of Geosciences, 13, 12, pp. 1-14, (2020); Duchi J., Hazan E., Singer Y., Adaptive subgradient methods for online learning and stochastic optimization, Journal of machine learning research, 12, 7, (2011); Grahn H., Geladi P., Techniques and applications of hyperspectral image analysis, (2007); Guo A. J., Zhu F., Spectral-spatial feature extraction and classification by ANN supervised with center loss in hyperspectral imagery, IEEE Transactions on Geoscience and Remote Sensing, 57, 3, pp. 1755-1767, (2018); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, International conference on machine learning, pp. 448-456, (2015); Jiang J., Sun H., Liu X., Ma J., Learning spatial-spectral prior for super-resolution of hyperspectral imagery, IEEE Transactions on Computational Imaging, 6, pp. 1082-1096, (2020); Kuo B. C., Chen I. L., Li C. H., Hung C. C., Combining ensemble technique of support vector machines with the optimal kernel method for hyperspectral image classification, 2011 IEEE International Geoscience and Remote Sensing Symposium, pp. 3903-3906, (2011); Li Y., Zhang H., Shen Q., Spectralspatial classification of hyperspectral imagery with 3D convolutional neural network, Remote Sensing, 9, 1, (2017); Myasnikov E., Evaluation of stochastic gradient descent methods for nonlinear mapping of hyperspectral data, International Conference on Image Analysis and Recognition, pp. 276-283, (2016); Ozdemir A., Polat K., Deep learning applications for hyperspectral imaging: a systematic review, Journal of the Institute of Electronics and Computer, 2, 1, pp. 39-56, (2020); Palsson B., Sigurdsson J., Sveinsson J. R., Ulfarsson M. O., Hyperspectral unmixing using a neural network autoencoder, IEEE Access, 6, pp. 25646-25656, (2018); Petrakos M., Benediktsson J. A., Kanellopoulos I., The effect of classifier agreement on the accuracy of the combined classifier in decision level fusion, IEEE Transactions on Geoscience and Remote Sensing, 39, 11, pp. 2539-2546, (2001); Prasad S., Bruce L. M., Limitations of principal components analysis for hyperspectral target recognition, IEEE Geoscience and Remote Sensing Letters, 5, 4, pp. 625-629, (2008); Yue J., Zhao W., Mao S., Liu H., Spectralspatial classification of hyperspectral images using deep convolutional neural networks, Remote Sensing Letters, 6, 6, pp. 468-477, (2015); Zhu X. X., Tuia D., Mou L., Xia G. S., Zhang L., Xu F., Fraundorfer F., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geoscience and Remote Sensing Magazine, 5, 4, pp. 8-36, (2017)","A. Galodha; Department of Earth and Space Sciences, Indian Institute of Space Science and Technology (IIST), Department of Space, DoS, Isro, Trivandrum, India; email: abhinavgalohda@gmail.com","Yilmaz A.; Wegner J.D.; Qin R.; Remondino F.; Fuse T.; Toschi I.","International Society for Photogrammetry and Remote Sensing","","2022 24th ISPRS Congress on Imaging Today, Foreseeing Tomorrow, Commission II","6 June 2022 through 11 June 2022","Nice","179853","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132045208"
"Berg P.; Pham M.-T.; Courty N.","Berg, Paul (57416048000); Pham, Minh-Tan (56070990300); Courty, Nicolas (23088029900)","57416048000; 56070990300; 23088029900","Self-Supervised Learning for Scene Classification in Remote Sensing: Current State of the Art and Perspectives","2022","Remote Sensing","14","16","3995","","","","1","10.3390/rs14163995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137764587&doi=10.3390%2frs14163995&partnerID=40&md5=896bf350d309ccd0c5d829eae3e5ff11","Institut de Recherche en Informatique et Systèmes Aléatoires (IRISA), Université Bretagne Sud, UMR 6074, Vannes, F-56000, France","Berg P., Institut de Recherche en Informatique et Systèmes Aléatoires (IRISA), Université Bretagne Sud, UMR 6074, Vannes, F-56000, France; Pham M.-T., Institut de Recherche en Informatique et Systèmes Aléatoires (IRISA), Université Bretagne Sud, UMR 6074, Vannes, F-56000, France; Courty N., Institut de Recherche en Informatique et Systèmes Aléatoires (IRISA), Université Bretagne Sud, UMR 6074, Vannes, F-56000, France","Deep learning methods have become an integral part of computer vision and machine learning research by providing significant improvement performed in many tasks such as classification, regression, and detection. These gains have been also observed in the field of remote sensing for Earth observation where most of the state-of-the-art results are now achieved by deep neural networks. However, one downside of these methods is the need for large amounts of annotated data, requiring lots of labor-intensive and expensive human efforts, in particular for specific domains that require expert knowledge such as medical imaging or remote sensing. In order to limit the requirement on data annotations, several self-supervised representation learning methods have been proposed to learn unsupervised image representations that can consequently serve for downstream tasks such as image classification, object detection or semantic segmentation. As a result, self-supervised learning approaches have been considerably adopted in the remote sensing domain within the last few years. In this article, we review the underlying principles developed by various self-supervised methods with a focus on scene classification task. We highlight the main contributions and analyze the experiments, as well as summarize the key conclusions, from each study. We then conduct extensive experiments on two public scene classification datasets to benchmark and evaluate different self-supervised models. Based on comparative results, we investigate the impact of individual augmentations when applied to remote sensing data as well as the use of self-supervised pre-training to boost the classification performance with limited number of labeled samples. We finally underline the current trends and challenges, as well as perspectives of self-supervised scene classification. © 2022 by the authors.","remote sensing; representation learning; scene classification; self-supervised learning","Classification (of information); Deep neural networks; Image representation; Learning systems; Medical imaging; Object detection; Semantic Segmentation; Semantics; Supervised learning; Integral part; Learning methods; Machine learning research; Remote-sensing; Representation learning; Scene classification; Self-supervised learning; Sensing currents; State of the art; Vision learning; Remote sensing","","","","","IDRIS; Grand Équipement National De Calcul Intensif, GENCI","This work is funded by the ANR AI chair OTTOPIA under reference ANR-20-CHIA-0030. It was granted access to the HPC resources of IDRIS under the allocation 202022-AD011013514 made by GENCI.","Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Et al., ImageNet Large Scale Visual Recognition Challenge, Int. J. Comput. Vis, 115, pp. 211-252, (2015); Huh M., Agrawal P., Efros A.A., What makes ImageNet good for transfer learning?, arXiv, (2016); Cheng G., Han J., Lu X., Remote Sensing Image Scene Classification: Benchmark and State of the Art, Proc. IEEE, 105, pp. 1865-1883, (2017); Pal M., Random forest classifier for remote sensing classification, Int. J. Remote Sens, 26, pp. 217-222, (2005); Mountrakis G., Im J., Ogole C., Support vector machines in remote sensing: A review, ISPRS J. Photogramm. Remote Sens, 66, pp. 247-259, (2011); Cheng G., Xie X., Han J., Guo L., Xia G.S., Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportunities, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 3735-3756, (2020); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 1, pp. 886-893; Lowe D., Object recognition from local scale-invariant features, Proceedings of the Seventh IEEE International Conference on Computer Vision, 2, pp. 1150-1157; Sivic, Zisserman, Video Google: A text retrieval approach to object matching in videos, Proceedings of the Ninth IEEE International Conference on Computer Vision, 2, pp. 1470-1477; Sanchez J., Perronnin F., Mensink T., Verbeek J., Image classification with the fisher vector: Theory and practice, Int. J. Comput. Vis, 105, pp. 222-245, (2013); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv, (2020); Neumann M., Pinto A.S., Zhai X., Houlsby N., Training general representations for remote sensing using in-domain knowledge, Proceedings of the IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium, pp. 6730-6733; Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems; Xia G.S., Hu J., Hu F., Shi B., Bai X., Zhong Y., Zhang L., Lu X., AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification, IEEE Trans. Geosci. Remote Sens, 55, pp. 3965-3981, (2017); Li H., Dou X., Tao C., Wu Z., Chen J., Peng J., Deng M., Zhao L., RSI-CB: A large-scale remote sensing image classification benchmark using crowdsourced data, Sensors, 20, (2020); Helber P., Bischke B., Dengel A., Borth D., Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 12, pp. 2217-2226, (2019); Sumbul G., Charfuelan M., Demir B., Markl V., Bigearthnet: A large-scale benchmark archive for remote sensing image understanding, Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5901-5904; Drusch M., Del Bello U., Carlier S., Colin O., Fernandez V., Gascon F., Hoersch B., Isola C., Laberinti P., Martimort P., Et al., Sentinel-2: ESA’s optical high-resolution mission for GMES operational services, Remote Sens. Environ, 120, pp. 25-36, (2012); Kaplan J., McCandlish S., Henighan T., Brown T.B., Chess B., Child R., Gray S., Radford A., Wu J., Amodei D., Scaling laws for neural language models, arXiv, (2020); Bengio Y., Courville A., Vincent P., Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell, 35, pp. 1798-1828, (2013); Qi G.J., Luo J., Small data challenges in big data era: A survey of recent progress on unsupervised and semi-supervised methods, IEEE Trans. Pattern Anal. Mach. Intell, 44, pp. 2168-2187, (2020); Jing L., Tian Y., Self-supervised visual feature learning with deep neural networks: A survey, IEEE Trans. Pattern Anal. Mach. Intell, 43, pp. 4037-4058, (2020); Ohri K., Kumar M., Review on self-supervised image recognition using deep neural networks, Knowl. Based Syst, 224, (2021); Liu X., Zhang F., Hou Z., Mian L., Wang Z., Zhang J., Tang J., Self-supervised learning: Generative or contrastive, IEEE Trans. Knowl. Data Eng, (2021); Vincent P., Larochelle H., Bengio Y., Manzagol P.A., Extracting and composing robust features with denoising autoencoders, Proceedings of the 25th International Conference on Machine Learning, pp. 1096-1103; Kingma D.P., Welling M., Auto-Encoding Variational Bayes, arXiv, (2014); He K., Chen X., Xie S., Li Y., Dollar P., Girshick R., Masked Autoencoders Are Scalable Vision Learners, arXiv, (2021); Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., Generative Adversarial Nets, Advances in Neural Information Processing Systems, 27, (2014); Radford A., Metz L., Chintala S., Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, arXiv, (2015); Doersch C., Gupta A., Efros A.A., Unsupervised Visual Representation Learning by Context Prediction, Proceedings of the International Conference on Computer Vision (ICCV); Zhang R., Isola P., Efros A.A., Colorful Image Colorization, Proceedings of the European Conference on Computer Vision ECCV; Gidaris S., Singh P., Komodakis N., Unsupervised representation learning by predicting image rotations, arXiv, (2018); Noroozi M., Favaro P., Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles, Proceedings of the European Conference on Computer Vision ECCV; Dosovitskiy A., Springenberg J.T., Riedmiller M., Brox T., Discriminative unsupervised feature learning with convolutional neural networks, Adv. Neural Inf. Process. Syst, 27, pp. 766-774, (2014); Jing L., Vincent P., LeCun Y., Tian Y., Understanding dimensional collapse in contrastive self-supervised learning, arXiv, (2021); Dong X., Shen J., Triplet Loss in Siamese Network for Object Tracking, Proceedings of the European Conference on Computer Vision (ECCV); Wu Z., Xiong Y., Yu S.X., Lin D., Unsupervised feature learning via non-parametric instance discrimination, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3733-3742; Chen T., Kornblith S., Norouzi M., Hinton G., A simple framework for contrastive learning of visual representations, Proceedings of the International Conference on Machine Learning PMLR, pp. 1597-1607; He K., Fan H., Wu Y., Xie S., Girshick R., Momentum Contrast for Unsupervised Visual Representation Learning, Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9726-9735; Caron M., Misra I., Mairal J., Goyal P., Bojanowski P., Joulin A., Unsupervised learning of visual features by contrasting cluster assignments, Adv. Neural Inf. Process. Syst, 33, pp. 9912-9924, (2020); Peyre G., Cuturi M., Computational optimal transport: With applications to data science, Found. Trends Mach. Learn, 11, pp. 355-607, (2019); Wang X., Zhang R., Shen C., Kong T., Li L., Dense contrastive learning for self-supervised visual pre-training, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3024-3033; Grill J.B., Strub F., Altche F., Tallec C., Richemond P., Buchatskaya E., Doersch C., Avila Pires B., Guo Z., Gheshlaghi Azar M., Et al., Bootstrap your own latent-a new approach to self-supervised learning, Adv. Neural Inf. Process. Syst, 33, pp. 21271-21284, (2020); Hinton G., Vinyals O., Dean J., Distilling the knowledge in a neural network, arXiv, 2, (2015); Chen X., He K., Exploring simple siamese representation learning, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750-15758; Caron M., Touvron H., Misra I., Jegou H., Mairal J., Bojanowski P., Joulin A., Emerging properties in self-supervised vision transformers, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650-9660; Zbontar J., Jing L., Misra I., LeCun Y., Deny S., Barlow twins: Self-supervised learning via redundancy reduction, Proceedings of the International Conference on Machine Learning, pp. 12310-12320; Saxe A.M., Bansal Y., Dapello J., Advani M., Kolchinsky A., Tracey B.D., Cox D.D., On the information bottleneck theory of deep learning, J. Stat. Mech. Theory Exp, 2019, (2019); Bardes A., Ponce J., LeCun Y., VICReg: Variance-Invariance-Covariance Regularization For Self-Supervised Learning, arXiv, (2022); Krizhevsky A., Learning Multiple Layers of Features from Tiny Images, (2009); Lin D., Fu K., Wang Y., Xu G., Sun X., MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification, IEEE Geosci. Remote Sens. Lett, 14, pp. 2092-2096, (2017); Penatti O.A., Nogueira K., Dos Santos J.A., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 44-51; Stojnic V., Risojevic V., Evaluation of Split-Brain Autoencoders for High-Resolution Remote Sensing Scene Classification, Proceedings of the 2018 International Symposium ELMAR, pp. 67-70; Zhang R., Isola P., Efros A.A., Split-brain autoencoders: Unsupervised learning by cross-channel prediction, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1058-1067; Tao C., Qi J., Lu W., Wang H., Li H., Remote sensing image scene classification with self-supervised paradigm under limited labeled samples, IEEE Geosci. Remote. Sens. Lett, 19, (2020); Zhao Z., Luo Z., Li J., Chen C., Piao Y., When self-supervised learning meets scene classification: Remote sensing scene classification based on a multitask learning framework, Remote Sens, 12, (2020); Xia G.S., Yang W., Delon J., Gousseau Y., Sun H., Maitre H., Structural high-resolution satellite image indexing, Proceedings of the ISPRS TC VII Symposium-100 Years ISPRS, 38, pp. 298-303; Yuan Y., Lin L., Self-Supervised Pretraining of Transformers for Satellite Image Time Series Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 474-487, (2021); Devlin J., Chang M.W., Lee K., Toutanova K., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1, pp. 4171-4186, (2019); Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need, Adv. Neural Inf. Process. Syst, 30, pp. 6000-6010, (2017); Jean N., Wang S., Samar A., Azzari G., Lobell D., Ermon S., Tile2vec: Unsupervised representation learning for spatially distributed data, Proceedings of the AAAI Conference on Artificial Intelligence, 33, pp. 3967-3974; Boryan C., Yang Z., Mueller R., Craig M., Monitoring US agriculture: The US department of agriculture, national agricultural statistics service, cropland data layer program, Geocarto Int, 26, pp. 341-358, (2011); Jung H., Jeon T., Self-supervised learning with randomised layers for remote sensing, Electron. Lett, 57, pp. 249-251, (2021); Stojnic V., Risojevic V., Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding, Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 1182-1191; Ayush K., Uzkent B., Meng C., Tanmay K., Burke M., Lobell D., Ermon S., Geography-aware self-supervised learning, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10181-10190; Chen X., Fan H., Girshick R., He K., Improved baselines with momentum contrastive learning, arXiv, (2020); Manas O., Lacoste A., Giro-i Nieto X., Vazquez D., Rodriguez P., Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data, Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9394-9403; Daudt R.C., Le Saux B., Boulch A., Gousseau Y., Urban change detection for multi-spectral earth observation using convolutional neural networks, Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 2115-2118; Jung H., Oh Y., Jeong S., Lee C., Jeon T., Contrastive Self-Supervised Learning With Smoothed Representation for Remote Sensing, IEEE Geosci. Remote Sens. Lett, 19, (2021); Lam D., Kuzma R., McGee K., Dooley S., Laielli M., Klaric M., Bulatov Y., McCord B., xview: Objects in context in overhead imagery, arXiv, (2018); Tao C., Qia J., Zhang G., Zhu Q., Lu W., Li H., TOV: The Original Vision Model for Optical Remote Sensing Image Understanding via Self-supervised Learning, arXiv, (2022); Miller G.A., WordNet: An Electronic Lexical Database, (1998); Zhou W., Newsam S., Li C., Shao Z., PatternNet: A benchmark dataset for performance evaluation of remote sensing image retrieval, ISPRS J. Photogramm. Remote Sens, 145, pp. 197-209, (2018); Scheibenreif L., Mommert M., Borth D., Contrastive self-supervised data fusion for satellite imagery, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 3, pp. 705-711, (2022); Ebel P., Meraner A., Schmitt M., Zhu X.X., Multisensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery, IEEE Trans. Geosci. Remote. Sens, 59, pp. 5866-5878, (2020); Yokoya N., Ghamisi P., Hansch R., Schmitt M., Report on the 2020 IEEE GRSS data fusion contest-global land cover mapping with weak supervision [technical committees], IEEE Geosci. Remote Sens. Mag, 8, pp. 134-137, (2020); Windsor R., Jamaludin A., Kadir T., Zisserman A., Self-supervised multi-modal alignment for whole body medical imaging, Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 90-101, (2021); Scheibenreif L., Hanna J., Mommert M., Borth D., Self-Supervised Vision Transformers for Land-Cover Segmentation and Classification, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1422-1431; Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo B., Swin transformer: Hierarchical vision transformer using shifted windows, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022; Huang H., Mou Z., Li Y., Li Q., Chen J., Li H., Spatial-temporal Invariant Contrastive Learning for Remote Sensing Scene Classification, IEEE Geosci. Remote Sens. Lett, 19, (2022); Perrot M., Courty N., Flamary R., Habrard A., Mapping estimation for discrete optimal transport, Adv. Neural Inf. Process. Syst, 29, pp. 4204-4212, (2016); Rubner Y., Tomasi C., Guibas L., A metric for distributions with applications to image databases, Proceedings of the Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271), pp. 59-66; Zheng X., Kellenberger B., Gong R., Hajnsek I., Tuia D., Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 732-741; Wang X., Liu Z., Yu S.X., Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination, Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12581-12590; Kellenberger B., Marcos D., Tuia D., Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning, Remote Sens. Environ, 216, pp. 139-153, (2018); Guldenring R., Nalpantidis L., Self-supervised contrastive learning on agricultural images, Comput. Electron. Agric, 191, (2021); He K., Zhang X., Ren S., Sun J., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1026-1034; Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A multiclass weed species image dataset for deep learning, Sci. Rep, 9, pp. 1-12, (2019); Chiu M.T., Xu X., Wei Y., Huang Z., Schwing A.G., Brunner R., Khachatrian H., Karapetyan H., Dozier I., Rose G., Et al., Agriculture-vision: A large aerial image database for agricultural pattern analysis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2828-2838; Risojevic V., Stojnic V., The role of pre-training in high-resolution remote sensing scene classification, arXiv, (2021); Guo D., Xia Y., Luo X., Self-supervised GANs with similarity loss for remote sensing image scene classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 2508-2521, (2021); Chen T., Zhai X., Ritter M., Lucic M., Houlsby N., Self-supervised gans via auxiliary rotation loss, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12154-12163; Zhang H., Goodfellow I., Metaxas D., Odena A., Self-attention generative adversarial networks, Proceedings of the International Conference on Machine Learning, pp. 7354-7363; Jain P., Schoen-Phelan B., Ross R., Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images, arXiv, (2022); Wang Y., Albrecht C.M., Zhu X.X., Self-supervised Vision Transformers for Joint SAR-optical Representation Learning, arXiv, (2022); Sumbul G., De Wall A., Kreuziger T., Marcelino F., Costa H., Benevides P., Caetano M., Demir B., Markl V., BigEarthNet-MM: A Large-Scale, Multimodal, Multilabel Benchmark Archive for Remote Sensing Image Classification and Retrieval [Software and Datasets], IEEE Geosci. Remote Sens. Mag, 9, pp. 174-180, (2021); Goodfellow I.J., Shlens J., Szegedy C., Explaining and harnessing adversarial examples, arXiv, (2014); Jiang Z., Chen T., Chen T., Wang Z., Robust pre-training by adversarial contrastive learning, Adv. Neural Inf. Process. Syst, 33, pp. 16199-16210, (2020); Kim M., Tack J., Hwang S.J., Adversarial self-supervised contrastive learning, Adv. Neural Inf. Process. Syst, 33, pp. 2983-2994, (2020); Xu Y., Sun H., Chen J., Lei L., Kuang G., Ji K., Robust remote sensing scene classification by adversarial self-supervised learning, Proceedings of the 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, pp. 4936-4939; Patel C., Sharma S., Gulshan V., Evaluating Self and Semi-Supervised Methods for Remote Sensing Segmentation Tasks, arXiv, (2021); Wang Y., Albrecht C.M., Braham N.A.A., Mou L., Zhu X.X., Self-supervised Learning in Remote Sensing: A Review, arXiv, (2022); Neumann M., Pinto A.S., Zhai X., Houlsby N., In-domain representation learning for remote sensing, arXiv, (2019); Cohen J., A coefficient of agreement for nominal scales, Educ. Psychol. Meas, 20, pp. 37-46, (1960); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, Proceedings of the International Conference on Machine Learning, pp. 448-456; Paszke A., Gross S., Massa F., Lerer A., Bradbury J., Chanan G., Killeen T., Lin Z., Gimelshein N., Antiga L., Et al., Pytorch: An imperative style, high-performance deep learning library, Adv. Neural Inf. Process. Syst, 32, pp. 8026-8037, (2019); Hinton G.E., Roweis S., Stochastic neighbor embedding, Adv. Neural Inf. Process. Syst, 15, pp. 857-864, (2002); Shen K., Jones R., Kumar A., Xie S.M., HaoChen J.Z., Ma T., Liang P., Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation, Proceedings of the International Conference on Machine Learning","P. Berg; Institut de Recherche en Informatique et Systèmes Aléatoires (IRISA), Université Bretagne Sud, UMR 6074, Vannes, F-56000, France; email: paul.berg@univ-ubs.fr","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85137764587"
"Bouraya S.; Belangour A.","Bouraya, Sara (57225175868); Belangour, Abdessamad (57200278748)","57225175868; 57200278748","Approaches to video real time multi-object tracking and object detection: A survey","2021","International Symposium on Image and Signal Processing and Analysis, ISPA","2021-September","","","145","151","6","1","10.1109/ISPA52656.2021.9552095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117031172&doi=10.1109%2fISPA52656.2021.9552095&partnerID=40&md5=3ae371fd1460c87c9f5363002a3f50c1","Ben m'Sik University Hassan the Second, LTIM Laboratory Faculty of Science, Casablanca, Morocco","Bouraya S., Ben m'Sik University Hassan the Second, LTIM Laboratory Faculty of Science, Casablanca, Morocco; Belangour A., Ben m'Sik University Hassan the Second, LTIM Laboratory Faculty of Science, Casablanca, Morocco","The world is living a major shift from information era to artificial intelligence (AI) era. Machines are giving the ability to sense the surrounding world and to take decisions. Computer vision and especially multi-object tracking(MOT), which relies on Deep Learning, is at the heart of this shift. Indeed, with the growth of deep learning, the methods and algorithms that are tackling this problem have gained better performance from the integration of deep learning models. Deep Learning has been demonstrated as MOT, which tackles the challenges of in-and-out objects, unlabeled data, confusing appearance and occlusion. Deep learning, which relied on MOT techniques, has recently gained a fast ground from representation learning to modelling the networks thanks to the advancement of deep learning hypothesis and benchmark arrangement. This paper sums up and analyzes deep learning based MOT techniques which are at a highest level. The paper also offers a comprehensive review about the different techniques applied in MOT of deep learning based on different methods. Furthermore, this study analyzes the benefits and the constraints of current strategies, techniques and methods.  © 2021 IEEE.","CNN; Convolutional Neural Networks; Deep Learning; Multi Object Tracking; Recurrent neural networks; Video Tracking","Object detection; Object recognition; Recurrent neural networks; Tracking (position); CNN; Convolutional neural network; Deep learning; Information eras; Learning models; Multi-object tracking; Performance; Tracking techniques; Video real-time; Video-tracking; Convolutional neural networks","","","","","","","Sushmitha S., Satheesh N., Kanchana V., Multiple car detection, recognition and tracking in traffic, 2020 Int. Conf. Emerg. Technol. INCET 2020, pp. 1-5, (2020); Sudha D., Priyadarshini J., An intelligent multiple vehicle detection and tracking using modified vibe algorithm and deep learning algorithm, Soft Comput., 24, 22, pp. 17417-17429, (2020); Mohamed H.E.D., Et al., MSR-YOLO: Method to enhance fish detection and tracking in fish farms, Procedia Comput. Sci., 170, 2019, pp. 539-546, (2020); Fernandes A., Baptista M., Fernandes L., Chaves P., Drone, aircraft and bird identification in video images using object tracking and residual neural networks, Proc. 11th Int. Conf. Electron. Comput. Artif. Intell. ECAI 2019, (2019); Ellouze A., Ksantini M., Delmotte F., Karray M., Multiple object tracking: Case of aircraft detection and tracking, 16th Int. Multi- Conference Syst. Signals Devices, SSD 2019, pp. 473-478, (2019); Yoganandhan A., Subhash S.D., Jothi J.H., Mohanavel V., Fundamentals and development of self-driving cars, Mater. Today Proc., 33, 40, pp. 3303-3310, (2020); Paravarzar S., Mohammad B., Motion Prediction on Self-driving Cars: A Review, pp. 1-9, (2020); Mohanapriya D., Mahesh K., An Efficient Framework for Object Tracking in Video Surveillance, (2020); Jha S., Seo C., Yang E., Joshi G.P., Real time object detection and trackingsystem for video surveillance system, Multimed. Tools Appl., 80, 3, pp. 3981-3996, (2021); Host K., Ivasic-Kos M., Pobar M., Tracking handball players with the Deepsort algorithm, ICPRAM 2020 - Proc. 9th Int. Conf. Pattern Recognit. Appl. Methods, pp. 593-599, (2020); Zhang R., Wu L., Yang Y., Wu W., Chen Y., Xu M., Multicamera multi-player tracking with deep player identification in sports video, Pattern Recognit., 102, (2020); Garcia-Fernandez A.F., Svensson L., Williams J.L., Xia Y., Granstrom K., Trajectory poisson multi-bernoulli filters, IEEE Trans. Signal Process., 68, pp. 4933-4945, (2020); Kim H.-U., Koh Y.J., Kim C.-S., Online multiple object tracking based on open-set few-shot learning, IEEE Access, 8, pp. 190312-190326, (2020); Rathnayake T., Gostar A.K., Hoseinnezhad R., Tennakoon R., Bab-Hadiashar A., On-line visual tracking with occlusion handling, Sensors (Switzerland), 20, 3, pp. 1-23, (2020); Huang Y., Essa I., Tracking Multiple Objects through Occlusions 2 . Related Work; Ma Y., Liu Y., Liu S., Zhang Z., Multiple object detection and tracking in complex background, Int. J. Pattern Recognit. Artif. Intell., 31, 2, pp. 1-20, (2017); Asha C.S., Narasimhadhan A.V., A Comparative study of illumination invariant techniques in video tracking perspective, IETE Tech. Rev. (Institution Electron. Telecommun. Eng. India), 37, 4, pp. 353-364, (2020); Ciaparrone G., Luque S.F., Tabik S., Troiano L., Tagliaferri R., Herrera F., Deep learning in video multi-object tracking: A survey, Neurocomputing, 381, pp. 61-88, (2020); Zhou S., Ke M., Qiu J., Wang J., A Survey of Multi-object Video Tracking Algorithms, 842, (2019); Thoreau M., Kottege N., Deep similarity metric learning for realtime pedestrian tracking, Australas. Conf. Robot. Autom. ACRA, 2019, (2019); Wen L., Et al., UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking, Comput. Vis. Image Underst., 193, pp. 1-27, (2020); Ramesh R., Mahesh K., A Performance analysis of pre-trained neural network and design of CNN for sports video classification, Proc. 2020 IEEE Int. Conf. Commun. Signal Process. ICCSP 2020, pp. 213-216, (2020); Jing L., Parag T., Wu Z., Tian Y., Wang H., VideoSSL: Semisupervised Learning for Video Classification, pp. 1110-1119, (2020); Perez-Hernandez F., Tabik S., Lamas A., Olmos R., Fujita H., Herrera F., Object Detection Binary Classifiers methodology based on deep learning to identify small objects handled similarly: Application in video surveillance, Knowledge-Based Syst., 194, 40, (2020); Jyotsna C., Amudha J., Detection from Surveillance Video, pp. 335-339, (2020); Sherstinsky A., Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network, Phys. D Nonlinear Phenom., 404, pp. 1-43, (2020); Suleiman D., Awajan A., Deep learning based abstractive text summarization: Approaches, datasets, evaluation measures, and challenges, Math. Probl. Eng., 2020, (2020); Luo W., Et al., Multiple Object Tracking: A Literature Review, pp. 1-18, (2014); Camplani M., Et al., Multiple human tracking in RGB-depth data: A survey, IET Comput. Vis., 11, 4, pp. 265-285, (2017); Emami P., Pardalos P.M., Elefteriadou L., Ranka S., Machine learning methods for data association in multi-object tracking, ACM Comput. Surv., 53, 4, pp. 1-33, (2020); Leal-Taixe L., Milan A., Schindler K., Cremers D., Reid I., Roth S., Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking, (2017); Leal-Taixe L., Milan A., Reid I., Roth S., Schindler K., MOT Challenge 2015: Towards A Benchmark for Multi-Target Tracking, pp. 1-15, (2015); Milan A., Leal-Taixe L., Reid I., Roth S., Schindler K., MOT16: A Benchmark for Multi-Object Tracking, pp. 1-12, (2016); Ciaparrone G., Luque S.F., Tabik S., Troiano L., Tagliaferri R., Herrera F., Deep learning in video multi-object tracking: A survey, Neurocomputing, 381, pp. 61-88, (2020); Dendorfer P., Et al., MOT Challenge: A Benchmark for Single-camera Multiple Target Tracking, (2020); Dendorfer P., Et al., CVPR19 Tracking and Detection Challenge : How Crowded Could It Get ?, pp. 1-7; Dendorfer P., Et al., MOT20: A Benchmark for Multi Object Tracking in Crowded Scenes, pp. 1-7, (2020); Wang Y.-Q., An analysis of the viola-jones face detection algorithm, Image Process. Line, 4, pp. 128-148, (2014); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real- time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Redmon J., Divvala S., Girshick R., Farhadi A., You only Look Once: Unified, real-time object detection; Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, IEEE Trans. Pattern Anal. Mach. Intell., 42, 2, pp. 318-327, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., pp. 580-587, (2014); Girshick R., Fast R-CNN, Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1440-1448, (2015); Erhan D., Szegedy C., Toshev A., Anguelov D., Scalable Object Detection Using Deep Neural Networks; Li X., Lai T., Wang S., Chen Q., Yang C., Chen R., Weighted feature pyramid networks for object detection, Proc. - 2019 IEEE Intl Conf Parallel Distrib. Process. With Appl. Big Data Cloud Comput. Sustain. Comput. Commun. Soc. Comput. Networking, ISPA/BDCloud/SustainCom/SocialCom 2019, pp. 1500-1504, (2019); Shrivastava A., Training Region-based Object Detectors with Online Hard Example Mining; Shrivastava A., Beyond Skip Connections: Top-Down Modulation for Object Detection; He K., Sun J., Deep Residual Learning for Image Recognition, pp. 1-9; He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, IEEE Trans. Pattern Anal. Mach. Intell., 42, 2, pp. 386-397, (2020); Zhao H., Shi J., Qi X., Wang X., Jia J., Pyramid scene parsing network, Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, 2017, pp. 6230-6239, (2017); Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S., End-to-End object detection with transformers, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), Vol 12346 LNCS, pp. 213-229, (2020); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., Lecun Y., Overfeat: Integrated recognition, localization and detection using convolutional networks, 2nd Int. Conf. Learn. Represent. ICLR 2014 - Conf. Track Proc.., (2014); Liu W., Et al., SSD: Single shot multibox detector, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), Vol 9905 LNCS, pp. 21-37, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, 2017, pp. 6517-6525, (2017); Redmon J., Farhadi A., Tech Rep., pp. 1-6, (2018); Huang J., Sun C., Murphy K., Guadarrama S., Speed / Accuracy Trade-offs for Modern Convolutional Object Detectors, pp. 7310-7319; Shafiee M.J., Chywl B., Li F., Wong A., Fast YOLO: A Fast You only Look Once System for Real-Time Embedded Object Detection in Video, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770-778, (2016); Redmon J., [YOLOv3]YOLOv3: An Incremental Improvement, (2017); Soleimanitaleb Z., Keyvanrad M.A., Jafari A., Object tracking methods: A review, 2019 9th Int. Conf. Comput. Knowl. Eng. ICCKE 2019, pp. 282-288, (2019); Abdennour S., Hicham T., Parallel particle filters for multiple target tracking, Int. Arab J. Inf. Technol., 13, 6, pp. 980-987, (2016); Li X., Wang K., Wang W., Li Y., A Multiple Object Tracking method using Kalman filter, 2010 IEEE Int. Conf. Inf. Autom. ICIA 2010, 1, 1, pp. 1862-1866, (2010); Najafzadeh N., Fotouhi M., Kasaei S., Object Tracking Using Kalman Filter with Adaptive Sampled Histogram, pp. 781-786, (2015); Santosh D.H., Mohan P.G.K., Multiple objects tracking using Extended Kalman Filter, GMM and Mean Shift Algorithm-A comparative study, Proc. 2014 IEEE Int. Conf. Adv. Commun. Control Comput. Technol. ICACCCT 2014, 978, pp. 1484-1488, (2015); Zhao Z., Yu S., Wu X., Wang C., Xu Y., A multi-target tracking algorithm using texture for real-time surveillance, 2008 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2008, pp. 2150-2155, (2009); Gunjal P.R., Gunjal B.R., Shinde H.A., Vanam S.M., Aher S.S., Moving object tracking using kalman filter, 2018 Int. Conf. Adv. Commun. Comput. Technol. ICACCT 2018, pp. 544-547, (2018); Son J., Jung I., Park K., Han B., Tracking-by-segmentation with online gradient boosting decision tree, Proc. IEEE Int. Conf. Comput. Vis. Vol 2015 Inter, pp. 3056-3064, (2015); Gu C., Lee M.C., Semiautomatic segmentation and tracking of semantic video objects, IEEE Trans. Circuits Syst. Video Technol., 8, 5, pp. 572-584, (1998); Schubert F., Casaburo D., Dickmanns D., Belagiannis V., Revisiting robust visual tracking using pixel-wise posteriors, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), 9163, pp. 275-288, (2015); Bibby C., Reid I., Robust real-time visual tracking using pixelwise posteriors, Lect. Notes Comput. Sci. (Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), Vol 5303 LNCS, 2, pp. 831-844, (2008); Pellegrini S., Ess A., Schindler K., Van Gool L., You'll never walk alone: Modeling social behavior for multi-target tracking, Proc. IEEE Int. Conf. Comput. Vis., pp. 261-268, (2009); Zhao A., Robust histogram-based object tracking in image sequences, Proc. - Digit. Image Comput. Tech. Appl. 9th Bienn. Conf. Aust. Pattern Recognit. Soc. DICTA 2007, pp. 45-52, (2007)","","Petkovic T.; University of Zagreb, Unska 3, Zagreb; Petrinovic D.; University of Zagreb, Unska 3, Zagreb; Loncaric S.; University of Zagreb, Unska 3, Zagreb","IEEE Computer Society","","12th International Symposium on Image and Signal Processing and Analysis, ISPA 2021","13 September 2021 through 15 September 2021","Virtual, Zagreb","172371","18455921","978-166542639-8","","","English","Int. Symp. Image Signal Process. Anal., ISPA","Conference paper","Final","","Scopus","2-s2.0-85117031172"
"Sa I.; Lim J.; Ahn H.; Macdonald B.","Sa, Inkyu (16643843300); Lim, Jongyoon (57197870753); Ahn, Hoseok (56218250500); Macdonald, Bruce (7101734705)","16643843300; 57197870753; 56218250500; 7101734705","deepNIR: Datasets for Generating Synthetic NIR Images and Improved Fruit Detection System Using Deep Learning Techniques","2022","Sensors","22","13","4721","","","","5","10.3390/s22134721","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132387935&doi=10.3390%2fs22134721&partnerID=40&md5=eb3bc91107c2dff0e79bc8aa5e459bfe","CSIRO Data61, Robot Perception Team, Robotics and Autonomous Systems Group, Brisbane, 4069, Australia; CARES, Department of Electrical, Computer and Software Engineering, University of Auckland, Auckland, 1010, New Zealand","Sa I., CSIRO Data61, Robot Perception Team, Robotics and Autonomous Systems Group, Brisbane, 4069, Australia; Lim J., CARES, Department of Electrical, Computer and Software Engineering, University of Auckland, Auckland, 1010, New Zealand; Ahn H., CARES, Department of Electrical, Computer and Software Engineering, University of Auckland, Auckland, 1010, New Zealand; Macdonald B., CARES, Department of Electrical, Computer and Software Engineering, University of Auckland, Auckland, 1010, New Zealand","This paper presents datasets utilised for synthetic near-infrared (NIR) image generation and bounding-box level fruit detection systems. A high-quality dataset is one of the essential building blocks that can lead to success in model generalisation and the deployment of data-driven deep neural networks. In particular, synthetic data generation tasks often require more training samples than other supervised approaches. Therefore, in this paper, we share the NIR+RGB datasets that are re-processed from two public datasets (i.e., nirscene and SEN12MS), expanded our previous study, deepFruits, and our novel NIR+RGB sweet pepper (capsicum) dataset. We oversampled from the original nirscene dataset at 10, 100, 200, and 400 ratios that yielded a total of 127 k pairs of images. From the SEN12MS satellite multispectral dataset, we selected Summer (45 k) and All seasons (180 k) subsets and applied a simple yet important conversion: digital number (DN) to pixel value conversion followed by image standardisation. Our sweet pepper dataset consists of 1615 pairs of NIR+RGB images that were collected from commercial farms. We quantitatively and qualitatively demonstrate that these NIR+RGB datasets are sufficient to be used for synthetic NIR image generation. We achieved Frechet inception distances (FIDs) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper datasets, respectively. In addition, we release manual annotations of 11 fruit bounding boxes that can be exported in various formats using cloud service. Four newly added fruits (blueberry, cherry, kiwi and wheat) compound 11 novel bounding box datasets on top of our previous work presented in the deepFruits project (apple, avocado, capsicum, mango, orange, rockmelon and strawberry). The total number of bounding box instances of the dataset is 162 k and it is ready to use from a cloud service. For the evaluation of the dataset, Yolov5 single stage detector is exploited and reported impressive mean-average-precision, mAP[0.5:0.95] results of min:0.49, max:0.812. We hope these datasets are useful and serve as a baseline for future studies. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","dataset; fruit detection; generative adversarial network; object detection; synthetic infrared image generation","Capsicum; Deep Learning; Fruit; Neural Networks, Computer; Deep neural networks; Distributed database systems; Fruits; Image enhancement; Image segmentation; Infrared devices; Infrared imaging; Learning systems; Object detection; Object recognition; Bounding-box; Dataset; Fruit detection; Image generations; Near Infrared; Near- infrared images; Near-infrared; Objects detection; Sweet pepper; Synthetic infrared image generation; fruit; pepper; Generative adversarial networks","","","","","","","Sun P., Kretzschmar H., Dotiwalla X., Chouard A., Patnaik V., Tsui P., Guo J., Zhou Y., Chai Y., Caine B., Et al., Scalability in Perception for Autonomous Driving: Waymo Open Dataset, (2019); Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I., Attention is all you need, Adv. Neural Inf. Process. Syst, pp. 6000-6010, (2017); Korshunov P., Marcel S., DeepFakes: A New Threat to Face Recognition?, Assessment and Detection, (2018); Jumper J., Evans R., Pritzel A., Green T., Figurnov M., Ronneberger O., Tunyasuvunakool K., Bates R., Zidek A., Potapenko A., Et al., Highly accurate protein structure prediction with AlphaFold, Nature, 596, pp. 583-589, (2021); Degrave J., Felici F., Buchli J., Neunert M., Tracey B., Carpanese F., Ewalds T., Hafner R., Abdolmaleki A., de Las Casas D., Et al., Magnetic control of tokamak plasmas through deep reinforcement learning, Nature, 602, pp. 414-419, (2022); Rouse J.W., Haas R.H., Schell J.A., Deering D.W., Monitoring the Vernal Advancement and Retrogradation (Green Wave Effect) of Natural Vegetation, (1973); An L., Zhao J., Di H., Generating infrared image from visible image using Generative Adversarial Networks, Proceedings of the 2019 IEEE International Conference on Unmanned Systems (ICUS), (2019); Yuan X., Tian J., Reinartz P., Generating artificial near infrared spectral band from RGB image using conditional generative adversarial network, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 3, pp. 279-285, (2020); Bhat N., Saggu N., Pragati, Kumar S., Generating Visible Spectrum Images from Thermal Infrared using Conditional Generative Adversarial Networks, Proceedings of the 2020 5th International Conference on Communication and Electronics Systems (ICCES), pp. 1390-1394, (2020); Saxena A., Chung S., Ng A., Learning depth from single monocular images, Adv. Neural Inf. Process. Syst, 18, pp. 1161-1168, (2005); Zheng C., Cham T.J., Cai J., T2net: Synthetic-to-realistic translation for solving single-image depth estimation tasks, Proceedings of the European Conference on Computer Vision (ECCV), pp. 767-783, (2018); Sa I., deepNIR, (2022); Brown M., Susstrunk S., Multi-spectral SIFT for scene category recognition, Proceedings of the CVPR, pp. 177-184, (2011); Schmitt M., Hughes L.H., Qiu C., Zhu X.X., SEN12MS—A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion, (2019); Sa I., Ge Z., Dayoub F., Upcroft B., Perez T., McCool C., DeepFruits: A Fruit Detection System Using Deep Neural Networks, Sensors, 16, (2016); Chebrolu N., Lottes P., Schaefer A., Winterhalter W., Burgard W., Stachniss C., Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields, Int. J. Rob. Res, 36, pp. 1045-1052, (2017); Sa I., Chen Z., Popovic M., Khanna R., Liebisch F., Nieto J., Siegwart R., weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming, IEEE Robot. Autom. Lett, 3, pp. 588-595, (2018); Sa I., Popovic M., Khanna R., Chen Z., Lottes P., Liebisch F., Nieto J., Stachniss C., Walter A., Siegwart R., WeedMap: A Large-Scale Semantic Weed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming, Remote Sens, 10, (2018); Sa I., McCool C., Lehnert C., Perez T., On Visual Detection of Highly-occluded Objects for Harvesting Automation in Horticulture, Proceedings of the ICRA, (2015); Di Cicco M., Potena C., Grisetti G., Pretto A., Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection, (2016); Sa I., Lehnert C., English A., McCool C., Dayoub F., Upcroft B., Perez T., Peduncle detection of sweet pepper for autonomous crop harvesting—Combined Color and 3-D Information, IEEE Robot. Autom. Lett, 2, pp. 765-772, (2017); Lehnert C., Sa I., McCool C., Upcroft B., Perez T., Sweet pepper pose detection and grasping for automated crop harvesting, Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 2428-2434, (2016); Lehnert C., McCool C., Sa I., Perez T., Performance improvements of a sweet pepper harvesting robot in protected cropping environments, J. Field Robot, 37, pp. 1197-1223, (2020); McCool C., Sa I., Dayoub F., Lehnert C., Perez T., Upcroft B., Visual detection of occluded crop: For automated harvesting, Proceedings of the 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 2506-2512, (2016); Haug S., Ostermann J., A Crop/Weed Field Image Dataset for the Evaluation of Computer Vision Based Precision Agriculture Tasks, Proceedings of the Computer Vision—ECCV 2014 Workshops, pp. 105-116, (2015); Segarra J., Buchaillot M.L., Araus J.L., Kefauver S.C., Remote Sensing for Precision Agriculture: Sentinel-2 Improved Features and Applications, Agronomy, 10, (2020); Goodfellow I.J., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., Generative Adversarial Networks, (2014); Mirza M., Osindero S., Conditional Generative Adversarial Nets, (2014); Isola P., Zhu J.Y., Zhou T., Efros A.A., Image-to-image translation with conditional adversarial networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125-1134, (2017); Wang T.C., Liu M.Y., Zhu J.Y., Tao A., Kautz J., Catanzaro B., High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2018); Karras T., Aittala M., Hellsten J., Laine S., Lehtinen J., Aila T., Training Generative Adversarial Networks with Limited Data, Proceedings of the NeurIPS, (2020); Zhu J.Y., Park T., Isola P., Efros A.A., Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), (2017); Schonfeld E., Sushko V., Zhang D., Gall J., Schiele B., Khoreva A., You Only Need Adversarial Supervision for Semantic Image Synthesis, Proceedings of the International Conference on Learning Representations, (2021); Aslahishahri M., Stanley K.G., Duddu H., Shirtliffe S., Vail S., Bett K., Pozniak C., Stavness I., From RGB to NIR: Predicting of near infrared reflectance from visible spectrum aerial images of crops, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1312-1322, (2021); Berg A., Ahlberg J., Felsberg M., Generating visible spectrum images from thermal infrared, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1143-1152, (2018); Li Q., Lu L., Li Z., Wu W., Liu Z., Jeon G., Yang X., Coupled GAN with relativistic discriminators for infrared and visible images fusion, IEEE Sens. J, 21, pp. 7458-7467, (2021); Ma J., Yu W., Liang P., Li C., Jiang J., FusionGAN: A generative adversarial network for infrared and visible image fusion, Inf. Fusion, 48, pp. 11-26, (2019); Ma Z., Wang F., Wang W., Zhong Y., Dai H., Deep learning for in vivo near-infrared imaging, Proc. Natl. Acad. Sci. USA, 118, (2021); Welander P., Karlsson S., Eklund A., Generative Adversarial Networks for Image-to-Image Translation on Multi-Contrast MR Images-A Comparison of CycleGAN and UNIT, (2018); Liu M.Y., Breuel T., Kautz J., Unsupervised Image-to-Image Translation Networks, (2017); Soni R., Arora T., A review of the techniques of images using GAN, Gener. Advers. Netw.-Image-Image Transl, 5, pp. 99-123, (2021); Fawakherji M., Potena C., Pretto A., Bloisi D.D., Nardi D., Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming, Rob. Auton. Syst, 146, (2021); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Adv. Neural Inf. Process. Syst, 28, pp. 91-99, (2015); Jocher G., Yolov5, (2021); Ge Z., Liu S., Wang F., Li Z., Sun J., YOLOX: Exceeding YOLO Series in 2021, (2021); Yang H., Deng R., Lu Y., Zhu Z., Chen Y., Roland J.T., Lu L., Landman B.A., Fogo A.B., Huo Y., CircleNet: Anchor-free Glomerulus Detection with Circle Representation, Med. Image Comput. Comput. Assist. Interv, 2020, pp. 35-44, (2020); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, (2017); Carion N., Massa F., Synnaeve G., Usunier N., Kirillov A., Zagoruyko S., End-to-End Object Detection with Transformers, Computer Vision—ECCV 2020, pp. 213-229, (2020); Chen K., Wang J., Pang J., Cao Y., Xiong Y., Li X., Sun S., Feng W., Liu Z., Xu J., Et al., MMDetection: Open MMLab Detection Toolbox and Benchmark, (2019); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietikainen M., Deep Learning for Generic Object Detection: A Survey, Int. J. Comput. Vis, 128, pp. 261-318, (2020); Buslaev A., Iglovikov V.I., Khvedchenya E., Parinov A., Druzhinin M., Kalinin A.A., Albumentations: Fast and Flexible Image Augmentations, Information, 11, (2020); Shorten C., Khoshgoftaar T.M., A survey on Image Data Augmentation for Deep Learning, J. Big Data, 6, (2019); Xie Q., Luong M.T., Hovy E., Le Q.V., Self-training with Noisy Student improves ImageNet classification, (2019); Karras T., Laine S., Aila T., A Style-Based Generator Architecture for Generative Adversarial Networks, IEEE Trans. Pattern Anal. Mach. Intell, 43, pp. 4217-4228, (2021); Radford A., Metz L., Chintala S., Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, (2015); Akyon F.C., Altinuc S.O., Temizel A., Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection, (2022); Wu Y., Kirillov A., Massa F., Lo W.Y., Girshick R., Detectron2, (2019); Wang C.Y., Liao H.Y.M., Yeh I.H., Wu Y.H., Chen P.Y., Hsieh J.W., CSPNet: A New Backbone that can Enhance Learning Capability of CNN, (2019); He K., Zhang X., Ren S., Sun J., Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, IEEE Trans. Pattern Anal. Mach. Intell, 37, pp. 1904-1916, (2015); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature Pyramid Networks for Object Detection, (2016); Liu S., Qi L., Qin H., Shi J., Jia J., Path Aggregation Network for Instance Segmentation, (2018); Rezatofighi H., Tsoi N., Gwak J., Sadeghian A., Reid I., Savarese S., Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression, (2019); Borji A., Pros and Cons of GAN Evaluation Measures, (2018); Padilla R., Netto S.L., da Silva E.A.B., A Survey on Performance Metrics for Object-Detection Algorithms, Proceedings of the 2020 International Conference on Systems, Signals and Image Processing (IWSSIP), pp. 237-242, (2020); Lin T.Y., Maire M., Belongie S., Bourdev L., Girshick R., Hays J., Perona P., Ramanan D., Lawrence Zitnick C., Dollar P., Microsoft COCO: Common Objects in Context, (2014); Birodkar V., Mobahi H., Bengio S., Semantic Redundancies in Image-Classification Datasets: The 10% You Don’t Need, (2019); Kodali N., Abernethy J., Hays J., Kira Z., On Convergence and Stability of GANs, (2017)","I. Sa; CSIRO Data61, Robot Perception Team, Robotics and Autonomous Systems Group, Brisbane, 4069, Australia; email: inkyu.sa@csiro.au","","MDPI","","","","","","14248220","","","35808218","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132387935"
"Zhang G.; Gao X.; Ran S.; Yang Y.; Li L.; Zhang Y.","Zhang, Guangbin (57226368168); Gao, Xianjun (55576939500); Ran, Shuhao (57217232907); Yang, Yuanwei (56261201600); Li, Lishan (58116926700); Zhang, Yan (57205360357)","57226368168; 55576939500; 57217232907; 56261201600; 58116926700; 57205360357","Accurate and lightweight cloud detection method based on cloud and snow coexistence region of high-resolution remote sensing images; [高分遥感影像云雪共存区轻量云高精度检测方法]","2023","Cehui Xuebao/Acta Geodaetica et Cartographica Sinica","52","1","","93","107","14","0","10.11947/j.AGCS.2023.20210686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148858297&doi=10.11947%2fj.AGCS.2023.20210686&partnerID=40&md5=6d9a8b151d1e54a319b009e4fa111c7e","School of Geosciences, Yangtze University, Wuhan, 430100, China; Key Laboratory of Mine Environmental Monitoring and Improving Around Poyang Lake, Ministry of Natural Resources, Nanchang, 330013, China; Hunan Provincial Key Laboratory of Geo-Information Engineering in Surveying, Hunan University of Science and Technology, Mapping and Remote Sensing, Xiangtan, 411201, China; Beijing Key Laboratory of Urban Spatial Information Engineering, Beijing, 100045, China","Zhang G., School of Geosciences, Yangtze University, Wuhan, 430100, China; Gao X., School of Geosciences, Yangtze University, Wuhan, 430100, China, Key Laboratory of Mine Environmental Monitoring and Improving Around Poyang Lake, Ministry of Natural Resources, Nanchang, 330013, China; Ran S., School of Geosciences, Yangtze University, Wuhan, 430100, China; Yang Y., School of Geosciences, Yangtze University, Wuhan, 430100, China, Hunan Provincial Key Laboratory of Geo-Information Engineering in Surveying, Hunan University of Science and Technology, Mapping and Remote Sensing, Xiangtan, 411201, China, Beijing Key Laboratory of Urban Spatial Information Engineering, Beijing, 100045, China; Li L., School of Geosciences, Yangtze University, Wuhan, 430100, China; Zhang Y., School of Geosciences, Yangtze University, Wuhan, 430100, China","Cloud detection is a critical stage in remote sensing image preprocessing. However, when there is snow on the underlying surface of scenes, the general cloud detection methods wouldbe easily affected. As a result, the cloud detection accuracy of these methods would reduce.Furthermore, most available cloud detection datasets are of medium-resolution and do not focus on the cloud and snow coexistence study areas. As a result, a cloud detection dataset has been created and released based on high-resolution cloud-snow coexistence remote sensing images.Meanwhile, this study suggests a convolution neural network termed RDC-Net for cloud detection in high-resolution cloud and snow coexistence images. The RDC-Net contains the reconstructible multiscale feature fusion module for multiscale cloud feature extraction, the dual adaptive feature fusion module for effective cloud feature representation reconstruction, and the controllably deep gradient guidance flows module for unbiased network gradient descent guidance. Benefiting from the above technical components, the network can enhance the robustness of cloud detection in complicated regions and facilitate lightweight deployment of the network. The experimental results show that the RDC-Net has an excellent anti-interference capacity for highlighted ground objects and has outstanding detection performance for thin clouds and clouds over snow. Furthermore, the RDC-Net has fewer parameters and floating-point operations, making it acceptable for industrial production and application. © 2023 SinoMaps Press. All rights reserved.","cloud and snow coexistence region; cloud detection; convolutional neural network; high accuracy; high-resolution remote sensing images; lightweight","Convolution; Convolutional neural networks; Deep neural networks; Gradient methods; Object detection; Remote sensing; Snow; Cloud and snow coexistence region; Cloud detection; Cloud detection method; Coexistence region; Convolutional neural network; High resolution; High-accuracy; High-resolution remote sensing images; Lightweight; Remote sensing images; accuracy assessment; cloud; data set; detection method; image resolution; remote sensing; satellite imagery; snow; Digital arithmetic","","","","","Open Fund of Beijing Key Laboratory of Urban Spatial Information Engineering, (20210205); Open Fund of Hunan Provincial Key Laboratory of Geo-Information Engineering in Surveying, Mapping and Remote Sensing; Open Research Fund of Key Laboratory of Earth Observation of Hainan Province, (2020LDE001); Ministry of Natural Resources of the People's Republic of China, MNR, (MEMI-2021-2022-08); Hunan University of Science and Technology, HNUST, (2020NGCM07, E22133, E22205)","Open Fund of Key Laboratory of Mine Environmental Monitoring and Improving around Poyang Lake, Ministry of Natural Resources (No. MEMI-2021-2022-08); The Open Fund of Hunan Provincial Key Laboratory of Geo-Information Engineering in Surveying, Mapping and Remote Sensing, Hunan University of Science and Technology (Nos. E22133; E22205); The Open Fund of the Key Laboratory of National Geographic Census and Monitoring, Ministry of Natural Resources (No. 2020NGCM07); The Open Fund of Beijing Key Laboratory of Urban Spatial Information Engineering (No. 20210205); The Open Research Fund of Key Laboratory of Earth Observation of Hainan Province (No. 2020LDE001)","IDSO S B, JACKSON R D, REGINATO R J., Remote-sensing of crop yields, Science (New York, N Y), 196, 4285, pp. 19-25, (1977); ZHENG Kai, LI Jiansheng, YANG Jianfeng, Et al., A cloud and snow detection method of TH-1 image based on combined ResNet and DeepLabV3+, Acta Geodaetica et Cartographica Sinica, 49, 10, pp. 1343-1353, (2020); IRISH R R, BARKER J L, GOWARD S N, Et al., Characterization of the landsat-7 ETM+ automated cloud-cover assessment (ACCA) algorithm, Photogrammetric Engineering & Remote Sensing, 72, 10, pp. 1179-1188, (2006); ZHAN Yongjie, WANG Jian, SHI Jianping, Et al., Distinguishing cloud and snow in satellite images via deep convolutional network, IEEE Geoscience and Remote Sensing Letters, 14, 10, pp. 1785-1789, (2017); IRISH R R., Landsat 7 automatic cloud cover assessment, Proceedings of SPIE-The International Society for Optical Engineering, 4049, pp. 348-355, (2000); ZHU Zhe, WOODCOCK C E., Object-based cloud and cloud shadow detection in Landsat imagery, Remote Sensing of Environment, 118, pp. 83-94, (2012); WANG Yongji, MING Yanfang, LIANG Tianchen, Et al., GF-6 WFV data cloud detection based on improved LCCD algorithm, Acta Optica Sinica, 40, 21, pp. 169-180, (2020); WIELAND M, LI Yu, MARTINIS S., Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network, Remote Sensing of Environment, 230, (2019); LI Pengfei, DONG Limin, XIAO Huachao, Et al., A cloud image detection method based on SVM vector machine, Neurocomputing, 169, pp. 34-42, (2015); GHASEMIAN N, AKHOONDZADEH M., Introducing two random forest based methods for cloud detection in remote sensing images, Advances in Space Research, 62, 2, pp. 288-303, (2018); HUGHES M, HAYES D., Automated detection of cloud and cloud shadow in single-date landsat imagery using neural networks and spatial post-processing, Remote Sensing, 6, 6, pp. 4907-4926, (2014); CHAI Dengfeng, NEWSAM S, ZHANG H K, Et al., Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks, Remote Sensing of Environment, 225, pp. 307-316, (2019); JEPPESEN J H, JACOBSEN R H, INCEOGLU F, Et al., A cloud detection algorithm for satellite imagery based on deep learning, Remote Sensing of Environment, 229, pp. 247-259, (2019); RONNEBERGER O, FISCHER P, BROX T., U-net: convolutional networks for biomedical image segmentation, Lecture Notes in Computer Science, pp. 234-241, (2015); LI Zhiwei, SHEN Huanfeng, CHENG Qing, Et al., Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors, ISPRS Journal of Photogrammetry and Remote Sensing, 150, pp. 197-212, (2019); GUO Jianhua, YANG Jingyu, YUE Huanjing, Et al., CDnetV2: CNN-based cloud detection for remote sensing imagery with cloud-snow coexistence, IEEE Transactions on Geoscience and Remote Sensing, 59, 1, pp. 700-713, (2021); HUANG Gao, LIU Zhuang, VAN DER MAATEN L, Et al., Densely connected convolutional networks, Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261-2269, (2017); SZEGEDY C, LIU Wei, JIA Yangqing, Et al., Going deeper with convolutions, Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, (2015); HE Kaiming, ZHANG Xiangyu, REN Shaoqing, Et al., Deep residual learning for image recognition, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); ZHANG Guangbin, GAO Xianjun, YANG Yuanwei, Et al., Controllably deep supervision and multi-scale feature fusion network for cloud and snow detection based on medium- and high-resolution imagery dataset, Remote Sensing, 13, 23, (2021); DING Xiaohan, ZHANG Xiangyu, MA Ningning, Et al., RepVGG: making VGG-style ConvNets great again, Proceedings of 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13728-13737, (2021); SZEGEDY C, IOFFE S, VANHOUCKE V, Et al., Inception-v4, inception-ResNet and the impact of residual connections on learning, Proceedings of the AAAI Conference on Artificial Intelligence, 31, 1, pp. 4278-4284, (2017); WANG Qilong, WU Banggu, ZHU Pengfei, Et al., ECA-net: efficient channel attention for deep convolutional neural networks, Proceedings of 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11531-11539, (2020); WOO S, PARK J, LEE J Y, Et al., CBAM: convolutional block attention module, Proceedings of 2018 Computer Vision-ECCV 2018, pp. 3-19, (2018); FOGA S, SCARAMUZZA P L, GUO Song, Et al., Cloud detection algorithm comparison and validation for operational Landsat data products, Remote Sensing of Environment, 194, pp. 379-390, (2017); MOHAJERANI S, SAEEDI P., Cloud and cloud shadow segmentation for remote sensing imagery via filtered jaccard loss function and parametric augmentation, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14, pp. 4254-4266, (2021); SCARAMUZZA P L, BOUCHARD M A, DWYER J L., Development of the landsat data continuity mission cloud-cover assessment algorithms, IEEE Transactions on Geoscience and Remote Sensing, 50, 4, pp. 1140-1154, (2012); VOIGT S, GIULIO-TONOLO F, LYONS J, Et al., Global trends in satellite-based emergency mapping, Science (New York, N Y), 353, 6296, pp. 247-252, (2016)","X. Gao; School of Geosciences, Yangtze University, Wuhan, 430100, China; email: junxgao@whu.edu.cn","","SinoMaps Press","","","","","","10011595","","CEXUE","","Chinese","Cehui Xuebao","Article","Final","","Scopus","2-s2.0-85148858297"
"Hulens D.; Van Ranst W.; Cao Y.; Goedemé T.","Hulens, Dries (56202320300); Van Ranst, Wiebe (56667681200); Cao, Ying (55257328600); Goedemé, Toon (6506388986)","56202320300; 56667681200; 55257328600; 6506388986","Autonomous Visual Navigation for a Flower Pollination Drone","2022","Machines","10","5","364","","","","3","10.3390/machines10050364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130517814&doi=10.3390%2fmachines10050364&partnerID=40&md5=cd26b353cf1446678968befbc2a46a38","EAVISE (Embedded and Artificially intelligent VISion Engineering), KU Leuven, Sint-Katelijne-Waver, 2860, Belgium; Magics Technologies NV, Geel, 2440, Belgium","Hulens D., EAVISE (Embedded and Artificially intelligent VISion Engineering), KU Leuven, Sint-Katelijne-Waver, 2860, Belgium; Van Ranst W., EAVISE (Embedded and Artificially intelligent VISion Engineering), KU Leuven, Sint-Katelijne-Waver, 2860, Belgium; Cao Y., Magics Technologies NV, Geel, 2440, Belgium; Goedemé T., EAVISE (Embedded and Artificially intelligent VISion Engineering), KU Leuven, Sint-Katelijne-Waver, 2860, Belgium","In this paper, we present the development of a visual navigation capability for a small drone enabling it to autonomously approach flowers. This is a very important step towards the development of a fully autonomous flower pollinating nanodrone. The drone we developed is totally autonomous and relies for its navigation on a small on-board color camera, complemented with one simple ToF distance sensor, to detect and approach the flower. The proposed solution uses a DJI Tello drone carrying a Maix Bit processing board capable of running all deep-learning-based image processing and navigation algorithms on-board. We developed a two-stage visual servoing algorithm that first uses a highly optimized object detection CNN to localize the flowers and fly towards it. The second phase, approaching the flower, is implemented by a direct visual steering CNN. This enables the drone to detect any flower in the neighborhood, steer the drone towards the flower and make the drone’s pollinating rod touch the flower. We trained all deep learning models based on an artificial dataset with a mix of images of real flowers, artificial (synthetic) flowers and virtually rendered flowers. Our experiments demonstrate that the approach is technically feasible. The drone is able to detect, approach and touch the flowers totally autonomously. Our 10 cm sized prototype is trained on sunflowers, but the methodology presented in this paper can be retrained for any flower type. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","neural network; pollination drone; two-stage approach; visual servoing","","","","","","Fonds Wetenschappelijk Onderzoek, FWO, (S003817N); Vlaamse regering","Funding: This work has been supported by the company MAGICS, the FWO SBO project OmniDrone under agreement S003817N and the Flemish Government under the AI Research Program.","Van Dijk M., Morley T., Rau M.L., Saghai Y., A meta-analysis of projected global food demand and population at risk of hunger for the period 2010–2050, Nat. Food, 2, pp. 494-501, (2021); Wagner D.L., Insect declines in the Anthropocene, Annu. Rev. Entomol, 65, pp. 457-480, (2020); Khalifa S.A., Elshafiey E.H., Shetaia A.A., El-Wahed A.A.A., Algethami A.F., Musharraf S.G., AlAjmi M.F., Zhao C., Masry S.H., Abdel-Daim M.M., Et al., Overview of bee pollination and its economic value for crop production, Insects, 12, (2021); Yuan T., Zhang S., Sheng X., Wang D., Gong Y., Li W., An autonomous pollination robot for hormone treatment of tomato flower in greenhouse, Proceedings of the 2016 3rd International Conference on Systems and Informatics (ICSAI), pp. 108-113, (2016); Ohi N., Lassak K., Watson R., Strader J., Du Y., Yang C., Hedrick G., Nguyen J., Harper S., Reynolds D., Et al., Design of an Autonomous Precision Pollination Robot, Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 7711-7718, (2018); Kalantari F., Mohd Tahir O., Mahmoudi Lahijani A., Kalantari S., A review of vertical farming technology: A guide for implementation of building integrated agriculture in cities, Advanced Engineering Forum, 24, pp. 76-91, (2017); Hulens D., Van Ranst W., Cao Y., Goedeme T., The Autonomous Pollination Drone, Proceedings of the 2nd IFSA Winter Conference on Automation, Robotics & Communications for Industry 4.0 (ARCI’ 2022), 1, pp. 38-41, (2022); Hashimoto K., Visual Servoing, 7, (1993); Doitsidis L., Valavanis K.P., Tsourveloudis N.C., Kontitsis M., A framework for fuzzy logic based UAV navigation and control, Proceedings of the IEEE International Conference on Robotics and Automation, 4, pp. 4041-4046, (2004); De Schutter J., De Laet T., Rutgeerts J., Decre W., Smits R., Aertbelien E., Claes K., Bruyninckx H., Constraint-based task specification and estimation for sensor-based robot systems in the presence of geometric uncertainty, Int. J. Robot. Res, 26, pp. 433-455, (2007); Strader J., Nguyen J., Tatsch C., Du Y., Lassak K., Buzzo B., Watson R., Cerbone H., Ohi N., Yang C., Et al., Flower Interaction Subsystem for a Precision Pollination Robot, Proceedings of the 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5534-5541, (2019); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826, (2016); Dias P.A., Tabb A., Medeiros H., Apple flower detection using deep convolutional networks, Comput. Ind, 99, pp. 17-28, (2018); Guglielmino P.C., Seker Z., Stallings N.A., Craigie C.A., Autonomous Drone Pollination, (2021); Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., Mobilenets: Efficient convolutional neural networks for mobile vision applications, (2017); Howard A., Sandler M., Chu G., Chen L.C., Chen B., Tan M., Wang W., Zhu Y., Pang R., Vasudevan V., Et al., Searching for mobilenetv3, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314-1324, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2016); He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietikainen M., Deep learning for generic object detection: A survey, Int. J. Comput. Vis, 128, pp. 261-318, (2020); Zhou B., Lapedriza A., Khosla A., Oliva A., Torralba A., Places: A 10 million image database for scene recognition, IEEE Trans. Pattern Anal. Mach. Intell, 40, pp. 1452-1464, (2017)","W. Van Ranst; EAVISE (Embedded and Artificially intelligent VISion Engineering), KU Leuven, Sint-Katelijne-Waver, 2860, Belgium; email: wiebe.vanranst@kuleuven.be; T. Goedemé; EAVISE (Embedded and Artificially intelligent VISion Engineering), KU Leuven, Sint-Katelijne-Waver, 2860, Belgium; email: toon.goedeme@kuleuven.be","","MDPI","","","","","","20751702","","","","English","Mach.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130517814"
"Seo D.; Cho B.-H.; Kim K.","Seo, Dasom (57347660200); Cho, Byeong-Hyo (57191619019); Kim, Kyoungchul (57225077819)","57347660200; 57191619019; 57225077819","Development of monitoring robot system for tomato fruits in hydroponic greenhouses","2021","Agronomy","11","11","2211","","","","17","10.3390/agronomy11112211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119475648&doi=10.3390%2fagronomy11112211&partnerID=40&md5=999c32c8f9b2eb7abb6b9710fa2fa360","Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea","Seo D., Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; Cho B.-H., Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; Kim K., Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea","Crop monitoring is highly important in terms of the efficient and stable performance of tasks such as planting, spraying, and harvesting, and for this reason, several studies are being conducted to develop and improve crop monitoring robots. In addition, the applications of deep learning algorithms are increasing in the development of agricultural robots since deep learning algorithms that use convolutional neural networks have been proven to show outstanding performance in image classification, segmentation, and object detection. However, most of these applications are focused on the development of harvesting robots, and thus, there are only a few studies that improve and develop monitoring robots through the use of deep learning. For this reason, we aimed to develop a real-time robot monitoring system for the generative growth of tomatoes. The presented method detects tomato fruits grown in hydroponic greenhouses using the Faster R-CNN (region-based convolutional neural network). In addition, we sought to select a color model that was robust to external light, and we used hue values to develop an image-based maturity standard for tomato fruits; furthermore, the developed maturity standard was verified through comparison with expert classification. Finally, the number of tomatoes was counted using a centroid-based tracking algorithm. We trained the detection model using an open dataset and tested the whole system in real-time in a hydroponic greenhouse. A total of 53 tomato fruits were used to verify the developed system, and the developed system achieved 88.6% detection accuracy when completely obscured fruits not captured by the camera were included. When excluding obscured fruits, the system’s accuracy was 90.2%. For the maturity classification, we conducted qualitative evaluations with the assistance of experts. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Hydroponic greenhouse; Maturity levels; Monitoring robot; Object detection","","","","","","Korea Smart Farm R&D Foundation; MSICT; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421031-04); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","Funding: This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation through Smart Farm Innovation Technology Development Program, funded by MAFRA, MSICT and RDA (421031-04).","Yang D., Li H., Zhang L., Study on the fruit recognition system based on machine vision, Adv. J. Food Sci. Technol, 10, pp. 18-21, (2016); Tang Y., Chen M., Wang C., Luo L., Li J., Lian G., Zou X., Recognition and localization methods for vision-based fruit picking robots: A review, Front. Plant Sci, 11, pp. 1-17, (2020); Zhang Q., Karkee M., Tabb A., The use of agricultural robots in orchard management, Robotics and Automation for Improving Agriculture, pp. 187-214, (2019); Srinivasan N., Prabhu P., Smruthi S.S., Sivaraman N.V., Gladwin S.J., Rajavel R., Natarajan A.R., Design of an autonomous seed plating robot, Proceedings of the 2016 IEEE Region 10 Humanitarian Technology Conference (R10-HTC), pp. 1-4; Santhi P.V., Kapileswar N., Chenchela V.K.R., Prasad C.H.V.S., Sensor and vision based autonomous AGRIBOT for sowing seeds, Proceedings of the 2017 International Conference on Energy, Communication, Data Analysis and Soft Computing (ICECDS), pp. 242-245; Khuantham C., Sonthitham A., Spraying robot controlled by application smartphone for pepper farm, Proceedings of the 2020 International Conference on Power, Energy and Innovations (ICPEI), pp. 225-228; Cantelli L., Bonaccorse F., Longo D., Melita C.D., Schillaci G., Muscato G., A small versatile electrical robot for autonomous spraying in agriculture, Agric. Eng, 1, pp. 391-402, (2019); Danton A., Roux J.C., Dance B., Cariou C., Lenain R., Development of a spraying robot for precision agriculture: An edge following approach, Proceedings of the 2020 IEEE Conference on Control Technology and Applications (CCTA), pp. 267-272; Murugan K., Shankar B.J., Sumanth A., Sudharshan C.V., Reddy G.V., Smart automated pesticide spraying bot, Proceedings of the 2020 3rd International Conference on Intelligent Sustainable Systems (ICISS), pp. 864-868; Mu L., Cui G., Liu Y., Cui Y., Fu L., Gejima Y., Design and simulation of an integrated end-effector for picking kiwifruit by robot, Inf. Process. Agric, 7, pp. 58-71, (2020); Arad B., Balendonck J., Barth R., Ben-Shahar O., Edan Y., Hellstrom T., Hemming J., Kurtser P., Ringdahl O., Tielen T., Et al., Development of a sweet pepper harvesting robot, J. Field Robot, 37, pp. 1027-1039, (2020); Xiong Y., Ge Y., Grimstad L., From P.J., An autonomous strawberry-harvesting robot: Design, development, integration, and field evaluation, J. Field Robot, 37, pp. 202-224, (2020); Kuznetsova A., Maleva T., Soloviev V., Using YOLOv3 algorithm with pre-and post-processing for apple detection in fruit-harvesting robot, Agronomy, 10, (2020); Taqi F., Al-Langawi F., Abdulraheem H., El-Abd M., A cherry-tomato harvesting robot, Proceedings of the 2017 18th International Conference on Advanced Robotics (ICAR), pp. 463-468; Badeka E., Vrochidou E., Papakostas G.A., Pachidis T., Kaburlasos V.G., Harvest crate detection for grapes harvesting robot based on YOLOv3 model, Proceedings of the 2020 Fourth International Conference On Intelligent Computing in Data Sciences (ICDS), pp. 1-5; Chou W.C., Tsai W.R., Chang H.H., Lu S.Y., Lin K.F., Lin P.L., Prioritization of pesticides in crops with a semi-quantitative risk ranking method for Taiwan postmarket monitoring program, J. Food Drug Anal, 27, pp. 347-354, (2019); Ravankar A., Ravankar A.A., Watanabe M., Hoshino Y., Rawankar A., Development of a low-cost semantic monitoring system for vineyards using autonomous robots, Agriculture, 10, (2020); Kim W.S., Lee D.H., Kim Y.J., Kim T., Lee W.S., Choi C.H., Stereo-vision-based crop height estimation for agricultural robots, Comput. Electron. Agric, 181, (2021); Fernando S., Nethmi R., Silva A., Perera A., De Silva R., Abeygunawardhana P.K.W., Intelligent disease detection system for greenhouse with a robotic monitoring system, Proceedings of the 2020 2nd International Conference on Advancements in Computing (ICAC), pp. 204-209; Yoon C., Lim D., Park C., Factors affecting adoption of smart farms: The case of Korea, Comput. Hum. Behav, 108, (2020); Santos L.C., Aguiar A.S., Santos F.N., Valente A., Petry M., Occupancy grid and topological maps extraction from satellite images for path planning in agricultural robots, Robotics, 9, (2020); Moysiadis V., Sarigiannidis P., Vitsas V., Khelifi A., Smart farming in Europe, Comput. Sci. Rev, 39, (2021); Rong J., Wang P., Yang Q., Huang F., A field-tested harvesting robot for oyster mushroom in greenhouse, Agronomy, 11, (2021); Liu G., Nouaze J.C., Mbouembe P.L.T., Kim J.H., YOLO-tomato: A robust algorithm for tomato detection based on YOLOv3, Sensors, 20, (2020); Lawal M.O., Tomato detection based on modified YOLOv3 framework, Sci. Rep, 11, (2021); Afonso M., Fonteijn H., Fiorentin F.S., Lensink D., Mooij M., Faber N., Polder G., Wehrens R., Tomato fruit detection and counting in greenhouses using deep learning, Front. Plant Sci, 11, (2020); Hu C., Liu X., Pan Z., Li P., Automatic detection of single ripe tomato on plant combining Faster R-CNN and intuitionistic Fuzzy set, IEEE Access, 7, pp. 154683-154696, (2019); Iwasaki Y., Yamane A., Itoh M., Goto C., Matsumuto H., Takaichi M., Demonstration of year-round production of tomato fruits with high soluble-solids content by low node-order pinching and high-density planting, Bull. NARO Crop. Sci, 3, pp. 41-51, (2019); Alexander L., Grierson D., Ethylene biosynthesis and action in tomato: A model for climacteric fruit ripening, J. Exp. Bot, 53, pp. 2039-2055, (2002); Garcia M.B., Ambat S., Adao R.T., Tomayto, tomahto: A machine learning approach for tomato ripening stage identification using pixel-based color image classification, Proceedings of the 2019 IEEE 11th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM), pp. 1-6; Rupanagudi S.R., Ranjani B.S., Nagaraj P., Bhat V.G., A cost effective tomato maturity grading system using image processing for farmers, Proceedings of the 2014 International Conference on Contemporary Computing and Informatics (IC3I), pp. 7-12; Pacheco W.D.N., Lopez F.R.J., Tomato classification according to organoleptic maturity (coloration) using machine learning algorithms K-NN, MLP, and K-Means Clustering, Proceedings of the 2019 XXII Symposium on Image, Signal Processing and Artificial Vision (STSIVA), pp. 1-5; Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the 2016 IEEE Conference of Computer Vision and Pattern Recogniton, pp. 779-788; Ren S., He R., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detectin with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single shot multibox detector, Proceedings of the European Conference on Computer Vision, pp. 21-37; He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recogniton, Proceedings of the 2016 IEEE Conference of Computer Vision and Pattern Recogniton, pp. 770-778; Make ML, Tomato Dataset, Make ML; He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision, pp. 2980-2988; Hallett S.H., Jones R.J.A., Compilation of an accumulated temperature databased for use in an environmental information system, Agric. For. Meteorol, 63, pp. 21-34, (1993); Harvest Timer; Hirsch R., Exploring Colour Photography: A Complete Guide, (2004)","B.-H. Cho; Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; email: cho2519@korea.kr; K. Kim; Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; email: kkcmole@korea.kr","","MDPI","","","","","","20734395","","","","English","Agronomy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119475648"
"Milicevic M.; Batos V.; Lipovac A.; Car Z.","Milicevic, Mario (8657285100); Batos, Vedran (8657285000); Lipovac, Adriana (55799622900); Car, Zeljka (16174303500)","8657285100; 8657285000; 55799622900; 16174303500","Deep Regression Neural Networks for Proportion Judgment","2022","Future Internet","14","4","100","","","","0","10.3390/fi14040100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127667286&doi=10.3390%2ffi14040100&partnerID=40&md5=2eda02cc21ac31ef7e49deb0bd3dd77d","Department of Electrical Engineering and Computing, University of Dubrovnik, Dubrovnik, 20000, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, 10000, Croatia","Milicevic M., Department of Electrical Engineering and Computing, University of Dubrovnik, Dubrovnik, 20000, Croatia; Batos V., Department of Electrical Engineering and Computing, University of Dubrovnik, Dubrovnik, 20000, Croatia; Lipovac A., Department of Electrical Engineering and Computing, University of Dubrovnik, Dubrovnik, 20000, Croatia; Car Z., Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, 10000, Croatia","Deep regression models are widely employed to solve computer vision tasks, such as human age or pose estimation, crowd counting, object detection, etc. Another possible area of application, which to our knowledge has not been systematically explored so far, is proportion judgment. As a prerequisite for successful decision making, individuals often have to use proportion judgment strategies, with which they estimate the magnitude of one stimulus relative to another (larger) stimulus. This makes this estimation problem interesting for the application of machine learning techniques. In regard to this, we proposed various deep regression architectures, which we tested on three original datasets of very different origin and composition. This is a novel approach, as the assumption is that the model can learn the concept of proportion without explicitly counting individual objects. With comprehensive experiments, we have demonstrated the effectiveness of the proposed models which can predict proportions on real-life datasets more reliably than human experts, considering the coefficient of determination (>0.95) and the amount of errors (MAE < 2, RMSE < 3). If there is no significant number of errors in determining the ground truth, with an appropriate size of the learning dataset, an additional reduction of MAE to 0.14 can be achieved. The used datasets will be publicly available to serve as reference data sources in similar projects. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","computer vision; convolutional neural networks; deep learning; deep regression; proportion judgment","Convolutional neural networks; Decision making; Deep neural networks; Object detection; Regression analysis; Convolutional neural network; Decisions makings; Deep learning; Deep regression; Estimation problem; Human age estimation; Human pose estimations; Proportion judgment; Regression modelling; Regression neural networks; Computer vision","","","","","","","Chesney D., Bjalkebring P., Peters E., How to estimate how well people estimate: Evaluating measures of individual differences in the approximate number system, Atten. Percept. Psycho, 77, pp. 2781-2802, (2015); Hollands J.G., Dyre B.P., Bias in proportion judgments: The cyclical power model, Psychol. Rev, 107, pp. 500-524, (2000); Sheridan T.B., Ferrell W.R., Man-Machine Systems: Information, Control, and Decision Models of Human Performance, (1974); Wickens C.D., Hollands J.G., Banbury S., Parasuraman R., Engineering Psychology and Human Performance, (2021); Lathuiliere S., Mesejo P., Alameda-Pineda X., Horaud R., A comprehensive analysis of deep regression, IEEE Trans. Pattern Anal, 42, pp. 2065-2081, (2019); Khan A., Sohail A., Zahoora U., Qureshi A.S., A survey of the recent architectures of deep convolutional neural networks, Artif. Intell. Rev, 53, pp. 5455-5516, (2020); Shen W., Guo Y., Wang Y., Zhao K., Wang B., Yuille A.L., Deep regression forests for age estimation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2304-2313, (2018); Shi L., Copot C., Vanlanduit S., A Deep Regression Model for Safety Control in Visual Servoing Applications, Proceedings of the 2020 Fourth IEEE International Conference on Robotic Computing (IRC), pp. 360-366, (2020); Milicevic M., Zubrinic K., Grbavac I., Keselj A., Ensemble Transfer Learning Framework for Vessel Size Estimation from 2D Images, Proceedings of the International Work-Conference on Artificial Neural Networks, pp. 258-269, (2019); Deng J., Bai Y., Li C., A Deep Regression Model with Low-Dimensional Feature Extraction for Multi-Parameter Manufacturing Quality Prediction, Appl. Sci, 10, (2020); Gao J., Zhang T., Yang X., Xu C., P2T: Part-to-target tracking via deep regression learning, IEEE Trans. Image Process, 27, pp. 3074-3086, (2018); Zhong Z., Li J., Zhang Z., Jiao Z., Gao X., An attention-guided deep regression model for landmark detection in cephalograms, Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 540-548, (2019); Fang C., Huang J., Cuan K., Zhuang X., Zhang T., Comparative study on poultry target tracking algorithms based on a deep regression network, Biosyst. Eng, 190, pp. 176-183, (2020); Wang Q., Yang D., Li Z., Zhang X., Liu C., Deep regression via multi-channel multi-modal learning for pneumonia screening, IEEE Access, 8, pp. 78530-78541, (2020); Wang Q., Wan J., Li X., Robust hierarchical deep learning for vehicular management, IEEE Trans. Veh. Technol, 68, pp. 4148-4156, (2018); Salehi S.S.M., Khan S., Erdogmus D., Gholipour A., Real-time deep pose estimation with geodesic loss for image-to-template rigid registration, IEEE Trans. Med. Imaging, 38, pp. 470-481, (2018); Abdi A.M., Land cover and land use classification performance of machine learning algorithms in a boreal landscape using Sentinel-2 data, Gisci. Remote Sens, 57, pp. 1-20, (2020); Jia K., Liang S., Gu X., Baret F., Wei X., Wang X., Yao Y., Yang L., Li Y., Fractional vegetation cover estimation algorithm for Chinese GF-1 wide field view data, Remote Sens. Environ, 177, pp. 184-191, (2016); Yu R., Li S., Zhang B., Zhang H., A Deep Transfer Learning Method for Estimating Fractional Vegetation Cover of Senti-nel-2 Multispectral Images, IEEE Geosci. Remote Sens, 19, pp. 1-5, (2021); Carpenter G.A., Gopal S., Macomber S., Martens S., Woodcock C.E., A neural network method for mixture estimation for vegetation mapping, Remote Sens. Environ, 70, pp. 138-152, (1999); Mohammed-Aslam M.A., Rokhmatloh-Salem Z.E., Javzandulam T.S., Linear mixture model applied to the land-cover classification in an alluvial plain using Landsat TM data, J. Environ. Inform, 7, pp. 95-101, (2006); Blinn C.E., Increasing the Precision of Forest Area Estimates through Improved Sampling for Nearest Neighbor Satellite Image Classification, (2005); Wu B., Li Q., Crop planting and type proportion method for crop acreage estimation of complex agricultural landscapes, Int. J. Appl. Earth Obs, 16, pp. 101-112, (2012); Drake N.A., Mackin S., Settle J.J., Mapping vegetation, soils, and geology in semiarid shrublands using spectral matching and mixture modeling of SWIR AVIRIS imagery, Remote Sens. Environ, 68, pp. 12-25, (1999); Gilbert M., Gregoire J.C., Visual, semi-quantitative assessments allow accurate estimates of leafminer population densities: An example comparing image processing and visual evaluation of damage by the horse chestnut leafminer Cameraria ohridella (Lep., Gracillariidae), Jpn. J. Appl. Entomol. Z, 127, pp. 354-359, (2003); Alaiz-Rodriguez R., Alegre E., Gonzalez-Castro V., Sanchez L., Quantifying the proportion of damaged sperm cells based on image analysis and neural networks, Proc. SMO, 8, pp. 383-388, (2008); Zhu Q., Chen J., Wang L., Guan Q., Proportion Estimation for Urban Mixed Scenes Based on Nonnegative Matrix Factorization for High-Spatial Resolution Remote Sensing Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 11257-11270, (2021); Milicevic M., Zubrinic K., Grbavac I., Obradovic I., Application of deep learning architectures for accurate detection of olive tree flowering phenophase, Remote Sens, 12, (2020); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can semantic labeling methods generalize to any city? The inria aerial image labeling benchmark, Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229, (2017); Chollet F., Deep Learning with Python, (2021); Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G.S., Davis A., Dean J., Devin M., Et al., Tensorflow: Large-scale machine learning on heterogeneous distributed systems, (2016); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Kingma D.P., Ba J., Adam: A method for stochastic optimization, (2014); Tieleman T., Hinton G., Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude, COURSERA Neural Netw. Mach. Learn, 4, pp. 26-31, (2012); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258, (2017); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., Inception-v4, inception-resnet and the impact of residual connections on learning, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, (2017); Tan C., Sun F., Kong T., Zhang W., Yang C., Liu C., A survey on deep transfer learning, Proceedings of the International Conference on Artificial Neural Networks, pp. 270-279, (2018); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Adv. Neural Inf. Process. Syst, 25, pp. 1097-1105, (2012); Huh M., Agrawal P., Efros A.A., What makes ImageNet good for transfer learning?, (2016); Scholkopf B., Smola A.J., Williamson R.C., Bartlett P.L., New support vector algorithms, Neural Comput, 12, pp. 1207-1245, (2000); Breiman L., Random forests, Mach. Learn, 45, pp. 5-32, (2001); Sagi O., Rokach L., Ensemble learning: A survey, Wires Data Min. Knowl, 8, (2018); Ganaie M.A., Hu M., Ensemble deep learning: A review, (2021); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, Proceedings of the International Conference on Machine Learning, pp. 448-456, (2015); Srivastava N., Hinton G., Krizhevsky A., Sutskever I., Salakhutdinov R., Dropout: A simple way to prevent neural networks from overfitting, J. Mach. Learn. Res, 15, pp. 1929-1958, (2014); Garbin C., Zhu X., Marques O., Dropout vs. batch normalization: An empirical study of their impact to deep learning, Multimed. Tools Appl, 79, pp. 12777-12815, (2020); Krig S., Ground truth data, content, metrics, and analysis, Computer Vision Metrics, pp. 247-271, (2016)","M. Milicevic; Department of Electrical Engineering and Computing, University of Dubrovnik, Dubrovnik, 20000, Croatia; email: mario.milicevic@unidu.hr","","MDPI","","","","","","19995903","","","","English","Future Internet","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127667286"
"Rodriguez-Conde I.; Campos C.; Fdez-Riverola F.","Rodriguez-Conde, Ivan (57284789800); Campos, Celso (14053531300); Fdez-Riverola, Florentino (35580091100)","57284789800; 14053531300; 35580091100","On-device object detection for more efficient and privacy-compliant visual perception in context-aware systems","2021","Applied Sciences (Switzerland)","11","19","9173","","","","3","10.3390/app11199173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116314527&doi=10.3390%2fapp11199173&partnerID=40&md5=9bd13b7ffd2d1cc84303c75a4d7ec16a","Department of Computer Science, University of Arkansas at Little Rock, 2801 South University Avenue, Little Rock, 72204, AR, United States; Department of Computer Science, ESEI—Escuela Superior de Ingeniería Informática, Universidade de Vigo, Ourense, 32004, Spain; CINBIO, Department of Computer Science, ESEI—Escuela Superior de Ingeniería Informática, Universidade de Vigo, Ourense, 32004, Spain; SING Research Group, Galicia Sur Health Research Institute (IIS Galicia Sur), SERGAS-UVIGO, Vigo, 36213, Spain","Rodriguez-Conde I., Department of Computer Science, University of Arkansas at Little Rock, 2801 South University Avenue, Little Rock, 72204, AR, United States; Campos C., Department of Computer Science, ESEI—Escuela Superior de Ingeniería Informática, Universidade de Vigo, Ourense, 32004, Spain; Fdez-Riverola F., CINBIO, Department of Computer Science, ESEI—Escuela Superior de Ingeniería Informática, Universidade de Vigo, Ourense, 32004, Spain, SING Research Group, Galicia Sur Health Research Institute (IIS Galicia Sur), SERGAS-UVIGO, Vigo, 36213, Spain","Ambient Intelligence (AmI) encompasses technological infrastructures capable of sensing data from environments and extracting high-level knowledge to detect or recognize users’ features and actions, as well as entities or events in their surroundings. Visual perception, particularly object detection, has become one of the most relevant enabling factors for this context-aware user-centered intelligence, being the cornerstone of relevant but complex tasks, such as object tracking or human action recognition. In this context, convolutional neural networks have proven to achieve state-of-the-art accuracy levels. However, they typically result in large and highly complex models that typically demand computation offloading onto remote cloud platforms. Such an approach has security-and latency-related limitations and may not be appropriate for some AmI use cases where the system response time must be as short as possible, and data privacy must be guaranteed. In the last few years, the on-device paradigm has emerged in response to those limitations, yielding more compact and efficient neural networks able to address inference directly on client machines, thus providing users with a smoother and better-tailored experience, with no need of sharing their data with an outsourced service. Framed in that novel paradigm, this work presents a review of the recent advances made along those lines in object detection, providing a comprehensive study of the most relevant lightweight CNN-based detection frameworks, discussing the most paradigmatic AmI domains where such an approach has been successfully applied, the different challenges arisen, the key strategies and techniques adopted to create visual solutions for image-based object classification and localization, as well as the most relevant factors to bear in mind when assessing or comparing those techniques, such as the evaluation metrics or the hardware setups used. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Ambient intelligence; Convolutional neural networks; Deep learning; Object detection; On-device","","","","","","Conseller?a de Educaci?n; Consellería de Educación, Universidades e Formación Profesional; European Regional Development Fund, ERDF, (ED431G2019/06); Xunta de Galicia, (2019-2022); Centro Singular de Investigación de Galicia, CINBIO","Funding text 1: Funding: This research was funded by the Consellería de Educación, Universidades e Formación Profesional (Xunta de Galicia) under the scope of the strategic funding ED431C2018/55-GRC Competitive Reference Group and the “Centro singular de investigación de Galicia” (accreditation 2019-2022) funded by the European Regional Development Fund (ERDF)-Ref. ED431G2019/06.; Funding text 2: This research was funded by the Conseller?a de Educaci?n, Universidades e Formaci?n Profesional (Xunta de Galicia) under the scope of the strategic funding ED431C2018/55-GRC Competitive Reference Group and the ?Centro singular de investigaci?n de Galicia? (accreditation 2019-2022) funded by the European Regional Development Fund (ERDF)-Ref. ED431G2019/06.","Wang Z., Zheng L., Liu Y., Li Y., Wang S., Towards Real-Time Multi-Object Tracking, Proceedings of the Computer Vision–ECCV 2020: 16th European Conference, pp. 107-122; Cao Z., Hidalgo G., Simon T., Wei S.E., Sheikh Y., OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields, IEEE Trans. Pattern Anal. Mach. Intell, 43, pp. 172-186, (2021); Zhang H.B., Zhang Y.X., Zhong B., Lei Q., Yang L., Du J.X., Chen D.S., A comprehensive survey of vision-based human action recognition methods, Sensors, 19, (2019); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2015); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015—Conference Track Proceedings, (2015); Dunne R., Morris T., Harper S., A Survey of Ambient Intelligence, ACM Comput. Surv, 54, pp. 1-27, (2021); Sadri F., Ambient intelligence: A survey, ACM Comput. Surv, 43, pp. 1-66, (2011); Remagnino P., Foresti G.L., Ambient Intelligence: A New Multidisciplinary Paradigm, IEEE Trans. Syst. Man Cybern. —Part A Syst. Hum, 35, pp. 1-6, (2005); Gandodhar P.S., Chaware S.M., Context Aware Computing Systems: A survey, Proceedings of the 2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), pp. 605-608; Augusto J., Aztiria A., Kramer D., Alegre U., A Survey on the Evolution of the Notion of Context-Awareness, Appl. Artif. Intell, 31, pp. 613-642, (2017); Cook D.J., Augusto J.C., Jakkula V.R., Review: Ambient intelligence: Technologies, applications, and opportunities, Pervasive Mob. Comput, 5, pp. 277-298, (2009); Mawela T., Ambient Intelligence Systems for the Elderly: A Privacy Perspective, Proceedings of the Computational Science and Its Applications—ICCSA 2020, pp. 875-888; Friedewald M., Vildjiounaite E., Punie Y., Wright D., The Brave New World of Ambient Intelligence: An Analysis of Scenarios Regarding Privacy, Identity and Security Issues, Proceedings of the Security in Pervasive Computing, pp. 119-133, (2006); Theoharidou M., Marias G., Dritsas S., Gritzalis D., The ambient intelligence paradigm A review of security and privacy strategies in leading economies, Proceedings of the 2006 2nd IET International Conference on Intelligent Environments—IE 06, pp. 213-219, (2006); Caire P., Moawad A., Efthymiou V., Bikakis A., Le Traon Y., Privacy challenges in Ambient Intelligence systems, J. Ambient. Intell. Smart Environ, 8, pp. 619-644, (2016); Gomez C., Chessa S., Fleury A., Roussos G., Preuveneers D., Internet of Things for enabling smart environments: A technology-centric perspective, J. Ambient. Intell. Smart Environ, 11, pp. 23-43, (2019); Cai Y., Genovese A., Piuri V., Scotti F., Siegel M., IoT-based Architectures for Sensing and Local Data Processing in Ambient Intelligence: Research and Industrial Trends, Proceedings of the 2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC), pp. 1-6, (2019); Streitz N., Charitos D., Kaptein M., Bohlen M., Grand challenges for ambient intelligence and implications for design contexts and smart societies, J. Ambient. Intell. Smart Environ, 11, pp. 87-107, (2019); Abhari M., Abhari K., Ambient Intelligence Applications in Architecture: Factors Affecting Adoption Decisions, Proceedings of the Advances in Information and Communication, pp. 235-250; Rocker C., Perceived Usefulness and Perceived Ease-of-Use of Ambient Intelligence Applications in Office Environments, Proceedings of the Human Centered Design, pp. 1052-1061, (2009); Hasanov A., Laine T.H., Chung T.-S., A survey of adaptive context-aware learning environments, J. Ambient. Intell. Smart Environ, 11, pp. 403-428, (2019); Kanagarajan S., Ramakrishnan S., Ubiquitous and Ambient Intelligence Assisted Learning Environment Infrastructures Development—A review, Educ. Inf. Technol, 23, pp. 569-598, (2018); Karthick G.S., Pankajavalli P.B., Ambient Intelligence for Patient-Centric Healthcare Delivery: Technologies, Framework, and Applications, Design Frameworks for Wireless Networks, pp. 223-254, (2020); Haque A., Milstein A., Fei-Fei L., Illuminating the dark spaces of healthcare with ambient intelligence, Nature, 585, pp. 193-202, (2020); Uddin M.Z., Khaksar W., Torresen J., Ambient Sensors for Elderly Care and Independent Living: A Survey, Sensors, 18, (2018); El murabet A., Abtoy A., Touhafi A., Tahiri A., Ambient Assisted living system’s models and architectures: A survey of the state of the art, J. King Saud Univ. —Comput. Inf. Sci, 32, pp. 1-10, (2020); Ramkumar M., Catharin S.S., Nivetha D., Survey of Cognitive Assisted Living Ambient System Using Ambient intelligence as a Companion, Proceedings of the 2019 IEEE International Conference on System, Computation, Automation and Networking (ICSCAN), pp. 1-5; Calvaresi D., Cesarini D., Sernani P., Marinoni M., Dragoni A.F., Sturm A., Exploring the ambient assisted living domain: A systematic review, J. Ambient. Intell. Humaniz. Comput, 8, pp. 239-257, (2017); Salih A.S.M., A Review of Ambient Intelligence Assisted Healthcare Monitoring, Int. J. Comput. Inf. Syst. Ind. Manag, 5, pp. 741-750, (2014); Dhar S., Guo J., Liu J., Tripathi S., Kurup U., Shah M., On-Device Machine Learning: An Algorithms and Learning Theory Perspective, (2019); Guo K., Zeng S., Yu J., Wang Y., Yang H., A Survey of FPGA-Based Neural Network Accelerator, (2017); Deng L., Li G., Han S., Shi L., Xie Y., Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey, Proc. IEEE, 108, pp. 485-532, (2020); Cheng J., Wang P., Li G., Hu Q., Lu H., Recent Advances in Efficient Computation of Deep Convolutional Neural Networks, Front. Inf. Technol. Electron. Eng, 19, pp. 64-77, (2018); Qin H., Gong R., Liu X., Bai X., Song J., Sebe N., Binary neural networks: A survey, Pattern Recognit, (2020); Wu X., Sahoo D., Hoi S.C.H., Recent advances in deep learning for object detection, Neurocomputing, 396, pp. 39-64, (2020); Chahal K., Dey K., A Survey of Modern Object Detection Literature using Deep Learning, (2018); Zhao Z., Zheng P., Xu S., Wu X., Object Detection With Deep Learning: A Review, IEEE Trans. Neural Netw. Learn. Syst, 30, pp. 3212-3232, (2019); Jiao L., Zhang F., Liu F., Yang S., Li L., Feng Z., Qu R., A survey of deep learning-based object detection, IEEE Access, 7, pp. 128837-128868, (2019); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietikainen M., Wang X., Fieguth P., Chen J., Et al., Deep Learning for Generic Object Detection: A Survey, Int. J. Comput. Vis, 128, pp. 261-318, (2020); Khan A., Sohail A., Zahoora U., Qureshi A.S., A survey of the recent architectures of deep convolutional neural networks, Artif. Intell. Rev, (2020); Sultana F., Sufian A., Dutta P., A review of object detection models based on convolutional neural network, Adv. Intell. Syst. Comput, 1157, pp. 1-16, (2020); Gams M., Gu I.Y.-H., Harma A., Munoz A., Tam V., Artificial intelligence and ambient intelligence, J. Ambient. Intell. Smart Environ, 11, pp. 71-86, (2019); Ramos C., Ambient Intelligence—A State of the Art from Artificial Intelligence Perspective, Proceedings of the Progress in Artificial Intelligence, pp. 285-295, (2007); Patel A., Shah J., Sensor-based activity recognition in the context of ambient assisted living systems: A review, J. Ambient. Intell. Smart Environ, 11, pp. 301-322, (2019); Bansal A., Ahirwar M.K., Shukla P.K., A Survey on Classification Algorithms Used in Healthcare Environment of the Internet of Things, Int. J. Comput. Sci. Eng, 6, pp. 883-887, (2018); Augusto J.C., Callaghan V., Cook D., Kameas A., Satoh I., Intelligent Environments: A manifesto, Hum.-Cent. Comput. Inf. Sci, 3, (2013); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., MobileNets: Efficient convolutional neural networks for mobile vision applications, (2017); Tripathi S., Dane G., Kang B., Bhaskaran V., Nguyen T., LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 411-420; Azimi S.M., ShuffleDet: Real-Time Vehicle Detection Network in On-Board Embedded UAV Imagery, European Conference on Computer Vision, pp. 88-99, (2019); Keeffe S.O., Villing R., Evaluating pruned object detection networks for real-time robot vision, Proceedings of the 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), pp. 91-96; Gao H., Tao W., Wen D., Liu J., Chen T., Osa K., Kato M., DupNet: Towards Very Tiny Quantized CNN With Improved Accuracy for Face Detection, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 168-177; Unel F.O., Ozkalayci B.O., Cigla C., The Power of Tiling for Small Object Detection, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 582-591; Deng J., Shi Z., Zhuo C., Energy-Efficient Real-Time UAV Object Detection on Embedded Platforms, IEEE Trans. Comput. Aided Des. Integr. Circuits Syst, 39, pp. 3123-3127, (2020); Ringwald T., Sommer L., Schumann A., Beyerer J., Stiefelhagen R., UAV-Net: A Fast Aerial Vehicle Detector for Mobile Platforms, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 544-552; Zhang P., Zhong Y., Li X., SlimYOLOv3: Narrower, faster and better for real-time UAV applications, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 37-45; Liu Y., Cao S., Lasang P., Shen S., Modular Lightweight Network for Road Object Detection Using a Feature Fusion Approach, IEEE Transaction on Systems, Man, and Cybernetics: Systems, 51, pp. 4716-4728, (2019); Vaddi S., Kumar C., Jannesari A., Efficient Object Detection Model for Real-Time UAV Applications, (2019); Yang Z., Xu W., Wang Z., He X., Yang F., Yin Z., Combining Yolov3-tiny Model with Dropblock for Tiny-face Detection, Proceedings of the 2019 IEEE 19th International Conference on Communication Technology (ICCT), pp. 1673-1677; Han S., Kwon J., Kwon S., Real-time Small Object Detection Model in the Bird-view UAV Imagery, Proceedings of the 3rd International Conference on Vision, Image and Signal Processing, (2019); Zhao X., Liang X., Zhao C., Tang M., Wang J., Real-Time Multi-Scale Face Detector on Embedded Devices, Sensors, 19, (2019); Yoo Y.J., Han D., Yun S., EXTD: Extremely tiny face detector via iterative filter reuse, (2019); Qi S., Yang J., Song X., Jiang C., Multi-Task FaceBoxes: A Lightweight Face Detector Based on Channel Attention and Context Information, KSII Trans. Internet Inf. Syst, 14, pp. 4080-4097, (2020); Li X., Tian Y., Zhang F., Quan S., Xu Y., Object detection in the context of mobile augmented reality, Proceedings of the 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR); Zhao Y., Wang L., Hou L., Gan C., Huang Z., Hu X., Shen H., Ye J., Real Time Object Detection for Traffic Based on Knowledge Distillation: 3rd Place Solution to Pair Competition, Proceedings of the 2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pp. 1-6; Barba-Guaman L., Eugenio Naranjo J., Ortiz A., Deep Learning Framework for Vehicle and Pedestrian Detection in Rural Roads on an Embedded GPU, Electronics, 9, (2020); Liu M., Ding X., Du W., Continuous, Real-Time Object Detection on Mobile Devices without Offloading, Proceedings of the 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS), pp. 976-986; Nikouei S.Y., Chen Y., Song S., Xu R., Choi B., Faughnan T., Smart Surveillance as an Edge Network Service: From Harr-Cascade, SVM to a Lightweight CNN, Proceedings of the 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC), pp. 256-265; Nguyen P.H., Arsalan M., Koo J.H., Naqvi R.A., Truong N.Q., Park K.R., LightDenseYOLO: A Fast and Accurate Marker Tracker for Autonomous UAV Landing by Visible Light Camera Sensor on Drone, Sensors, 18, (2018); Kyrkou C., Plastiras G., Theocharides T., Venieris S.I., Bouganis C., DroNet: Efficient convolutional neural network detector for real-time UAV applications, Proceedings of the 2018 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 967-972; Mithun N.C., Munir S., Guo K., Shelton C., ODDS: Real-Time Object Detection Using Depth Sensors on Embedded GPUs, Proceedings of the 2018 17th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), pp. 230-241; Ghazi P., Happonen A.P., Boutellier J., Huttunen H., Embedded Implementation of a Deep Learning Smile Detec-tor, Proceedings of the 2018 7th European Workshop on Visual Information Processing (EUVIP), (2018); Melinte D.O., Dumitriu D., Margaritescu M., Ancuta P.-N., Deep Learning Computer Vision for Sorting and Size Determination of Municipal Waste, Proceedings of the International Conference of Mechatronics and Cyber-MixMechatronics—2019, pp. 142-152, (2020); Yang A., Bakhtari N., Langdon-Embry L., Redwood E., Grandjean Lapierre S., Rakotomanga P., Rafalimanantsoa A., De Dios Santos J., Vigan-Womas I., Knoblauch A.M., Et al., Kankanet: An artificial neural network-based object detection smartphone application and mobile microscope as a point-of-care diagnostic aid for soil-transmitted helminthiases, PLoS Negl Trop Dis, 13, (2019); Pang S., Wang S., Rodriguez-Paton A., Li P., Wang X., Rodriguez-Paton A., Li P., Wang X., An artificial intelligent diagnostic system on mobile Android terminals for cholelithiasis by lightweight convolutional neural network, PLoS ONE, 14, (2019); Lage E.S., Santos R.L., Junior S.M.T., Andreotti F., Low-Cost IoT Surveillance System Using Hardware-Acceleration and Convolutional Neural Networks, Proceedings of the 2019 IEEE 5th World Forum on Internet of Things (WF-IoT), pp. 931-936; Bresilla K., Perulli G.D., Boini A., Morandi B., Corelli Grappadelli L., Manfrini L., Single-Shot Convolution Neural Networks for Real-Time Fruit Detection Within the Tree, Front. Plant Sci, 10, (2019); Xiong Q., Lin J., Yue W., Liu S., Liu Y., Ding C., A Deep Learning Approach to Driver Distraction Detection of Using Mobile Phone, Proceedings of the 2019 IEEE Vehicle Power and Propulsion Conference (VPPC), pp. 1-5; Shakeel M.F., Bajwa N.A., Anwaar A.M., Sohail A., Khan A., Haroon-ur-Rashid, Detecting Driver Drowsiness in Real Time Through Deep Learning Based Object Detection, Proceedings of the International Work-Conference on Artificial Neural Networks, pp. 283-296, (2019); Xiao D., Li H., Liu C., He Q., Alonso-Betanzos A., Large-Truck Safety Warning System Based on Lightweight SSD Model, Intell. Neurosci, 2019, (2019); Ramcharan A., McCloskey P., Baranowski K., Mbilinyi N., Mrisho L., Ndalahwa M., Legg J., Hughes D.P., A Mobile-Based Deep Learning Model for Cassava Disease Diagnosis, Front. Plant Sci, 10, (2019); Wang F., Tan J.T.C., Improving Deep Learning Based Object Detection of Mobile Robot Vision by HSI Preprocessing Method and CycleGAN Method Under Inconsistent Illumination Conditions in Real Environment, Proceedings of the 2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM), pp. 583-588; Chen P., Hsieh J., Gochoo M., Wang C., Liao H.M., Smaller Object Detection for Real-Time Embedded Traffic Flow Estimation Using Fish-Eye Cameras, Proceedings of the 2019 IEEE International Conference on Image Processing (ICIP), pp. 2956-2960; Zhao H., Zhang W., Sun H., Xue B., Embedded Deep Learning for Ship Detection and Recognition, Future Internet, 11, (2019); Ding S., Long F., Fan H., Liu L., Wang Y., A novel YOLOv3-tiny network for unmanned airship obstacle detection, Proceedings of the Proceedings of 2019 IEEE 8th Data Driven Control and Learning Systems Conference, DDCLS 2019, pp. 277-281; Putro M.D., Nguyen D.L., Jo K.H., Fast Eye Detector Using CPU Based Lightweight Convolutional Neural Network, Proceedings of the 2020 20th International Conference on Control, Automation and Systems (ICCAS), pp. 12-16; Long Z., Suyuan W., Zhongma C., Jiaqi F., Xiaoting Y., Wei D., Lira-YOLO: A lightweight model for ship detection in radar images, J. Syst. Eng. Electron, 31, pp. 950-956, (2020); Zhou Z., Song Z., Fu L., Gao F., Li R., Cui Y., Real-time kiwifruit detection in orchard using deep learning on Android™ smartphones for yield estimation, Comput. Electron. Agric, 179, (2020); Khaled N., Mohsen S., El-Din K.E., Akram S., Metawie H., Mohamed A., In-Door Assistant Mobile Application Using CNN and TensorFlow, Proceedings of the 2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE), pp. 1-6; Ji H., Zeng X., Li H., Ding W., Nie X., Zhang Y., Xiao Z., Human abnormal behavior detection method based on T-TINY-YOLO, Proceedings of the 5th International Conference on Multimedia and Image Processing, pp. 1-5; Han C., Zhu J., Li F., Wan S., Chen H., Design of lightweight pedestrian detection network in railway scenes, J. Phys. Conf. Ser, 1544, (2020); Pinto de Aguiar A.S., Neves dos Santos F.B., Feliz dos Santos L.C., de Jesus Filipe V.M., Miranda de Sousa A.J., Vineyard trunk detection using deep learning—An experimental device benchmark, Comput. Electron. Agric, 175, (2020); Choi J., Chun D., Lee H., Kim H., Uncertainty-based Object Detector for Autonomous Driving Embedded Platforms, Proceedings of the 2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS), pp. 16-20; Seo J., Ahn H., Kim D., Lee S., Chung Y., Park D., EmbeddedPigDet—Fast and Accurate Pig Detection for Embedded Board Implementations, Appl. Sci, 10, (2020); Ai Y.B., Rui T., Yang X.Q., He J.L., Fu L., Li J.B., Lu M., Visual SLAM in dynamic environments based on object detection, Def. Technol, (2020); Guo Q., Liu Q., Wang W., Zhang Y., Kang Q., A fast occluded passenger detector based on MetroNet and Tiny MetroNet, Inf. Sci, 534, pp. 16-26, (2020); Gong J., Zhao J., Li F., Zhang H., Vehicle detection in thermal images with an improved yolov3-tiny, Proceedings of the 2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS), pp. 253-256; Anisuzzaman D.M., Patel Y., Niezgoda J., Gopalakrishnan S., Yu Z., A Mobile App for Wound Localization using Deep Learning; Sun H., Yang J., Shen J., Liang D., Ning-Zhong L., Zhou H., TIB-Net: Drone Detection Network with Tiny Iterative Backbone, IEEE Access, 8, pp. 130697-130707, (2020); Jia J., Zhai G., Ren P., Zhang J., Gao Z., Min X., Yang X., Tiny-BDN: An Efficient and Compact Barcode Detection Network, IEEE J. Sel. Top. Signal Process, 14, pp. 688-699, (2020); Melinte D.O., Travediu A.-M., Dumitriu D.N., Deep Convolutional Neural Networks Object Detector for Real-Time Waste Identification, Appl. Sci, 10, (2020); Zhang S., Wu Y., Men C., Ren N., Li X., Channel Compression Optimization Oriented Bus Passenger Object Detection, Math. Probl. Eng, 2020, (2020); Wang X., Wang S., Cao J., Wang Y., Data-Driven Based Tiny-YOLOv3 Method for Front Vehicle Detection Inducing SPP-Net, IEEE Access, 8, pp. 110227-110236, (2020); Mazzia V., Khaliq A., Salvetti F., Chiaberge M., Real-Time Apple Detection System Using Embedded Systems With Hardware Accelerators: An Edge AI Application, IEEE Access, 8, pp. 9102-9114, (2020); Mishra B., Garg D., Narang P., Mishra V., Drone-surveillance for search and rescue in natural disaster, Comput. Commun, 156, pp. 1-10, (2020); Oh S., You J.-H., Kim Y.-K., FRDet: Balanced and Lightweight Object Detector based on Fire-Residual Modules for Embedded Processor of Autonomous Driving, (2020); Etxeberria-Garcia M., Ezaguirre F., Plazaola J., Munoz U., Zamalloa M., Embedded object detection applying Deep Neural Networks in railway domain, Proceedings of the 2020 23rd Euromicro Conference on Digital System Design (DSD), pp. 565-569; Liu J., Wang X., Early recognition of tomato gray leaf spot disease based on MobileNetv2-YOLOv3 model, Plant Methods, 16, (2020); Zhang X., Gao Y., Xiao G., Feng B., Chen W., A Real-Time Garbage Truck Supervision and Data Statistics Method Based on Object Detection, Wirel. Commun. Mob. Comput, 2020, (2020); Roy B., Nandy S., Ghosh D., Dutta D., Biswas P., Das T., MOXA: A Deep Learning Based Unmanned Approach For Real-Time Monitoring of People Wearing Medical Masks, Trans. Indian Natl. Acad. Eng, 5, pp. 509-518, (2020); von Atzigen M., Liebmann F., Hoch A., Bauer D.E., Snedeker J.G., Farshad M., Furnstahl P., HoloYolo: A proof-of-concept study for marker-less surgical navigation of spinal rod implants with augmented reality and on-device machine learning, Int. J. Med Robot. Comput. Assist. Surg, 17, pp. 1-10, (2021); Kazemi F.M., Samadi S., Poorreza H.R., Akbarzadeh-T M.R., Vehicle recognition using curvelet transform and SVM, Proceedings of the International Conference on Information Technology-New Generations, ITNG 2007, pp. 516-521; Breiman L., Random forests, Mach. Learn, 45, pp. 5-32, (2001); Wu S., Nagahashi H., Parameterized adaboost: Introducing a parameter to speed up the training of real adaboost, IEEE Signal Process. Lett, 21, pp. 687-691, (2014); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Et al., ImageNet Large Scale Visual Recognition Challenge, Int. J. Comput. Vis, 115, pp. 211-252, (2015); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The pascal visual object classes (VOC) challenge, Int. J. Comput. Vis, 88, pp. 303-338, (2010); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 770-778; Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, IEEE Trans. Pattern Anal. Mach. Intell, 42, pp. 318-327, (2020); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single shot multibox detector, Proceedings of the European Conference on Computer Vision, 9905, pp. 21-37, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the Proceedings—30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, pp. 6517-6525; Redmon J., Farhadi A., YOLOv3: An incremental improvement, (2018); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 779-788; Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G.S., Davis A., Dean J., Devin M., Et al., TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems, (2016); Paszke A., Gross S., Chintala S., Chanan G., Yang E., DeVito Z., Lin Z., Desmaison A., Antiga L., Lerer A., Automatic differentiation in pytorch, Proceedings of the NIPS 2017 The Thirty-first Annual Conference on Neural Information Processing Systems, (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common Objects in Context, Proceedings of the European conference on computer vision, pp. 740-755, (2014); Geiger A., Lenz P., Stiller C., Urtasun R., Vision meets robotics: The KITTI dataset, Int. J. Robot. Res, 32, pp. 1231-1237, (2013); Zhu P., Wen L., Du D., Bian X., Hu Q., Ling H., Vision Meets Drones: Past, Present and Future; Yang S., Luo P., Loy C.C., Tang X., WIDER FACE: A face detection benchmark, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 5525-5533, (2016); Kukreja V., Kumar D., Kaur A., Geetanjali Sakshi, GAN-based synthetic data augmentation for increased CNN performance in Vehicle Number Plate Recognition, Proceedings of the 2020 4th International Conference on Electronics, Communication and Aerospace Technology (ICECA), pp. 1190-1195; Bhattarai B., Baek S., Bodur R., Kim T.K., Sampling Strategies for GAN Synthetic Data, Proceedings of the ICASSP 2020—2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2303-2307; Qiu W., Yuille A., UnrealCV: Connecting Computer Vision to Unreal Engine, Proceedings of the Computer Vision—ECCV 2016 Workshops, pp. 909-916, (2016); Perez-Rua J.M., Zhu X., Hospedales T.M., Xiang T., Incremental Few-Shot Object Detection, Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13843-13852; Fan Q., Zhuo W., Tang C.K., Tai Y.W., Few-Shot Object Detection With Attention-RPN and Multi-Relation Detector, Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4012-4021","I. Rodriguez-Conde; Department of Computer Science, University of Arkansas at Little Rock, 2801 South University Avenue, Little Rock, 72204, United States; email: irconde@ualr.edu","","MDPI","","","","","","20763417","","","","English","Appl. Sci.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116314527"
"Naga Venkata Satya Sirisha T.S.; Venkata Sai Mada N.; Haritha S.; Tumuluru P.; Rachapudi V.","Naga Venkata Satya Sirisha, Tummuri Sri (58182121700); Venkata Sai Mada, Naga (58181317000); Haritha, Sriram (58181589200); Tumuluru, Praveen (57195534253); Rachapudi, Venubabu (57201913804)","58182121700; 58181317000; 58181589200; 57195534253; 57201913804","Evaluating the Performance of YOLO V5 for Electronic Device Classification","2023","Proceedings of the 3rd International Conference on Artificial Intelligence and Smart Energy, ICAIS 2023","","","","992","997","5","0","10.1109/ICAIS56108.2023.10073671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152401304&doi=10.1109%2fICAIS56108.2023.10073671&partnerID=40&md5=4553a247baad2531f05b2e9eda13df5e","Koneru Lakshmaiah Education Foundation, Department of Cse, AP, Vaddeswaram, India","Naga Venkata Satya Sirisha T.S., Koneru Lakshmaiah Education Foundation, Department of Cse, AP, Vaddeswaram, India; Venkata Sai Mada N., Koneru Lakshmaiah Education Foundation, Department of Cse, AP, Vaddeswaram, India; Haritha S., Koneru Lakshmaiah Education Foundation, Department of Cse, AP, Vaddeswaram, India; Tumuluru P., Koneru Lakshmaiah Education Foundation, Department of Cse, AP, Vaddeswaram, India; Rachapudi V., Koneru Lakshmaiah Education Foundation, Department of Cse, AP, Vaddeswaram, India","This study explores YOLO V5 (You Look Only Once version 5) for image classification. With the input data primarily concentrating on this area, four distinct types of electronic devices were used in this experiment. The task of object detection for electronic device classification is within the scope of computer vision and involves the identification and categorization of various electronic device kinds in both still photos and moving pictures. Among the well-known object detection techniques are Support vector machines (SVMs), decision trees, and more contemporary deep learning techniques such as convolutional neural networks (CNNs), You Only Look Once (YOLO). A variety of factors including scale variation, occlusion, and changes in illumination and position, can have an impact on how well YOLO performs when classifying electronic devices. The usage of ensembles of models, multi -scale training and testing, and data augmentation are a few of the strategies that researchers have suggested to overcome these issues. Despite tremendous advancement, object detection for electronic device classification using YOLO is still an active field of study, and new methods are constantly being developed to boost the precision and effectiveness of this approach. YOLO V5 operates more quickly and accurately. Thus, YOLO V5 has been selected for the most effective training approach. Where the overall average is 90% and above. © 2023 IEEE.","Classification; Computer Vision; Detection; Device classification; Machine Learning","Computer vision; Convolutional neural networks; Decision trees; Learning systems; Object detection; Object recognition; Support vector machines; Thermoelectric equipment; Detection; Device classifications; Electronics devices; Images classification; Input datas; Machine-learning; Moving pictures; Objects detection; Performance; Photo pictures; Deep learning","","","","","","","Hnoohom N., Yuenyong S., Thai fast food image classification using deep learning, 2018 International ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI-NCON), pp. 116-119, (2018); Zhao Z.-Q., Zheng P., Xu S.-T., Wu X., Object Detection with Deep Learning: A Review, (2018); Mahmood A., Uzair M., Al-Maadeed S., Multi-order statistical descriptors for real-time face recognition and object classification, IEEE Access, 6, pp. 12993-13004, (2018); Jaya G., Sunil P., Gireesh K., Bare skin image classification using convolution neural network, IJETAE Exploring research and innovations, (2022); Zhao Z.-Q., Zheng P., Xu S.-T., Wu X., Object detection with deep learning: A review, IEEE Transactions on Neural Networks and Learning Systems, 30, 11, pp. 3212-3232, (2019); Wang J., Zheng Y., Wang M., Shen Q., Huang J., Object-scale adaptive convolutional neural networks for high-spatial resolution remote sensing image classification, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14, pp. 283-299, (2021); Bird J.J., Barnes C.M., Manso L.J., Ekart A., Faria D.R., Fruit quality and defect image classification with conditional gan data augmentation, Scientia Horticulturae, (2021); Abu M.A., Indra N.H., Rahman A.H.A., Sapiee N.A., Ahmad I., A study on image classification based on deep learning and tensorflow, (2019); Islam M.T., Siddique B.M.N.K., Rahman S., Jabid T., Image recognition with deep learning, 2018 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS), pp. 106-110, (2018); Minija S.J., Emmanuel W.R.S., Food image classification using sphere shaped-Support vector machine, 2017 International Conference on Inventive Computing and Informatics (ICICI), pp. 109-113, (2017); Deng J., Xuan X., Wang W., Li Z., Yao H., Wang Z., A review of research on object detection based on deep learning, Journal of Physics: Conference Series-IOPscience, (2020); Krishna M.M., Neelima M., Harshali M., Rao M.V.G., Image classification using Deep learning, ITJRT publications, (2018); Corominas O.L., Smolinska I.R., Rana A.S.S., Image Classification with Classic and Deep Learning Techniques, (2021); Li Y., Fan Y., Wang S., Bai J., Li K., Application of yolov5 based on attention mechanism and receptive field in identifying defects of thangka images, IEEE Access, 10, (2022); Liu K., Stbi-yolo: A real-time object detection method for lung nodule recognition, IEEE Access, 10, (2022); Jung H.-K., Choi G.-S., Improved yolov5: Efficient object detection using drone images under various conditions, Applied Sciences., 12, (2022); MacHado G.R., Silva E., Goldschmidt B.R.R., Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective!, (2021); Khan D.M., Recent trends on object detection and image classification: A review, 2020 International Conference on Computational Performance Evaluation (ComPE), pp. 427-535, (2020); Rehman S., Ajmal H., Farooq U., Ain Q., Riaz F., Hassan A., Convolutional neural network based image segmentation: A review, 26, (2018); Zhong Y., Wang J., Peng J., Zhang L., Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer, (2020)","T.S. Naga Venkata Satya Sirisha; Koneru Lakshmaiah Education Foundation, Department of Cse, Vaddeswaram, AP, India; email: sirishathummuri77@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","3rd International Conference on Artificial Intelligence and Smart Energy, ICAIS 2023","2 February 2023 through 4 February 2023","Coimbatore","187553","","978-166546216-7","","","English","Proc. Int. Conf. Artif. Intell. Smart Energy, ICAIS","Conference paper","Final","","Scopus","2-s2.0-85152401304"
"Mohan V.; Simske S.J.","Mohan, Vinay (58151329400); Simske, Steven J. (7003506391)","58151329400; 7003506391","Cross-sensor vision system for maritime object detection","2023","Frontiers in Marine Science","10","","1112955","","","","0","10.3389/fmars.2023.1112955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150620432&doi=10.3389%2ffmars.2023.1112955&partnerID=40&md5=13e462ee0f627bb7a29dab2a71cab934","Department of Systems Engineering, Colorado State University, Fort Collins, CO, United States","Mohan V., Department of Systems Engineering, Colorado State University, Fort Collins, CO, United States; Simske S.J., Department of Systems Engineering, Colorado State University, Fort Collins, CO, United States","Accurate and automated detection of maritime vessels present in aerial images is a considerable challenge. While significant progress has been made in recent years by adopting neural network architectures in detection and classification systems, these systems are usually designed specific to a sensor, dataset or location. In this paper, we present a system which uses multiple sensors and a convolutional neural network (CNN) architecture to test cross-sensor object detection resiliency. The system is composed of five main subsystems: Image Capture, Image Processing, Model Creation, Object-of-Interest Detection and System Evaluation. We show that the system has a high degree of cross-sensor vessel detection accuracy, paving the way for the design of similar systems which could prove robust across applications, sensors, ship types and ship sizes. Copyright © 2023 Mohan and Simske.","convolutional neural network; deep learning; maritime vessel; object detection; optical satellite system; synthetic aperture radar; vessel detection system","","","","","","","","Antelo J., Ambrosio G., Gonzalez J., Galindo C., Ship detection and recognition in high-resolution satellite images, 2009 IEEE International Geoscience and Remote Sensing Symposium, 4, pp. 514-517, (2009); Copernicus Open access hub; Sentinel data access overview; U.I for computer research – the MASATI dataset; Bi F., Liu F., Gao L., A hierarchical salient-region based algorithm for ship detection in remote sensing images, Advances in neural network research and applications, pp. 729-738, (2010); Bi F., Zhu B., Gao L., Bian M., A visual search inspired computational model for ship detection in optical satellite images, IEEE Geosci. Remote Sens. Lett, 9, pp. 749-753, (2012); Buck H., Sharghi E., Bromley K., Guilas C., Chheng T., Ship detection and classification from overhead imagery, Applications of Digital Image Processing XXX, 6696, pp. 522-536, (2007); Chen Y., Duan T., Wang C., Zhang Y., Huang M., End-to-End ship detection in SAR images for complex scenes based on deep CNNs, J. Sensors, 2021, (2020); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1251-1258, (2017); Corbane C., Pecoul E., Demagistri L., Petit M., Fully automated procedure for ship detection using optical satellite imagery, Remote Sensing of Inland, Coastal, and Oceanic Waters, 7150, (2008); Dekker R.J., Bouma H., Breejen E., den, Broek A.C., van den, Hanckmann P., Hogervorst M.A., Et al., Maritime situation awareness capabilities from satellite and terrestrial sensor systems, Proc. Maritime Systems and Technologies MAST Europe, (2013); Dong L., Yali L., Fei H., Shengjin W., Object detection in image with complex background, 3rd International Conference on Multimedia Technology (ICMT-13), pp. 471-478, (2013); Sentinel-2 User Handbook, 2015, (2015); Gallego A.-J., Pertusa A., Gil P., Automatic ship classification from optical aerial images with convolutional neural network, Remote Sens, 10, 4, (2018); Guo J., Zhu C.R., A novel method of ship detection from spaceborne optical image based on spatial pyramid matching, Appl. Mech. Mater, 190–191, pp. 1099-1103, (2012); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, (2016); Heiselberg H., A direct and fast methodology for ship recognition in sentinel-2 multispectral imagery, Remote Sens, 8, (2016); Jin T., Zhang J., Ship detection from high-resolution imagery based on land masking and cloud filtering, Seventh International Conference on Graphic and Image Processing (ICGIP 2015), 9817, pp. 268-273, (2015); Johansson P., Small vessel detection in high quality optical satellite imagery, Tech. report chalmers university of technology sweden. JR, (2011); Kumar S.S., Selvi M.U., Sea Object detection using colour and texture classification, Int. J. Comput. Appl. Eng, 1, pp. 59-63, (2011); Lavalle C., Rocha Gomes C., Baranzelli C., Batista e Silva F., Coastal zones: Policy alternatives impacts on European coastal zones 2000–2050, (2011); Li Z., Itti L., Saliency and gist features for target detection in satellite images, IEEE Trans. Image Process, 20, pp. 2017-2029, (2011); Li J., Qu C., Shao J., Ship detection in SAR images based on an improved faster R-CNN, 2017 SAR in Big Data Era: Models, Methods and Applications (BIGSARDATA), pp. 1-6, (2017); Lin J., Yang X., Xiao S., Yu Y., Jia C., A line segment based inshore ship detection method, Future control and automation, pp. 261-269, (2012); Ramani S., Prabakaran N., Kannadasan R., Rajkumar S., Real time detection and segmentation of ships in satellite images, Int. J. Sci. Technol. Res, 8, 12, (2019); Satyanarayana M.S., Aparna G., A method of ship detection from spaceborne optical image, Int. J. Adv. Comput. Math. Sci, 3, pp. 535-540, (2012); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv, 1409, (2014); Song Z., Sui H., Wang Y., Automatic ship detection for optical satellite images based on visual attention model and LBP, 2014 IEEE Workshop on Electronics, Computer and Applications, pp. 722-725, (2014); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z.B., Rethinking the inception architecture for computer vision, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826, (2016); Willhauck G., Caliz J.J., Hoffmann C., Lingenfelder I., Heynen M., Object oriented ship detection from VHR satellite images, (2005); Xia Y., Wan S., Yue L., A novel algorithm for ship detection based on dynamic fusion model of multi-feature and support vector machine, 6th International Conference on Image and Graphics (ICIG), pp. 521-526, (2011); Yang G., Li B., Ji S., Gao F., Xu Q., Ship detection from optical satellite images based on sea surface analysis, IEEE Geosci. Remote Sens. Lett, 11, pp. 641-645, (2014); Yang X., Sun H., Fu K., Yang J., Sun X., Yan M., Et al., Automatic ship detection in remote sensing images from Google earth of complex scenes based on multiscale rotation dense feature pyramid networks, Remote Sens, 10, (2018); Yang X., Sun H., Fu K., Yang J., Sun X., Yan M., Et al., A sea-land segmentation scheme based on statistical model of sea, 4th International Congress on Image and Signal Processing. CISP, 2011, pp. 1155-1159, (2011); Zhang W., Bian C., Zhao X., Hou Q., Ship target segmentation and detection in complex optical remote sensing image based on component tree characteristics discrimination, Proc. SPIE 8558, Optoelectronic Imaging and Multimedia Technology II, 85582F, (2012); Zhang S., Wu R., Xu K., Wang J., Sun W., R-CNN-Based ship detection from high resolution remote sensing imagery, Remote Sens, 11, 6, (2019)","V. Mohan; Department of Systems Engineering, Colorado State University, Fort Collins, United States; email: vinay.mohan@colostate.edu; S.J. Simske; Department of Systems Engineering, Colorado State University, Fort Collins, United States; email: steve.simske@colostate.edu","","Frontiers Media S.A.","","","","","","22967745","","","","English","Front. Mar. Sci.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85150620432"
"Li J.; Wang E.; Qiao J.; Li Y.; Li L.; Yao J.; Liao G.","Li, Jie (7410068291); Wang, Enguo (58205265800); Qiao, Jiangwei (56020128900); Li, Yi (57873067600); Li, Li (57037167200); Yao, Jian (55311401100); Liao, Guisheng (10042143700)","7410068291; 58205265800; 56020128900; 57873067600; 57037167200; 55311401100; 10042143700","Automatic rape flower cluster counting method based on low-cost labelling and UAV-RGB images","2023","Plant Methods","19","1","40","","","","0","10.1186/s13007-023-01017-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154034684&doi=10.1186%2fs13007-023-01017-x&partnerID=40&md5=6b3499b7b1135910fce793753657d26b","Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System, Hubei University of Technology, Wuhan, 430068, China; Oil Crops Research Institute of the Chinese Academy of Agricultural Sciences, Key Laboratory of Biology and Genetic Improvement of Oil Crops, Chinese Academy of Agricultural Sciences, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; National Lab of Radar Signal Processing, Xidian University, Xian, China","Li J., Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System, Hubei University of Technology, Wuhan, 430068, China; Wang E., Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System, Hubei University of Technology, Wuhan, 430068, China; Qiao J., Oil Crops Research Institute of the Chinese Academy of Agricultural Sciences, Key Laboratory of Biology and Genetic Improvement of Oil Crops, Chinese Academy of Agricultural Sciences, Wuhan, China; Li Y., Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System, Hubei University of Technology, Wuhan, 430068, China; Li L., School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Yao J., School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Liao G., National Lab of Radar Signal Processing, Xidian University, Xian, China","Background: The flowering period is a critical time for the growth of rape plants. Counting rape flower clusters can help farmers to predict the yield information of the corresponding rape fields. However, counting in-field is a time-consuming and labor-intensive task. To address this, we explored a deep learning counting method based on unmanned aircraft vehicle (UAV). The proposed method developed the in-field counting of rape flower clusters as a density estimation problem. It is different from the object detection method of counting the bounding boxes. The crucial step of the density map estimation using deep learning is to train a deep neural network that maps from an input image to the corresponding annotated density map. Results: We explored a rape flower cluster counting network series: RapeNet and RapeNet+. A rectangular box labeling-based rape flower clusters dataset (RFRB) and a centroid labeling-based rape flower clusters dataset (RFCP) were used for network model training. To verify the performance of RapeNet series, the paper compares the counting result with the real values of manual annotation. The average accuracy (Acc), relative root mean square error (rrMSE) and R2 of the metrics are up to 0.9062, 12.03 and 0.9635 on the dataset RFRB, and 0.9538, 5.61 and 0.9826 on the dataset RFCP, respectively. The resolution has little influence for the proposed model. In addition, the visualization results have some interpretability. Conclusions: Extensive experimental results demonstrate that the RapeNet series outperforms other state-of-the-art counting approaches. The proposed method provides an important technical support for the crop counting statistics of rape flower clusters in field. © 2023, The Author(s).","Attention mechanism; Bayesian loss; Pyramidal convolution; Rape flower clusters","","","","","","Open Foundation of Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System, (HBSEES202206); National Natural Science Foundation of China, NSFC, (62071172); Chinese Academy of Agricultural Sciences, CAAS; Wuhan University, WHU; Agricultural Science and Technology Innovation Program, ASTIP, (CAAS-ZDRW202105)","Funding text 1: This research was funded by National Natural Science Foundation of China grant number 62071172 and Agricultural Science and Technology Innovation Project grant number CAAS-ZDRW202105 and the Open Foundation of Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System grant number HBSEES202206. ; Funding text 2: The authors would like to thank the Oil Crops Research Institute of the Chinese Academy of Agricultural Sciences and Wuhan University for the effective advice provided in the study. We also thank the anonymous reviewers and academic editors for their valuable comments and constructive suggestions, which helped to improve the manuscript.","Zhang X., He Y., Rapid estimation of seed yield using hyperspectral images of oilseed rape leaves, Ind Crops Prod, 42, pp. 416-420, (2013); Amiri M., Raeisi-Dehkordi H., Sarrafzadegan N., Forbes S.C., Salehi-Abargouei A., The effects of canola oil on cardiovascular risk factors: a systematic review and meta-analysis with dose–response analysis of controlled clinical trials, Nutr Metab Cardiovasc Dis, 30, 12, pp. 2133-2145, (2020); George B., Loeser E., Oilseeds: world markets and trade, (2021); Asare E., Scarisbrick D., Rate of nitrogen and sulphur fertilizers on yield, yield components and seed quality of oilseed rape (brassica napus l.), Field Crops Res, 44, 1, pp. 41-46, (1995); Luo Y., On farm harvest and storage losses of oil crops and the impact on resources and environment in China, Chin J Oil Crop Sci, 44, 2, pp. 249-256, (2022); Stankevych S., Yevtushenko M., Vilna V., Zabrodina I., Yushchuk D., Sirous L.Y., Lutytska N., Molchanova O., Melenti V., Golovan L., Et al., Efficiency of chemical protection of spring rape and mustard from rape blossom beetle, Ukrain J Ecol, 9, 4, pp. 584-598, (2019); Riar A., Gill G., McDonald G., Different post-sowing nitrogen management approaches required to improve nitrogen and water use efficiency of canola and mustard, Front Plant Sci., (2020); Bouchet A.-S., Laperche A., Bissuel-Belaygue C., Snowdon R., Nesi N., Stahl A., Nitrogen use efficiency in rapeseed. A review, Agron Sustain Dev, 36, 2, pp. 1-20, (2016); Diepenbrock W., Yield analysis of winter oilseed rape (brassica napus l.): a review, Field Crops Res, 67, 1, pp. 35-49, (2000); Behrens T., Muller J., Diepenbrock W., Utilization of canopy reflectance to predict properties of oilseed rape (brassica napus l.) and barley (hordeum vulgare l.) during ontogenesis, Eur J Agron, 25, 4, pp. 345-355, (2006); d'Andrimont R., Taymans M., Lemoine G., Ceglar A., Yordanov M., van der Velde M., Detecting flowering phenology in oil seed rape parcels with sentinel-1 and -2 time series, Rem Sens Environ, 239, (2020); Feng A., Zhou J., Vories E., Sudduth K.A., Evaluation of cotton emergence using uav-based imagery and deep learning, Comput Electron Agric, 177, (2020); Oh S., Chang A., Ashapure A., Jung J., Dube N., Maeda M., Gonzalez D., Landivar J., Plant counting of cotton from uas imagery using deep learning-based object detection framework, Rem Sens, (2020); Wang L., Xiang L., Tang L., Jiang H., A convolutional neural network-based method for corn stand counting in the field, Sensors, (2021); Li L., Zhang Q., Huang D., A review of imaging techniques for plant phenotyping, Sensors, 14, 11, pp. 20078-20111, (2014); Vikram P., Anand N., Linesh R., Agriculture drones: a modern breakthrough in precision agriculture, Int J Rem Sens, 20, 10, pp. 507-518, (2017); Feng L., Chen S., Zhang C., Zhang Y., He Y., A comprehensive review on recent applications of unmanned aerial vehicle remote sensing with various sensors for high-throughput plant phenotyping, Comput Electron Agric, 182, (2021); Fang S., Tang W., Peng Y., Gong Y., Dai C., Chai R., Liu K., Remote estimation of vegetation fraction and flower fraction in oilseed rape with unmanned aerial vehicle data, Rem Sens., (2016); Wan L., Li Y., Cen H., Zhu J., Yin W., Wu W., Zhu H., Sun D., Zhou W., He Y., Combining uav-based vegetation indices and image classification to estimate flower number in oilseed rape, Rem Sens., (2018); Zang Y., Chen X., Chen J., Tian Y., Shi Y., Cao X., Cui X., Remote sensing index for mapping canola flowers using modis data, Rem Sens., (2020); Zhang T., Vail S., Duddu H.S.N., Parkin I.A.P., Guo X., Johnson E.N., Shirtliffe S.J., Phenotyping flowering in canola (brassica napus l.) and estimating seed yield using an unmanned aerial vehicle-based imagery, Front Plant Sci., (2021); Sulik J.J., Long D.S., Spectral indices for yellow canola flowers, Int J Rem Sens, 36, 10, pp. 2751-2765, (2015); Zhang G., Zhao S., Li W., Du Q., Ran Q., Tao R., Htd-net: a deep convolutional neural network for target detection in hyperspectral imagery, Rem Sens, 12, 9, (2020); Gouiaa R., Akhloufi M.A., Shahbazi M., Advances in convolution neural networks based crowd counting and density estimation, Big Data Cogn Comput, (2021); Samiei S., Rasti P., Ly Vu J., Buitink J., Rousseau D., Deep learning-based detection of seedling development, Plant Methods., 16, 1, pp. 1-11, (2020); Jiang Y., Li C., Paterson A.H., Robertson J.S., Deepseedling: deep convolutional network and Kalman filter for plant seedling detection and counting in the field, Plant Methods, 15, 1, pp. 1-19, (2019); Yang B., Gao Z., Gao Y., Zhu Y., Rapid detection and counting of wheat ears in the field using yolov4 with attention module, Agronomy, (2021); Lu H., Cao Z., Xiao Y., Zhuang B., Shen C., Tasselnet: counting maize tassels in the wild via local counts regression network, Plant Methods, 13, 1, pp. 1-17, (2017); Xiong H., Cao Z., Lu H., Madec S., Liu L., Shen C., Tasselnetv2: in-field counting of wheat spikes with context-augmented local regression networks, Plant Methods, 15, 1, pp. 1-14, (2019); Lu H., Cao Z., Tasselnetv2+: a fast implementation for high-throughput plant counting from high-resolution rgb imagery, Front Plant Sci, (2020); Lu H., Liu L., Li Y.-N., Zhao X.-M., Wang X.-Q., Cao Z.-G., Tasselnetv3: explainable plant counting with guided upsampling and background suppression, IEEE Trans Geosci Rem Sens, 60, pp. 1-15, (2021); Madec S., Jin X., Lu H., De Solan B., Liu S., Duyme F., Heritier E., Baret F., Ear density estimation from high resolution rgb imagery using deep learning technique, Agric For Meteorol, 264, pp. 225-234, (2019); Liu L., Lu H., Li Y., Cao Z., High-throughput rice density estimation from transplantation to tillering stages using deep networks, Plant Phenom (Washington, D.C.), 2020, (2020); Wang B., Liu H., Samaras D., Nguyen M.H., Distribution matching for crowd counting, Adv Neural Inf Process Syst, 33, pp. 1595-1607, (2020); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc IEEE, 86, 11, pp. 2278-2324, (1998); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Commun ACM, 60, 6, pp. 84-90, (2017); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, , Arxiv Preprint Arxiv, (2014); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Targ S., Almeida D., Lyman K., Resnet in resnet: Generalizing residual architectures, 2016, Arxiv Preprint Arxiv, (1603); Duta I.C., Liu L., Zhu F., Shao L., . Pyramidal convolution: Rethinking convolutional neural networks for visual recognition, Arxiv Preprint Arxiv, (2020); Ma Z., Wei X., Hong X., Gong Y., Bayesian loss for crowd count estimation with point supervision, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6142-6151, (2019); Zhu L., Geng X., Li Z., Liu C., Improving yolov5 with attention mechanism for detecting boulders from planetary images, Rem Sens., (2021); Li R., Wu Y., Improved yolo v5 wheat ear detection algorithm based on attention mechanism, Electronics, (2022); Dong Y., Liu Y., Kang H., Li C., Liu P., Liu Z., Lightweight and efficient neural network with spsa attention for wheat ear detection, PeerJ Comput Sci, 8, (2022); Wang Y., Qin Y., Cui J., Occlusion robust wheat ear counting algorithm based on deep learning, Front Plant Sci, (2021); Hou Q., Zhou D., Feng J., Coordinate attention for efficient mobile network design, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13713-13722, (2021); Wang D., Zhang D., Yang G., Xu B., Luo Y., Yang X., Ssrnet: in-field counting wheat ears using multi-stage convolutional neural network, IEEE Trans Geosci Rem Sens, 60, pp. 1-11, (2021); Ghorbani M.A., Shamshirband S., Haghi D.Z., Azani A., Bonakdari H., Ebtehaj I., Application of firefly algorithm-based support vector machines for prediction of field capacity and permanent wilting point, Soil Tillage Res, 172, pp. 32-38, (2017); Li M.-F., Tang X.-P., Wu W., Liu H.-B., General models for estimating daily global solar radiation for different solar radiation zones in mainland china, Energy Convers Manag, 70, pp. 139-148, (2013); Alkhudaydi T., Et al., Counting spikelets from infield wheat crop images using fully convolutional networks, Neural Comput Appl, pp. 1-22, (2022); Banerjee B.P., Sharma V., Spangenberg G., Kant S., Machine learning regression analysis for estimation of crop emergence using multispectral uav imagery, Rem Sens., (2021); Tan M., Chen B., Pang R., Vasudevan V., Sandler M., Howard A., Le Q.V., Mnasnet: Platform-aware neural architecture search for mobile, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820-2828, (2019); Iandola F., Moskewicz M., Karayev S., Girshick R., Darrell T., Keutzer K. Densenet: Implementing efficient convnet descriptor pyramids, 2014. Arxiv Preprint Arxiv, (1869); Tanefficientnet M.Q., Rethinking model scaling for convolutional neural networks, International Conference on Machine Learning, pp. 6105-6114, (2019); Tan L., Lv X., Lian X., Wang G., Yolov4\_drone: Uav image target detection based on an improved yolov4 algorithm, Comput Electr Eng, 93, (2021)","J. Qiao; Oil Crops Research Institute of the Chinese Academy of Agricultural Sciences, Key Laboratory of Biology and Genetic Improvement of Oil Crops, Chinese Academy of Agricultural Sciences, Wuhan, China; email: qiaojiangwei@caas.cn","","BioMed Central Ltd","","","","","","17464811","","","","English","Plant Methods","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85154034684"
"Weinstein B.G.; Garner L.; Saccomanno V.R.; Steinkraus A.; Ortega A.; Brush K.; Yenni G.; McKellar A.E.; Converse R.; Lippitt C.D.; Wegmann A.; Holmes N.D.; Edney A.J.; Hart T.; Jessopp M.J.; Clarke R.H.; Marchowski D.; Senyondo H.; Dotson R.; White E.P.; Frederick P.; Ernest S.K.M.","Weinstein, Ben G. (56188889300); Garner, Lindsey (57192877656); Saccomanno, Vienna R. (36714255900); Steinkraus, Ashley (57419141000); Ortega, Andrew (57202779101); Brush, Kristen (57639411200); Yenni, Glenda (55199138000); McKellar, Ann E. (25936587000); Converse, Rowan (57274924500); Lippitt, Christopher D. (14060686500); Wegmann, Alex (56435817100); Holmes, Nick D. (8835919700); Edney, Alice J. (57218994603); Hart, Tom (25521692600); Jessopp, Mark J. (6506459557); Clarke, Rohan H. (7403070847); Marchowski, Dominik (56955761600); Senyondo, Henry (56509511400); Dotson, Ryan (57638379700); White, Ethan P. (7401613097); Frederick, Peter (7006749233); Ernest, S. K. Morgan (6603760208)","56188889300; 57192877656; 36714255900; 57419141000; 57202779101; 57639411200; 55199138000; 25936587000; 57274924500; 14060686500; 56435817100; 8835919700; 57218994603; 25521692600; 6506459557; 7403070847; 56955761600; 56509511400; 57638379700; 7401613097; 7006749233; 6603760208","A general deep learning model for bird detection in high-resolution airborne imagery","2022","Ecological Applications","32","8","e2694","","","","4","10.1002/eap.2694","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135870449&doi=10.1002%2feap.2694&partnerID=40&md5=5a1da84e887a169711bebfc575bd074f","Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; California Oceans Program, The Nature Conservancy, Sacramento, CA, United States; Geomatics Program, University of Florida, Gainesville, FL, United States; Montana State University, Bozeman, MT, United States; Environment and Climate Change Canada, Saskatoon, SK, Canada; Center for the Advancement of Spatial Informatics Research and Education, University of New Mexico, Albuquerque, NM, United States; Department of Zoology, University of Oxford, Oxford, United Kingdom; School of Biological, Earth and Environmental Sciences, University College Cork, Cork, Ireland; School of Biological Sciences, Monash University, Melbourne, VIC, Australia; Ornithological Station, Museum and Institute of Zoology, Polish Academy of Sciences, Gdańsk, Poland; Quantaero, Reno, NV, United States","Weinstein B.G., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; Garner L., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; Saccomanno V.R., California Oceans Program, The Nature Conservancy, Sacramento, CA, United States; Steinkraus A., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; Ortega A., Geomatics Program, University of Florida, Gainesville, FL, United States; Brush K., Montana State University, Bozeman, MT, United States; Yenni G., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; McKellar A.E., Environment and Climate Change Canada, Saskatoon, SK, Canada; Converse R., Center for the Advancement of Spatial Informatics Research and Education, University of New Mexico, Albuquerque, NM, United States; Lippitt C.D., Center for the Advancement of Spatial Informatics Research and Education, University of New Mexico, Albuquerque, NM, United States; Wegmann A., California Oceans Program, The Nature Conservancy, Sacramento, CA, United States; Holmes N.D., California Oceans Program, The Nature Conservancy, Sacramento, CA, United States; Edney A.J., Department of Zoology, University of Oxford, Oxford, United Kingdom; Hart T., Department of Zoology, University of Oxford, Oxford, United Kingdom; Jessopp M.J., School of Biological, Earth and Environmental Sciences, University College Cork, Cork, Ireland; Clarke R.H., School of Biological Sciences, Monash University, Melbourne, VIC, Australia; Marchowski D., Ornithological Station, Museum and Institute of Zoology, Polish Academy of Sciences, Gdańsk, Poland; Senyondo H., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; Dotson R., Quantaero, Reno, NV, United States; White E.P., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; Frederick P., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States; Ernest S.K.M., Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, FL, United States","Advances in artificial intelligence for computer vision hold great promise for increasing the scales at which ecological systems can be studied. The distribution and behavior of individuals is central to ecology, and computer vision using deep neural networks can learn to detect individual objects in imagery. However, developing supervised models for ecological monitoring is challenging because it requires large amounts of human-labeled training data, requires advanced technical expertise and computational infrastructure, and is prone to overfitting. This limits application across space and time. One solution is developing generalized models that can be applied across species and ecosystems. Using over 250,000 annotations from 13 projects from around the world, we develop a general bird detection model that achieves over 65% recall and 50% precision on novel aerial data without any local training despite differences in species, habitat, and imaging methodology. Fine-tuning this model with only 1000 local annotations increases these values to an average of 84% recall and 69% precision by building on the general features learned from other data sources. Retraining from the general model improves local predictions even when moderately large annotation sets are available and makes model training faster and more stable. Our results demonstrate that general models for detecting broad classes of organisms using airborne imagery are achievable. These models can reduce the effort, expertise, and computational resources necessary for automating the detection of individual organisms across large scales, helping to transform the scale of data collection in ecology and the questions that can be addressed. © 2022 The Ecological Society of America.","airborne monitoring; bird detection; computer vision; deep learning; unoccupied aerial vehicle","Animals; Artificial Intelligence; Birds; Deep Learning; Ecosystem; Humans; Neural Networks, Computer; airborne sensing; artificial intelligence; computer vision; resolution; unmanned vehicle; animal; artificial intelligence; bird; ecosystem; human","","","","","Microsoft Azure; Gordon and Betty Moore Foundation, GBMF, (GBMF4563); U.S. Army Corps of Engineers, USACE, (W912HZ‐20‐2‐0022/3); South Florida Water Management District, SFWMD, (4500126520)","We thank the many researchers for contributing published data and images to model training, including John Neill, Christian Pfeifer, Kyle Landolt, Heather Lynch, Madeline Hayes, Rodrigo Valle, Benjamin Kellenberger, Gary Andrew, and Francie Cuthbert. Dan Morris was instrumental in hosting data and organizing the AI4Earth community. Thanks to Mark Koneff and the Branch of Migratory Bird Surveys for their assistance in gathering data and providing insight on model application. This work was supported by an AI4Earth grant from Microsoft Azure to BW. This research was supported by the Gordon and Betty Moore Foundation's Data‐Driven Discovery Initiative (GBMF4563) to Ethan P. White, by a grant from the Army Corps of Engineers (W912HZ‐20‐2‐0022/3) to S. K. Morgan Ernest and Peter Frederick, and by grants from the South Florida Water Management District (4500126520) to S. K. Morgan Ernest and Peter Frederick. ","Afan I., Manez M., Diaz-Delgado R., Drone Monitoring of Breeding Waterbird Populations: The Case of the Glossy Ibis, Drones, 2, (2018); Ahumada J.A., Fegraus E., Birch T., Flores N., Kays R., O'Brien T.G., Palmer J., Et al., Wildlife Insights: A Platform to Maximize the Potential of Camera Trap and Other Passive Sensor Wildlife Data for the Planet, Environmental Conservation, 47, pp. 1-6, (2020); Beery S., Wu G., Rathod V., Votel R., Huang J., Context R-CNN: Long Term Temporal Context for per-Camera Object Detection, pp. 13075-13085, (2020); Beijbom O., Edmunds P.J., Roelfsema C., Smith J., Kline D.I., Neal B.P., Dunlap M.J., Et al., Towards Automated Annotation of Benthic Survey Images: Variability of Human Experts and Operational Modes of Automation, PLoS One, 10, (2015); Berger-Wolf T.Y., Rubenstein D.I., Stewart C.V., Holmberg J.A., Parham J., Menon S., Crall J., Van Oast J., Kiciman E., Joppa L., Wildbook: Crowdsourcing, Computer Vision, and Data Science for Conservation, (2017); Bondi E., Dey D., Kapoor A., Piavis J., Shah S., Fang F., Dilkina B., Et al., AirSim-W: A Simulation Environment for Wildlife Conservation with UAVs, Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies, pp. 1-12, (2018); Bowley C., Mattingly M., Barnas A., Ellis-Felege S., Desell T., Detecting Wildlife in Unmanned Aerial Systems Imagery Using Convolutional Neural Networks Trained with an Automated Feedback Loop, Computational Science – ICCS 2018, pp. 69-82, (2018); Chabot D., Dillon C., Francis C., An Approach for Using off-the-Shelf Object-Based Image Analysis Software to Detect and Count Birds in Large Volumes of Aerial Imagery, Avian Conservation and Ecology, 13, (2018); Crall J.P., Stewart C.V., Berger-Wolf T.Y., Rubenstein D.I., Sundaresan S.R., “HotSpotter — Patterned Species Instance Recognition.” In 2013 IEEE Workshop on Applications of Computer Vision (WACV), pp. 230-237, (2013); Dulava S., Bean W.T., Richmond O.M.W., Environmental Reviews and Case Studies: Applications of Unmanned Aircraft Systems (UAS) for Waterbird Surveys, Environmental Practice, 17, pp. 201-210, (2015); Graves A., Bellemare M.G., Menick J., Munos R., Kavukcuoglu K., Automated Curriculum Learning for Neural Networks, Proceedings of the 34th International Conference on Machine Learning, 70, pp. 1311-1320, (2017); Gregory R.D., van Strien A., Wild Bird Indicators: Using Composite Population Trends of Birds as Measures of Environmental Health, Ornithological Science, 9, pp. 3-22, (2010); Groom G., Stjernholm M., Nielsen R.D., Fleetwood A., Petersen I.K., Remote Sensing Image Data and Automated Analysis to Describe Marine Bird Distributions and Abundances, Ecological Informatics, 14, pp. 2-8, (2013); Hayes M.C.P.C., Gray G., Harris W.C., Sedgwick V.D., Crawford N., Chazal S., Croftsjohnston D.W., Drones and Deep Learning Produce Accurate and Efficient Monitoring of Large-Scale Seabird Colonies, Ornithological Applications, 123, 3, (2021); Kawaguchi K., Kaelbling L.P., Bengio Y., Generalization in Deep Learning, (2020); Kellenberger B., Marcos D., Tuia D., Detecting Mammals in UAV Images: Best Practices to Address a Substantially Imbalanced Dataset with Deep Learning, Remote Sensing of Environment, 216, pp. 139-153, (2018); Kellenberger B., Tuia D., Morris D., AIDE: Accelerating Image-Based Ecological Surveys with Interactive Machine Learning, Methods in Ecology and Evolution, 11, pp. 1716-1727, (2020); Kim S., Kim M., Learning of Counting Crowded Birds of Various Scales Via Novel Density Activation Maps, IEEE Access, 8, pp. 155296-155305, (2020); LaRue M.A., Stapleton S., Anderson M., Feasibility of Using High-Resolution Satellite Imagery to Assess Vertebrate Wildlife Populations, Conservation Biology, 31, pp. 213-220, (2017); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., “Focal Loss for Dense Object Detection.” In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, (2017); Liu Y., Shah V., Borowicz A., Wethington M., Strycker N., Forrest S., Lynch H., Singh H., “Towards Efficient Machine Learning Methods for Penguin Counting in Unmanned Aerial System Imagery.” In 2020 IEEE/OES Autonomous Underwater Vehicles Symposium (AUV), pp. 1-7, (2020); McKellar A.E., Shephard N.G., Chabot D., Dual Visible-Thermal Camera Approach Facilitates Drone Surveys of Colonial Marshbirds, Remote Sensing in Ecology and Conservation, 7, pp. 214-226, (2021); Moreland E.E., Cameron M.F., Angliss R.P., Boveng P.L., Evaluation of a Ship-Based Unoccupied Aircraft System (UAS) for Surveys of Spotted and Ribbon Seals in the Bering Sea Pack Ice, Journal of Unmanned Vehicle Systems, 3, pp. 114-122, (2015); Pan S.J., Yang Q., A Survey on Transfer Learning, IEEE Transactions on Knowledge and Data Engineering, 22, pp. 1345-1359, (2010); Pfeifer C., Rummler M.-C., Mustafa O., Assessing Colonies of Antarctic Shags by Unmanned Aerial Vehicle (UAV) at South Shetland Islands, Antarctica, Antarctic Science, 33, pp. 133-149, (2021); Reintsma K.M., McGowan P.C., Callahan C., Collier T., Gray D., Sullivan J.D., Prosser D.J., Preliminary Evaluation of Behavioral Response of Nesting Waterbirds to Small Unmanned Aircraft Flight, Waterbirds, 41, pp. 326-331, (2018); Torney C.J., Lloyd-Jones D.J., Chevallier M., Moyer D.C., Maliti H.T., Mwita M., Kohi E.M., Hopcraft G.C., A Comparison of Deep Learning and Citizen Science Techniques for Counting Wildlife in Aerial Survey Images, Methods in Ecology and Evolution, 10, pp. 779-787, (2019); Van Horn G., Mac Aodha O., Song Y., Cui Y., Sun C., Shepard A., Adam H., Perona P., Belongie S., The iNaturalist Species Classification and Detection Dataset, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8769-8778, (2018); Weinstein B., “weecology/BirdDetector: Paper Submission (1.1).” Zenodo, (2021); Weinstein B.G., A Computer Vision for Animal Ecology, Journal of Animal Ecology, 87, pp. 533-545, (2018); Weinstein B., Fang D., Senyondo H., White E., Munshi D., “weecology/DeepForest: Pytorch release (1.0.0).” Zenodo, (2021); Weinstein B., Garner L., Saccomanno V.R., Steinkraus A., Ortega A., Brush K., Yenni G., Et al., A Global Model of Bird Detection in High Resolution Airborne Images Using Computer Vision, (2021); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks, Remote Sensing, 11, (2019); Weinstein B.G., Marconi S., Aubry-Kientz M., Vincent G., Senyondo H., White E.P., DeepForest: A Python Package for RGB Deep Learning Tree Crown Delineation, Methods in Ecology and Evolution, 11, pp. 1743-1751, (2020); Weissensteiner M.H., Poelstra J.W., Wolf J.B.W., Low-Budget Ready-to-Fly Unmanned Aerial Vehicles: An Effective Tool for Evaluating the Nesting Status of Canopy-Breeding Bird Species, Journal of Avian Biology, 46, pp. 425-430, (2015); Willi M., Pitman R.T., Cardoso A.W., Locke C., Swanson A., Boyer A., Veldthuis M., Fortson L., Identifying Animal Species In Camera Trap Images Using Deep Learning and Citizen Science, Methods in Ecology and Evolution, 10, pp. 80-91, (2019); Zoph B., Cubuk E.D., Ghiasi G., Lin T.-Y., Shlens J., Le Q.V., Learning Data Augmentation Strategies for Object Detection, (2019)","B.G. Weinstein; Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, United States; email: ben.weinstein@weecology.org","","Ecological Society of America","","","","","","10510761","","ECAPE","35708073","English","Ecol. Appl.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135870449"
"Han R.-Z.; Feng W.; Guo Q.; Hu Q.-H.","Han, Rui-Ze (57195939589); Feng, Wei (56471162500); Guo, Qing (57191163500); Hu, Qing-Hua (7403214664)","57195939589; 56471162500; 57191163500; 7403214664","Single Object Tracking Research: A Survey; [视频单目标跟踪研究进展综述]","2022","Jisuanji Xuebao/Chinese Journal of Computers","45","9","","1877","1907","30","1","10.11897/SP.J.1016.2022.01877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137158934&doi=10.11897%2fSP.J.1016.2022.01877&partnerID=40&md5=863b4f04a2b8e75a9a42f830cfea783f","College of Intelligence and Computing, Tianjin University, Tianjin, 300350, China; Key Research Center for Surface Monitoring and Analysis of Cultural Relics, Tianjin, 300350, China","Han R.-Z., College of Intelligence and Computing, Tianjin University, Tianjin, 300350, China, Key Research Center for Surface Monitoring and Analysis of Cultural Relics, Tianjin, 300350, China; Feng W., College of Intelligence and Computing, Tianjin University, Tianjin, 300350, China, Key Research Center for Surface Monitoring and Analysis of Cultural Relics, Tianjin, 300350, China; Guo Q., College of Intelligence and Computing, Tianjin University, Tianjin, 300350, China, Key Research Center for Surface Monitoring and Analysis of Cultural Relics, Tianjin, 300350, China; Hu Q.-H., College of Intelligence and Computing, Tianjin University, Tianjin, 300350, China","Visual object tracking is an important and fundamental task in computer vision, which has many real-world applications, e.g., video surveillance, visual navigation and robotic service. Visual object tracking also has many challenges, such as object loss, object deformation, background clutters, and object fast motion. To solve the above problems and track the target accurately and efficiently, many visual object tracking algorithms have been emerged in recent years. In this paper, we first review the two most popular tracking frameworks in the past ten years, i.e., the Correlation Filter(CF) and Siamese network based visual object tracking. We present the rationale, the improvement strategy, and the representative works of the above two frameworks in detail. Specifically, the CF technology has been used in visual object tracking for over ten years, which has a good balance between the tracking accuracy and running speed. In CF tracking, the target is located by applying a circular convolution operation on the learned filter and the current frame, which can be efficiently achieved by the Fast Fourier Transform(FFT). The Siamese network based trackers locate the target from the candidate patches through a matching function offline learned on abundant training data in terms of image pairs. The matching function is modeled by a two-branch convolutional neural network(CNN) with shared parameters to learn the similarity between the target and the candidate patches. Besides the above two frameworks, we then present some other deep learning based tracking methods categorized by different network structures, e.g., RNN(Recurrent Neural Network), GCN(Graph Convolutional Network), etc. We also introduce some classical strategies for handling the challenges in the visual object tracking problem. From the recent tracking methods, we find that the development direction of the methods shows a diversified trend. More new network structures and skills have been applied to object tracking task. Although other deep tracking methods show a diversified trend, they have not formed a complete system. In the past ten years, the mainstream frameworks were the Correlation Filter and Siamese network. For the development trend of the visual tracking in the next few years, the CF tracking method has been relatively mature, and the development space in the future is limited. The deep learning algorithm based on CNN, especially the tracking algorithm under Siamese framework, could still be the mainstream framework. Further, this paper detailedly presents and compares the benchmarks and challenges for visual object tracking task, including the OTB benchmark, LaSOT benchmark, and VOT challenges, etc. Based on the data statistics of the datasets and the performance evaluation of the algorithms, we summarize the characteristics and advantages of various visual object tracking algorithms. For the future development of visual object tracking, which would be applied in real-world scenes before some problems to be addressed, such as the problems in long-term tracking, low-power high-speed tracking and attack-robust tracking. In the future, we can consider the integration of the traditional color(RGB) image together with the multi-modal data, such as the depth image, the thermal image, for joint analysis, which will provide more solutions for the visual object tracking task. Moreover, the visual tracking task will develop together with some other related tasks for mutual promotion, e.g., the video object detection, the video object segmentation task. © 2022, Science Press. All right reserved.","Correlation filter based tracking; Development of visual tracking; Siamese network based tracking; Survey; Visual object tracking; Visual tracking benchmark","Computer vision; Convolution; Convolutional neural networks; Fast Fourier transforms; Learning algorithms; Learning systems; Security systems; Target tracking; Correlation filter based tracking; Correlation filters; Development of visual tracking; Filter-based; Network-based; Siamese network based tracking; Visual object tracking; Visual Tracking; Visual tracking benchmark; Recurrent neural networks","","","","","National Natural Science Foundation of China, NSFC, (62072334, U1803264); Natural Science Foundation of Tianjin City, (18JCYBJC15200); Tianjin Research Innovation Project for Postgraduate Students, (2021YJSB174)","This work was supported by the Natural Science Foundation of Tianjin under Grant No.18JCYBJC15200, the Tianjin Research Innovation Project for Postgraduate Students under Grant No.2021YJSB174, and the National Natural Science Foundation of China under Grant Nos. U1803264 and 62072334.","Huang Kai-Qi, Chen Xiao-Tang, Kang Yun-Feng, Tan Tie-Niu, Intelligent visual surveillance: A review, Chinese Journal of Computers, 38, 6, pp. 1093-1118, (2015); Han R, Feng W, Zhao J, Et al., Complementary-view multiple human tracking, Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10917-10924, (2020); Han R, Feng W, Zhang Y, Et al., Multiple human association and tracking from egocentric and complementary top views, IEEE Transactions on Pattern Analysis and Machine Intelligence, (2021); Han R, Zhao J, Feng W, Et al., Complementary-view co-interest person detection, Proceedings of the 28th ACM International Conference on Multimedia, pp. 2746-2754, (2020); Liu Cai-Hong, Zhang Lei, Huang Hua, Visualization of cross-view multi-object tracking for surveillance videos in crossroad, Chinese Journal of Computers, 41, 1, pp. 221-235, (2018); Ge Bao-Yi, Zuo Xian-Zhang, Hu Yong-Jiang, Review of visual object tracking technology, Journal of Image and Graphics, 23, 8, pp. 1091-1107, (2018); Meng Lu, Yang Xu, A survey of object tracking algorithms, ACTA Automatica Sinica, 45, 7, pp. 1244-1260, (2019); Avidan S., Support vector tracking, IEEE Transactions on Pattern Analysis and Machine Intelligence, 26, 8, pp. 1064-1072, (2004); Ning J, Yang J, Jiang S, Et al., Object tracking via dual linear structured SVM and explicit feature map, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4266-4274, (2016); Babenko B, Yang M H, Belongie S., Visual tracking with online multiple instance learning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 983-990, (2009); Zhang T, Ghanem B, Liu S, Ahuja N., Robust visual tracking via multi-task sparse learning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2042-2049, (2012); Bolme D S, Beveridge J R, Draper B A, Lui Y M., Visual object tracking using adaptive correlation filters, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2544-2550, (2010); Chen Xu, Meng Zhao-Hui, Survey on video object tracking algorithms based on deep learning, Computer Systems and Applications, 28, 1, pp. 1-9, (2019); Li Xi, Zha Yu-Fei, Zhang Tian-Zhu, Et al., Survey of visual object tracking algorithms based on deep learning, Journal of Image and Graphics, 24, 12, pp. 2057-2080, (2019); Chen Yun-Fang, Wu Yi, Zhang Wei, Survey of target tracking algorithm based on Siamesenetwork structur, Computer Engineering and Applications, 56, 6, pp. 10-18, (2020); Wu Y, Lim J, Yang M H., Online object tracking: A benchmark, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2411-2418, (2013); Fan H, Ling H, Lin L, Et al., LaSOT: A high-quality benchmark for large-scale single object tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5374-5383, (2019); Yilmaz A, Javed O, Shah M., Object tracking: A survey, ACM Computer Survey, 38, 4, (2006); Smeulders A W M, Chu D M, Cucchiara R, Et al., Visual tracking: An experimental survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, 36, 7, pp. 1442-1468, (2014); Wu Y, Lim J, Yang M H., Object tracking benchmark, IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, 9, pp. 1834-1848, (2015); Ross D A, Lim J, Lin R S, Yang M H., Incremental learning for robust visual tracking, International Journal on Computer Vision, 77, 1, pp. 125-141, (2007); Nam H, Han B., Learning multi-domain convolutional neural networks for visual tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4293-4302, (2016); Fan H, Ling H., SANet: Structure-aware network for visual tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2217-2224, (2017); Bertinetto L, Valmadre J, Henriques J F, Et al., Fully-convolutional Siamese networks for object tracking, Proceedings of the European Conference on Computer Vision, pp. 850-865, (2016); Feng W, Han R, Guo Q, Et al., Dynamic saliency-aware regularization for correlation filter-based object tracking, IEEE Transactions on Image Processing, 28, 7, pp. 3232-3245, (2019); Henriques J F, Caseiro R, Martins P, Batista J., Exploiting the circulant structure of tracking-by-detection with kernels, Proceedings of the European Conference on Computer Vision, pp. 702-715, (2012); Henriques J F, Caseiro R, Martins P, Batista J., High-speed tracking with kernelized correlation filters, IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, 3, pp. 583-596, (2015); Dalal N, Triggs B., Histograms of oriented gradients for human detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 886-893, (2005); Danelljan M, Khan F S, Felsberg M, Weijer J., Adaptive color attributes for real-time visual tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1090-1097, (2014); Li Y, Zhu J., A scale adaptive kernel correlation filter tracker with feature integration, Proceedings of the European Conference on Computer Vision, pp. 254-265, (2014); Possegger H, Mauthner T, Bischof H., In defense of color-based model-free tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2113-2120, (2015); Bertinetto L, Valmadre J, Golodetz, Et al., Staple: Complementary learners for real-time tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1401-1409, (2015); Lukei A, Vojir T, Cehovin Zajc L, Et al., Discriminative correlation filter with channel and spatial reliability, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6309-6318, (2017); Danelljan M, Hger G, Khan F S, Felsberg M., Convolutional features for correlation filter based visual tracking, Proceedings of the IEEE International Conference on Computer Vision Workshop, pp. 58-66, (2015); Ma C, Huang J B, Yang X, Yang M H., Hierarchical convolutional features for visual tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 3074-3082, (2015); Qi Y, Zhang S, Qin L, Et al., Hedged deep tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4303-4311, (2016); Danelljan M, Robinson F S, Khan A, Felsberg M., Beyond correlation filters: Learning continuous convolution operators for visual tracking, Proceedings of the European Conference on Computer Vision, pp. 472-488, (2016); Danelljan M, Bhat G, Khan F S, Felsberg M., ECO: Efficient convolution operators for tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6638-6646, (2016); Gundogdu E, Alatan A A., Good features to correlate for visual tracking, IEEE Transactions on Image Processing, 27, 5, pp. 2526-2540, (2018); Huang C, Lucey S, Ramanan D., Learning policies for adaptive tracking with deep feature cascades, Proceedings of the IEEE International Conference on Computer Vision, pp. 105-114, (2017); Bhat G, Johnander J, Danelljan M, Et al., Unveiling the power of deep tracking, Proceedings of the European Conference on Computer Vision, pp. 493-509, (2018); Danelljan M, Hger G, Khan F S, Felsberg M., Accurate scale estimation for robust visual tracking, Proceedings of the British Machine Vision Conference, pp. 1-11, (2014); Danelljan M, Hger G, Khan F S, Felsberg M., Discriminative scale space tracking, IEEE transactions on Pattern Analysis and Machine Intelligence, 39, 8, pp. 1561-1575, (2016); Galoogahi H K, Sim T, Lucey S., Correlation filters with limited boundaries, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4630-4638, (2015); Danelljan M, Hager G, Khan F S, Felsberg M., Learning spatially regularized correlation filters for visual tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 4310-4318, (2015); Li F, Tian C, Zuo W, Et al., Learning spatial-temporal regularized correlation filters for visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4904-4913, (2018); Han R, Guo Q, Feng W., Content-related spatial regularization for visual object tracking, Proceedings of the IEEE International Conference on Multimedia and Expo, pp. 1-6, (2018); Galoogahi H K, Fagg A, Lucey S., Learning background-aware correlation filters for visual tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1135-1143, (2017); Dai K, Wang D, Lu H, Et al., Visual tracking via adaptive spatially-regularized correlation filters, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4670-4679, (2019); Han R, Feng W, Wang S., Fast learning of spatially regularized and content aware correlation filter for visual tracking, IEEE Transactions on Image Processing, 29, pp. 7128-7140, (2020); Bromley J, Guyon I, Lecun Y, Et al., Signature verification using a ""Siamese"" time delay neural network, Proceedings of the Neural Information Processing Systems, pp. 737-744, (1993); Tao R, Gavves E, Smeulders A W M., Siamese instance search for tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1420-1429, (2016); Li B, Yan J, Wu W, Et al., High performance visual tracking with Siamese region proposal network, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8971-8980, (2018); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Zhu Z, Wang Q, Bo L, Et al., Distractor-aware Siamese networks for visual object tracking, Proceedings of the European Conference on Computer Vision, pp. 101-117, (2018); Wang G, Luo C, Xiong Z, Zeng W., SPM-tracker: Series-parallel matching for real-time visual object tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3643-3652, (2019); Fan H, Ling H., Siamese cascaded region proposal networks for real-time visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7952-7961, (2019); Ren S, He K, Girshick R, Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 31, 6, pp. 2164-2173, (2019); Cai Z, Vasconcelos N., Cascade R-CNN: Delving into high quality object detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6154-6162, (2018); Li B, Wu W, Wang Q, Et al., SiamRPN++: Evolution of Siamese visual tracking with very deep networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4282-4291, (2019); Zhang Z, Peng H., Deeper and wider Siamese networks for real-time visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4591-4600, (2019); Wang Q, Teng Z, Xing J, Et al., Learning attentions: Residual attentional Siamese network for high performance online visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4854-4863, (2018); Yu Y, Xiong Y, Huang W, Scott M R., Deformable Siamese attention networks for visual object tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6728-6737, (2020); Du F, Liu P, Zhao W, Tang X., Correlation-guided attention for corner detection based visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6836-6845, (2020); Guo Q, Feng W, Zhou C, Et al., Learning dynamic Siamese network for visual object tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 1781-1789, (2017); Dong X, Shen J, Shao L, Porikli F., CLNet: A compact latent network for fast adjusting Siamese trackers, Proceedings of the European Conference on Computer Vision, pp. 378-395, (2020); Liu Y, Li R, Cheng Y, Et al., Object tracking using spatio-temporal networks for future prediction location, Proceedings of the European Conference on Computer Vision, pp. 1-17, (2020); Yang T, Chan A B., Learning dynamic memory networks for object tracking, Proceedings of the European Conference on Computer Vision, pp. 153-169, (2018); Yang T, Chan A B., Visual tracking via dynamic memory networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 1, pp. 360-374, (2019); Xu Y, Wang Z, Li Z, Et al., SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines, Proceedings of the AAAI Conference on Artificial Intelligence, pp. 12549-12556, (2020); Guo D, Wang J, Cui Y, Et al., SiamCAR: Siamese fully convolutional classification and regression for visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6269-6277, (2020); Chen Z, Zhong B, Li G, Et al., Siamese box adaptive network for visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6668-6677, (2020); Zhang Z, Peng H., Learning object-aware anchor-free networks for real-time object tracking, Proceedings of the European Conference on Computer Vision, pp. 771-787, (2020); Tian Z, Shen C, Chen H, He T., FOCS: Fully convolutional one-stage object detection, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9627-9636, (2019); Dong X, Shen J., Triplet loss in Siamese network for object tracking, Proceedings of the European Conference on Computer Vision, pp. 472-488, (2018); Jung I, Son J, Baek M, Han B., Real-time MDNet, Proceedings of the European Conference on Computer Vision, pp. 83-98, (2018); Nam H, Baek M, Han B., Modeling and propagating CNNs in a tree structure for visual tracking, (2016); Song Y, Ma C, Gong L, Et al., CREST: Convolutional residual learning for visual tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 2555-2564, (2017); Yang T, Chan A B., Recurrent filter learning for visual tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 2010-2019, (2017); Gao J, Zhang T, Xu C., Graph convolutional tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4649-4659, (2019); Danelljan M, Bhat G, Khan F S, Felsberg M., ATOM: Accurate tracking by overlap maximization, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4660-4669, (2019); Jiang B, Luo R, Mao J, Et al., Acquisition of localization confidence for accurate object detection, Proceedings of the European Conference on Computer Vision, pp. 784-799, (2018); Bhat G, Danelljan M, Gool L V, Timofte R., Learning discriminative model prediction for tracking, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6182-6191, (2019); Danelljan M, Gool L V, Timofte R., Probabilistic regression for visual tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7183-7192, (2020); Wang M, Liu Y, Huang Z., Large margin object tracking with circulant feature maps, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4021-4029, (2017); Zhou C, Guo Q, Wan L, Feng W., Selective object and context tracking, Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1947-1951, (2017); Guo Q, Han R, Feng W, Et al., Selective spatial regularization by reinforcement learned decision making for object tracking, IEEE Transactions on Image Processing, 29, pp. 2999-3013, (2019); Voigtlaender P, Luiten J, Torr P H S, Leibe B., Siam R-CNN: Visual tracking by re-detection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6578-6588, (2020); Wang Q, Zhang L, Bertinetto L, Et al., Fast online object tracking and segmentation: A unifying approach, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1328-1338, (2019); Lukei A, Matas J, Kristan M., D3S-A discriminative single shot segmentation tracker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7133-7142, (2020); Mueller M, Smith N, Ghanem B., Context- aware correlation filter tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1396-1404, (2017); Liao B, Wang C, Wang Y, Et al., PG-Net: Pixel to global matching network for visual tracking, Proceedings of the European Conference on Computer Vision, pp. 429-444, (2020); Bhat G, Danelljan M, Van Gool L, Timofte R., Know your surroundings: Exploiting scene information for object tracking, Proceedings of the European Conference on Computer Vision, pp. 205-221, (2020); Zhang Y, Yang Y, Zhou W, Et al., Motion-aware correlation filters for online visual tracking, Sensors, 18, 11, (2018); Wu Y, Hu J, Li F, Et al., Kernel-based motion-blurred target tracking, Proceedings of the International Symposium on Visual Computing, pp. 486-495, (2011); Ma B, Huang L, Shen J, Et al., Visual tracking under motion blur, IEEE Transactions on Image Processing, 25, 12, pp. 5867-5876, (2016); Wu Y, Ling H, Yu J, Et al., Blurred target tracking by blur-driven tracker, Proceedings of the IEEE International Conference on Computer Vision, pp. 1100-1107, (2011); Seibold C, Hilsmann A, Eisert P., Model-based motion blur estimation for the improvement of motion tracking, Computer Vision and Image Understanding, 160, pp. 45-56, (2017); Gladh S, Danelljan M, Khan F S, Felsberg M., Deep motion features for visual tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1243-1248, (2016); Guo Q, Feng W, Gao R, Et al., Exploring the effects of blur and deblurring to visual object tracking, IEEE Transactions on Image Processing, 30, pp. 1812-1824, (2021); Liang P, Blasch E, Ling H., Encoding color information for visual tracking: Algorithms and benchmark, IEEE Transactions on Image Processing, 24, 12, pp. 5630-5644, (2015); Mueller M, Smith N, Ghanem B., A benchmark and simulator for UAV tracking, Proceedings of the European Conference on Computer Vision, pp. 445-461, (2016); Li A, Lin M, Wu Y, Et al., NUS-PRO: A new visual tracking challenge, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 2, pp. 335-349, (2016); Galoogahi H K, Fagg A, Huang C, Et al., Need for Speed: A benchmark for higher frame rate object tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 1125-1134, (2017); Valmadre J, Bertinetto L, Henriques J F, Et al., Long-term tracking in the wild: A benchmark, Proceedings of the European Conference on Computer Vision, pp. 670-685, (2018); Muller M, Bibi A, Giancola S, Et al., TrackingNet: A large-scale dataset and benchmark for object tracking in the wild, Proceedings of the European Conference on Computer Vision, pp. 300-317, (2018); Huang L, Zhao X, Huang K., GOT-10k: A large high-diversity benchmark for generic object tracking in the wild, IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 5, pp. 1562-1577, (2021); Lukei A, Kart U, Kapyla J, Et al., CDTB: A color and depth visual object tracking dataset and benchmark, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10013-10022, (2019); Li C, Liang X, Lu L, Et al., RGB-T object tracking: Benchmark and baseline, Pattern Recognition, 96, (2019); Miller G A., WordNet: A lexical database for English, Communications of the ACM, 38, 11, pp. 39-41, (1995); Kristan M, Pflugfelder R, Leonardis A, Et al., The VOT2013 challenge: Overview and additional results, Proceedings of the Nineteenth Computer Vision Winter Workshop, pp. 1-8, (2014); Hadfield S, Lebeda K, Bowden R., The visual object tracking VOT2014 challenge results, Proceedings of the European Conference on Computer Vision, pp. 191-217, (2014); Kristan M, Matas J, Leonardis A, Et al., The visual object tracking VOT2015 challenge results, Proceedings of the IEEE International Conference on Computer Vision Workshop, pp. 1-23, (2015); Kristan M, Leonardis A, Matas J, Et al., The visual object tracking VOT2016 challenge results, Proceedings of the European Conference on Computer Vision Workshop, pp. 777-823, (2016); Kristan M, Leonardis A, Matas J, Et al., The visual object tracking VOT2017 challenge results, Proceedings of the IEEE International Conference on Computer Vision Workshop, pp. 1-24, (2017); Kristan M, Leonardis A, Matas J, Et al., The sixth visual object tracking VOT2018 challenge results, Proceedings of the European Conference on Computer Vision Workshop, pp. 3-53, (2018); Kristan M, Berg A, Zheng L, Et al., The seventh visual object tracking VOT2019 challenge results, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop, pp. 1-36, (2019); Kristan M, Leonardis A, Matas J, Et al., The eighth visual object tracking VOT2020 challenge results, Proceedings of the European Conference on Computer Vision Workshop, pp. 547-601, (2020); Kristan M, Matas J, Leonardis A, Et al., A novel performance evaluation methodology for single-target trackers, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 11, pp. 2137-2155, (2016); Danelljan M, Hager G, Khan F S, Felsberg M., Convolutional features for correlation filter based visual tracking, Proceedings of the IEEE International Conference on Computer Vision, pp. 621-629, (2015); He A, Luo C, Tian X, Zeng W., A twofold Siamese network for real-time object tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4834-4843, (2018); He Z, Fan Y, Zhuang J, Et al., Correlation filters with weighted convolution responses, Proceedings of the IEEE International Conference on Computer Vision, pp. 1992-2000, (2017); Ma C, Yang X, Zhang C, Yang M H., Long-term correlation tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5388-5396, (2015); Yan B, Zhao H, Wang D, Et al., Skimming-Perusal Tracking: A framework for real-time and robust long-term tracking, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2385-2393, (2019); Dai K, Zhang Y, Wang D, Et al., High-performance long-term tracking with meta-updater, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6298-6307, (2020); Wang N, Song Y, Ma C, Et al., Unsupervised deep tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308-1317, (2019); Yan B, Wang D, Lu H, Yang Y., Cooling-shrinking attack: Blinding the tracker with imperceptible noises, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 990-999, (2020); Guo Q, Xie X, Juefei-Xu F, Et al., Spark: Spatial-aware online incremental attack against visual tracking, Proceedings of the European Conference on Computer Vision, pp. 202-219, (2020); Liang S, Wei X, Yao S, Cao X., Efficient adversarial attacks for visual object tracking, Proceedings of the European Conference on Computer Vision, pp. 34-50, (2020); Jia S, Ma C, Song Y, Yang X., Robust tracking against adversarial attacks, Proceedings of the European Conference on Computer Vision, pp. 69-84, (2020); Du D, Qi Y, Yu H, Et al., The unmanned aerial vehicle benchmark: Object detection and tracking, Proceedings of the European Conference on Computer Vision, pp. 370-386, (2018); Li S, Yeung D Y., Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models, Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4140-4146, (2017); Du B, Cai S, Wu C., Object tracking in satellite videos based on a multi-frame optical flow tracker, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12, 8, pp. 3043-3055, (2019); Huang Z, Fu C, Li Y, Et al., Learning aberrance repressed correlation filters for real-time UAV tracking, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2891-2900, (2019); Li Y, Fu C, Ding F, Et al., AutoTrack: Towards high-performance visual tracking for UAV with automatic spatio-temporal regularization, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11923-11932, (2020); Shao J, Du B, Wu C, Zhang L., Tracking objects from satellite videos: A velocity feature based correlation filter, IEEE Transactions on Geoscience and Remote Sensing, 57, 10, pp. 7860-7871, (2019); Shao J, Du B, Wu C, Zhang L., Can we track targets from space? A hybrid kernel correlation filter tracker for satellite video, IEEE Transactions on Geoscience and Remote Sensing, 57, 11, pp. 8719-8731, (2019); Song S, Xiao J., Tracking revisited using RGBD camera: Unified benchmark and baselines, Proceedings of the IEEE International Conference on Computer Vision, pp. 233-240, (2013); Liu W, Tang X, Zhao C., Robust RGBD tracking via weighted convolution operators, IEEE Sensors Journal, 20, 8, pp. 4496-4503, (2020); Camplani M, Hannuna S L, Mirmehdi M, Et al., Real-time RGB-D tracking with depth scaling kernelised correlation filters and occlusion handling, Proceedings of the British Machine Vision Conference, pp. 1-145, (2015); Kart U, Kamarainen J K, Matas J., How to make an RGBD tracker, Proceedings of the European Conference on Computer Vision, pp. 148-161, (2018); Li C, Wu X, Zhao N, Et al., Fusing two-stream convolutional neural networks for RGB-T object tracking, Neurocomputing, 281, pp. 78-85, (2018); Zhu Y, Li C, Luo B, Et al., Dense feature aggregation and pruning for RGBT tracking, Proceedings of the ACM International Conference on Multimedia, pp. 465-472, (2019); Zhang L, Danelljan M, Gonzalez-Garcia A, Et al., Multi-modal fusion for end-to-end RGB-T tracking, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop, pp. 1-10, (2019); Wang C, Xu C, Cui Z, Et al., Cross-modal pattern-propagation for RGB-T tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7064-7073, (2020); Li C, Zhu C, Huang Y, Et al., Cross-modal ranking with soft consistency and noisy labels for robust RGB-T tracking, Proceedings of the European Conference on Computer Vision, pp. 808-823, (2018); Li C, Liu L, Lu A, Et al., Challenge-aware RGBT tracking, Proceedings of the European Conference on Computer Vision, pp. 222-237, (2020); Huang L, Zhao X, Huang K., Bridging the gap between detection and tracking: A unified approach, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3999-4009, (2019); Wang G, Luo C, Sun X, Et al., Tracking by instance detection: A meta-learning approach, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6288-6297, (2020); Chen X, Li Z, Yuan Y, Et al., State-aware tracker for real-time video object segmentation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9384-9393, (2020); Sun M, Xiao J, Lim E G, Et al., Fast template matching and update for video object tracking and segmentation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10791-10799, (2020)","W. Feng; College of Intelligence and Computing, Tianjin University, Tianjin, 300350, China; email: wfeng@tju.edu.cn; W. Feng; Key Research Center for Surface Monitoring and Analysis of Cultural Relics, Tianjin, 300350, China; email: wfeng@tju.edu.cn","","Science Press","","","","","","02544164","","JIXUD","","Chinese","Jisuanji Xuebao","Review","Final","","Scopus","2-s2.0-85137158934"
"Karwowska K.","Karwowska, K. (57608598400)","57608598400","TRANSFER LEARNING IN THE CLASSIFICATION OF SATELLITE IMAGES SHOWING AMAZON RAINFOREST","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2022","","113","118","5","0","10.5194/isprs-archives-XLIII-B3-2022-113-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131914665&doi=10.5194%2fisprs-archives-XLIII-B3-2022-113-2022&partnerID=40&md5=34ace95de533d282a8edd81687398953","Department of Imagery Intelligence, Faculty of Civil Engineering and Geodesy, Military University of Technology, Warsaw, 00-908, Poland","Karwowska K., Department of Imagery Intelligence, Faculty of Civil Engineering and Geodesy, Military University of Technology, Warsaw, 00-908, Poland","In recent years, we have been dealing with the dynamic technological progress of the space sector, which allows for the observation of the Earth with better temporal, spatial and spectral resolution. The increasing availability of satellite data has contributed to the development of data processing algorithms. Thanks to the use of digital image processing methods and deep neural networks, it is possible to perform automatic image classification, segmentation or detection and recognition of objects on the images. This article presents the methodology that allows to accelerate the classification process of satellite images representing the Amazon rainforest based on the Transfer Learning method. Additionally, the influence of the choice of optimization, i.e. the network weight estimation strategy, on the classification of objects was checked. In order to verify the method, an additional raster image classifier was created on the basis of Lidar data. Research shows that the transfer learning method allows the preparation of an image classifier based on a small database (less than 100 images representing one class). The network training process can be shortened to a few minutes.  © Authors 2022","convolutional neural networks; high-resolution satellite imagery; image analysis; rainforest; scene classification; urban land cover classification","Convolution; Convolutional neural networks; Data handling; Deep neural networks; Earth (planet); Image classification; Image segmentation; Object detection; Satellite imagery; Amazon rain forest; Convolutional neural network; High resolution satellite imagery; Image Classifiers; Image-analysis; Rainfor; Satellite images; Scene classification; Transfer learning methods; Urban land cover classification; Classification (of information)","","","","","","","Alem A., Kumar S., Transfer Learning Models for Land Cover and Land Use Classification in Remote Sensing Image, Appl. Artif. Intell, pp. 1-19, (2021); Chandak S., Chitters V., Honnungar S., Understanding the Amazon Rainforest from Space using CNNs; Chollet F., Xception: Deep Learning with Depthwise Separable Convolutions, (2017); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, 2009 IEEE Conference on Computer Vision and Pattern Recognition. Presented at the 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, (2009); Duchi J., Hazan E., Singer Y., Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, J. Mach. Learn. Res, 12, pp. 2121-2159, (2011); Efremova N., Seddik M.E.A., Erten E., Soil Moisture Estimation using Sentinel-1/-2 Imagery Coupled with cycleGAN for Time-series Gap Filing, IEEE Trans. Geosci. Remote Sens, pp. 1-1, (2021); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, (2015); Hossin M., A Review on Evaluation Metrics for Data Classification Evaluations, Int. J. Data Min. Knowl. Manag. Process, 5, pp. 01-11, (2015); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Khan M.E., Liu Z., Tangkaratt V., Gal Y., Vprop: Variational Inference using RMSprop, (2017); Kingma D.P., Ba J., Adam: A Method for Stochastic Optimization, (2017); Kudli S., Qian S., Pastel B., Kaggle Competition: Understanding the Amazon from Space; Metzger N., Turkoglu M.O., D'Aronco S., Wegner J.D., Schindler K., Crop Classification Under Varying Cloud Cover With Neural Ordinary Differential Equations, IEEE Trans. Geosci. Remote Sens, pp. 1-12, (2021); Pires de Lima R., Marfurt K., Convolutional Neural Network for Remote-Sensing Scene Classification: Transfer Learning Analysis, Remote Sens, 12, (2020); Planet: Understanding the Amazon from Space [WWW Document], (2015); Risojevi V., Stojni V., The Role of Pre-Training in High-Resolution Remote Sensing Scene Classification, (2021); Ru C., Duan S.-B., Jiang X.-G., Li Z.-L., Jiang Y., Ren H., Leng P., Gao M., Land Surface Temperature Retrieval From Landsat 8 Thermal Infrared Data Over Urban Areas Considering Geometry Effect: Method and Application, IEEE Trans. Geosci. Remote Sens, pp. 1-16, (2021); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2015); Sun G., Chen X., Jia X., Yao Y., Wang Z., Combinational Build-Up Index (CBI) for Effective Impervious Surface Mapping in Urban Areas, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 9, pp. 2081-2092, (2016); Sutskever I., Martens J., Dahl G., Hinton G., On the importance of initialization and momentum in deep learning, Proceedings of the 30th International Conference on Machine Learning, pp. 1139-1147, (2013); Yuan Y., Lin L., Liu Q., Hang R., Zhou Z.-G., SITS-Former: A pre-trained spatio-spectral-temporal representation model for Sentinel-2 time series classification, Int. J. Appl. Earth Obs. Geoinformation, 106, (2022); Zeiler M.D., ADADELTA: An Adaptive Learning Rate Method, (2012); Zhong B., Yang A., Nie A., Yao Y., Zhang H., Wu S., Liu Q., Finer Resolution Land-Cover Mapping Using Multiple Classifiers and Multisource Remotely Sensed Data in the Heihe River Basin, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 8, pp. 4973-4992, (2015)","K. Karwowska; Department of Imagery Intelligence, Faculty of Civil Engineering and Geodesy, Military University of Technology, Warsaw, 00-908, Poland; email: kinga.karwowska@wat.edu.pl","Jiang J.; Shaker A.; Zhang Z.","International Society for Photogrammetry and Remote Sensing","","2022 24th ISPRS Congress on Imaging Today, Foreseeing Tomorrow, Commission III","6 June 2022 through 11 June 2022","Nice","179854","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131914665"
"Weyler J.; Magistri F.; Seitz P.; Behley J.; Stachniss C.","Weyler, Jan (57222163936); Magistri, Federico (57208212994); Seitz, Peter (57483928700); Behley, Jens (35207956200); Stachniss, Cyrill (6507517732)","57222163936; 57208212994; 57483928700; 35207956200; 6507517732","In-Field Phenotyping Based on Crop Leaf and Plant Instance Segmentation","2022","Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022","","","","2968","2977","9","5","10.1109/WACV51458.2022.00302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126093044&doi=10.1109%2fWACV51458.2022.00302&partnerID=40&md5=a6ecd76bdb8d0aee40af34295a301b03","University of Bonn, Germany; Robert Bosch GmbH, Germany","Weyler J., University of Bonn, Germany; Magistri F., University of Bonn, Germany; Seitz P., Robert Bosch GmbH, Germany; Behley J., University of Bonn, Germany; Stachniss C., University of Bonn, Germany","A detailed analysis of a plant's phenotype in real field conditions is critical for plant scientists and breeders to understand plant function. In contrast to traditional phenotyping performed manually, vision-based systems have the potential for an objective and automated assessment with high spatial and temporal resolution. One of such systems' objectives is to detect and segment individual leaves of each plant since this information correlates to the growth stage and provides phenotypic traits, such as leaf count, cover-age, and size. In this paper, we propose a vision-based approach that performs instance segmentation of individual crop leaves and associates each with its corresponding crop plant in real fields. This enables us to compute relevant basic phenotypic traits on a per-plant level. We employ a convolutional neural network and operate directly on drone imagery. The network generates two different representations of the input image that we utilize to cluster individual crop leaf and plant instances. We propose a novel method to compute clustering regions based on our network's predictions that achieves high accuracy. Furthermore, we com-pare to other state-of-the-art approaches and show that our system achieves superior performance. The source code of our approach is available 1.  © 2022 IEEE.","Grouping and Shape; Object Detection/Recognition/Categorization Deep Learning; Remote Sensing; Scene Understanding; Segmentation","Convolutional neural networks; Crops; Deep learning; Object detection; Plants (botany); Crop leaves; Crop plants; Grouping and shape; Object detection/recognition/categorization deep learning; Objects detection; Phenotypic traits; Phenotyping; Remote-sensing; Scene understanding; Segmentation; Remote sensing","","","","","Robert Bosch; Deutsche Forschungsgemeinschaft, DFG, (EXC-2070 - 390732324)","Acknowledgments This work has partially been funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy, EXC-2070 - 390732324 - PhenoRob and the Robert Bosch GmbH.","Berman M., Triki A.R., Blaschko M.B., The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks, Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), (2018); De Brabandere B., Neven D., Van Gool L., Semantic Instance Segmentation with a Discriminative Loss Function, Deep Learning for Robotic Vision Workshop, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), (2017); Champ J., Mora-Fallas A., Goeau H., Mata-Montero E., Bonnet P., Joly A., Instance Segmentation for the Fine Detection of Crop andWeed Plants by Precision Agricultural Robots, Applications in Plant Sciences, 8, 7, (2020); Chen D., Chen M., Altmann T., Klukas C., Bridging Genomics and Phenomics, Approaches in Integrative Bioinformatics, pp. 299-333, (2014); Everingham M., Van Gool L., Williams I.C.K., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, Intl. Journal of Computer Vision (IJCV), 88, 2, pp. 303-338, (2010); Forstner W., Wrobel B., Photogrammetric Computer Vision-Statistics, Geometry, Orientation and Reconstruction, (2016); Gockenbach M.S., Finite-Dimensional Linear Algebra, (2011); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proc. of the IEEE Intl. Conf. on Computer Vision (ICCV), (2017); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), (2016); Itzhaky Y., Farjon G., Khoroshevsky F., Shpigler A., Bar-Hillel A., Leaf Counting: Multiple Scale Regression and Detection Using Deep CNNs, Proc. of British Machine Vision Conference (BMVC), (2018); Kingma D.P., Ba J., Adam: A Method for Stochastic Optimization, (2014); Kulikov V., Lempitsky V., Instance Segmentation of Biological Images using Harmonic Embeddings, Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), (2020); Lancashire P.D., Bleiholder H., Van Den Boom T., Langeluddeke P., Stauss R., Weber E., Witzenberger A., A Uniform Decimal Code for Growth Stages of Crops andWeeds, Annals of Applied Biology, 119, 3, pp. 561-601, (1991); Lin T., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick L.C., Microsoft COCO: Common Objects in Context, Proc. of the Europ. Conf. on Computer Vision (ECCV), (2014); Lottes P., Behley J., Milioto A., Stachniss C., Fully Convolutional Networks with Sequential Information for Robust Crop and Weed Detection in Precision Farming, IEEE Robotics and Automation Letters (RAL), 3, pp. 3097-3104, (2018); Magistri F., Chebrolu N., Stachniss C., Segmentation-Based 4D Registration of Plants Point Clouds for Phenotyping, Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), (2020); McCool C., Perez T., Upcroft B., Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics, Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), (2017); Milioto A., Lottes P., Stachniss C., Realtime blob-wise sugar beets vs weeds classification for monitoring fields using convolutional neural networks, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, (2017); Milioto A., Lottes P., Stachniss C., Realtime Semantic Segmentation of Crop andWeed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs, Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), (2018); Minervini M., Scharr H., Tsaftaris S.A., Image Analysis: The New Bottleneck in Plant Phenotyping, IEEE Signal Processing Magazine, 32, 4, pp. 126-131, (2015); Morris D., A Pyramid CNN for Dense-Leaves Segmentation, Proc. of the Conf. on Computer and Robot Vision (CRV), (2018); Neven D., De Brabandere B., Proesmans M., Van Gool L., Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth, Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), (2019); Papandreou G., Zhu T., Chen L., Gidaris S., Tompson J., Murphy K., Personlab: Person Pose Estimation and Instance Segmentation with a Bottom-up, Part-based, Geometric Embedding Model, Proc. of the Europ. Conf. on Computer Vision (ECCV), (2018); Papeand J., Klukas C., 3-D Histogram-Based Segmentation and Leaf Detection for Rosette Plants, Proc. of the Europ. Conf. on Computer Vision (ECCV), pp. 61-74, (2014); Ren M., Zemel R.S., End-to-End Instance Segmentation and Counting with Recurrent Attention, Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), (2017); Romera E., Alvarez J.M., Bergasa L.M., Arroyo R., ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation, IEEE Trans. on Intelligent Transportation Systems (ITS), 19, 1, pp. 263-272, (2018); Romera-Paredes B., Hilaire Sean Torr P., Recurrent Instance Segmentation, Proc. of the Europ. Conf. on Computer Vision (ECCV), (2016); Scharr H., Minervini M., Fischbach A., Tsaftaris S.A., Annotated Image Datasets of Rosette Plants, European Conference on Computer Vision, pp. 6-12, (2014); Scharr H., Minervini M., French A.P., Klukas C., Kramer D.M., Liu X., Luengo I., Pape J., Polder G., Vukadinovic D., Leaf Segmentation in Plant Phenotyping: A Collation Study, Machine Vision and Applications, 27, 4, pp. 585-606, (2016); Shi W., Van De Zedde R., Jiang H., Kootstra G., Plant-part Segmentation Using Deep Learning and Multi-view Vision, Biosystems Engineering, 187, pp. 81-95, (2019); Strang G., Linear Algebra and Learning from Data, (2019); Weyler J., Milioto A., Falck T., Behley J., Stachniss C., Joint Plant Instance Detection and Leaf Count Estimation for In-Field Plant Phenotyping, IEEE Robotics and Automation Letters (RA-L), (2021); Wu Y., Chen L., Merhof D., Improving Pixel Embedding Learning through Intermediate Distance Regression Supervision for Instance Segmentation, Proc. of the Europ. Conf. on Computer Vision (ECCV), pp. 213-227, (2020); Wu Y., Kirillov A., Massa F., Lo W., Girshick R., Detectron2, (2019)","","","Institute of Electrical and Electronics Engineers Inc.","CVF; IEEE Computer Society","22nd IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022","4 January 2022 through 8 January 2022","Waikoloa","177326","","978-166540915-5","","","English","Proc. - IEEE/CVF Winter Conf. Appl. Comput. Vis., WACV","Conference paper","Final","","Scopus","2-s2.0-85126093044"
"Al-Shammary A.A.; Zaghden N.; Bouhlel M.S.","Al-Shammary, Ali Abbas (57984772500); Zaghden, Nizar (26430224600); Bouhlel, Med Salim (6507076729)","57984772500; 26430224600; 6507076729","Automatic image annotation system using deep learning method to analyse ambiguous images","2023","Periodicals of Engineering and Natural Sciences","11","2","","176","185","9","0","10.21533/pen.v11i2.3517.g1268","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152652843&doi=10.21533%2fpen.v11i2.3517.g1268&partnerID=40&md5=498363bbd1824437a3021eda99d66be6","National School of Electronics and Telecommunications of Sfax, University of Sfax, Tunisia; Higher School of Business, University of Sfax, Tunisia; Higher School of Biotechnology, University of Sfax, Tunisia; Smart systems for Engineering & E-health based on Technologies of Image & Telecommunications (SETIT), ISBS, University of Sfax, Tunisia","Al-Shammary A.A., National School of Electronics and Telecommunications of Sfax, University of Sfax, Tunisia, Smart systems for Engineering & E-health based on Technologies of Image & Telecommunications (SETIT), ISBS, University of Sfax, Tunisia; Zaghden N., Higher School of Business, University of Sfax, Tunisia, Smart systems for Engineering & E-health based on Technologies of Image & Telecommunications (SETIT), ISBS, University of Sfax, Tunisia; Bouhlel M.S., Higher School of Biotechnology, University of Sfax, Tunisia, Smart systems for Engineering & E-health based on Technologies of Image & Telecommunications (SETIT), ISBS, University of Sfax, Tunisia","Image annotation has gotten a lot of attention recently because of how quickly picture data has expanded. Together with image analysis and interpretation, image annotation, which may semantically describe images, has a variety of uses in allied industries including urban planning engineering. Even without big data and image identification technologies, it is challenging to manually analyze a diverse variety of photos. The improvements to the Automated Image Annotation (AIA) label system have been the subject of several scholarly research. The authors will discuss how to use image databases and the AIA system in this essay. The proposed method extracts image features from photos using an improved VGG-19, and then uses nearby features to automatically forecast picture labels. The proposed study accounts for both correlations between labels and images as well as correlations within images. The number of labels is also estimated using a label quantity prediction (LQP) model, which improves label prediction precision. The suggested method addresses automatic annotation methodologies for pixel-level images of unusual things while incorporating supervisory information via interactive spherical skins. The genuine things that were converted into metadata and identified as being connected to pre-existing categories were categorized by the authors using a deep learning approach called a conventional neural network (CNN) - supervised. Certain object monitoring systems strive for a high item detection rate (true-positive), followed by a low availability rate (falsepositive). The authors created a KD-tree based on k-nearest neighbors (KNN) to speed up annotating. In order to take into account for the collected image backdrop. The proposed method transforms the conventional two-class object detection problem into a multi-class classification problem, breaking the separated and identical distribution estimations on machine learning methodologies. It is also simple to use because it only requires pixel information and ignores any other supporting elements from various color schemes. The following factors are taken into consideration while comparing the five different AIA approaches: main idea, significant contribution, computational framework, computing speed, and annotation accuracy. A set of publicly accessible photos that serve as standards for assessing AIA methods is also provided, along with a brief description of the four common assessment signs. © The Author 2023. This work is licensed under a Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/) that allows others to share and adapt the material for any purpose (even commercially), in any medium with an acknowledgement of the work's authorship and initial publication in this journal","AIA model; Annotated Image; Image Analysis; VGG19 algorithm","","","","","","Ministry of Higher Education and Scientific Research, MOHESR; Ministry of Higher Education and Scientific Research, MHESR","The authors are grateful to the Iraqi Ministry of Higher Education and Scientific Research (MOHESR) for technically supporting the current research.","Nizar Zaghden B. K. A. A. R. M., Text Recognition in both ancient and cartographic documents, 1308, 6309, (2013); Chen J., Ying P., Fu X., Luo X., Guan H., Wei K., Automatic tagging by leveraging visual and annotated features in social media, IEEE, 9210, pp. 1-12, (2021); Stangl A., Morris M., Gurari D. P. S., Tree. Is the Person Naked? What People with Vision Impairments Want in Image Descriptions, Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, (2020); Ben H., Pan Y., Li Y., Yao T., Hong R., Wang M., Mei T., Unpaired Image Captioning with Semantic-Constrained Self-Learning, IEEE Trans. Multimed, (2021); Zaghden N., Ben Moussa S., Alimi A. M, Reconnaissance des fontes arabes par l'utilisation des dimensions fractales et des ondelettes, Actes du 9eme Colloque International Francophone sur l'Ecrit et le Document, (2006); Ambrosi C, Strozzi T., Study of landslides in Ticino: Photointerpretation and analysis of deformations with satellite radar interferometry, Bull. Of the Ticino Society of Sci. Nat, pp. 19-27, (2008); El-Baz F. a. W. J., Remote Sensing In Archaeology, (2007); Kurimo L. L. J. N. J. G. I. a. J. L. E., The Effect of Motion Blur and Signal Noise on Image Quality in Low Light Imaging, Proc. Scandinavian Conference on Image Analysis, (2009); HAMOUDA M., BOUHLEL M. S., Modified Convolutional Neural Networks Architecture for Hyperspectral Image Classification (Extra-Convolutional Neural Networks), IET Image Processing, pp. 1-8, (2021); Kulkarni D. S. a. S., Different types of Noises in Images and Noise Removing Technique, International Journal of Advanced Technology in Engineering and Science, pp. 50-62, (2015); Goswami S. J. a. S., A Comparative Study of Various Image Restoration Techniques with Different Types of Blur, International Journal of Research in Computer Applications and Robotics, 3, 11, pp. 54-60, (2015); Cheng Q., Zhang Q., Fu P., Tu C., Li S., A survey and analysis on automatic image annotation, Pattern Recognition, 79, pp. 242-259, (2018); Nizar ZAGHDEN R. M. A. M., A proposition of a robust system for historical document images indexation, International Journal of Computer Applications, 11, pp. 224-235, (2010); Hanbury A., A survey of methods for image annotation, Elsevier, 19, pp. 617-627, (2008); Marina Ivasic-Kos I. I., A Lightweight Network for Building Extraction from Remote Sensing Images, IEEE, 196-2892, pp. 99-112, (2021); Kang Jian, PiCoCo: Pixelwise Contrast and Consistency Learning for Semisupervised Building Footprint Segmentation, IEEE, 14, pp. 10548-10559, (2021); Ammar S., Bouwmans T., ZAGHDEN Nizar, Neji M., Deep detector classifier (DeepDC) for moving objects, IET Image Processing, 14, 1212, pp. 1490-1501, (2020); Ferentinos K. P., Deep learning models for plant disease detection and diagnosis, Computers and Electronics in Agriculture, 145, pp. 311-318, (2018); Mahajan V. J. P. A. P. L. B. N. S. a. S. C. G. P., Perceptual Quality Evaluation of Hazy Natural Images, IEEE Transactions on Industrial Informatics, (2021); Jonathan Long T. D., Fully Convolutional Networks for Semantic Segmentation, IEEE, pp. 3431-3440, (2015); Krizhevsky I. S. a. G. E. H. A., Imagenet classification with deep convolutional neural networks, NIPS, 3, 5, pp. 1-2, (2012); Zisserman K. S. a. A., Very deep convolutional networks for large-scale image recognition, CoRR, 4, 5, pp. 1-3, (2014); Szegedy W. L. Y. J. P. S. S. R. D. A. D. E. V. V. a. A. R. C., Going deeper with convolutions, CoRR, 1409, 4842, pp. 1-3, (2014); ZAGHDEN Nizar, M. B. M. B., Jasim MS, Identified of Collision Alert in Vehicle Ad hoc based on Machine learning, (2021); Long J. S. E. D. T., Fully convolutional networks for semantic segmentation, proceedings of IEEE conference on Computer Vision and pattern recognition, pp. 3431-3440, (2015); Krizhevsky A., Sutskever I., Hinton G., Imagenet classification with deep convolutional neural networks communication, ACM, 60, pp. 84-90, (2017); Le M. T. a. Q. V., EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, International Conference on Machine Learning, (2019); Chenga Q., Zhang Q., Fu P., Tu C., Li S., A survey and analysis on automatic image annotation, ELSEVIER, 79, pp. 242-259, (2018); Brodzicki J. J.-K. P. K. M. G. a. M. B. A., Pre-Trained Deep Convolutional Neural Network for Clostridioides Difficile Bacteria Cytotoxicity Classification Based on Fluorescence Images, sensors, (2020); Hanbay M. T. a. D., Plant disease and pest detection using deep learning-based features, Turkish Journal of Electrical Engineering & Computer Sciences, pp. 1636-1651, (2019); Guo X., Comparison and evaluation of annual NDVI time series in china, NOAA AVHRR LTDR and terra MODIS mod13c1 products, (2017); Wei Wei G. D. X. L., Automatic image annotation based on an improved nearest neighbor technique with tag semantic extension model, Elsever, 1877, pp. 616-623, (2021); Yang X., Pixel-level automatic annotation for forest fire image, pp. 1-14, (2021); Calum N. W. D. R. C. J. W. M. M. a. C. W., Wilson R., Receiver Operating Characteristic curve analysis determines association of individual potato foliage volatiles with onion thrips preference, cultivar and plant age, pp. 5-9, (2019); Jaouedi F. P. J. B. N. B. M. B. N, Prediction of Human Activities Based on a New Structure of Skeleton Features and Deep Learning Model, Sensors, 17, 4944, pp. 1-15, (2020)","A.A. Al-Shammary; National School of Electronics and Telecommunications of Sfax, University of Sfax, Tunisia; email: alidyala1987@gmail.com","","International University of Sarajevo","","","","","","23034521","","","","English","Period. eng. nat. sci.","Article","Final","","Scopus","2-s2.0-85152652843"
"Li X.; Yan L.; Qi P.; Zhang L.; Goudail F.; Liu T.; Zhai J.; Hu H.","Li, Xiaobo (56365004800); Yan, Lei (8328907300); Qi, Pengfei (57219935563); Zhang, Liping (57221977938); Goudail, François (57207536959); Liu, Tiegen (7405910538); Zhai, Jingsheng (57194603149); Hu, Haofeng (55612861500)","56365004800; 8328907300; 57219935563; 57221977938; 57207536959; 7405910538; 57194603149; 55612861500","Polarimetric Imaging via Deep Learning: A Review","2023","Remote Sensing","15","6","1540","","","","2","10.3390/rs15061540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151487904&doi=10.3390%2frs15061540&partnerID=40&md5=35b2e760b55f292683417901b078b17a","School of Marine Science and Technology, Tianjin University, Tianjin, 300072, China; Spatial Information Integration and 3S Engineering Application Beijing Key Laboratory, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, 100871, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Department of Imaging and Interventional Radiology, The Chinese University of Hong Kong, 999077, Hong Kong; Laboratoire Charles Fabry, CNRS, Institut d’Optique Graduate School, Université Paris-Saclay, Palaiseau, 91120, France; School of Precision Instrument and Opto-Electronics Engineering, Tianjin University, Tianjin, 300072, China","Li X., School of Marine Science and Technology, Tianjin University, Tianjin, 300072, China; Yan L., Spatial Information Integration and 3S Engineering Application Beijing Key Laboratory, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, 100871, China; Qi P., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Zhang L., Department of Imaging and Interventional Radiology, The Chinese University of Hong Kong, 999077, Hong Kong; Goudail F., Laboratoire Charles Fabry, CNRS, Institut d’Optique Graduate School, Université Paris-Saclay, Palaiseau, 91120, France; Liu T., School of Precision Instrument and Opto-Electronics Engineering, Tianjin University, Tianjin, 300072, China; Zhai J., School of Marine Science and Technology, Tianjin University, Tianjin, 300072, China; Hu H., School of Marine Science and Technology, Tianjin University, Tianjin, 300072, China","Polarization can provide information largely uncorrelated with the spectrum and intensity. Therefore, polarimetric imaging (PI) techniques have significant advantages in many fields, e.g., ocean observation, remote sensing (RS), biomedical diagnosis, and autonomous vehicles. Recently, with the increasing amount of data and the rapid development of physical models, deep learning (DL) and its related technique have become an irreplaceable solution for solving various tasks and breaking the limitations of traditional methods. PI and DL have been combined successfully to provide brand-new solutions to many practical applications. This review briefly introduces PI and DL’s most relevant concepts and models. It then shows how DL has been applied for PI tasks, including image restoration, object detection, image fusion, scene classification, and resolution improvement. The review covers the state-of-the-art works combining PI with DL algorithms and recommends some potential future research directions. We hope that the present work will be helpful for researchers in the fields of both optical imaging and RS, and that it will stimulate more ideas in this exciting research field. © 2023 by the authors.","convolutional neural network; deep learning; polarimetric imaging; polarization; remote sensing; synthetic aperture radar","Convolutional neural networks; Deep neural networks; Image enhancement; Image fusion; Image reconstruction; Object detection; Optical remote sensing; Polarimeters; Polarization; Radar imaging; Autonomous Vehicles; Biomedical diagnosis; Breakings; Convolutional neural network; Deep learning; Ocean observations; Physical modelling; Polarimetric imaging; Remote-sensing; Spectra's; Synthetic aperture radar","","","","","National Natural Science Foundation of China, NSFC, (62075161, 62205243)","This work was supported by the National Natural Science Foundation of China (62205243, 62075161).","Bass M., Van Stryland E.W., Williams D.R., Wolfe W.L., Handbook of Optics, 2, (1995); Tyson R.K., Principles of Adaptive Optics, (2015); Fowles G.R., Introduction to Modern Optics, (1989); Goldstein D.H., Polarized Light, (2017); Li X., Li H., Lin Y., Guo J., Yang J., Yue H., Li K., Li C., Cheng Z., Hu H., Et al., Learning-based denoising for polarimetric images, Opt. Express, 28, pp. 16309-16321, (2020); Li X., Hu H., Zhao L., Wang H., Yu Y., Wu L., Liu T., Polarimetric image recovery method combining histogram stretching for underwater imaging, Sci. Rep, 8, pp. 1-10, (2018); Wang H., Hu H., Li X., Guan Z., Zhu W., Jiang J., Liu K., Liu T., An angle of polarization (AoP) visualization method for DoFP polarization image sensors Based on three dimensional HSI color space, Sensors, 19, (2019); Li X., Zhang L., Qi P., Zhu Z., Xu J., Liu T., Zhai J., Hu H., Are indices of polarimetric purity excellent metrics for object identification in scattering media?, Remote Sens, 14, (2022); Song L.M.W.K., Adler D.G., Conway J.D., Diehl D.L., Farraye F.A., Kantsevoy S.V., Kwon R., Mamula P., Rodriguez B., Shah R.J., Et al., Narrow band imaging and multiband imaging, Gastrointest. Endosc, 67, pp. 581-589, (2008); Zhao Y., Yi C., Kong S.G., Pan Q., Cheng Y., Multi-band polarization imaging, Multi-Band Polarization Imaging and Applications, pp. 47-71, (2016); Hu H., Lin Y., Li X., Qi P., Liu T., IPLNet: A neural network for intensity-polarization imaging in low light, Opt. Lett, 45, pp. 6162-6165, (2020); Guan Z., Goudail F., Yu M., Li X., Han Q., Cheng Z., Hu H., Liu T., Contrast optimization in broadband passive polarimetric imaging based on color camera, Opt. Express, 27, pp. 2444-2454, (2019); Hariharan P., Optical Holography: Principles, Techniques, and Applications, (1998); Kim M.K., Full color natural light holographic camera, Opt. Express, 21, pp. 9636-9642, (2013); Levoy M., Light fields and computational imaging, Computer, 39, pp. 46-55, (2006); Tyo J.S., Goldstein D.L., Chenault D.B., Shaw J.A., Review of passive imaging polarimetry for remote sensing applications, Appl. Opt, 45, pp. 5453-5469, (2006); Morio J., Refregier P., Goudail F., Dubois-Fernandez P.C., Dupuis X., A characterization of Shannon entropy and Bhattacharyya measure of contrast in polarimetric and interferometric SAR image, Proc. IEEE, 97, pp. 1097-1108, (2009); Li X., Xu J., Zhang L., Hu H., Chen S.C., Underwater image restoration via Stokes decomposition, Opt. Lett, 47, pp. 2854-2857, (2022); Chen W., Yan L., Chandrasekar V., Optical polarization remote sensing, Int. J. Remote Sens, 41, pp. 4849-4852, (2020); Liu T., Guan Z., Li X., Cheng Z., Han Y., Yang J., Li K., Zhao J., Hu H., Polarimetric underwater image recovery for color image with crosstalk compensation, Opt. Lasers Eng, 124, (2020); Meriaudeau F., Ferraton M., Stolz C., Morel O., Bigue L., Polarization imaging for industrial inspection, Image Process. Mach. Vis. Appl. Int. Soc. Opt. Photonics, 6813, (2008); Liu X., Li X., Chen S.C., Enhanced polarization demosaicking network via a precise angle of polarization loss calculation method, Opt. Lett, 47, pp. 1065-1069, (2022); Li X., Han Y., Wang H., Liu T., Chen S.C., Hu H., Polarimetric Imaging Through Scattering Media: A Review, Front. Phys, 10, (2022); Cheng G., Xie X., Han J., Guo L., Xia G.S., Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportunities, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 3735-3756, (2020); Demos S., Alfano R., Optical polarization imaging, Appl. Opt, 36, pp. 150-155, (1997); Liu Y., York T., Akers W.J., Sudlow G.P., Gruev V., Achilefu S., Complementary fluorescence-polarization microscopy using division-of-focal-plane polarization imaging sensor, J. Biomed. Opt, 17, (2012); Fade J., Panigrahi S., Carre A., Frein L., Hamel C., Bretenaker F., Ramachandran H., Alouini M., Long-range polarimetric imaging through fog, Appl. Opt, 53, pp. 3854-3865, (2014); Li X., Hu H., Zhao L., Wang H., Han Q., Cheng Z., Liu T., Pseudo-polarimetric method for dense haze removal, IEEE Photonics J, 11, (2019); Li X., Wang H., Hu H., Liu T., Polarimetric underwater image recovery based on circularly polarized illumination and histogram stretching, AOPC 2019: Optical Sensing and Imaging Technology, (2019); Zhanghao K., Chen L., Yang X.S., Wang M.Y., Jing Z.L., Han H.B., Zhang M.Q., Jin D., Gao J.T., Xi P., Super-resolution dipole orientation mapping via polarization demodulation, Light. Sci. Appl, 5, (2016); Hao X., Kuang C., Wang T., Liu X., Effects of polarization on the de-excitation dark focal spot in STED microscopy, J. Opt, 12, (2010); Li X., Goudail F., Chen S.C., Self-calibration for Mueller polarimeters based on DoFP polarization imagers, Opt. Lett, 47, pp. 1415-1418, (2022); Li X., Liu W., Goudail F., Chen S.C., Optimal nonlinear Stokes–Mueller polarimetry for multi-photon processes, Opt. Lett, 47, pp. 3287-3290, (2022); Goudail F., Terrier P., Takakura Y., Bigue L., Galland F., DeVlaminck V., Target detection with a liquid-crystal-based passive Stokes polarimeter, Appl. Opt, 43, pp. 274-282, (2004); Schechner Y.Y., Narasimhan S.G., Nayar S.K., Instant dehazing of images using polarization, Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, 1; Treibitz T., Schechner Y.Y., Active polarization descattering, IEEE Trans. Pattern Anal. Mach. Intell, 31, pp. 385-399, (2008); Schechner Y.Y., Narasimhan S.G., Nayar S.K., Polarization-based vision through haze, Appl. Opt, 42, pp. 511-525, (2003); Ghosh N., Vitkin A.I., Tissue polarimetry: Concepts, challenges, applications, and outlook, J. Biomed. Opt, 16, (2011); Rehbinder J., Haddad H., Deby S., Teig B., Nazac A., Novikova T., Pierangelo A., Moreau F., Ex vivo Mueller polarimetric imaging of the uterine cervix: A first statistical evaluation, J. Biomed. Opt, 21, (2016); Jacques S.L., Ramella-Roman J.C., Lee K., Imaging skin pathology with polarized light, J. Biomed. Opt, 7, pp. 329-340, (2002); Wang W., Lim L.G., Srivastava S., Bok-Yan So J., Shabbir A., Liu Q., Investigation on the potential of Mueller matrix imaging for digital staining, J. Biophotonics, 9, pp. 364-375, (2016); Pierangelo A., Benali A., Antonelli M.R., Novikova T., Validire P., Gayet B., De Martino A., Ex-vivo characterization of human colon cancer by Mueller polarimetric imaging, Opt. Express, 19, pp. 1582-1593, (2011); Parikh H., Patel S., Patel V., Classification of SAR and PolSAR images using deep learning: A review, Int. J. Image Data Fusion, 11, pp. 1-32, (2020); Pierangelo A., Manhas S., Benali A., Fallet C., Totobenazara J.L., Antonelli M.R., Novikova T., Gayet B., De Martino A., Validire P., Multispectral Mueller polarimetric imaging detecting residual cancer and cancer regression after neoadjuvant treatment for colorectal carcinomas, J. Biomed. Opt, 18, (2013); Lee J.S., Grunes M.R., De Grandi G., Polarimetric SAR speckle filtering and its implication for classification, IEEE Trans. Geosci. Remote Sens, 37, pp. 2363-2373, (1999); Lee J.S., Pottier E., Polarimetric Radar Imaging: From Basics to Applications, (2017); Yan L., Li Y., Chandrasekar V., Mortimer H., Peltoniemi J., Lin Y., General review of optical polarization remote sensing, Int. J. Remote Sens, 41, pp. 4853-4864, (2020); Mullissa A.G., Tolpekin V., Stein A., Perissin D., Polarimetric differential SAR interferometry in an arid natural environment, Int. J. Appl. Earth Obs. Geoinf, 59, pp. 9-18, (2017); Shang R., He J., Wang J., Xu K., Jiao L., Stolkin R., Dense connection and depthwise separable convolution based CNN for polarimetric SAR image classification, Knowl.-Based Syst, 194, (2020); Pourshamsi M., Xia J., Yokoya N., Garcia M., Lavalle M., Pottier E., Balzter H., Tropical forest canopy height estimation from combined polarimetric SAR and LiDAR using machine-learning, ISPRS J. Photogramm. Remote Sens, 172, pp. 79-94, (2021); Yang X., Pan T., Yang W., Li H.C., PolSAR image despeckling using trained models on single channel SAR images, Proceedings of the 2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR), pp. 1-4; Hu J., Mou L., Schmitt A., Zhu X.X., FusioNet: A two-stream convolutional neural network for urban scene classification using PolSAR and hyperspectral data, Proceedings of the 2017 Joint Urban Remote Sensing Event (JURSE), pp. 1-4; Ferro-Famil L., Pottier E., Lee J.S., Unsupervised classification of multifrequency and fully polarimetric SAR images based on the H/A/Alpha-Wishart classifier, IEEE Trans. Geosci. Remote Sens, 39, pp. 2332-2342, (2001); Singha S., Johansson A.M., Doulgeris A.P., Robustness of SAR sea ice type classification across incidence angles and seasons at L-band, IEEE Trans. Geosci. Remote Sens, 59, pp. 9941-9952, (2020); Pallotta L., Orlando D., Polarimetric covariance eigenvalues classification in SAR images, IEEE Geosci. Remote Sens. Lett, 16, pp. 746-750, (2018); Tadono T., Ohki M., Abe T., Summary of natural disaster responses by the Advanced Land Observing Satellite-2 (ALOS-2), Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 42, pp. 69-72, (2019); Natsuaki R., Hirose A., L-Band SAR Interferometric Analysis for Flood Detection in Urban Area-a Case Study in 2015 Joso Flood, Japan, Proceedings of the IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6592-6595; Hu H., Zhang Y., Li X., Lin Y., Cheng Z., Liu T., Polarimetric underwater image recovery via deep learning, Opt. Lasers Eng, 133, (2020); Li X., Li Z., Feng R., Luo S., Zhang C., Jiang M., Shen H., Generating high-quality and high-resolution seamless satellite imagery for large-scale urban regions, Remote Sens, 12, (2020); Pan T., Peng D., Yang W., Li H.C., A filter for SAR image despeckling using pre-trained convolutional neural network model, Remote Sens, 11, (2019); Zhang Q., Yuan Q., Li J., Yang Z., Ma X., Learning a dilated residual network for SAR image despeckling, Remote Sens, 10, (2018); Goudail F., Noise minimization and equalization for Stokes polarimeters in the presence of signal-dependent Poisson shot noise, Opt. Lett, 34, pp. 647-649, (2009); Denis L., Dalsasso E., Tupin F., In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium IGARSS, pp. 411-414; Qi P., Li X., Han Y., Zhang L., Xu J., Cheng Z., Liu T., Zhai J., Hu H., U2R-pGAN: Unpaired underwater-image recovery with polarimetric generative adversarial network, Opt. Lasers Eng, 157, (2022); Akiyama K., Ikeda S., Pleau M., Fish V.L., Tazaki F., Kuramochi K., Broderick A.E., Dexter J., Moscibrodzka M., Gowanlock M., Et al., Superresolution full-polarimetric imaging for radio interferometry with sparse modeling, Astron. J, 153, (2017); Ahmed A., Zhao X., Gruev V., Zhang J., Bermak A., Residual interpolation for division of focal plane polarization image sensors, Opt. Express, 25, pp. 10651-10662, (2017); Tao Y., Muller J.P., Super-resolution restoration of misr images using the ucl magigan system, Remote Sens, 11, (2019); Goudail F., Beniere A., Optimization of the contrast in polarimetric scalar images, Opt. Lett, 34, pp. 1471-1473, (2009); Ma X., Wu P., Wu Y., Shen H., A review on recent developments in fully polarimetric SAR image despeckling, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 11, pp. 743-758, (2017); Zhang L., Dong H., Zou B., Efficiently utilizing complex-valued PolSAR image data via a multi-task deep learning framework, ISPRS J. Photogramm. Remote Sens, 157, pp. 59-72, (2019); Li N., Zhao Y., Pan Q., Kong S.G., Chan J.C.W., Full-Time Monocular Road Detection Using Zero-Distribution Prior of Angle of Polarization, Proceedings of the European Conference on Computer Vision, pp. 457-473, (2020); Dickson C.N., Wallace A.M., Kitchin M., Connor B., Long-wave infrared polarimetric cluster-based vehicle detection, JOSA A, 32, pp. 2307-2315, (2015); Carnicer A., Javidi B., Polarimetric 3D integral imaging in photon-starved conditions, Opt. Express, 23, pp. 6408-6417, (2015); Hagen N., Otani Y., Stokes polarimeter performance: General noise model and analysis, Appl. Opt, 57, pp. 4283-4296, (2018); Li X., Hu H., Liu T., Huang B., Song Z., Optimal distribution of integration time for intensity measurements in degree of linear polarization polarimetry, Opt. Express, 24, pp. 7191-7200, (2016); Li X., Hu H., Wang H., Liu T., Optimal Measurement Matrix of Partial Polarimeter for Measuring Ellipsometric Parameters with Eight Intensity Measurements, IEEE Access, 7, pp. 31494-31500, (2019); Goudail F., Li X., Boffety M., Roussel S., Liu T., Hu H., Precision of retardance autocalibration in full-Stokes division-of-focal-plane imaging polarimeters, Opt. Lett, 44, pp. 5410-5413, (2019); Hu H., Qi P., Li X., Cheng Z., Liu T., Underwater imaging enhancement based on a polarization filter and histogram attenuation prior, J. Phys. D Appl. Phys, 54, (2021); Lopez-Martinez C., Fabregas X., Polarimetric SAR speckle noise model, IEEE Trans. Geosci. Remote Sens, 41, pp. 2232-2242, (2003); Jaffe J.S., Computer modeling and the design of optimal underwater imaging systems, IEEE J. Ocean. Eng, 15, pp. 101-111, (1990); Cariou J., Jeune B.L., Lotrian J., Guern Y., Polarization effects of seawater and underwater targets, Appl. Opt, 29, (1990); Shen H., Lin L., Li J., Yuan Q., Zhao L., A residual convolutional neural network for polarimetric SAR image super-resolution, ISPRS J. Photogramm. Remote Sens, 161, pp. 90-108, (2020); Li X., Le Teurnier B., Boffety M., Liu T., Hu H., Goudail F., Theory of autocalibration feasibility and precision in full Stokes polarization imagers, Opt. Express, 28, pp. 15268-15283, (2020); Li X., Hu H., Goudail F., Liu T., Fundamental precision limits of full Stokes polarimeters based on DoFP polarization cameras for an arbitrary number of acquisitions, Opt. Express, 27, pp. 31261-31272, (2019); Li X., Goudail F., Hu H., Han Q., Cheng Z., Liu T., Optimal ellipsometric parameter measurement strategies based on four intensity measurements in presence of additive Gaussian and Poisson noise, Opt. Express, 26, pp. 34529-34546, (2018); Li X., Hu H., Wang H., Wu L., Liu T.G., Influence of noise statistics on optimizing the distribution of integration time for degree of linear polarization polarimetry, Opt. Eng, 57, (2018); Li X., Hu H., Wu L., Liu T., Optimization of instrument matrix for Mueller matrix ellipsometry based on partial elements analysis of the Mueller matrix, Opt. Express, 25, pp. 18872-18884, (2017); Li X., Liu T., Huang B., Song Z., Hu H., Optimal distribution of integration time for intensity measurements in Stokes polarimetry, Opt. Express, 23, pp. 27690-27699, (2015); Dubreuil M., Delrot P., Leonard I., Alfalou A., Brosseau C., Dogariu A., Exploring underwater target detection by imaging polarimetry and correlation techniques, Appl. Opt, 52, pp. 997-1005, (2013); Sun R., Sun X., Chen F., Pan H., Song Q., An artificial target detection method combining a polarimetric feature extractor with deep convolutional neural networks, Int. J. Remote Sens, 41, pp. 4995-5009, (2020); Fan Q., Chen F., Cheng M., Lou S., Xiao R., Zhang B., Wang C., Li J., Ship detection using a fully convolutional network with compact polarimetric sar images, Remote Sens, 11, (2019); Liu F., Jiao L., Tang X., Yang S., Ma W., Hou B., Local restricted convolutional neural network for change detection in polarimetric SAR images, IEEE Trans. Neural Networks Learn. Syst, 30, pp. 818-833, (2018); Goudail F., Tyo J.S., When is polarimetric imaging preferable to intensity imaging for target detection?, JOSA A, 28, pp. 46-53, (2011); Wolff L.B., Polarization-based material classification from specular reflection, IEEE Trans. Pattern Anal. Mach. Intell, 12, pp. 1059-1071, (1990); Tominaga S., Kimachi A., Polarization imaging for material classification, Opt. Eng, 47, (2008); Fernandez-Michelli J.I., Hurtado M., Areta J.A., Muravchik C.H., Unsupervised classification algorithm based on EM method for polarimetric SAR images, ISPRS J. Photogramm. Remote Sens, 117, pp. 56-65, (2016); Wen Z., Wu Q., Liu Z., Pan Q., Polar-spatial feature fusion learning with variational generative-discriminative network for PolSAR classification, IEEE Trans. Geosci. Remote Sens, 57, pp. 8914-8927, (2019); Solomon J.E., Polarization imaging, Appl. Opt, 20, pp. 1537-1544, (1981); Daily M., Elachi C., Farr T., Schaber G., Discrimination of geologic units in Death Valley using dual frequency and polarization imaging radar data, Geophys. Res. Lett, 5, pp. 889-892, (1978); Leader J., Polarization discrimination in remote sensing, Proceedings of the AGARD Electromagnetic Wave Propagation Involving Irregular Surfaces and Inhomogeneous Media 12 p (SEE N75-22045 13-70); Gruev V., Perkins R., York T., CCD polarization imaging sensor with aluminum nanowire optical filters, Opt. Express, 18, pp. 19087-19094, (2010); Zhong H., Liu G., Nonlocal Means Filter for Polarimetric SAR Data Despeckling Based on Discriminative Similarity Measure, IEEE Geosci. Remote Sens. Lett, 11, pp. 514-518, (2014); Zhao Y., Liu J.G., Zhang B., Hong W., Wu Y.R., Adaptive Total Variation Regularization Based SAR Image Despeckling and Despeckling Evaluation Index, IEEE Trans. Geosci. Remote Sens, 53, pp. 2765-2774, (2015); Nie X., Qiao H., Zhang B., Wang Z., PolSAR image despeckling based on the Wishart distribution and total variation regularization, Proceedings of the 11th World Congress on Intelligent Control and Automation, pp. 1479-1484; Zhong H., Zhang J., Liu G., Robust polarimetric SAR despeckling based on nonlocal means and distributed Lee filter, IEEE Trans. Geosci. Remote Sens, 52, pp. 4198-4210, (2013); Zhang J., Luo H., Liang R., Zhou W., Hui B., Chang Z., PCA-based denoising method for division of focal plane polarimeters, Optics Express, 25, pp. 2391-2400, (2017); Wenbin Y., Shiting L., Xiaojin Z., Abubakar A., Amine B., A K Times Singular Value Decomposition Based Image Denoising Algorithm for DoFP Polarization Image Sensors with Gaussian Noise, IEEE Sens. J, 18, pp. 6138-6144, (2018); Song S., Xu B., Yang J., Ship detection in polarimetric SAR images via variational Bayesian inference, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 10, pp. 2819-2829, (2017); Abubakar A., Zhao X., Li S., Takruri M., Bastaki E., Bermak A., A Block-Matching and 3-D Filtering Algorithm for Gaussian Noise in DoFP Polarization Images, IEEE Sens. J, 18, pp. 7429-7435, (2018); Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community, J. Appl. Remote Sens, 11, (2017); Yuan Q., Shen H., Li T., Li Z., Li S., Jiang Y., Xu H., Tan W., Yang Q., Wang J., Et al., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ, 241, (2020); Jordan M.I., Mitchell T.M., Machine learning: Trends, perspectives, and prospects, Science, 349, pp. 255-260, (2015); Khedri E., Hasanlou M., Tabatabaeenejad A., Estimating Soil Moisture Using Polsar Data: A Machine Learning Approach, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 42, pp. 133-137, (2017); Mahendru A., Sarkar M., Bio-inspired object classification using polarization imaging, Proceedings of the 2012 Sixth International Conference on Sensing Technology (ICST), pp. 207-212; Zhang L., Shi L., Cheng J.C.Y., Chu W.C., Yu S.C.H., LPAQR-Net: Efficient Vertebra Segmentation from Biplanar Whole-spine Radiographs, IEEE J. Biomed. Health Inform, 25, pp. 2710-2721, (2021); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag, 4, pp. 22-40, (2016); Takruri M., Abubakar A., Alnaqbi N., Al Shehhi H., Jallad A.H.M., Bermak A., DoFP-ML: A Machine Learning Approach to Food Quality Monitoring Using a DoFP Polarization Image Sensor, IEEE Access, 8, pp. 150282-150290, (2020); Hansch R., Hellwich O., Skipping the real world: Classification of PolSAR images without explicit feature extraction, ISPRS J. Photogramm. Remote Sens, 140, pp. 122-132, (2018); Wang H., Xu F., Jin Y.Q., A review of PolSAR image classification: From polarimetry to deep learning, Proceedings of the IGARSS 2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 3189-3192; Pourshamsi M., Garcia M., Lavalle M., Pottier E., Balzter H., Machine-Learning Fusion of PolSAR and LiDAR Data for Tropical Forest Canopy Height Estimation, Proceedings of the IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 8108-8111; Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Deng L., Yu D., Deep learning: Methods and applications, Found. Trends Signal Process, 7, pp. 197-387, (2014); Paoletti M., Haut J., Plaza J., Plaza A., Deep learning classifiers for hyperspectral imaging: A review, ISPRS J. Photogramm. Remote Sens, 158, pp. 279-317, (2019); Reichstein M., Camps-Valls G., Stevens B., Jung M., Denzler J., Carvalhais N., Deep learning and process understanding for data-driven Earth system science, Nature, 566, pp. 195-204, (2019); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, ISPRS J. Photogramm. Remote Sens, 173, pp. 24-49, (2021); Liu L., Lei B., Can SAR images and optical images transfer with each other?, Proceedings of the IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 7019-7022; Wang H., Zhang Z., Hu Z., Dong Q., SAR-to-Optical Image Translation with Hierarchical Latent Features, IEEE Trans. Geosci. Remote Sens, 60, pp. 1-12, (2022); Yang X., Zhao J., Wei Z., Wang N., Gao X., SAR-to-optical image translation based on improved CGAN, Pattern Recognit, 121, (2022); Fuentes Reyes M., Auer S., Merkle N., Henry C., Schmitt M., Sar-to-optical image translation based on conditional generative adversarial networks—Optimization, opportunities and limits, Remote Sens, 11, (2019); (2020); Zebker H.A., Van Zyl J.J., Imaging radar polarimetry: A review, Proc. IEEE, 79, pp. 1583-1606, (1991); Boerner W.M., Cram L.A., Holm W.A., Stein D.E., Wiesbeck W., Keydel W., Giuli D., Gjessing D.T., Molinet F.A., Brand H., Direct and Inverse Methods in Radar Polarimetry, 350, (2013); Jones R.C., A new calculus for the treatment of optical systemsi. description and discussion of the calculus, JOSA A, 31, pp. 488-493, (1941); Jones R.C., A new calculus for the treatment of optical systems. IV, JOSA A, 32, pp. 486-493, (1942); Jones R.C., A new calculus for the treatment of optical systemsv. A more general formulation, and description of another calculus, JOSA A, 37, pp. 107-110, (1947); Perez J.J.G., Ossikovski R., Polarized Light and the Mueller Matrix Approach, (2017); Oyama K., Hirose A., Phasor quaternion neural networks for singular point compensation in polarimetric-interferometric synthetic aperture radar, IEEE Trans. Geosci. Remote Sens, 57, pp. 2510-2519, (2018); Shang R., Wang G., A Okoth M., Jiao L., Complex-valued convolutional autoencoder and spatial pixel-squares refinement for polarimetric SAR image classification, Remote Sens, 11, (2019); Henderson F., Lewis A., Reyerson R., Polarimetry in Radar Remote Sensing: Basic and Applied Concepts, (1998); Yang C., Hou B., Ren B., Hu Y., Jiao L., CNN-based polarimetric decomposition feature selection for PolSAR image classification, IEEE Trans. Geosci. Remote Sens, 57, pp. 8796-8812, (2019); Krogager E., New decomposition of the radar target scattering matrix, Electron. Lett, 26, pp. 1525-1527, (1990); Touzi R., Target scattering decomposition of one-look and multi-look SAR data using a new coherent scattering model: The TSVM, Proceedings of the IGARSS 2004—2004 IEEE International Geoscience and Remote Sensing Symposium, 4, pp. 2491-2494; Holm W.A., Barnes R.M., On radar polarization mixed target state decomposition techniques, Proceedings of the 1988 IEEE National Radar Conference, pp. 249-254; Huynen J.R., Phenomenological Theory of Radar Targets, (1970); Van Zyl J.J., Application of Cloude’s target decomposition theorem to polarimetric imaging radar data, Radar Polarimetry, 1748, pp. 184-191, (1993); Cloude S.R., Pottier E., An entropy based classification scheme for land applications of polarimetric SAR, IEEE Trans. Geosci. Remote Sens, 35, pp. 68-78, (1997); Zhang L., Zou B., Cai H., Zhang Y., Multiple-component scattering model for polarimetric SAR image decomposition, IEEE Geosci. Remote Sens. Lett, 5, pp. 603-607, (2008); Ballester-Berman J.D., Lopez-Sanchez J.M., Applying the Freeman–Durden decomposition concept to polarimetric SAR interferometry, IEEE Trans. Geosci. Remote Sens, 48, pp. 466-479, (2009); Arii M., Van Zyl J.J., Kim Y., Adaptive model-based decomposition of polarimetric SAR covariance matrices, IEEE Trans. Geosci. Remote Sens, 49, pp. 1104-1113, (2010); Serre T., Kreiman G., Kouh M., Cadieu C., Knoblich U., Poggio T., A quantitative theory of immediate visual recognition, Prog. Brain Res, 165, pp. 33-56, (2007); Khan A., Sohail A., Zahoora U., Qureshi A.S., A survey of the recent architectures of deep convolutional neural networks, Artif. Intell. Rev, 53, pp. 5455-5516, (2020); Zhang L., Yu S.C.H., Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal CT Images, arXiv, (2021); Koyama C.N., Watanabe M., Sano E.E., Hayashi M., Nagatani I., Tadono T., Shimada M., Improving L-Band SAR Forest Monitoring by Big Data Deep Learning Based on ALOS-2 5 Years Pan-Tropical Observations, Proceedings of the 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, pp. 6747-6750; Li Z., Yang W., Peng S., Liu F., A survey of convolutional neural networks: Analysis, applications, and prospects, arXiv, (2020); Gu J., Wang Z., Kuen J., Ma L., Shahroudy A., Shuai B., Liu T., Wang X., Wang G., Cai J., Et al., Recent advances in convolutional neural networks, Pattern Recognit, 77, pp. 354-377, (2018); Zhang X., Wang Y., Zhang N., Xu D., Chen B., Research on Scene Classification Method of High-Resolution Remote Sensing Images Based on RFPNet, Appl. Sci, 9, (2019); Fukushima K., A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position, Biol. Cybern, 36, pp. 193-202, (1980); LeCun Y., Boser B., Denker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation applied to handwritten zip code recognition, Neural Comput, 1, pp. 541-551, (1989); Rahmani B., Loterie D., Konstantinou G., Psaltis D., Moser C., Multimode optical fiber transmission with a deep learning network, Light. Sci. Appl, 7, (2018); Rivenson Y., Zhang Y., Gunaydin H., Teng D., Ozcan A., Phase recovery and holographic image reconstruction using deep learning in neural networks, Light. Sci. Appl, 7, (2018); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Adv. Neural Inf. Process. Syst, 25, pp. 1097-1105, (2012); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., LeCun Y., Overfeat: Integrated recognition, localization and detection using convolutional networks, arXiv, (2013); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Nair V., Hinton G.E., Rectified linear units improve restricted boltzmann machines, Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814; Glorot X., Bordes A., Bengio Y., Deep sparse rectifier neural networks, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, pp. 315-323; Barbastathis G., Ozcan A., Situ G., On the use of deep learning for computational imaging, Optica, 6, pp. 921-943, (2019); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, pp. 2278-2324, (1998); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell, 37, pp. 1904-1916, (2015); Huang G., Liu Z., Van Der Maaten L., Weinberger K.Q., Densely connected convolutional networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700-4708; Seydi S.T., Hasanlou M., Amani M., A new end-to-end multi-dimensional CNN framework for land cover/land use change detection in multi-source remote sensing datasets, Remote Sens, 12, (2020); Zhang J., Shao J., Chen J., Yang D., Liang B., Liang R., PFNet: An unsupervised deep network for polarization image fusion, Opt. Lett, 45, pp. 1507-1510, (2020); Blin R., Ainouz S., Canu S., Meriaudeau F., A new multimodal RGB and polarimetric image dataset for road scenes analysis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 216-217; Zhang Z., Wang H., Xu F., Jin Y.Q., Complex-valued convolutional neural network and its application in polarimetric SAR image classification, IEEE Trans. Geosci. Remote Sens, 55, pp. 7177-7188, (2017); Makhzani A., Frey B., K-sparse autoencoders, arXiv, (2013); Luo W., Li J., Yang J., Xu W., Zhang J., Convolutional sparse autoencoders for image classification, IEEE Trans. Neural Netw. Learn. Syst, 29, pp. 3289-3294, (2017); Kusner M.J., Paige B., Hernandez-Lobato J.M., Grammar variational autoencoder, Proceedings of the International Conference on Machine Learning PMLR, pp. 1945-1954; Chen W., Gou S., Wang X., Li X., Jiao L., Classification of PolSAR images using multilayer autoencoders and a self-paced learning approach, Remote Sens, 10, (2018); Zhang L., Ma W., Zhang D., Stacked sparse autoencoder in PolSAR data classification using local spatial information, IEEE Geosci. Remote Sens. Lett, 13, pp. 1359-1363, (2016); Hou B., Kou H., Jiao L., Classification of polarimetric SAR images using multilayer autoencoders and superpixels, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 9, pp. 3072-3081, (2016); Hu Y., Fan J., Wang J., Classification of PolSAR images based on adaptive nonlocal stacked sparse autoencoder, IEEE Geosci. Remote Sens. Lett, 15, pp. 1050-1054, (2018); Geng J., Ma X., Fan J., Wang H., Semisupervised classification of polarimetric SAR image via superpixel restrained deep neural network, IEEE Geosci. Remote Sens. Lett, 15, pp. 122-126, (2017); Hinton G.E., Osindero S., Teh Y.W., A fast learning algorithm for deep belief nets, Neural Comput, 18, pp. 1527-1554, (2006); Liu F., Jiao L., Hou B., Yang S., POL-SAR image classification based on Wishart DBN and local spatial information, IEEE Trans. Geosci. Remote Sens, 54, pp. 3292-3308, (2016); Tanase R., Datcu M., Raducanu D., A convolutional deep belief network for polarimetric SAR data feature extraction, Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 7545-7548; Lv Q., Dou Y., Niu X., Xu J., Xu J., Xia F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks, J. Sens, 2015, (2015); Guo Y., Wang S., Gao C., Shi D., Zhang D., Hou B., Wishart RBM based DBN for polarimetric synthetic radar data classification, Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1841-1844; Shah Hosseini R., Entezari I., Homayouni S., Motagh M., Mansouri B., Classification of polarimetric SAR images using Support Vector Machines, Can. J. Remote Sens, 37, pp. 220-233, (2011); Wang L., Xu X., Dong H., Gui R., Yang R., Pu F., Exploring Convolutional Lstm for PolSAR Image Classification, Proceedings of the IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 8452-8455; Wang L., Xu X., Gui R., Yang R., Pu F., Learning Rotation Domain Deep Mutual Information Using Convolutional LSTM for Unsupervised PolSAR Image Classification, Remote Sens, 12, (2020); Jiao L., Liu F., Wishart deep stacking network for fast POLSAR image classification, IEEE Trans. Image Process, 25, pp. 3273-3286, (2016)","H. Hu; School of Marine Science and Technology, Tianjin University, Tianjin, 300072, China; email: haofeng_hu@tju.edu.cn; L. Yan; Spatial Information Integration and 3S Engineering Application Beijing Key Laboratory, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, 100871, China; email: lyan@pku.edu.cn; F. Goudail; Laboratoire Charles Fabry, CNRS, Institut d’Optique Graduate School, Université Paris-Saclay, Palaiseau, 91120, France; email: francois.goudail@institutoptique.fr","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85151487904"
"Tiwari R.; Dubey A.K.","Tiwari, Rishabh (57226358774); Dubey, Ashwani Kumar (7103176828)","57226358774; 7103176828","Detection of Camouflaged Drones using Computer Vision and Deep Learning Techniques","2022","Proceedings of the Confluence 2022 - 12th International Conference on Cloud Computing, Data Science and Engineering","","","","380","383","3","2","10.1109/Confluence52989.2022.9734191","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127539154&doi=10.1109%2fConfluence52989.2022.9734191&partnerID=40&md5=98a5fa1af40a10a62ae8609e7e814cab","Amity University Uttar Pradesh, Aset, Dept. Of Electronics And Communication Engineering, Noida, India","Tiwari R., Amity University Uttar Pradesh, Aset, Dept. Of Electronics And Communication Engineering, Noida, India; Dubey A.K., Amity University Uttar Pradesh, Aset, Dept. Of Electronics And Communication Engineering, Noida, India","Nowadays the world is seeing rapid advancement in robotics and especially flying unmanned vehicles or drones. These drones are controlled manually or sometimes automated as unmanned scouts to scan entire areas. Unfortunately, because of the expanding fame of these drones, their capability to be abused has increased drastically, due to which security locked important locations are now vulnerable to either getting attacked or spied upon. An automated drone detection is not only a robust solution, but it is the most needed one. This paper highlights a drone detection method using computer vision to detect and track drones by creating a deep learning model. The YOLOv4 model is tested on the testing dataset, which consists of both usual and custom drone images. The model is also tested on unseen videos and performance of drone detection was tested and the FPS was recorded during the detection to recognize the speed of detection at various elevations levels of the drone. The technique utilized in this task is for certain changes in the YOLOv4 architecture which causes the model to perform better compared to what past related works evaluations offer and has been fruitful in scoring a mAP of 94.36%. The normal FPS seen on various recordings for drone identification was near 36 on recordings with sky as background and 30.5 on recordings with dense background like trees and structures.  © 2022 IEEE.","deep neural network; object detection; R-CNN; YOLOv4","Aircraft detection; Computer vision; Convolutional neural networks; Drones; Object detection; Object recognition; Statistical tests; Detection methods; Learning models; Learning techniques; Performance; R-CNN; Related works; Robust solutions; Work evaluations; YOLOv4; Deep neural networks","","","","","","","Bodla N., Singh B., Chellappa R., Davis L.S., Soft-NMS; Hu Y., Wu X., Zheng G., Liu X., Object Detection of UAV for Anti-UAV Based on Improved YOLO v3», Proc. of the CCC; Guanzhou; Zhang T., Zhang H., Zhang Z., Xie J., Li M., Bag of tricks for image classification with convolutional neural networks, Proc. of the; Cui N., K Chen' Zhang B., Zhang Y., Liu X., Zhou J., Effects of route guidance strategies on traffic emissions in intelligent transportation systems, Phystca A: Statistical Mechanics and Us; OkutTnTand I., Stephanedes Y.J., Dynamic Prediction of Traffic Volume; Wang J., Chen R., He Z., Traffic speed prediction for urban transportation network: a path based deep learning approach, Transportation Research Part C: Emerging Technologies, 100, pp. 372-385, (2019); Du S., Li T., Gong X., Homg S.J., A Hybrid Method For Traffic Flow; Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand. M Andreetto T., Adam H., Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications, Advances tn Neural Information Processing Systems (NIPS, pp. 10727-10737, (2018); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, 2077 IEEE International Conference on Computer Vision fICCV), (2017); Redmon J., Girshick S.R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, 2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Tao L., Wang H., Zhang X., Li X., Yang H., An object detection system based on YOLO in traffic scene, 2017 6th International; Sadykova D., Pemebayeva D., Bagheri M., James A., IN-YOLO: Real-Time Detection of Outdoor High Voltage Insulators Using UAV; Kharchenko V., Chyrka I., Detection of Airplanes on the Ground Using YOLO Neural Network, 2018 IEEE 17th International; Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, 2077 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6517-6525, (2017); Wei Xun D.T., Lim Y.L., Srigrarom S., Drone detection using YOLOV3 with transfer learning on NVIDIA Jetson TX2, 2021 Second International Symposium on Instrumentation, Control, Artificial Intelligence, and Robotics (ICA-SYMP), pp. 1-6, (2021); Saqib M., Daud Khan S., Sharma N., Blumenstein M., A study on detecting drones using deep convolutional neural networks, 201714th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pp. 1-5, (2017); Al-Emadi S., Al-Ali A., Mohammad A., Al-Ali A., Audio Based Drone Detection and Identification using Deep Learning, 2019 15th International Wireless Communications & Mobile Computing Conference (IWCMC), pp. 459-464, (2019); Lee D., La W.G., Kim H., Drone Detection, and Identification System Using Artificial Intelligence, Proc. of the 2018 International Conference on Information and Communication Technology Convergence (ICTC), Busan, Korea, 22-24 October;, pp. 1131-1133; Shi W., Caballero J., Huszar F., Totz J., Aitken A.P., Bishop R., Et al., Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition; Szegedy C., Loffe S., Vanhoucke V., Alemi A.A., Inception-v4 inception-resnet and the impact of residual connections on learning; G Srivastava' Hinton N., Krizhevsky A., Sutskever I., Salakhutdinov R., Dropout: a simple way to prevent neural networks from over-fitting, The Journal of Machine Learning Research, 15, 1, pp. 1929-1958, (2014); Howard A., Sandler M., Chu G., Chen L.C., Chen B., Tan M., Wang W., Zhu Y., Pang R., Vasudevan V., Le Q.V., Adam H., Searching for MobileNefV3, Proc. of the IEEE/CVF International Conference on Computer Vtston (ICCV), pp. 1314-1324, (2019)","","","Institute of Electrical and Electronics Engineers Inc.","IEEE UP Section","12th International Conference on Cloud Computing, Data Science and Engineering, Confluence 2022","27 January 2022 through 28 January 2022","Virtual, Online","177256","","978-166543701-1","","","English","Proc. Conflu. - Int. Conf. Cloud Comput., Data Sci. Eng.","Conference paper","Final","","Scopus","2-s2.0-85127539154"
"Sozzi M.; Cantalamessa S.; Cogato A.; Kayad A.; Marinello F.","Sozzi, Marco (57202641618); Cantalamessa, Silvia (57419565900); Cogato, Alessia (57202649746); Kayad, Ahmed (57189044443); Marinello, Francesco (16230574000)","57202641618; 57419565900; 57202649746; 57189044443; 16230574000","Automatic Bunch Detection in White Grape Varieties Using YOLOv3, YOLOv4, and YOLOv5 Deep Learning Algorithms","2022","Agronomy","12","2","319","","","","66","10.3390/agronomy12020319","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124282895&doi=10.3390%2fagronomy12020319&partnerID=40&md5=fcf8a12127e1c805068039d37c093853","Department of Land Environment Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy; Department of Agronomy, Food, Natural Resources, Animals, and Environment, University of Padova, Legnaro, 35020, Italy; Department of Agricultural, Food, Environmental and Animal Sciences, University of Udine, Udine, 33100, Italy","Sozzi M., Department of Land Environment Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy; Cantalamessa S., Department of Agronomy, Food, Natural Resources, Animals, and Environment, University of Padova, Legnaro, 35020, Italy; Cogato A., Department of Agricultural, Food, Environmental and Animal Sciences, University of Udine, Udine, 33100, Italy; Kayad A., Department of Land Environment Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy; Marinello F., Department of Land Environment Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy","Over the last few years, several Convolutional Neural Networks for object detection have been proposed, characterised by different accuracy and speed. In viticulture, yield estimation and prediction is used for efficient crop management, taking advantage of precision viticulture techniques. Convolutional Neural Networks for object detection represent an alternative methodology for grape yield estimation, which usually relies on manual harvesting of sample plants. In this paper, six versions of the You Only Look Once (YOLO) object detection algorithm (YOLOv3, YOLOv3-tiny, YOLOv4, YOLOv4-tiny, YOLOv5x, and YOLOv5s) were evaluated for real-time bunch detection and counting in grapes. White grape varieties were chosen for this study, as the identification of white berries on a leaf background is trickier than red berries. YOLO models were trained using a heterogeneous dataset populated by images retrieved from open datasets and acquired on the field in several illumination conditions, background, and growth stages. Results have shown that YOLOv5x and YOLOv4 achieved an F1-score of 0.76 and 0.77, respectively, with a detection speed of 31 and 32 FPS. Differently, YOLO5s and YOLOv4-tiny achieved an F1-score of 0.76 and 0.69, respectively, with a detection speed of 61 and 196 FPS. The final YOLOv5x model for bunch number, obtained considering bunch occlusion, was able to estimate the number of bunches per plant with an average error of 13.3% per vine. The best combination of accuracy and speed was achieved by YOLOv4-tiny, which should be considered for real-time grape yield estimation, while YOLOv3 was affected by a False Positive–False Negative compensation, which decreased the RMSE. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Agricultural robot; Cluster detection; Crop load mapping; Early yield estimation; Precision viticulture; Real-time detection; Smart farming; Viticulture; Yield estimation","","","","","","Università degli Studi di Padova","Funding: This research was financially supported by the project financed with BIRD 2020 funds, Dept. TESAF, University of Padova—Italy.","Arno J., Casasnovas J.A.M., Dasi M.R., Rosell J.R., Review. Precision viticulture. Research topics, challenges and opportunities in site-specific vineyard management, Span. J. Agric. Res, 7, (2009); Matese A., Di Gennaro S.F., Technology in precision viticulture: A state of the art review, Int. J. Wine Res, 7, (2015); Marinello F., Bramley R.G.V., Cohen Y., Fountas S., Guo H., Karkee M., Martinez-Casasnovas J.A., Paraforos D.S., Sartori L., Sorensen C.G., Et al., Agriculture and Digital Sustainability: A Digitization Footprint, Proceedings of the Precision Agriculture ‘21, ECPA 2021, Proceedings of the 13th European Conference on Precision Agriculture, pp. 83-89, (2021); Kayad A., Sozzi M., Gatto S., Whelan B., Sartori L., Marinello F., Ten years of corn yield dynamics at field scale under digital agriculture solutions: A case study from North Italy, Comput. Electron. Agric, 185, (2021); Seng K.P., Ang L.M., Schmidtke L.M., Rogiers S.Y., Computer vision and machine learning for viticulture technology, IEEE Access, 6, pp. 67494-67510, (2018); Liakos K.G., Busato P., Moshou D., Pearson S., Bochtis D., Machine learning in agriculture: A review, Sensors, 18, (2018); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Voulodimos A., Doulamis N., Doulamis A., Protopapadakis E., Deep Learning for Computer Vision: A Brief Review, Comput. Intell. Neurosci, 2018, (2018); Zhao Z.Q., Zheng P., Xu S.T., Wu X., Object Detection with Deep Learning: A Review, IEEE Trans. Neural Netw. Learn. Syst, 30, pp. 3212-3232, (2019); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 580-587, (2014); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, 39, pp. 1137-1149, (2017); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, IEEE Trans. Pattern Anal. Mach. Intell, 42, pp. 386-397, (2020); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y.Y., Berg A.C., SSD: Single Shot MultiBox Detector, Proceedings of the Computer Vision—ECCV, 9905, pp. 21-37, (2016); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning—Method overview and review of use for fruit detection and yield estimation, Comput. Electron. Agric, 162, pp. 219-234, (2019); Laurent C., Oger B., Taylor J.A., Scholasch T., Metay A., Tisseyre B., A review of the issues, methods and perspectives for yield estimation, prediction and forecasting in viticulture, Eur. J. Agron, 130, (2021); Sabbatini P., Dami I., Howell G.S., Predicting Harvest Yield in Juice and Wine Grape Vineyards, Mich. State Univ. Ext, pp. 1-12, (2012); De La Fuente M., Linares R., Baeza P., Miranda C., Lissarrague J.R., Comparison of different methods of grapevine yield prediction in the time window between fruitset and veraison, OENO One, 49, pp. 27-35, (2015); Coombe B.G., McCarthy M.G., Dynamics of grape berry growth and physiology of ripening, Aust. J. Grape Wine Res, 6, pp. 131-135, (2000); Taylor J.A., Sanchez L., Sams B., Haggerty L., Jakubowski R., Djafour S., Bates T.R., Evaluation of a commercial grape yield monitor for use mid-season and at-harvest, OENO One, 50, pp. 57-63, (2016); Sozzi M., Kayad A., Tomasi D., Lovat L., Marinello F., Sartori L., Assessment of grapevine yield and quality using a canopy spectral index in white grape variety, Proceedings of the Precision Agriculture ‘19, ECPA 2019, Proceedings of the 12th European Conference on Precision Agriculture, pp. 181-186, (2019); Hall A., Remote sensing applications for viticultural terroir analysis, Elements, 14, pp. 185-190, (2018); Cogato A., Meggio F., Collins C., Marinello F., Medium-Resolution Multispectral Data from Sentinel-2 to Assess the Damage and the Recovery Time of Late Frost on Vineyards, Remote Sens, 12, (2020); Sozzi M., Kayad A., Giora D., Sartori L., Marinello F., Cost-effectiveness and performance of optical satellites constellation for Precision Agriculture, Proceedings of the Precision Agriculture ‘19, ECPA 2019, Proceedings of the 12th European Conference on Precision Agriculture, pp. 501-507, (2019); Sozzi M., Kayad A., Gobbo S., Cogato A., Sartori L., Marinello F., Singh V., Huang Y., Economic Comparison of Satellite, Plane and UAV-Acquired NDVI Images for Site-Specific Nitrogen Application: Observations from Italy, Agronomy, 11, (2021); Cogato A., Wu L., Jewan S.Y.Y., Meggio F., Marinello F., Sozzi M., Pagay V., Evaluating the Spectral and Physiological Responses of Grapevines (Vitis vinifera L.) to Heat and Water Stresses under Different Vineyard Cooling and Irrigation Strategies, Agronomy, 11, (2021); Bramley R.G.V., Le Moigne M., Evain S., Ouzman J., Florin L., Fadaili E.M., Hinze C.J., Cerovic Z.G., On-the-go sensing of grape berry anthocyanins during commercial harvest: Development and prospects, Aust. J. Grape Wine Res, 17, pp. 316-326, (2011); Henry D., Aubert H., Veronese T., Proximal Radar Sensors for Precision Viticulture, IEEE Trans. Geosci. Remote Sens, 57, pp. 4624-4635, (2019); Gongal A., Amatya S., Karkee M., Zhang Q., Lewis K., Sensors and systems for fruit detection and localization: A review, Comput. Electron. Agric, 116, pp. 8-19, (2015); Aquino A., Millan B., Diago M.P., Tardaguila J., Automated early yield prediction in vineyards from on-the-go image acquisition, Comput. Electron. Agric, 144, pp. 26-36, (2018); Nuske S., Achar S., Bates T., Narasimhan S., Singh S., Yield estimation in vineyards by visual grape detection, Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2352-2358, (2011); Nuske S., Wilshusen K., Achar S., Yoder L., Singh S., Automated visual yield estimation in vineyards, J. Field Robot, 31, pp. 837-860, (2014); Di Gennaro S.F., Toscano P., Cinat P., Berton A., Matese A., A precision viticulture UAV-based approach for early yield prediction in vineyard, Proceedings of the Precision Agriculture ‘19, ECPA 2019, Proceedings of the 12th European Conference on Precision Agriculture, pp. 373-379, (2019); Comba L., Biglia A., Aimonino D.R., Gay P., Unsupervised detection of vineyards by 3D point-cloud UAV photogrammetry for precision agriculture, Comput. Electron. Agric, 155, pp. 84-95, (2018); Santos T.T., de Souza L.L., dos Santos A.A., Avila S., Grape detection, segmentation, and tracking using deep neural networks and three-dimensional association, Comput. Electron. Agric, 170, (2020); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Zhang Y., Shen Y., Zhang J., An improved tiny-yolov3 pedestrian detection algorithm, Optik, 183, pp. 17-23, (2019); Hendry, Chen R.C., Automatic License Plate Recognition via sliding-window darknet-YOLO deep learning, Image Vis. Comput, 87, pp. 47-56, (2019); Zhou J., Jing J., Zhang H., Zhen W., Wang Z., Huang H., Real-time fabric defect detection algorithm based on s-yolov3 model, Laser Optoelectron. Prog, 57, (2020); Bresilla K., Perulli G.D., Boini A., Morandi B., Grappadelli L.C., Manfrini L., Single-Shot Convolution Neural Networks for Real-Time Fruit Detection Within the Tree, Front. Plant Sci, 10, (2019); Tian Y., Yang G., Wang Z., Wang H., Li E., Liang Z., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agric, 157, pp. 417-426, (2019); Li X., Qin Y., Wang F., Guo F., Yeow J.T.W., Pitaya detection in orchards using the MobileNet-YOLO model, Proceedings of the Chinese Control Conference, 2020, pp. 6274-6278, (2020); Morbekar A., Parihar A., Jadhav R., Crop disease detection using YOLO, Proceedings of the 2020 International Conference for Emerging Technology, INCET 2020, (2020); Ponnusamy V., Coumaran A., Shunmugam A.S., Rajaram K., Senthilvelavan S., Smart Glass: Real-Time Leaf Disease Detection using YOLO Transfer Learning, Proceedings of the 2020 IEEE International Conference on Communication and Signal Processing, ICCSP 2020, pp. 1150-1154, (2020); Zhong Y., Gao J., Lei Q., Zhou Y., A Vision-Based Counting and Recognition System for Flying Insects in Intelligent Agriculture, Sensors, 18, (2018); Abdulsalam M., Aouf N., Deep Weed Detector/Classifier Network for Precision Agriculture, pp. 1087-1092, (2020); Yin Y., Li H., Fu W., Faster-YOLO: An accurate and faster object detection method, Digit. Signal Process, 102, (2020); Wang C.Y., Liao H.Y.M., Wu Y.H., Chen P.Y., Hsieh J.W., Yeh I.H., CSPNet: A new backbone that can enhance learning capability of CNN, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 1571-1580, (2020); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020); Zheng Z., Wang P., Liu W., Li J., Ye R., Ren D., Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression, Proceedings of the AAAI Conference on Artificial Intelligence, (2016); Jocher G., Stoken A., Borovec J., Christopher S.T.A.N., Laughing L.C., ultralytics/yolov5: v4.0-nn.SiLU() activations, Weights & Biases logging, PyTorch Hub integration, Zenodo, (2021); Iyer R., Shashikant Ringe P., Varadharajan Iyer R., Prabhulal Bhensdadiya K., Comparison of YOLOv3, YOLOv5s and MobileNet-SSD V2 for Real-Time Mask Detection, Artic. Int. J. Res. Eng. Technol, 8, pp. 1156-1160, (2021); Yang G., Feng W., Jin J., Lei Q., Li X., Gui G., Wang W., Face Mask Recognition System with YOLOV5 Based on Image Recognition, Proceedings of the 2020 IEEE 6th International Conference on Computer and Communications (ICCC), 1, pp. 1398-1404, (2020); Chetlur S., Woolley C., Vandermersch P., Cohen J., Tran J., Catanzaro B., Shelhamer E., cuDNN: Efficient Primitives for Deep Learning, (2014); Kuznetsova A., Rom H., Alldrin N., Uijlings J., Krasin I., Pont-Tuset J., Kamali S., Popov S., Malloci M., Kolesnikov A., Et al., The Open Images Dataset V4: Unified Image Classification, Object Detection, and Visual Relationship Detection at Scale, Int. J. Comput. Vis, 128, pp. 1956-1981, (2020); Kwon Y., Choi W., Marrable D., Abdulatipov R., Loick J., Yolo_label 2020; Dice L.R., Measures of the Amount of Ecologic Association between Species, Ecology, 26, pp. 297-302, (1945); Li S., Gu X., Xu X., Xu D., Zhang T., Liu Z., Dong Q., Detection of concealed cracks from ground penetrating radar images based on deep learning algorithm, Constr. Build. Mater, 273, (2021); Lema D.G., Pedrayes O.D., Usamentiaga R., Garcia D.F., Alonso A., Cost-performance evaluation of a recognition service of livestock activity using aerial images, Remote Sens, 13, (2021); Aguiar A.S., Magalhaes S.A., Dos Santos F.N., Castro L., Pinho T., Valente J., Martins R., Boaventura-Cunha J., Grape Bunch Detection at Different Growth Stages Using Deep Learning Quantized Models, Agronomy, 11, (2021); Taylor J.A., Dresser J.L., Hickey C.C., Nuske S.T., Bates T.R., Considerations on spatial crop load mapping, Aust. J. Grape Wine Res, 25, pp. 144-155, (2019); Sozzi M., Cantalamessa S., Cogato A., Kayad A., Marinello F., Grape Yield Spatial Variability Assessment Using YOLOv4 Object Detection Algorithm, Proceedings of the Precision Agriculture ‘21, ECPA 2021, Proceedings of the 13th European Conference on Precision Agriculture, pp. 193-198, (2021); Pollock K.H., A Capture-Recapture Design Robust to Unequal Probability of Capture, J. Wildl. Manag, 46, (1982)","M. Sozzi; Department of Land Environment Agriculture and Forestry, University of Padova, Legnaro, 35020, Italy; email: marco.sozzi@unipd.it","","MDPI","","","","","","20734395","","","","English","Agronomy","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85124282895"
"Mansuri L.E.; Patel D.A.","Mansuri, Lukman E. (57212571689); Patel, D.A. (55755999400)","57212571689; 55755999400","Artificial intelligence-based automatic visual inspection system for built heritage","2022","Smart and Sustainable Built Environment","11","3","","622","646","24","20","10.1108/SASBE-09-2020-0139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100302130&doi=10.1108%2fSASBE-09-2020-0139&partnerID=40&md5=4d044eb65a50bd6b9dc1e9c470251a22","Department of Civil Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India","Mansuri L.E., Department of Civil Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India; Patel D.A., Department of Civil Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India","Purpose: Heritage is the latent part of a sustainable built environment. Conservation and preservation of heritage is one of the United Nations' (UN) sustainable development goals. Many social and natural factors seriously threaten heritage structures by deteriorating and damaging the original. Therefore, regular visual inspection of heritage structures is necessary for their conservation and preservation. Conventional inspection practice relies on manual inspection, which takes more time and human resources. The inspection system seeks an innovative approach that should be cheaper, faster, safer and less prone to human error than manual inspection. Therefore, this study aims to develop an automatic system of visual inspection for the built heritage. Design/methodology/approach: The artificial intelligence-based automatic defect detection system is developed using the faster R-CNN (faster region-based convolutional neural network) model of object detection to build an automatic visual inspection system. From the English and Dutch cemeteries of Surat (India), images of heritage structures were captured by digital camera to prepare the image data set. This image data set was used for training, validation and testing to develop the automatic defect detection model. While validating this model, its optimum detection accuracy is recorded as 91.58% to detect three types of defects: “spalling,” “exposed bricks” and “cracks.” Findings: This study develops the model of automatic web-based visual inspection systems for the heritage structures using the faster R-CNN. Then it demonstrates detection of defects of spalling, exposed bricks and cracks existing in the heritage structures. Comparison of conventional (manual) and developed automatic inspection systems reveals that the developed automatic system requires less time and staff. Therefore, the routine inspection can be faster, cheaper, safer and more accurate than the conventional inspection method. Practical implications: The study presented here can improve inspecting the built heritages by reducing inspection time and cost, eliminating chances of human errors and accidents and having accurate and consistent information. This study attempts to ensure the sustainability of the built heritage. Originality/value: For ensuring the sustainability of built heritage, this study presents the artificial intelligence-based methodology for the development of an automatic visual inspection system. The automatic web-based visual inspection system for the built heritage has not been reported in previous studies so far. © 2020, Emerald Publishing Limited.","Artificial intelligence; Automatic defect detection; Automatic inspection system; Deep learning; Visual inspection","Gujarat; India; Surat; Convolutional neural networks; Deep learning; Errors; Inspection; Neural network models; Object detection; Spalling; Statistical tests; Sustainable development; Websites; Automatic defect detections; Automatic inspection system; Automatic systems; Automatic visual inspection systems; Built heritage; Deep learning; Heritage structures; Human errors; Manual inspection; Visual inspection; artificial intelligence; data set; detection method; machine learning; visual analysis; Brick","","","","","SVNIT Surat; Ministry of Education, India, MoE","This work is part of the first author’s doctoral research, supported by a fellowship from the Ministry of Education (MoE), Government of India, and SVNIT Surat. The authors are thankful to the Archaeological Survey of India (ASI) Vadodara Circle for permitting data collection of this work.","Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Kudlur M., Levenberg J., Monga R., Moore S., Murray D., Steiner B., Tucker P., Vasudevan V., Warden P., Wicke M., Yu Y., Zheng X., Tensorflow: a system for large-scale machine learning, 12th Symposium on Operating Systems Design and Implementation, pp. 265-283, (2016); Aras F., Krstevska L., Altay G., Tashkov L., Experimental and numerical modal analyses of a historical masonry palace, Construction and Building Materials, 25, 1, pp. 81-91, (2011); Atha D.J., Jahanshahi M.R., Evaluation of deep learning approaches based on convolutional neural networks for corrosion detection, Structural Health Monitoring, 17, 5, pp. 1110-1128, (2018); Beckman G.H., Polyzois D., Cha Y.J., Deep learning-based automatic volumetric damage quantification using depth camera, Automation in Construction, 99, pp. 114-124, (2019); Boscato G., Dal Cin A., Ientile S., Russo S., Optimized procedures and strategies for the dynamic monitoring of historical structures, Journal of Civil Structural Health Monitoring, 6, 2, pp. 265-289, (2016); Cha Y., Choi W., Buyukozturk O., Deep learning‐based crack damage detection using convolutional neural networks, Computer-Aided Civil and Infrastructure Engineering, 32, 5, pp. 361-378, (2017); Cha Y.J., Choi W., Suh G., Mahmoudkhani S., Buyukozturk O., Autonomous structural visual inspection using region-based deep learning for detecting multiple damage types, Computer-Aided Civil and Infrastructure Engineering, 33, 9, pp. 731-747, (2018); Choi J.I., Lee Y., Kim Y.Y., Lee B.Y., Image-processing technique to detect carbonation regions of concrete sprayed with a phenolphthalein solution, Construction and Building Materials, 154, pp. 451-461, (2017); D'Ayala D., Speranza E., Definition of collapse mechanisms and seismic vulnerability of historic masonry buildings, Earthquake Spectra, 19, 3, pp. 479-509, (2003); Dai J., Li Y., He K., Sun J., R-fcn: object detection via region-based fully convolutional networks, Advances in Neural Information Processing Systems, pp. 379-387, (2016); Conservation of Heritage Buildings - A Guide, (2013); Elghaish F., Matarneh S., Talebi S., Kagioglou M., Hosseini M.R., Abrishami S., Toward digitalization in the construction industry with immersive and drones technologies: a critical literature review, Smart and Sustainable Built Environment, (2020); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The pascal visual object classes (voc) challenge, International Journal of Computer Vision, 88, 2, pp. 303-338, (2010); Faisal F., Tarek Z., A comparative review of building component rating systems, Journal of Building Engineering, 33, (2020); Flores-Colen I., Manuel Calico Lopes de Brito J., Peixoto de Freitas V., On-site performance assessment of rendering façades for predictive maintenance, Structural Survey, 29, 2, pp. 133-146, (2011); Gentile C., Gallino N., Condition assessment and dynamic system identification of a historic suspension footbridge, Structural Control and Health Monitoring, 15, 3, pp. 369-388, (2008); Girshick R., Fast r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Gopalakrishnan K., Khaitan S.K., Choudhary A., Agrawal A., Deep Convolutional Neural Networks with transfer learning for computer vision-based data-driven pavement distress detection, Construction and Building Materials, 157, pp. 322-330, (2017); Goulding J.S., Rahimian F.P., Industry preparedness: advanced learning paradigms for exploitation, Construction Innovation and Process Improvement, pp. 409-433, (2012); Halawa L.J., Wibowo A., Ernawan F., Face recognition using faster R-CNN with inception-V2 architecture for CCTV camera, 2019 3rd International Conference on Informatics and Computational Sciences (ICICoS), pp. 1-6, (2019); Hallermann N., Morgenthal G., Rodehorst V., Vision-based monitoring of heritage monuments: unmanned Aerial Systems (UAS) for detailed inspection and high-accuracy survey of structures, WIT Transactions on The Built Environment, 153, pp. 621-632, (2015); Hassan S.I., Dang L.M., Mehmood I., Im S., Choi C., Kang J., Park Y.S., Moon H., Underground sewer pipe condition assessment based on convolutional neural networks, Automation in Construction, 106, (2019); Hosagrahar J., Soule J., Girard L.F., Potts A., Cultural heritage, the UN sustainable development goals, and the new urban agenda, BDC: Bollettino Del Centro Calza Bini, 16, 1, pp. 37-54, (2016); Huang J., Rathod V., Chow D., Sun C., Zhu M., Fathi A., Lu Z., Tensorflow object detection api, Code: Github. Com/Tensorflow/Models/Tree/Master/Object Detection, (2017); Ibarra-Castanedo C., Sfarra S., Klein M., Maldague X., Solar loading thermography: time-lapsed thermographic survey and advanced thermographic signal processing for the inspection of civil engineering and cultural heritage structures, Infrared Physics and Technology, 82, pp. 56-74, (2017); Jalal M.P., Roushan T.Y., Noorzai E., Alizadeh M., A BIM-based construction claim management model for early identification and visualization of claims, Smart and Sustainable Built Environment, (2020); Kayan B.A., Sustainable built heritage: maintenance management appraisal approach, Journal of Cultural Heritage Management and Sustainable Development, 9, (2019); Kumar S.S., Wang M., Abraham D.M., Jahanshahi M.R., Iseley T., Cheng J.C.P., Deep learning–based automated detection of sewer defects in CCTV videos, Journal of Computing in Civil Engineering, 34, 1, (2020); Li R., Zhang W., Suk H.I., Wang L., Li J., Shen D., Ji S., Deep learning based imaging data completion for improved brain disease diagnosis, International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 305-312, (2014); Liu S., Liu S., Cai W., Pujol S., Kikinis R., Feng D., Early diagnosis of Alzheimer's disease with deep learning, 2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI), pp. 1015-1018, (2014); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Mansuri L., Udeaja C., Trillo C., Kwasi G., Patel D., Jha K., Makore C.B., Gupta S., Scientometric analysis and mapping of digital technologies used in cultural heritage field, Association of Researchers in Construction Management, ARCOM 2019 - Proceedings of the 35th Annual Conference, pp. 255-264, (2019); Marshall J., Conservation Manual, (1923); Mishra M., Machine learning techniques for structural health monitoring of heritage buildings: a state-of-the-art review and case studies, Journal of Cultural Heritage, (2020); Mohanty S.P., Hughes D.P., Salathe M., Using deep learning for image-based plant disease detection, Frontiers in Plant Science, 7, pp. 1-10, (2016); Moshtaghian F., Golabchi M., Noorzai E., A framework to dynamic identification of project risks, Smart and Sustainable Built Environment, 9, 4, (2020); Murphy J., An overview of convolutional neural network architectures for deep learning, Microway, (2016); Nair V., Hinton G.E., Rectified linear units improve restricted Boltzmann machines, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807-814, (2010); Newman C., Edwards D., Martek I., Lai J., Thwala W.D., Rillie I., Industry 4.0 deployment in the construction industry: a bibliometric literature review and UK-based case study, Smart and Sustainable Built Environment, (2020); Nishikawa T., Yoshida J., Sugiyama T., Fujino Y., Concrete crack detection by multiple sequential image filtering, Computer-Aided Civil and Infrastructure Engineering, 27, 1, pp. 29-47, (2012); Panella F., Roecklinger N., Vojnovic L., Loo Y., Boehm J., cost-benefit analysis of rail tunnel inspection for photogrammetry and laser scanning, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 43, pp. 1137-1144, (2020); Perez-Gracia V., Caselles J.O., Clapes J., Martinez G., Osorio R., Non-destructive analysis in cultural heritage buildings: evaluating the Mallorca cathedral supporting structures, NDT & E International, 59, pp. 40-47, (2013); Petti L., Trillo C., Makore B.N., Cultural heritage and sustainable development targets: a possible harmonisation? Insights from the European Perspective, Sustainability, 12, 3, (2020); Pieraccini M., Fratini M., Parrini F., Pinelli G., Atzeni C., Dynamic survey of architectural heritage by high-speed microwave interferometry, IEEE Geoscience and Remote Sensing Letters, 2, 1, pp. 28-30, (2005); Pour Rahimian F., Ibrahim R., Wirza Binti OK Rahmat R., B Abdullah M.T., Jaafar B.H.J.M.S., Mediating cognitive transformation with VR 3d sketching during conceptual architectural design process, Archnet-IJAR, International Journal of Architectural Research, 5, 1, pp. 99-113, (2011); Ramcharan A., Baranowski K., Mccloskey P., Ahmed B., Legg J., Hughes D.P., Deep learning for image-based cassava disease detection, Frontiers in Plant Science, 8, pp. 1-7, (2017); Ren S., He K., Girshick R., Sun J., Faster r-cnn: towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, pp. 91-99, (2015); Riggio M., Parisi M.A.V., Tardini C., Tsakanika E., D'Ayala D., Ruggieri N., Tampone G., Augelli F., Existing timber structures: proposal for an assessment template, SHATIS, 2015, pp. 100-107, (2015); Riggio M., D'Ayala D., Parisi M.A., Tardini C., Assessment of heritage timber structures: review of standards, guidelines and procedures, Journal of Cultural Heritage, 31, pp. 220-235, (2018); Ross P., Appraisal and Repair of Timber Structures, (2002); Saadi A., Belhadef H., Deep neural networks for Arabic information extraction, Smart and Sustainable Built Environment, 9, 4, (2020); Salman M., Mathavan S., Kamal K., Rahman M., Pavement crack detection using the Gabor filter, 16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013), pp. 2039-2044, (2013); Sankarasrinivasan S., Balasubramanian E., Karthik K., Chandrasekar U., Gupta R., Health monitoring of civil structures with integrated UAV and image processing system, Procedia Computer Science, 54, pp. 508-515, (2015); Scarre C., Roberts J., The English cemetery at Surat: pre-colonial cultural encounters in western India, The Antiquaries Journal, 85, pp. 251-291, (2005); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826, (2016); Tlhobogang B., Wannous M., Design of plant disease detection System : a transfer learning approach work in progress, 2018 IEEE International Conference on Applied System Invention (ICASI), pp. 158-161, (2018); Tzutalin, LabelImg, (2015); Udeaja C., Trillo C., Awuah K.G.B., Makore B.C.N., Patel D.A., Mansuri L.E., Jha K.N., Urban heritage conservation and rapid urbanization: insights from Surat, India, Sustainability, 12, 6, (2020); Valero E., Forster A., Bosche F., Hyslop E., Wilson L., Turmel A., Automated defect detection and classification in ashlar masonry walls using machine learning, Automation in Construction, 106, (2019); Wang N., Zhao Q., Li S., Zhao X., Zhao P., Damage classification for masonry historic structures using convolutional neural networks based on still images, Computer-Aided Civil and Infrastructure Engineering, 33, 12, pp. 1073-1089, (2018); Wang N., Zhao X., Zhao P., Zhang Y., Zou Z., Ou J., Automatic damage detection of historic masonry buildings based on mobile deep learning, Automation in Construction, 103, pp. 53-66, (2019); Yeum C.M., Dyke S.J., Vision‐based automated crack detection for bridge inspection, Computer-Aided Civil and Infrastructure Engineering, 30, 10, pp. 759-770, (2015); Zhang Z., Wang Y., Zhang J., Mu X., Comparison of multiple feature extractors on Faster RCNN for breast tumor detection, 2019 8th International Symposium on Next Generation Electronics (ISNE), pp. 1-4, (2019); Zolkafli U.K., Zakaria N., Mazlan A.M., Ali A.S., Maintenance work for heritage buildings in Malaysia: owners' perspectives, International Journal of Building Pathology and Adaptation, 37, 2, (2019); Zou Z., Zhao X., Zhao P., Qi F., Wang N., CNN-based statistics and location estimation of missing components in routine inspection of historic buildings, Journal of Cultural Heritage, 38, pp. 221-230, (2019)","L.E. Mansuri; Department of Civil Engineering, Sardar Vallabhbhai National Institute of Technology, Surat, India; email: erlukman@gmail.com","","Emerald Publishing","","","","","","20466099","","","","English","Smart Sustain. Built Environ.","Article","Final","","Scopus","2-s2.0-85100302130"
"García-Aguilar I.; García-González J.; Luque-Baena R.M.; López-Rubio E.","García-Aguilar, Iván (57224824090); García-González, Jorge (57204468921); Luque-Baena, Rafael Marcos (58314199100); López-Rubio, Ezequiel (6602352538)","57224824090; 57204468921; 58314199100; 6602352538","Object detection in traffic videos: an optimized approach using super-resolution and maximal clique algorithm","2023","Neural Computing and Applications","","","","","","","0","10.1007/s00521-023-08741-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162041669&doi=10.1007%2fs00521-023-08741-4&partnerID=40&md5=0417df37a0aa6ec4e3a158337292d378","Department of Computer Languages and Computer Science, University of Málaga, Bulevar Louis Pasteur, 35, Málaga, 29071, Spain; Biomedical Research Institute of Málaga (IBIMA), C/ Doctor Miguel Díaz Recio, 28, Málaga, 29010, Spain","García-Aguilar I., Department of Computer Languages and Computer Science, University of Málaga, Bulevar Louis Pasteur, 35, Málaga, 29071, Spain, Biomedical Research Institute of Málaga (IBIMA), C/ Doctor Miguel Díaz Recio, 28, Málaga, 29010, Spain; García-González J., Department of Computer Languages and Computer Science, University of Málaga, Bulevar Louis Pasteur, 35, Málaga, 29071, Spain, Biomedical Research Institute of Málaga (IBIMA), C/ Doctor Miguel Díaz Recio, 28, Málaga, 29010, Spain; Luque-Baena R.M., Department of Computer Languages and Computer Science, University of Málaga, Bulevar Louis Pasteur, 35, Málaga, 29071, Spain, Biomedical Research Institute of Málaga (IBIMA), C/ Doctor Miguel Díaz Recio, 28, Málaga, 29010, Spain; López-Rubio E., Department of Computer Languages and Computer Science, University of Málaga, Bulevar Louis Pasteur, 35, Málaga, 29071, Spain, Biomedical Research Institute of Málaga (IBIMA), C/ Doctor Miguel Díaz Recio, 28, Málaga, 29010, Spain","Detection of small objects is one of the main challenges to be improved in deep learning, mainly due to the small number of pixels and scene’s context, leading to a loss in performance. In this paper, we present an optimized approach based on deep object detection models that allow the detection of a higher number of elements and improve the score obtained for their class inference. The main advantage of the presented methodology is that it is not necessary to modify the internal structure of the selected convolutional neural network model or re-training for a specific scene. Our proposal is based on detecting initial regions to generate several sub-images using super-resolution (SR) techniques, increasing the number of pixels of the elements, and re-infer over these areas using the same pre-trained model. A reduced set of windows is calculated in the super-resolved image by analyzing a computed graph that describes the distances among the preliminary object detections. This analysis is done by finding maximal cliques on it. This way, the number of windows to be examined is diminished, significantly speeding up the detection process. This framework has been successfully tested on real traffic sequences obtained from the U.S. Department of Transportation. An increase of up to 44.6% is achieved, going from an average detection rate for the EfficientDet D4 model of 14.5% compared to 59.1% using the methodology presented for the first sequence. Qualitative experiments have also been performed over the Cityscapes and VisDrone datasets. © 2023, The Author(s).","Convolutional neural networks; Object detection; Small objects; Super-resolution; Test time augmentation","Convolution; Convolutional neural networks; Deep learning; Object recognition; Optical resolving power; Pixels; Convolutional neural network; Detection models; Maximal cliques algorithms; Objects detection; Performance; Small objects; Superresolution; Test time; Test time augmentation; Traffic videos; Object detection","","","","","Autonomous Government of Andalusia (Spain), (UMA20-FEDERJA-108); European Regional Development Fund; Instituto de Investigación Biomédica de Málaga y Plataforma en Nanomedicina-IBIMA; Universidad de Málaga; University of Málaga, (B1-2019_01, B1-2019_02, B1-2021_20, B1-2022_14, B4-2022); Young Employment operative program, (SNGJ5Y6-15)","This work was partially supported by the Autonomous Government of Andalusia (Spain) under project UMA20-FEDERJA-108, project name Detection, characterization and prognosis value of the non-obstructive coronary disease with deep learning. It includes funds from the European Regional Development Fund (ERDF). It was also partially supported by the University of Málaga (Spain) under grants B1-2019_01, project name Anomaly detection on roads by moving cameras; B1-2019_02, project name Self-Organizing Neural Systems for Non-Stationary Environments; B1-2021_20, project name Detection of coronary stenosis using deep learning applied to coronary angiography, B4-2022, project name Intelligent Clinical Decision Support System for Non-Obstructive Coronary Artery Disease in Coronarographies, and B1-2022_14, project name Deteccion de trayectorias anomalas de vehiculos en camaras de trafico. Iván García-Aguilar was funded by a scholarship from the Autonomous Government of Andalusia (Spain) under the Young Employment operative program [grant number SNGJ5Y6-15]. The authors thankfully acknowledge the computer resources, technical expertise, and assistance provided by the SCBI (Supercomputing and Bioinformatics Center) of the University of Málaga. They also gratefully acknowledge the support of NVIDIA Corporation with the donation of an RTX A6000 GPU with 48Gb. The authors also thankfully acknowledge the grant of the Universidad de Málaga and the Instituto de Investigación Biomédica de Málaga y Plataforma en Nanomedicina-IBIMA Plataforma BIONAND. ","Shen L., Tao H., Ni Y., Wang Y., Stojanovic V., Improved Yolov3 model with feature map cropping for multi-scale road object detection, Meas Sci Technol, 34, 4, (2023); Zhuang Z., Tao H., Chen Y., Stojanovic V., Paszke W., An optimal iterative learning control approach for linear systems with nonuniform trial lengths under input constraints, IEEE Trans Syst Man Cybern Syst, 53, 6, pp. 3461-3473, (2023); Zhou L., Tao H., Paszke W., Stojanovic V., Yang H., Pd-type iterative learning control for uncertain spatially interconnected systems, Mathematics, 8, 9, (2020); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., pp. 21-37, (2016); Lee Y., Hwang J-Wlee S., Bae Y., Park J., An energy and GPU-computation efficient backbone network for real-time object detection, (2019); Benito-Picazo J., Dominguez E., Palomo E., Lopez-Rubio E., Deep learning-based video surveillance system managed by low cost hardware and panoramic cameras, Integr Comput-Aided Eng, 27, pp. 1-15, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, (2016); Tan M., Le Q., EfficientNet: Rethinking model scaling for convolutional neural networks, In: Proceedings of the 36Th International Conference on Machine Learning, 97, pp. 6105-6114, (2019); Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.-C., (2019); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., SSD: Single shot MultiBox detector, Computer vision–ECCV 2016, pp. 21-37, (2016); Bochkovskiy A., Wang C.-Y., Liao H.-Y., (2020); Zhu X., Lyu S., Wang X., Zhao Q., Tph-Yolov5: Improved Yolov5 Based on Transformer Prediction Head for Object Detection on Drone-Captured Scenarios, (2021); Subudhi B.N., Nanda P.K., Ghosh A., A change information based fast algorithm for video object detection and tracking, IEEE Trans Circuits Syst Video Technol, 21, 7, pp. 993-1004, (2011); Mandel T., Jimenez M., Risley E., Nammoto T., Williams R., Panoff M., Ballesteros M., Suarez B., Detection confidence driven multi-object tracking to recover reliable tracks from unreliable detections, Pattern Recogn, 135, (2023); Kavitha R., Chitra D., An extreme learning machine and action recognition algorithm for generalized maximum clique problem in video event recognition, Dyn Syst Appl, 30, 8, pp. 1228-1249, (2021); Ren S., Li J., Tu T., Peng Y., Jiang J., Towards efficient video detection object super-resolution with deep fusion network for public safety, Secur Commun Netw, 2021, pp. 1-14, (2021); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, pp. 248-255, (2009); Lin T.-Y., Maire M., Belongie S., Bourdev L., Girshick R., Hays J., Perona P., Ramanan D., Zitnick C.L., Dollar P., Microsoft COCO: Common objects in context, (2015); Tan M., Pang R., Le Q.V., Efficientdet: Scalable and Efficient Object Detection, (2020); Rabbi J., Ray N., Schubert M., Chowdhury S., Chao D., Small-object detection in remote sensing images with end-to-end edge-enhanced gan and object detector network, Remote Sens, 12, 9, (2020); Deng C., Wang M., Liu L., Liu Y., Jiang Y., Extended feature pyramid network for small object detection, IEEE Trans Multimed, 24, pp. 1968-1979, (2022); Su P., Li W., Sha L., Shi Y., Dong T., Traffic sign recognition algorithm based on feature pyramid attention, In: Journal of Physics: Conference Series, 2035, (2021); Khan K., Imran A., Rehman H.Z.U., Fazil A., Zakwan M., Mahmood Z., Performance enhancement method for multiple license plate recognition in challenging environments, Eurasip J Image Video Process, 2021, 1, pp. 1-23, (2021); Dong C., Loy C.C., He K., Tang X., Learning a deep convolutional network for image super-resolution, Computer vision-ECCV 2014, pp. 184-199, (2014); Dong C., Loy C.C., Tang X., Accelerating the super-resolution convolutional neural network, Corr, (2016); Kim J., Lee J.K., Lee K.M., Accurate image super-resolution using very deep convolutional networks, In: Corr Arxiv, 1511, (2015); Kim J., Lee J.K., Lee K.M., Deeply-recursive convolutional network for image super-resolution, In: Corr Arxiv, 1511, (2015); Kong D., Han M., Xu W., Tao H., Gong Y., ) Video super-resolution with scene-specific priors, . In: Procedings of the British Machine Vision Conference 2006, British Machine Vision Association, (2006); Camargo A., He Q., Palaniappan K., Performance evaluation of optimization methods for super-resolution mosaicking on UAS surveillance videos, SPIE proceedings, (2012); Garcia-Aguilar I., Luque-Baena R.M., Lopez-Rubio E., Improved detection of small objects in road network sequences using CNN and super resolution, Expert Syst, 39, 2, (2021); Zhu P., Wen L., Du D., Bian X., Fan H., Hu Q., Ling H., Detection and tracking meet drones challenge, IEEE Trans Pattern Anal Mach Intell, 44, pp. 7380-7399, (2021)","I. García-Aguilar; Department of Computer Languages and Computer Science, University of Málaga, Málaga, Bulevar Louis Pasteur, 35, 29071, Spain; email: ivangarcia@lcc.uma.es","","Springer Science and Business Media Deutschland GmbH","","","","","","09410643","","","","English","Neural Comput. Appl.","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85162041669"
"Xavier A.I.; Villavicencio C.; Macrohon J.J.; Jeng J.-H.; Hsieh J.-G.","Xavier, Alphonse Inbaraj (57760677000); Villavicencio, Charlyn (57224008163); Macrohon, Julio Jerison (57215077956); Jeng, Jyh-Horng (35762841800); Hsieh, Jer-Guang (7401435589)","57760677000; 57224008163; 57215077956; 35762841800; 7401435589","Object Detection via Gradient-Based Mask R-CNN Using Machine Learning Algorithms","2022","Machines","10","5","340","","","","6","10.3390/machines10050340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132641468&doi=10.3390%2fmachines10050340&partnerID=40&md5=98cb66ef39a111b350a00897f24062de","Department of Information Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan; College of Information and Communications Technology, Bulacan State University, Bulacan, 3000, Philippines; Department of Electrical Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan","Xavier A.I., Department of Information Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan; Villavicencio C., Department of Information Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan, College of Information and Communications Technology, Bulacan State University, Bulacan, 3000, Philippines; Macrohon J.J., Department of Information Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan; Jeng J.-H., Department of Information Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan; Hsieh J.-G., Department of Electrical Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan","Object detection has received a lot of research attention in recent years because of its close association with video analysis and image interpretation. Detecting objects in images and videos is a fundamental task and considered as one of the most difficult problems in computer vision. Many machine learning and deep learning models have been proposed in the past to solve this issue. In the current scenario, the detection algorithm must calculate from beginning to end in the shortest amount of time possible. This paper proposes a method called GradCAM-MLRCNN that combines Gradient-weighted Class Activation Mapping++ (Grad-CAM++) for localization and Mask Regional Convolution Neural Network (Mask R-CNN) for object detection along with machine learning algorithms. In our proposed method, images are used to train the network, together with masks that shows where the objects are in the image. A bounding box is regressed around the region of interest in most localization networks. Furthermore, just like any classification task, the multi-class log loss is minimized during training. This model enhances the calculation time and speed, as well as the efficiency, which recognizes objects in images accurately by comparing state-of-the-art machine learning algorithms, such as decision tree, Gaussian algorithm, k-means clustering, k-nearest neighbor, and logistic regression. Among these methods, we found logistic regression performed well with an accuracy rate of 98.4%, recall rate of 99.6%, and precision rate of 97.3% with respect to ResNet 152 and VGG 19. Furthermore, we proved the goodness of fit of our proposed model using chi-square statistical method and demonstrated that our solution can achieve great precision while maintaining a fair recall level. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Grad-CAM++; GradCAM-MLRCNN; Mask R-CNN; object detection; object localization","","","","","","I-Shou University, ISU; Ministry of Science and Technology, Taiwan, MOST","Funding: This research was funded as a scholar of the Ministry of Science and Technology (MOST), Taiwan and I-Shou University, Kaohsiung City, Taiwan.","Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Et al., Imagenet large scale visual recognition challenge, Int. J. Comput. Vis, 115, pp. 211-252, (2015); Kuznetsova A., Rom H., Alldrin N., Uijlings J., Krasin I., Pont-Tuset J., Kamali S., Popov S., Malloci M., Duerig T., Et al., The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale, Int. J. Comput. Vis, 128, pp. 1956-1981, (2020); Zhu P., Wen L., Bian X., Ling H., Hu Q., Vision meets drones: A challenge, (2018); Chattopadhay A., Sarkar A., Howlader P., Balasubramanian V.N., Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks, (2018); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Uijlings J.R.R., van de Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int. J. Comput. Vis, 104, pp. 154-171, (2013); Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A., Learning Deep Features for Discriminative Localization, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2016); Selvaraju R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D., Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), (2017); Schottl A., A light-weight method to foster the (Grad)CAM interpretability and explainability of classification networks, Proceedings of the 2020 10th International Conference on Advanced Computer Information TechnologiesAlfred Schöttl, (2020); Ennadifi E., Laraba S., Vincke D., Mercatoris B., Gosselin B., Wheat Diseases Classification and Localization Utilizing system network, Proceedings of the 2020 International Conference on Intelligent Systems and Computer Vision (ISCV), (2020); Liu R., Yu Z., Mo D., Cai Y., An Improved Faster-RCNN Algorithm for Object Detection in Remote Sensing Images, Proceedings of the 2020 39th Chinese Control Conference (CCC), pp. 7188-7192, (2020); Yin X., Yang Y., Xu H., Li W., Deng J., Enhanced Faster-RCNN Algorithm for Object Detection in Aerial Images, Proceedings of the 2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC), pp. 2355-2358, (2020); Songhui M., Mingming S., Chufeng H., Objects detection and location based on mask RCNN and stereo vision, Proceedings of the 2019 14th IEEE International Conference on Electronic Measurement & Instruments (ICEMI), pp. 369-373, (2019); Song X., Jiang P., Zhu H., Research on Unmanned Vessel Surface Object Detection Based on Fusion of SSD and Faster-RCNN, Proceedings of the 2019 Chinese Automation Congress (CAC), pp. 3784-3788, (2019); Ouadiay F.Z., Bouftaih H., Bouyakhf E.H., Himmi M.M., Simultaneous object detection and localization using convolutional neural networks, Proceedings of the 2018 International Conference on Intelligent Systems and Computer Vision (ISCV), pp. 1-8, (2018); Gupta S., Bagga S., Dharandher S.K., Sharma D.K., GPOL: Gradient and Probabilistic approach for Object Localization to understand the working of CNNs, Proceedings of the 2019 IEEE Bombay Section Signature Conference (IBSSC), pp. 1-6, (2019); Lin Q., Ding Y., Xu H., Lin W., Li J., Xie X., ECascade-RCNN: Enhanced Cascade RCNN for Multi-scale Object Detection in UAV Images, Proceedings of the 2021 7th International Conference on Automation, Robotics and Applications (ICARA), pp. 268-272, (2021); Yao N., Shan G., Zhu X., Substation Object Detection Based on Enhance RCNN Model, Proceedings of the 2021 6th Asia Conference on Power and Electrical Engineering (ACPEE), pp. 463-469, (2021); Pramanik A., Pal S.K., Maiti J., Mitra P., Granulated RCNN and Multi-Class Deep SORT for Multi-Object Detection and Tracking, IEEE Trans. Emerg. Top. Comput. Intell, 6, pp. 171-181, (2022); Krishna N.M., Reddy R.Y., Reddy M.S.C., Madhav K.P., Sudham G., Object Detection and Tracking Using Yolo, Proceedings of the 2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA), pp. 1-7, (2021); Mukherjee S., Valenzise G., Cheng I., Potential of deep features for opinion-unaware, distortion-unaware, no-reference image quality assessment, Proceedings of the International Conference on Smart Multimedia (Springer), (2019); Zeiler M.D., Fergus R., Visualizing and understanding convolutional networks, European Conference on Computer Vision; Lecture Notes in Computer Science, 8689, (2014); JSpringenberg T., Dosovitskiy A., Brox T., Riedmiller M., Striving for simplicity: The all convolutional net, (2014); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems (NIPS), pp. 91-99, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Girshick R., Fast R-CNN, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1440-1448, (2015)","A.I. Xavier; Department of Information Engineering, I-Shou University, Kaohsiung City, 84001, Taiwan; email: xalphonse@gmail.com","","MDPI","","","","","","20751702","","","","English","Mach.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132641468"
"Bouguettaya A.; Zarzour H.; Kechida A.; Taberkit A.M.","Bouguettaya, Abdelmalek (57219439334); Zarzour, Hafed (55611979500); Kechida, Ahmed (54415863600); Taberkit, Amine Mohammed (57201033566)","57219439334; 55611979500; 54415863600; 57201033566","Vehicle Detection From UAV Imagery With Deep Learning: A Review","2022","IEEE Transactions on Neural Networks and Learning Systems","33","11","","6047","6067","20","35","10.1109/TNNLS.2021.3080276","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107176671&doi=10.1109%2fTNNLS.2021.3080276&partnerID=40&md5=f5e0cf0103457ff70fc584b9db0c7355","Research Center in Industrial Technologies (CRTI), Signal Processing and Imagery Division, Analysis and Control of Embedded Drone Systems Research Team, Algiers, 16014, Algeria; University of Souk Ahras, LIM Research, Department of Mathematics and Computer Science, Souk Ahras, 41000, Algeria","Bouguettaya A., Research Center in Industrial Technologies (CRTI), Signal Processing and Imagery Division, Analysis and Control of Embedded Drone Systems Research Team, Algiers, 16014, Algeria; Zarzour H., University of Souk Ahras, LIM Research, Department of Mathematics and Computer Science, Souk Ahras, 41000, Algeria; Kechida A., Research Center in Industrial Technologies (CRTI), Signal Processing and Imagery Division, Analysis and Control of Embedded Drone Systems Research Team, Algiers, 16014, Algeria; Taberkit A.M., Research Center in Industrial Technologies (CRTI), Signal Processing and Imagery Division, Analysis and Control of Embedded Drone Systems Research Team, Algiers, 16014, Algeria","Vehicle detection from unmanned aerial vehicle (UAV) imagery is one of the most important tasks in a large number of computer vision-based applications. This crucial task needed to be done with high accuracy and speed. However, it is a very challenging task due to many characteristics related to the aerial images and the used hardware, such as different vehicle sizes, orientations, types, density, limited datasets, and inference speed. In recent years, many classical and deep-learning-based methods have been proposed in the literature to address these problems. Handed engineering- and shallow learning-based techniques suffer from poor accuracy and generalization to other complex cases. Deep-learning-based vehicle detection algorithms achieved better results due to their powerful learning ability. In this article, we provide a review on vehicle detection from UAV imagery using deep learning techniques. We start by presenting the different types of deep learning architectures, such as convolutional neural networks, recurrent neural networks, autoencoders, generative adversarial networks, and their contribution to improve the vehicle detection task. Then, we focus on investigating the different vehicle detection methods, datasets, and the encountered challenges all along with the suggested solutions. Finally, we summarize and compare the techniques used to improve vehicle detection from UAV-based images, which could be a useful aid to researchers and developers to select the most adequate method for their needs.  © 2012 IEEE.","Autoencoders; computer vision; convolutional neural networks (CNNs); deep learning; generative adversarial networks (GANs); recurrent neural networks (RNNs); unmanned aerial vehicle (UAV); vehicle detection","Aircraft detection; Antennas; Computer architecture; Computer vision; Convolution; Generative adversarial networks; Image enhancement; Network architecture; Object detection; Unmanned aerial vehicles (UAV); Aerial vehicle; Auto encoders; Convolutional neural network; Deep learning; Generative adversarial network; Neural-networks; Objects detection; Recurrent neural network; Task analysis; Unmanned aerial vehicle; Vehicle detection.; Vehicles detection; article; autoencoder; computer vision; convolutional neural network; deep learning; detection algorithm; human; human experiment; imagery; recurrent neural network; unmanned aerial vehicle; velocity; Recurrent neural networks","","","","","","","Du D., Et al., The unmanned aerial vehicle benchmark: Object detection and tracking, Proc. Eur. Conf. Comput. Vis., pp. 370-386, (2018); Mueller M., Smith N., Ghanem B., A benchmark and simulator for UAV tracking, Proc. Eur. Conf. Comput. Vis., 9905, pp. 445-461, (2016); Robicquet A., Sadeghian A., Alahi A., Savarese S., Learning social etiquette: Human trajectory understanding in crowded scenes, Proc. Eur. Conf. Comput. Vis., in Lecture Notes in Computer Science, 9912, pp. 549-565, (2016); Zhu P., Wen L., Bian X., Ling H., Hu Q., Vision Meets Drones: A Challenge, (2018); Liu K., Mattyus G., Fast multiclass vehicle detection on aerial images, Ieee Geosci. Remote Sens. Lett., 12, 9, pp. 1938-1942, (2015); Lecun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. Ieee, 86, 11, pp. 2278-2324, (1998); Mujawar S., Kiran D., Ramasangu H., An efficient CNN architecture for image classification on FPGA accelerator, Proc. 2nd Int. Conf. Adv. Electron., Comput. Commun. (ICAECC), pp. 2018-2021, (2018); Peng Y., Et al., FB-CNN: Feature fusion-based bilinear CNN for classification of fruit fly image, Ieee Access, 8, pp. 3987-3995, (2020); Blot M., Cord M., Thome N., Max-min convolutional neural networks for image classification, Proc. Ieee Int. Conf. Image Process. (ICIP), pp. 3678-3682, (2016); Howard A.G., Some improvements on deep convolutional neural network based image classification, Proc. 2nd Int. Conf. Learn. Represent. (ICLR), pp. 1-6, (2014); Li J., Liang X., Shen S., Xu T., Feng J., Yan S., Scale-aware fast R-CNN for pedestrian detection, Ieee Trans. Multimedia, 20, 4, pp. 985-996, (2018); Agrawal T., Urolagin S., Multi-angle parking detection system using mask R-CNN, Proc. 2nd Int. Conf. Big Data Eng. Technol., pp. 76-80, (2020); Zhang W., Wang S., Thachan S., Chen J., Qian Y., Deconv R-CNN for small object detection on remote sensing images, Proc. Ieee Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 2491-2494, (2018); Sai B.N.K., Sasikala T., Object detection and count of objects in image using tensor flow object detection API, Proc. Int. Conf. Smart Syst. Inventive Technol. (ICSSIT), pp. 542-546, (2019); Kayalibay B., Jensen G., Smagt Der P.Van, CNN-based segmentation of medical imaging data, CoRR, pp. 1-24, (2017); Vardhana M., Arunkumar N., Lasrado S., Abdulhay E., Ramirez-Gonzalez G., Convolutional neural network for bio-medical image segmentation with hardware acceleration, Cognit. Syst. Res., 50, pp. 10-14, (2018); Tian Y., Yang G., Wang Z., Li E., Liang Z., Instance segmentation of apple flowers using the improved mask R-CNN model, Biosyst. Eng., 193, pp. 264-278, (2020); Zeiler M.D., Fergus R., Visualizing and understanding convolutional networks, Proc. Eur. Conf. Comput. Vis. (ECCV), 8689, pp. 818-833, (2014); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Commun. Acm, 1, pp. 1097-1105, (2012); Szegedy C., Et al., Going deeper with convolutions, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1-9, (2015); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Proc. 3rd Int. Conf. Learn. Represent. (ICLR), pp. 1-14, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Howard A.G., Et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Sandler M., Howard A., Zhu M., Zhmoginov A., Chen L.-C., MobileNetV2: Inverted residuals and linear bottlenecks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4510-4520, (2018); Howard A., Et al., Searching for MobileNetV3, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 1314-1324, (2019); Iandola F.N., Han S., Moskewicz M.W., Ashraf K., Dally W.J., Keutzer K., SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5 MB model size, CoRR, pp. 1-13, (2016); Zhang X., Zhou X., Lin M., Sun J., ShuffleNet: An extremely efficient convolutional neural network for mobile devices, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 6848-6856, (2018); Ma N., Zhang X., Zheng H.T., Sun J., ShuffleNet V2: Practical guidelines for efficient CNN architecture design, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 116-131, (2018); Wang R.J., Li X., Ling C.X., Pelee: A real-time object detection system on mobile devices, Proc. Adv. Neural Inf. Process. Syst., pp. 1963-1972, (2018); Li J., Zhao R., Hu H., Gong Y., Improving RNN transducer modeling for end-to-end speech recognition, Proc. Ieee Autom. Speech Recognit. Understand. Workshop (ASRU), pp. 114-121, (2019); Amberkar A., Awasarmol P., Deshmukh G., Dave P., Speech recognition using recurrent neural networks, Proc. Int. Conf. Current Trends Towards Converging Technol. (ICCTCT), pp. 2018-2021, (2018); Wang H., Wang H., Xu K., Evolutionary recurrent neural network for image captioning, Neurocomputing, 401, pp. 249-256, (2020); Wang M., Song L., Yang X., Luo C., A parallel-fusion RNNLSTM architecture for image caption generation, Proc. Ieee Int. Conf. Image Process. (ICIP), pp. 4448-4452, (2016); Li Y., Yang J., Hydrological time series prediction model based on attention-LSTM neural network, Proc. 2nd Int. Conf. Mach. Learn. Mach. Intell., pp. 21-25, (2019); Chen Y., Wang K., Prediction of satellite time series data based on long short term memory-autoregressive integrated moving average model (LSTM-ARIMA), Proc. Ieee 4th Int. Conf. Signal Image Process. (ICSIP), pp. 308-312, (2019); Song X., Et al., Time-series well performance prediction based on long short-term memory (LSTM) neural network model, J. Petroleum Sci. Eng., 186, (2020); Yao L., Guan Y., An improved LSTM structure for natural language processing, Proc. Ieee Int. Conf. Saf. Produce Informatization (IICSPI), pp. 565-569, (2018); Li J., Xu Y., Shi H., Bidirectional LSTM with hierarchical attention for text classification, Proc. Ieee 4th Adv. Inf. Technol., Electron. Automat. Control Conf. (IAEAC), pp. 456-459, (2019); Ganai A.F., Khursheed F., Predicting next word using RNN and LSTM cells: Stastical language modeling, Proc. 5th Int. Conf. Image Inf. Process. (ICIIP), pp. 469-474, (2019); Su C., Huang H., Shi S., Jian P., Shi X., Neural machine translation with Gumbel tree-LSTM based encoder, J. Vis. Commun. Image Represent., 71, (2020); Zhang C., Kim J., Modeling long-and short-term temporal context for video object detection, Proc. Ieee Int. Conf. Image Process. (ICIP), pp. 71-75, (2019); Lu Y., Lu C., Tang C.-K., Online video object detection using association LSTM, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 2344-2352, (2017); Carrio A., Sampedro C., Rodriguez-Ramos A., Campoy P., A review of deep learning methods and applications for unmanned aerial vehicles, J. Sensors, 2017, pp. 1-13, (2017); Hochreiter S., Schmidhuber J., Long short-term memory, Neural Comput., 9, 8, pp. 1735-1780, (1997); Li J., Liang X., Wei Y., Xu T., Feng J., Yan S., Perceptual generative adversarial networks for small object detection, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1222-1230, (2017); Liou C.-Y., Cheng W.-C., Liou J.-W., Liou D.-R., Autoencoder for words, Neurocomputing, 139, pp. 84-96, (2014); Vincent P., Larochelle H., Lajoie I., Bengio Y., Manzagol P.-A., Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion, J. Mach. Learn. Res., 11, 12, pp. 3371-3408, (2010); Masci J., Meier U., Ciresan D., Schmidhuber J., Stacked convolutional auto-encoders for hierarchical feature extraction, Proc. Int. Conf. Artif. Neural Netw., pp. 52-59, (2011); Xu J., Et al., Stacked sparse autoencoder (SSAE) for nuclei detection on breast cancer histopathology images, Ieee Trans. Med. Imag., 35, 1, pp. 119-130, (2016); Schreyer M., Sattarov T., Borth D., Dengel A., Reimer B., Detection of Anomalies in Large Scale Accounting Data Using Deep Autoencoder Networks, (2017); Pu Y., Et al., Variational autoencoder for deep learning of images, labels and captions, Proc. 30th Int. Conf. Neural Inf. Process., pp. 2360-2368, (2016); Zhu P., Wen L., Du D., Bian X., Hu Q., Ling H., Vision Meets Drones: Past, Present and Future, (2020); Bajaj K., Singh D.K., Ansari M.A., Autoencoders based deep learner for image denoising, Procedia Comput. Sci., 171, pp. 1535-1541, (2020); Fang Z., Jia T., Chen Q., Xu M., Yuan X., Wu C., Laser stripe image denoising using convolutional autoencoder, Results Phys., 11, pp. 96-104, (2018); Qiu Y., Yang Y., Lin Z., Chen P., Luo Y., Huang W., Improved denoising autoencoder for maritime image denoising and semantic segmentation of USV, China Commun., 17, 3, pp. 46-57, (2020); Keser R.K., Toreyin B.U., Autoencoder based dimensionality reduction of feature vectors for object recognition, Proc. 15th Int. Conf. Signal-Image Technol. Internet-Based Syst. (SITIS), pp. 577-584, (2019); Hu C., Hou X., Lu Y., Improving the architecture of an autoencoder for dimension reduction, Proc. Ieee 11th Int. Conf. Ubiquitous Intell. Comput., Ieee 11th Int. Conf. Autonomic Trusted Comput., Ieee 14th Int. Conf. Scalable Comput. Commun. Associated Workshops, pp. 855-858, (2014); Zabalza J., Et al., Novel segmented stacked autoencoder for effective dimensionality reduction and feature extraction in hyperspectral imaging, Neurocomputing, 185, pp. 1-10, (2016); Kuppili V., Edla D.R., Bablani A., Novel fitness function for 3D image reconstruction using bat algorithm based autoencoder, Proc. 23rd Int. Acm Conf. 3D Web Technol., pp. 2-3, (2018); Tan C.C., Eswaran C., Reconstruction of handwritten digit images using autoencoder neural networks, Proc. Can. Conf. Electr. Comput. Eng., pp. 465-470, (2008); Dong Z., Qu W., Infrared image colorization using an edge aware auto encoder decoder with the multi-resolution fusion, Proc. Chin. Automat. Congr. (CAC), pp. 1011-1016, (2019); Goodfellow I.J., Et al., Generative adversarial nets, Proc. 27th Int. Conf. Neural Inf. Process. Syst., 2, pp. 2672-2680, (2014); Emmert-Streib F., Yang Z., Feng H., Tripathi S., Dehmer M., An introductory review of deep learning for prediction models with big data, Frontiers Artif. Intell., 3, pp. 1-23, (2020); Bugdol M., Segiet Z., Krecichwost M., Kasperek P., Vehicle detection system using magnetic sensors, Transp. Problems, 9, 1, pp. 49-60, (2014); Ali S.S.M., George B., Vanajakshi L., Venkatraman J., A multiple inductive loop vehicle detection system for heterogeneous and laneless traffic, Ieee Trans. Instrum. Meas., 61, 5, pp. 1353-1360, (2012); Hickman M., Mirchandani P., Airborne traffic flow data and traffic management, Proc. Greenshields Symp., pp. 121-132, (2008); Leitloff J., Rosenbaum D., Kurz F., Meynberg O., Reinartz P., An operational system for estimating road traffic information from aerial images, Remote Sens., 6, 11, pp. 11315-11341, (2014); Najiya K.V., Archana M., UAV video processing for traffic surveillence with enhanced vehicle detection, Proc. 2nd Int. Conf. Inventive Commun. Comput. Technol. (ICICCT), pp. 662-668, (2018); Elloumi M., Dhaou R., Escrig B., Idoudi H., Saidane L.A., Monitoring road traffic with a UAV-based system, Proc. Ieee Wireless Commun. Netw. Conf. (WCNC), pp. 1-6, (2018); Zhang H., Liptrott M., Bessis N., Cheng J., Real-time traffic analysis using deep learning techniques and UAV based video, Proc. 16th Ieee Int. Conf. Adv. Video Signal Based Surveill. (AVSS), pp. 1-5, (2019); Zhao X., Pu F., Wang Z., Chen H., Xu Z., Detection, tracking, and geolocation of moving vehicle from UAV using monocular camera, Ieee Access, 7, pp. 101160-101170, (2019); Li S., Zhang W., Li G., Su L., Huang Q., Vehicle detection in UAV traffic video based on convolution neural network, Proc. Ieee 1st Conf. Multimedia Inf. Process. Retr. (MIPR), pp. 1-6, (2018); Coifman B., McCord M., Mishalani R.G., Redmill K., Surface transportation surveillance from unmanned aerial vehicles, Proc. 83rd Annu. Meeting Transp. Res. Board, (2004); Coifman B., McCord M., Mishalani R.G., Iswalt M., Ji Y., Roadway traffic monitoring from an unmanned aerial vehicle, Iee Proc. Intell. Transp. Syst., 153, 1, pp. 11-20, (2006); Xi X., Yu Z., Zhan Z., Yin Y., Tian C., Multi-task cost-sensitiveconvolutional neural network for car detection, Ieee Access, 7, pp. 98061-98068, (2019); Ke R., Li Z., Kim S., Ash J., Cui Z., Wang Y., Real-time bidirectional traffic flow parameter estimation from aerial videos, Ieee Trans. Intell. Transp. Syst., 18, 4, pp. 890-901, (2017); Pan Q., Wen X., Lu Z., Li L., Jing W., Dynamic speed control of unmanned aerial vehicles for data collection under Internet of Things, Sensors, 18, 11, pp. 1-18, (2018); Benjdira B., Khursheed T., Koubaa A., Ammar A., Ouni K., Car detection using unmanned aerial vehicles: Comparison between faster R-CNN and YOLOv3, Proc. 1st Int. Conf. Unmanned Vehicle Systems-Oman (UVS), pp. 1-6, (2019); Ayalew A., A review on object detection from unmanned aerial vehicle using CNN, Int. J. Advance Res., Ideas Innov. Technol., 5, 4, pp. 241-243, (2019); Koga Y., Miyazaki H., Shibasaki R., A CNN-based method of vehicle detection from aerial images using hard example mining, Remote Sens., 10, 1, (2018); Xu Y., Yu G., Wang Y., Wu X., Ma Y., Car detection from low-altitude UAV imagery with the faster R-CNN, J. Adv. Transp., 2017, pp. 1-10, (2017); Nguyen H., Improving faster R-CNN framework for fast vehicle detection, Math. Problems Eng., 2019, pp. 1-11, (2019); Shi K., Bao H., Ma N., Forward vehicle detection based on incremental learning and fast R-CNN, Proc. 13th Int. Conf. Comput. Intell. Secur. (CIS), pp. 73-76, (2017); Sun K., Xiao B., Liu D., Wang J., Deep high-resolution representation learning for human pose estimation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 5693-5703, (2019); Li Y., Chen Y., Wang N., Zhang Z.-X., Scale-aware trident networks for object detection, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 6054-6063, (2019); Cheng Z., Wu Y., Xu Z., Lukasiewicz T., Wang W., Segmentation Is All You Need, (2019); Li Z., Peng C., Yu G., Zhang X., Deng Y., Sun J., DetNet: Design backbone for object detection, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 334-350, (2018); Law H., Deng J., CornerNet: Detecting objects as paired keypoints, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 734-750, (2018); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 580-587, (2014); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, Ieee Trans. Pattern Anal. Mach. Intell., 37, 9, pp. 1904-1916, (2015); Girshick R., Fast R-CNN, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Ieee Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Dai J., Li Y., He K., Sun J., R-FCN: Object detection via regionbased fully convolutional networks, Proc. Adv. Neural Inf. Process. Syst., pp. 379-387, (2016); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 2961-2969, (2017); Sommer L., Schuchert T., Beyerer J., Comprehensive analysis of deep learning-based vehicle detection in aerial images, Ieee Trans. Circuits Syst. Video Technol., 29, 9, pp. 2733-2747, (2019); Sommer L.W., Schuchert T., Beyerer J., Fast deep vehicle detection in aerial images, Proc. Ieee Winter Conf. Appl. Comput. Vis. (WACV), pp. 311-319, (2017); Uijlings J.R.R., Sande De Van A K.E., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 779-788, (2016); Liu W., Et al., SSD: Single shot MultiBox detector, Proc. Eur. Conf. Comput. Vis., pp. 21-37, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 6517-6525, (2017); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 2980-2988, (2017); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020); Ammar A., Koubaa A., Ahmed M., Saad A., Aerial Images Processing for Car Detection Using Convolutional Neural Networks: Comparison between Faster R-CNN and YoloV3, (2019); Radovic M., Adarkwa O., Wang Q., Object recognition in aerial images using convolutional neural networks, J. Imag., 3, 2, (2017); Lu J., Et al., A vehicle detection method for aerial image based on YOLO, J. Comput. Commun., 6, 11, pp. 98-107, (2018); Tang T., Deng Z., Zhou S., Lei L., Zou H., Fast vehicle detection in UAV images, Proc. Int. Workshop Remote Sens. with Intell. Process. (RSIP), pp. 1-5, (2017); Amato G., Ciampi L., Falchi F., Gennaro C., Counting vehicles with deep learning in onboard UAV imagery, Proc. Ieee Symp. Comput. Commun. (ISCC), pp. 1-5, (2019); Hsieh M.-R., Lin Y.-L., Hsu W.H., Drone-based object counting by spatially regularized regional proposal network, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 4165-4173, (2017); Tang T., Zhou S., Deng Z., Zou H., Lei L., Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining, Sensors, 17, 2, (2017); Shen J., Liu N., Sun H., Tao X., Li Q., Vehicle detection in aerial images based on hyper feature map in deep convolutional network, Ieee Access, 13, 4, pp. 1989-2011, (2019); Tang T., Zhou S., Deng Z., Lei L., Zou H., Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks, Remote Sens., 9, 11, (2017); Guo Y., Xu Y., Li S., Dense construction vehicle detection based on orientation-aware feature fusion convolutional neural network, Autom. Construct., 112, (2020); Li Q., Mou L., Xu Q., Zhang Y., Zhu X.X., R3-Net: A deep network for multioriented vehicle detection in aerial images and videos, Ieee Trans. Geosci. Remote Sens., 57, 7, pp. 5028-5042, (2019); Gu Y., Wang B., Xu B., A FPN-based framework for vehicle detection in aerial images, Proc. 2nd Int. Conf. Video Image Process., pp. 60-64, (2018); Vaddi S., Kumar C., Jannesari A., Efficient Object Detection Model for Real-time Uav Applications, (2019); Tayara H., Chong K., Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network, Sensors, 18, 10, (2018); Maiti S., Gidde P., Saurav S., Singh Dhiraj S., Chaudhury S., Real-time vehicle detection in aerial images using skip-connected convolution network with region proposal networks, Pattern Recognition and Machine Intelligence (Lecture Notes in Computer Science), (2019); Yang J., Xie X., Yang W., Effective contexts for UAV vehicle detection, Ieee Access, 7, pp. 85042-85054, (2019); Xie X., Et al., Real-time vehicle detection from UAV imagery, Proc. Ieee 4th Int. Conf. Multimedia Big Data (BigMM), pp. 1-5, (2018); Ghodrati A., Diba A., Pedersoli M., Tuytelaars T., Van Gool L., DeepProposals: Hunting objects and actions by cascading deep convolutional layers, Int. J. Comput. Vis., 124, 2, pp. 115-131, (2017); Wang L., Liao J., Xu C., Vehicle detection based on drone images with the improved faster R-CNN, Proc. 11th Int. Conf. Mach. Learn. Comput. (ICMLC), pp. 466-471, (2019); Sommer L.W., Schuchert T., Beyerer J., Deep learning based multi-category object detection in aerial images, Proc. Spie, (2017); Herrmann C., Willersinn D., Beyerer J., Low-resolution convolutional neural networks for video face recognition, Proc. 13th Ieee Int. Conf. Adv. Video Signal Based Surveill. (AVSS), pp. 221-227, (2016); Sommer L., Schumann A., Schuchert T., Beyerer J., Multi feature deconvolutional faster R-CNN for precise vehicle detection in aerial imagery, Proc. Ieee Winter Conf. Appl. Comput. Vis. (WACV), pp. 635-642, (2018); Acatay O., Sommer L., Schumann A., Beyerer J., Comprehensive evaluation of deep learning based detection methods for vehicle detection in aerial imagery, Proc. 15th Ieee Int. Conf. Adv. Video Signal Based Surveill. (AVSS), pp. 1-6, (2018); Tayara H., Gil Soo K., Chong K.T., Vehicle detection and counting in high-resolution aerial images using convolutional regression neural network, Ieee Access, 6, pp. 2220-2230, (2018); Tang T., Zhou S., Deng Z., Lei L., Zou H., Fast multidirectional vehicle detection on aerial images using region based convolutional neural networks, Proc. Ieee Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 1844-1847, (2017); Deng Z., Sun H., Zhou S., Zhao J., Zou H., Toward fast and accurate vehicle detection in aerial images using coupled regionbased convolutional neural networks, Ieee J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, 8, pp. 3652-3664, (2017); Zhang S., Wen L., Bian X., Lei Z., Li S.Z., Single-shot refinement neural network for object detection, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4203-4212, (2018); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2117-2125, (2017); Zhou J., Vong C.-M., Liu Q., Wang Z., Scale adaptive image cropping for UAV object detection, Neurocomputing, 366, pp. 305-313, (2019); Wu Z., Suresh K., Narayanan P., Xu H., Kwon H., Wang Z., Delving into robust object detection from unmanned aerial vehicles: A deep nuisance disentanglement approach, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 1201-1210, (2019); Eris And U Cevik H., Implementation of target tracking methods on images taken from unmanned aerial vehicles, Proc. Ieee 17th World Symp. Appl. Mach. Intell. Inform. (SAMI), pp. 311-316, (2019); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 248-255, (2009); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal visual object classes (VOC) challenge, Int. J. Comput. Vis., 88, 2, pp. 303-338, (2010); Lin T.Y., Et al., Microsoft COCO: Common objects in context, Proc. Eur. Conf. Comput. Vis., pp. 740-755, (2014); Kamran F., Shahzad M., Shafait F., Automated military vehicle detection from low-altitude aerial images, Proc. Digit. Image Comput., Techn. Appl. (DICTA), pp. 1-8, (2018); Xu Z., Shi H., Li N., Xiang C., Zhou H., Vehicle detection under UAV based on optimal dense YOLO method, Proc. 5th Int. Conf. Syst. Informat. (ICSAI), pp. 407-411, (2018); Zhou Y., Rui T., Li Y., Zuo X., A UAV patrol system using panoramic stitching and object detection, Comput. Electr. Eng., 80, (2019); Sommer L., Nie K., Schumann A., Schuchert T., Beyerer J., Semantic labeling for improved vehicle detection in aerial imagery, Proc. 14th Ieee Int. Conf. Adv. Video Signal Based Surveill. (AVSS), pp. 1-6, (2017); Kouris A., Kyrkou C., Bouganis C.-S., Informed region selection for efficient UAV-based object detectors: Altitude-aware vehicle detection with CyCAR dataset, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 51-58, (2019); Shen J., Liu N., Sun H., Zhou H., Vehicle detection in aerial images based on lightweight deep convolutional network and generative adversarial network, Ieee Access, 7, pp. 148119-148130, (2019); Kompella A., Kulkarni R.V., A semi-supervised recurrent neural network for video salient object detection, Neural Comput. Appl., 33, pp. 1-19, (2020); Feng X., Jonathan Wu Q.M., Yang Y., Cao L., An autuencoderbased data augmentation strategy for generalization improvement of DCNNs, Neurocomputing, 402, pp. 283-297, (2020); Shorten C., Khoshgoftaar T.M., A survey on image data augmentation for deep learning, J. Big Data, 6, 1, (2019); Chen Y., Li J., Niu Y., He J., Small object detection networks based on classification-oriented super-resolution GAN for UAV aerial imagery, Proc. Chin. Control Decis. Conf. (CCDC), pp. 4610-4615, (2019); Zheng K., Wei M., Sun G., Anas B., Li Y., Using vehicle synthesis generative adversarial networks to improve vehicle detection in remote sensing images, Isprs Int. J. Geo-Inf., 8, 9, (2019); Lakhal M.I., Escalera S., Cevikalp H., CRN: End-to-end convolutional recurrent network structure applied to vehicle classification, Proc. 13th Int. Joint Conf. Comput. Vis., Imag. Comput. Graph. Theory Appl., pp. 137-144, (2018); Ning G., Et al., Spatially supervised recurrent convolutional neural networks for visual object tracking, Proc. Ieee Int. Symp. Circuits Syst. (ISCAS), pp. 1-4, (2017); Zhang S., Wu G., Costeira J.P., Moura J.M.F., FCN-rLSTM: Deep spatio-temporal neural networks for vehicle counting in city cameras, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), pp. 3687-3696, (2017); Ringwald T., Sommer L., Schumann A., Beyerer J., Stiefelhagen R., UAV-Net: A fast aerial vehicle detector for mobile platforms, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 544-552, (2019); Gschwend D., ZynqNet: An FPGA-accelerated Embedded Convolutional Neural Network, (2020); Kyrkou C., Plastiras G., Theocharides T., Venieris S.I., Bouganis C.-S., DroNet: Efficient convolutional neural network detector for real-time UAV applications, Proc. Design, Automat. Test Eur. Conf. Exhib. (DATE), pp. 967-972, (2018); Azimi S.M., ShuffleDet: Real-time vehicle detection network in onboard embedded UAV imagery, Proc. Eur. Conf. Comput. Vis., pp. 88-99, (2018); Razakarivony S., Jurie F., Vehicle detection in aerial imagery: A small target detection benchmark, J. Vis. Commun. Image Represent., 34, pp. 187-203, (2016); Utah Mapping Portal, (2020); Xia G.-S., Et al., DOTA: A large-scale dataset for object detection in aerial images, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 3974-3983, (2018); Mundhenk T.N., Konjevod G., Sakla W.A., Boakye K., A large contextual dataset for classification, detection and counting of cars with deep learning, Proc. Eur. Conf. Comput. Vis., pp. 785-800, (2016); Liu X., Liu W., Ma H., Fu H., Large-scale vehicle re-identification in urban surveillance videos, Proc. Ieee Int. Conf. Multimedia Expo (ICME), pp. 1-6, (2016); Cheng G., Han J., Zhou P., Guo L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors, Isprs J. Photogramm. Remote Sens., 98, pp. 119-132, (2014); Tanner F., Et al., Overhead imagery research data set-An annotated data library & tools to aid in the development of computer vision algorithms, Proc. Appl. Imag. Pattern Recognit. Workshop, pp. 1-8, (2009); Chen W., Baojun Z., Linbo T., Boya Z., Small vehicles detection based on UAV, J. Eng., 2019, 21, pp. 7894-7897, (2019); Sakla W., Konjevod G., Mundhenk T.N., Deep multi-modal vehicle detection in aerial ISR imagery, Proc. Ieee Winter Conf. Appl. Comput. Vis. (WACV), pp. 916-923, (2017); Li W., Li H., Wu Q., Chen X., Ngan K.N., Simultaneously detecting and counting dense vehicles from drone images, Ieee Trans. Ind. Electron., 66, 12, pp. 9651-9662, (2019); Wang J., Simeonova S., Shahbazi M., Orientation-and scaleinvariant multi-vehicle detection and tracking from unmanned aerial videos, Remote Sens., 11, 18, (2019); Cai Y., Et al., Guided Attention Network for Object Detection and Counting on Drones, (2019); Sommer L., Schmidt N., Schumann A., Beyerer J., Search area reduction fast-RCNN for fast vehicle detection in large aerial imagery, Proc. 25th Ieee Int. Conf. Image Process. (ICIP), pp. 3054-3058, (2018); Zhang M., Li H., Xia G., Zhao W., Ren S., Wang C., Research on the application of deep learning target detection of engineering vehicles in the patrol and inspection for military optical cable lines by UAV, Proc. 11th Int. Symp. Comput. Intell. Design (ISCID), pp. 97-101, (2018); Gao Z., Ji H., Mei T., Ramesh B., Liu X., EOVNet: Earthobservation image-based vehicle detection network, Ieee J. Sel. Topics Appl. Earth Observ. Remote Sens., 12, 9, pp. 3552-3561, (2019); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2818-2826, (2016); Szegedy C., Ioffe S., Vanhoucke V., Alemi A., Inceptionv4, Inception-ResNet and the Impact of Residual Connections on Learning, (2016); Huang G., Liu Z., Maaten Der L.Van, Weinberger K.Q., Densely connected convolutional networks, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 4700-4708, (2017); Xie S., Girshick R., Dollar P., Tu Z., He K., Aggregated residual transformations for deep neural networks, Proc. Cvpr, pp. 1492-1500, (2017); Hu J., Shen L., Sun G., Squeeze-and-excitation networks, Proc. Cvpr, pp. 7132-7141, (2018)","H. Zarzour; University of Souk Ahras, LIM Research, Department of Mathematics and Computer Science, Souk Ahras, 41000, Algeria; email: hafed.zarzour@gmail.com","","Institute of Electrical and Electronics Engineers Inc.","","","","","","2162237X","","","34029200","English","IEEE Trans. Neural Networks Learn. Sys.","Article","Final","","Scopus","2-s2.0-85107176671"
"Zhang L.; Su G.; Yin J.; Li Y.; Lin Q.; Zhang X.; Shao L.","Zhang, Luming (35231925400); Su, Ge (57650408700); Yin, Jianwei (8249720800); Li, Ying (55945677900); Lin, Qiuru (57220176427); Zhang, Xiaoqin (35232030000); Shao, Ling (55643855000)","35231925400; 57650408700; 8249720800; 55945677900; 57220176427; 35232030000; 55643855000","Bioinspired Scene Classification by Deep Active Learning with Remote Sensing Applications","2022","IEEE Transactions on Cybernetics","52","7","","5682","5694","12","2","10.1109/TCYB.2020.2981480","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101805922&doi=10.1109%2fTCYB.2020.2981480&partnerID=40&md5=4e3bb3d71607c3396da304e1935fc669","Wenzhou University, College of Computer Science and Artificial Intelligence, Wenzhou, China; Jinhua Polytechnic, Key Laboratory of Crop Harvesting Equipment Technology of Zhejiang Province, Jinhua, 321016, China; Zhejiang University, College of Computer Sciences, Hangzhou, 310027, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","Zhang L., Wenzhou University, College of Computer Science and Artificial Intelligence, Wenzhou, China, Jinhua Polytechnic, Key Laboratory of Crop Harvesting Equipment Technology of Zhejiang Province, Jinhua, 321016, China; Su G., Zhejiang University, College of Computer Sciences, Hangzhou, 310027, China; Yin J., Zhejiang University, College of Computer Sciences, Hangzhou, 310027, China; Li Y., Zhejiang University, College of Computer Sciences, Hangzhou, 310027, China; Lin Q., Zhejiang University, College of Computer Sciences, Hangzhou, 310027, China; Zhang X., Wenzhou University, College of Computer Science and Artificial Intelligence, Wenzhou, China; Shao L., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","Accurately classifying sceneries with different spatial configurations is an indispensable technique in computer vision and intelligent systems, for example, scene parsing, robot motion planning, and autonomous driving. Remarkable performance has been achieved by the deep recognition models in the past decade. As far as we know, however, these deep architectures are incapable of explicitly encoding the human visual perception, that is, the sequence of gaze movements and the subsequent cognitive processes. In this article, a biologically inspired deep model is proposed for scene classification, where the human gaze behaviors are robustly discovered and represented by a unified deep active learning (UDAL) framework. More specifically, to characterize objects' components with varied sizes, an objectness measure is employed to decompose each scenery into a set of semantically aware object patches. To represent each region at a low level, a local-global feature fusion scheme is developed which optimally integrates multimodal features by automatically calculating each feature's weight. To mimic the human visual perception of various sceneries, we develop the UDAL that hierarchically represents the human gaze behavior by recognizing semantically important regions within the scenery. Importantly, UDAL combines the semantically salient region detection and the deep gaze shifting path (GSP) representation learning into a principled framework, where only the partial semantic tags are required. Meanwhile, by incorporating the sparsity penalty, the contaminated/redundant low-level regional features can be intelligently avoided. Finally, the learned deep GSP features from the entire scene images are integrated to form an image kernel machine, which is subsequently fed into a kernel SVM to classify different sceneries. Experimental evaluations on six well-known scenery sets (including remote sensing images) have shown the competitiveness of our approach. © 2013 IEEE.","Active learning; bioinspired; contaminated; gaze behavior; machine learning; multimodal; remote sensing","Humans; Neural Networks, Computer; Remote Sensing Technology; Semantics; Visual Perception; Behavioral research; Biomimetics; Computer vision; Formal languages; Intelligent systems; Remote sensing; Robot programming; Semantics; Support vector machines; Vision; Biologically inspired; Experimental evaluation; Human visual perception; Remote sensing applications; Remote sensing images; Robot motion planning; Salient region detections; Spatial configuration; human; remote sensing; semantics; vision; Deep learning","","","","","","","Harel J., Koch C., Perona P., Graph-based visual saliency, Proc. Neural Inf. Process. Syst., pp. 545-553, (2006); Achanta R., Shaji A., Smith K., Lucchi A., Fua P., Susstrunk S., SLIC superpixels compared to state-of-the-art superpixel methods, IEEE Trans. Pattern Anal. Mach. Intell., 34, 11, pp. 2274-2282, (2012); Avidan S., Shamir A., Seam carving for content-aware image resizing, ACM Trans. Graph., 26, 3, (2007); Zhang L., Wang M., Nie L., Hong R., Xia Y., Zimmermann R., Biologically inspired media quality modeling, Proc. ACM Multimedia, pp. 491-500, (2015); Zhang L., Song M., Zhao Q., Liu X., Bu J., Chen C., Probabilistic graphlet transfer for photo cropping, IEEE Trans. Image Process., 22, 2, pp. 802-815, (2013); Wang Y.-S., Tai C.-L., Sorkine O., Lee T.-Y., Optimized scale-and-stretch for image resizing, ACM Trans. Graph., 27, 5, (2008); Zhang L., Han Y., Yang Y., Song M., Yan S., Tian Q., Discovering discriminative graphlets for aerial image categories recognition, IEEE Trans. Image Process., 22, 12, pp. 5071-5084, (2013); Lu X., Li X., Mou L., Semi-supervised multi-task learning for scene recognition, IEEE Trans. Cybern., 45, 9, pp. 1967-1976, (2015); Li X., Mou L., Lu X., Scene parsing from an MAP perspective, IEEE Trans. Cybern., 45, 9, pp. 1876-1886, (2015); Yuan Y., Mou L., Lu X., Scene recognition by manifold regularized deep learning architecture, IEEE Trans. Cybern., 26, 10, pp. 2222-2233, (2015); Cai D., He X., Zhou K., Han J., Bao H., Locality sensitive discriminative analysis, Proc. Int. Joint Conf. Artif. Intell. (IJCAI), (2005); Xiao J., Hays J., Ehinger K.A., Oliva A., Torralba A., SUN database: Large-scale scene recognition from abbey to zoo, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 3485-3492, (2010); Park D.K., Jeon Y.S., Won C.S., Efficient use of local edge histogram descriptor, Proc. ACM Multimedia Workshop, pp. 51-54, (2000); Zhou B., Lapedriza A., Xiao J., Torralba A., Oliva A., Learning deep features for scene recognition using places database, Proc. Neural Inf. Process. Syst., pp. 487-495, (2014); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Proc. Neural Inf. Process. Syst., pp. 1106-1114, (2012); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Pizzuti C., Socievole A., Multiobjective optimization and local merge for clustering attributed graphs, IEEE Trans. Cybern.; Wu E.Q., Zhou G.-R., Zhu L.-M., Wei C.-F., Ren H., Sheng R.S.F., Rotated sphere haar wavelet and deep contractive auto-encoder network with fuzzy Gaussian SVM for pilot’s pupil center detection, IEEE Trans. Cybern.; Kebria P.M., Khosravi A., Nahavandi S., Shi P., Alizadehsani R., Robust adaptive control scheme for teleoperation systems with delay and uncertainties, IEEE Trans. Cybern.; Rizvi S.A.A., Lin Z., Reinforcement learning-based linear quadratic regulation of continuous-time systems using dynamic output feedback, IEEE Trans. Cybern.; Tembine H., Deep learning meets game theory: Bregman-based algorithms for interactive deep generative adversarial networks, IEEE Trans. Cybern., 50, 3, pp. 1132-1145; Hernandez V.A.S., Schutze O., Wang H., Deutz A., Emmerich M., The set-based hypervolume Newton method for bi-objective optimization, IEEE Trans. Cybern.; Duda P., Rutkowski L., Jaworski M., Rutkowska D., On the Parzen kernel-based probability density function learning procedures over time-varying streaming data with applications to pattern classification, IEEE Trans. Cybern., 50, 4, pp. 1683-1696, (2020); Kim U.-H., Kim J.-H., A stabilized feedback episodic memory (SF-EM) and home service provision framework for robot and IoT collaboration, IEEE Trans. Cybern.; Duenas V.H., Cousin C.A., Rouse C., Fox E.J., Dixon W.E., Distributed repetitive learning control for cooperative cadence tracking in functional electrical stimulation cycling, IEEE Trans. Cybern., 50, 3, pp. 1084-1095; Nguyen B., Morell C., Baets B.D., Scalable large-margin distance metric learning using stochastic gradient descent, IEEE Trans. Cybern., 50, 3, pp. 1072-1083; Lazebnik S., Schmid C., Ponce J., Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 2169-2178, (2006); Kendall M.G., Smith B.B., On the method of paired comparisons, Biometrica, 31, 3-4, pp. 324-345, (1940); Quattoni A., Torralba A., Recognizing indoor scenes, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 413-420, (2009); von Luxburg U., A tutorial on spectral clustering, Max Planck Inst. Biol. Cybern., (2006); Bickel S., Scheffer T., Multi-view clustering, Proc. Ind. Conf. Data Min. (ICDM), pp. 1-8, (2014); Li F.-F., Perona P., A Bayesian hierarchical model for learning natural scene categories, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 524-531, (2005); Zhang L., Wang M., Nie L., Hong L., Rui Y., Tian Q., Retargeting semantically-rich photos, IEEE Trans. Multimedia, 17, 9, pp. 1538-1549, (2015); Wang M., Hua X.-S., Yuan X., Song Y., Dai L.-R., Optimizing multi-graph learning: Towards a unified video annotation scheme, Proc. ACM Multimedia, pp. 862-871, (2007); Guo Y., Liu F., Shi J., Zhou Z.-H., Gleicher M., Image retargeting using mesh parameterization, IEEE Trans. Multimedia, 11, 5, pp. 856-867, (2009); Harchaoui Z., Bach F.R., Image classification with segmentation graph kernels, Proc. Comput. Vis. Pattern Recognit. (CVPR), (2007); Wang J., Yang J., Yu K., Lv F., Huang T., Gong Y., Locality-constrained linear coding for image classification, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 3360-3367, (2010); Yang J., Yu K., Gong Y., Huang T., Linear spatial pyramid matching using sparse coding for image classification, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 1794-1801, (2009); Li L.-J., Su H., Xing E.P., Fei-Fei L., Object bank: A high-level image representation for scene classification and semantic feature sparsification, Proc. Neural Inf. Process. Syst. (NIPS), pp. 1378-1386, (2010); Zhou X., Yu K., Zhang T., Huang T.S., Image classification using super-vector coding of local image descriptors, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 141-154, (2010); Yang J., Yu K., Huang T.S., Supervised translation-invariant sparse coding, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 3517-3524, (2010); Wu Y., Chang E., Chang K., Smith J., Optimal multimodal fusion for multimedia data analysis, Proc. ACM Multimedia, pp. 572-579, (2004); Adams W.H., Et al., Semantic indexing of multimedia content using visual, audio, and text cues, EURASIP J. Adv. Signal Process., 2003, 2, pp. 170-185, (2003); Hall D.L., Llinas J., An introduction to multisensor data fusion, Proc. IEEE, 85, 1, pp. 6-23, (1997); Snoek C.G.M., Worring M., Smeulders A.W.M., Early versus late fusion in semantic video analysis, Proc. ACM Multimedia, pp. 399-402, (2005); Hadjidemetriou E., Grossberg M.D., Nayar S.K., Multiresolution histograms and their use for recognition, IEEE Trans. Pattern Anal. Mach. Intell., 26, 7, pp. 831-847, (2004); Li Y., Liu L., Shen C., Mid-level deep pattern mining, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 971-980, (2015); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 37, 9, pp. 1904-1916, (2015); Arbelaez P., Pont-Tuset J., Barron J.T., Marques F., Malik J., Multiscale combinatorial grouping, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 328-335, (2014); Mesnil G., Rifai S., Bordes A., Glorot X., Bengio Y., Vincent P., Unsupervised learning of semantics of object detections for scene categorizations, Proc. PRAM, (2015); Xiao Y., Wu J., Yuan J., McEntrist: A multi-channel feature generation mechanism for scene categorization, IEEE Trans. Image Process., 23, 2, pp. 823-836, (2014); Cong Y., Liu J., Yuan J., Luo J., Self-supervised online metric learning with low rank constraint for scene categorization, IEEE Trans. Image Process., 22, 8, pp. 3179-3191, (2013); Rubinstein M., Gutierrez D., Sorkine O., Shamir A., Comparative study of image retargeting, ACM Trans. Graph., 29, 5, (2010); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 248-255, (2009); Uijlings J.R.R., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 580-587, (2014); Wu R., Wang B., Wang W., Yu Y., Harvesting discriminative meta objects with deep CNN features for scene classification, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 1287-1295, (2015); Torralba A., Fergus R., Freeman W.T., 80 million tiny images: A large dataset for non-parametric object and scene recognition, IEEE Trans. Pattern Anal. Mach. Intell., 30, 11, pp. 1958-1970, (2008); Zhang L., Tong M.H., Marks T.K., Shan H., Cottrell G.W., SUN: A Bayesian framework for saliency using natural statistics, J. Vis., 8, 7, (2008); Luo W., Wang X., Tang X., Content-based photo quality assessment, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 2206-2213, (2011); Nishiyama M., Okabe T., Sato Y., Sato I., Sensation-based photo cropping, Proc. ACM Multimedia, pp. 669-672, (2009); Marchesotti L., Perronnin F., Larlus D., Csurka G., Assessing the aesthetic quality of photographs using generic image descriptors, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 1784-1791, (2011); Yao X., Han J., Cheng G., Qian X., Guo L., Semantic annotation of high-resolution satellite images via weakly supervised learning, IEEE Trans. Geosci. Remote Sens., 54, 6, pp. 3660-3671, (2016); Lu X., Lin Z., Jin H., Yang J., Wang J.Z., Rapid: Rating pictorial aesthetics using deep learning, Proc. ACM Int. Conf. Multimedia (MM), pp. 457-466, (2014); Lu X., Lin Z., Shen X., Mech R., Wang J.Z., Deep multipatch aggregation network for image style, aesthetics, and quality estimation, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 990-998, (2015); Wolfe J.M., Horowitz T.S., What attributes guide the deployment of visual attention and how do they do it?, Nat. Rev. Neurosci., 5, pp. 1-7, (2004); Bruce N.D.B., Tsotsos J.K., Saliency, attention, and visual search: An information theoretic approach, J. Vis., 9, 3, (2009); Dhar S., Ordonez V., Berg T.L., High level describable attributes for predicting aesthetics and interestingness, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 1657-1664, (2011); Mai L., Jin H., Liu F., Composition-preserving deep photo aesthetics assessment, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 497-506, (2016); Cheng M.-M., Zhang Z., Lin W.-Y., Torr P., Bing: Binarized normed gradients for objectness estimation at 300fps, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 3286-3293, (2014); Marchesotti L., Murray N., Perronnin F., Discovering beautiful attributes for aesthetic image analysis, Int. J. Comput. Vis., 113, 3, pp. 246-266, (2015); Cheng B., Ni B., Yan S., Tian Q., Learning to photograph, Proc. ACM Int. Conf. Multimedia (MM), pp. 291-300, (2010); Stricker M.A., Orengo M., Similarity of color images, Proc. Storage Retrieval Image Video Databases, pp. 381-392, (1995); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proc. Comput. Vis. Pattern Recognit. (CVPR), pp. 886-893, (2005)","X. Zhang; Wenzhou University, College of Computer Science and Artificial Intelligence, Wenzhou, China; email: xqzhang@wzu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21682267","","","33635802","English","IEEE Trans. Cybern.","Article","Final","","Scopus","2-s2.0-85101805922"
"Lu D.; Li D.; Zhu X.; Nie S.; Zhou G.; Zhang X.; Yang C.","Lu, Dajin (57386942900); Li, Dong (57226876832); Zhu, Xiaoxiao (57214687733); Nie, Sheng (56378784900); Zhou, Guoqing (55554955100); Zhang, Xingyi (57412271300); Yang, Chao (57412000200)","57386942900; 57226876832; 57214687733; 56378784900; 55554955100; 57412271300; 57412000200","Denoising and Classification of ICESat-2 Photon Point Cloud based on Convolutional Neural Network; [基于卷积神经网络的ICESat-2光子点云去噪分类]","2021","Journal of Geo-Information Science","23","11","","2086","2095","9","1","10.12082/dqxxkx.2021.210103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122820499&doi=10.12082%2fdqxxkx.2021.210103&partnerID=40&md5=5dddf32719eb8dc1b20dc7346f40c32a","Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, 650093, China; Key Lab of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; College of Geomatics and Geoinformation, Guilin University of Technology, Guilin, 541006, China","Lu D., Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, 650093, China; Li D., Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, 650093, China, Key Lab of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Zhu X., Key Lab of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Nie S., Key Lab of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Zhou G., College of Geomatics and Geoinformation, Guilin University of Technology, Guilin, 541006, China; Zhang X., Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, 650093, China; Yang C., Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, 650093, China","ICESat-2 (Ice, Cloud, and land Elevation Satellite-2) launched by NASA (National Aeronautics and Space Administration) in 2018 is a laser altitude measurement satellite. The advanced topographic laser altimeter system (ATLAS) instrument on-board ICESat-2 employs a micro-pulse and multi-beam photon counting laser altimeter system with low energy consumption, high detection sensitivity, and high repetition rates, and thus greatly improves the sampling density in the along-track distance. However, it introduces a significant number of solar noise photons in the raw data. How to effectively remove the noise photons and classify the signal photons into ground photons and canopy photons is critical for subsequent applications such as the estimation of terrain elevation and forest height, and it has been a hot and challenging topic in the current research. In this paper, a denoising and classification algorithm based on convolutional neural network was proposed. The convolutional neural network has made a series of breakthrough research results in the fields of image classification, object detection, semantic segmentation, and so on. To remove obvious noise photons, the photons were first divided into grids in the along-track distance and elevation direction, and the rough signal photons were gridded into pictures. Then, the convolutional neural network was employed to perform the final denoising and classification. Finally, the proposed algorithm was tested with the airborne LiDAR datasets, including DSM (Digital Surface Model) and DTM (Digital Terrain Model), and was further compared with ATL08 (land and vegetation height) products. Experimental results show that our proposed algorithm can remove noise photons effectively in bare land and forest areas. Moreover, this algorithm can simultaneously remove noise photons and classify signal photons into ground photons and canopy photons in forest areas. The R2 and RMSE values of the retrieved ground surface in the bare land areas were 1.0 and 0.72 m, respectively. In the forest areas, the R 2 of the estimated ground surface and canopy surface were 1.0 and 0.70 with the RMSE values of 1.11 m and 4.99 m, respectively. The reason for this result may be that it is difficult for photons to penetrate the forest canopy and reach the ground surface in forest areas, which causes the RMSE value of the forest area to be larger than that of the bare land area. In this paper, the deep learning algorithm was used to realize the denoising and classification of photon counting data, and good results were achieved in bare land and forest areas, which provides a reference for subsequent photon counting LiDAR data processing. 2021, Science Press. All right reserved.","Convolutional neural network; Deep learning; Denoising and classification; ICESat-2; Photon point cloud; Photon-counting; Rasterized; Supervised learning","Aneroid altimeters; Convolution; Convolutional neural networks; Deep neural networks; Energy utilization; Forestry; Image segmentation; NASA; Object detection; Pulse repetition rate; Radio altimeters; Semantics; Convolutional neural network; De-noising; Deep learning; Denoising and classification; Ice clouds; Ice, cloud, and land elevation satellite-2; Land elevation satellites; Photon counting; Photon point cloud; Point-clouds; Rasterized; algorithm; artificial neural network; ICESat; image classification; raster; segmentation; supervised learning; Optical radar","","","","","Guangxi Natural Science Fund for Innovation Research Team, (2019GXNSFGA245001); National Natural Science Foundation of China, NSFC, (42071405); Youth Innovation Promotion Association of the Chinese Academy of Sciences, YIPA CAS, (2019130)","Foundation items: Guangxi Natural Science Fund for Innovation Research Team, No.2019GXNSFGA245001; National Natural Science Foundation of China, No.42071405; The Youth Innovation Promotion Association Chinese Academy of Sciences, No.2019130.","Fang Y, Cao BC, Gao L, Et al., Development and application of lidar mapping satellite, Infrared and Laser Engineering, 49, 11, pp. 19-27, (2020); Pang Y, Li ZY, Chen BW, Et al., Status and development of spaceborne lidar applications in forestry, Aerospace Shanghai, 36, 3, pp. 20-28, (2019); Yue CY, Zheng YC, Xing YQ, Et al., Technical and application development study of space-borne LiDAR in forestry remote sensing, Infrared and Laser Engineering, 49, 11, pp. 97-106, (2020); Zhu XX, Wang C, Xi XH, Et al., Research progress of ICESat-2/ATLAS data processing and applications, Infrared and Laser Engineering, 49, 11, pp. 68-77, (2020); Xing Y Q, Huang J P, Gruen A, Et al., Assessing the performance of ICESat-2/ATLAS multi-channel photon data for estimating ground topography in forested terrain, Remote Sensing, 12, 13, (2020); Yao JQ, Tang XM, Li GY, Et al., Cloud detection of laser altimetry satellite ICESat-2 and the related algorithm, Laser & Optoelectronics Progress, 57, 13, (2020); Narine L L, Popescu S C, Malambo L., Using ICESat-2 to estimate and map forest aboveground biomass: A first example, Remote Sensing, 12, 11, (2020); Neuenschwander A, Pitts K., The ATL08 land and vegetation product for the ICESat-2 Mission, Remote Sensing of Environment, 221, pp. 247-259, (2019); Markus T, Neumann T, Martino A, Et al., The Ice, Cloud, and land Elevation Satellite-2 (ICESat-2): Science requirements, concept, and implementation, Remote Sensing of Environment, 190, pp. 260-273, (2017); Yang F, Wen JH., Icesat and icesat-2 applications: Progress and prospect, Chinese Journal of Polar Research, 23, 2, pp. 138-148, (2011); Parrish C E, Magruder L A, Neuenschwander A L, Et al., Validation of ICESat-2 ATLAS bathymetry and analysis of ATLAS's bathymetric mapping performance, Remote Sensing, 11, 14, (2019); Xia SB, Wang C, Xi XH, Et al., Point cloud filtering and tree height estimation using airborne experiment data of ICESat-2, Journal of Remote Sensing, 18, 6, pp. 1199-1207, (2014); Cao BC, Fang Y, Jiang ZZ, Et al., Implementation and accuracy evaluation of ICESat-2 ATL08 denoising algorithms, Bulletin of Surveying and Mapping, 5, pp. 25-30, (2020); Zhu X X, Nie S, Wang C, Et al., A ground elevation and vegetation height retrieval algorithm using micro-pulse photon-counting lidar data, Remote Sensing, 10, 12, (2018); Chen BW, Pang Y, Li ZY, Et al., Photon-counting LiDAR point cloud data filtering based on the random forest algorithm, Journal of Geo-information Science, 21, 6, pp. 898-906, (2019); Awadallah M, Ghannam S, Abbott AL, Et al., Active contour models for extracting ground and forest canopy curves from discrete laser altimeter data, Proceedings: 13th International Conference on LiDAR Applications for Assessing Forest Ecosystems, pp. 129-136, (2013); Nie S, Wang C, Xi X H, Et al., Estimating the vegetation canopy height using micro-pulse photon-counting LiDAR data, Optics Express, 26, 10, pp. A520-A540, (2018); Wang X A, Pan Z G, Glennie C., A novel noise filtering model for photon-counting laser altimeter data, IEEE Geoscience and Remote Sensing Letters, 13, 7, pp. 947-951, (2016); Zhang J S, Kerekes J., An adaptive density-based model for extracting surface returns from photon-counting laser altimeter data, IEEE Geoscience and Remote Sensing Letters, 12, 4, pp. 726-730, (2015); Zhu X, Nie S, Wang C, Et al., A noise removal algorithm based on OPTICS for photon-counting LiDAR data, IEEE Geoscience and Remote Sensing Letters, 2, pp. 1-5, (2020); Chen B W, Pang Y, Li Z Y, Et al., Ground and top of canopy extraction from photon-counting LiDAR data using local outlier factor with ellipse searching area, IEEE Geoscience and Remote Sensing Letters, 16, 9, pp. 1447-1451, (2019); Neuenschwander A, Guenther E, White J C, Et al., Validation of ICESat-2 terrain and canopy heights in boreal forests, Remote Sensing of Environment, 251, (2020); Chatterjee A., Soil carbon pools of six ecological regions of the United States, Journal of Forestry Research, 31, 5, pp. 1933-1938, (2020); Popescu S C, Zhou T, Nelson R, Et al., Photon counting LiDAR: An adaptive ground and canopy height retrieval algorithm for ICESat-2 data, Remote Sensing of Environment, 208, pp. 154-170, (2018); Li YD, Hao ZB, Lei H., Survey of convolutional neural network, Journal of Computer Applications, 36, 9, pp. 2508-2515, (2016); Zhou FY, Jin LP, Dong J., Review of convolutional neural network, Chinese Journal of Computers, 40, 6, pp. 1229-1251, (2017)","D. Li; Faculty of Land Resource Engineering, Kunming University of Science and Technology, Kunming, 650093, China; email: lidong@aircas.ac.cn; D. Li; Key Lab of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; email: lidong@aircas.ac.cn","","Science Press","","","","","","15608999","","","","Chinese","J. Geo-Inf. Sci.","Article","Final","","Scopus","2-s2.0-85122820499"
"Gallo I.; Rehman A.U.; Dehkordi R.H.; Landro N.; La Grassa R.; Boschetti M.","Gallo, Ignazio (7003336792); Rehman, Anwar Ur (58071783100); Dehkordi, Ramin Heidarian (57218627173); Landro, Nicola (57214364871); La Grassa, Riccardo (57204648786); Boschetti, Mirco (6701354038)","7003336792; 58071783100; 57218627173; 57214364871; 57204648786; 6701354038","Deep Object Detection of Crop Weeds: Performance of YOLOv7 on a Real Case Dataset from UAV Images","2023","Remote Sensing","15","2","539","","","","8","10.3390/rs15020539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146552598&doi=10.3390%2frs15020539&partnerID=40&md5=698f007824969bbc0ec9fed32dbc0d55","Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; Institute for Electromagnetic Sensing of the Environment, National Research Council, Milan, 20133, Italy; The Italian National Institute for Astrophysics, Rome, 00100, Italy","Gallo I., Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; Rehman A.U., Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; Dehkordi R.H., Institute for Electromagnetic Sensing of the Environment, National Research Council, Milan, 20133, Italy; Landro N., Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; La Grassa R., The Italian National Institute for Astrophysics, Rome, 00100, Italy; Boschetti M., Institute for Electromagnetic Sensing of the Environment, National Research Council, Milan, 20133, Italy","Weeds are a crucial threat to agriculture, and in order to preserve crop productivity, spreading agrochemicals is a common practice with a potential negative impact on the environment. Methods that can support intelligent application are needed. Therefore, identification and mapping is a critical step in performing site-specific weed management. Unmanned aerial vehicle (UAV) data streams are considered the best for weed detection due to the high resolution and flexibility of data acquisition and the spatial explicit dimensions of imagery. However, with the existence of unstructured crop conditions and the high biological variation of weeds, it remains a difficult challenge to generate accurate weed recognition and detection models. Two critical barriers to tackling this challenge are related to (1) a lack of case-specific, large, and comprehensive weed UAV image datasets for the crop of interest, (2) defining the most appropriate computer vision (CV) weed detection models to assess the operationality of detection approaches in real case conditions. Deep Learning (DL) algorithms, appropriately trained to deal with the real case complexity of UAV data in agriculture, can provide valid alternative solutions with respect to standard CV approaches for an accurate weed recognition model. In this framework, this paper first introduces a new weed and crop dataset named Chicory Plant (CP) and then tests state-of-the-art DL algorithms for object detection. A total of 12,113 bounding box annotations were generated to identify weed targets (Mercurialis annua) from more than 3000 RGB images of chicory plantations, collected using a UAV system at various stages of crop and weed growth. Deep weed object detection was conducted by testing the most recent You Only Look Once version 7 (YOLOv7) on both the CP and publicly available datasets (Lincoln beet (LB)), for which a previous version of YOLO was used to map weeds and crops. The YOLOv7 results obtained for the CP dataset were encouraging, outperforming the other YOLO variants by producing value metrics of 56.6%, 62.1%, and 61.3% for the mAP@0.5 scores, recall, and precision, respectively. Furthermore, the YOLOv7 model applied to the LB dataset surpassed the existing published results by increasing the mAP@0.5 scores from 51% to 61%, 67.5% to 74.1%, and 34.6% to 48% for the total mAP, mAP for weeds, and mAP for sugar beets, respectively. This study illustrates the potential of the YOLOv7 model for weed detection but remarks on the fundamental needs of large-scale, annotated weed datasets to develop and evaluate models in real-case field circumstances. © 2023 by the authors.","Convolutional Neural Network; Deep Learning; object detection; UAV imagery; YOLOv7","Agricultural chemicals; Agricultural robots; Aircraft detection; Antennas; Convolution; Convolutional neural networks; Crops; Data acquisition; Deep learning; Drones; Large dataset; Object recognition; Plants (botany); Statistical tests; Aerial vehicle; Convolutional neural network; Deep learning; Objects detection; Real case; Unmanned aerial vehicle imagery; Vehicle images; Weed detection; Weed recognition; You only look once version 7; Object detection","","","","","CNR-DIPARTIMENTO DI INGEGNERIA; Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR, (ARS01_01136)","The work was supported by the project “E-crops—Technologies for Digital and Sustainable Agriculture”, funded by the Italian Ministry of University and Research (MUR) under the PON Agrifood Program (Contract ARS01_01136). Ramin Heidarian Dehkordi activity was founded by E-crops and by the CNR-DIPARTIMENTO DI INGEGNERIA, ICT E TECNOLOGIE PER L’ENERGIA E I TRASPORTI project “DIT.AD022.180 Transizione industriale e resilienza delle Società post-Covid19 (FOE 2020)”, sub task activity “Agro-Sensing”.","Young S.L., Beyond precision weed control: A model for true integration, Weed Technol, 32, pp. 7-10, (2018); Barnes E., Morgan G., Hake K., Devine J., Kurtz R., Ibendahl G., Sharda A., Rains G., Snider J., Maja J.M., Et al., Opportunities for robotic systems and automation in cotton production, AgriEngineering, 3, pp. 339-362, (2021); Pandey P., Dakshinamurthy H.N., Young S.N., Frontier: Autonomy in Detection, Actuation, and Planning for Robotic Weeding Systems, Trans. ASABE, 64, pp. 557-563, (2021); Bauer M.V., Marx C., Bauer F.V., Flury D.M., Ripken T., Streit B., Thermal weed control technologies for conservation agriculture—A review, Weed Res, 60, pp. 241-250, (2020); Kennedy H., Fennimore S.A., Slaughter D.C., Nguyen T.T., Vuong V.L., Raja R., Smith R.F., Crop signal markers facilitate crop detection and weed removal from lettuce and tomato by an intelligent cultivator, Weed Technol, 34, pp. 342-350, (2020); Van Der Weide R., Bleeker P., Achten V., Lotz L., Fogelberg F., Melander B., Innovation in mechanical weed control in crop rows, Weed Res, 48, pp. 215-224, (2008); Lamm R.D., Slaughter D.C., Giles D.K., Precision weed control system for cotton, Trans. ASAE, 45, (2002); Chostner B., See & Spray: The next generation of weed control, Resour. Mag, 24, pp. 4-5, (2017); Gerhards R., Andujar Sanchez D., Hamouz P., Peteinatos G.G., Christensen S., Fernandez-Quintanilla C., Advances in site-specific weed management in agriculture—A review, Weed Res, 62, pp. 123-133, (2022); Chen D., Lu Y., Li Z., Young S., Performance evaluation of deep transfer learning on multi-class identification of common weed species in cotton production systems, Comput. Electron. Agric, 198, (2022); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A multiclass weed species image dataset for deep learning, Sci. Rep, 9, pp. 1-12, (2019); Suh H.K., Ijsselmuiden J., Hofstee J.W., van Henten E.J., Transfer learning for the classification of sugar beet and volunteer potato under field conditions, Biosyst. Eng, 174, pp. 50-65, (2018); Espejo-Garcia B., Mylonas N., Athanasakos L., Fountas S., Vasilakoglou I., Towards weeds identification assistance through transfer learning, Comput. Electron. Agric, 171, (2020); Dyrmann M., Karstoft H., Midtiby H.S., Plant species classification using deep convolutional neural network, Biosyst. Eng, 151, pp. 72-80, (2016); Girshick R., Donahue J., Darrell T., Malik J., Region-based convolutional networks for accurate object detection and segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 38, pp. 142-158, (2015); Jiao L., Zhang F., Liu F., Yang S., Li L., Feng Z., Qu R., A survey of deep learning-based object detection, IEEE Access, 7, pp. 128837-128868, (2019); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Proceedings of the 28th International Conference on Neural Information Processing Systems; He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969; Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788; Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271; Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Bochkovskiy A., Wang C.Y., Liao H.Y.M., Yolov4: Optimal speed and accuracy of object detection, arXiv, (2020); Glenn J., What Is YOLOv5?, (2020); Li C., Li L., Jiang H., Weng K., Geng Y., Li L., Ke Z., Li Q., Cheng M., Nie W., Et al., Yolov6: A single-stage object detection framework for industrial applications, arXiv, (2022); Wang C.Y., Bochkovskiy A., Liao H.Y.M., YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, arXiv, (2022); Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters J.G., Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, pp. 1-12, (2020); Ahmad A., Saraswat D., Aggarwal V., Etienne A., Hancock B., Performance of deep learning models for classifying and detecting common weeds in corn and soybean production systems, Comput. Electron. Agric, 184, (2021); Sharpe S.M., Schumann A.W., Boyd N.S., Goosegrass detection in strawberry and tomato using a convolutional neural network, Sci. Rep, 10, pp. 1-8, (2020); Sun C., Shrivastava A., Singh S., Gupta A., Revisiting unreasonable effectiveness of data in deep learning era, Proceedings of the IEEE International Conference on Computer Vision, pp. 843-852; Lu Y., Young S., A survey of public datasets for computer vision tasks in precision agriculture, Comput. Electron. Agric, 178, (2020); Mylonas N., Malounas I., Mouseti S., Vali E., Espejo-Garcia B., Fountas S., Eden library: A long-term database for storing agricultural multi-sensor datasets from uav and proximal platforms, Smart Agric. Technol, 2, (2022); Wu Z., Chen Y., Zhao B., Kang X., Ding Y., Review of weed detection methods based on computer vision, Sensors, 21, (2021); Salazar-Gomez A., Darbyshire M., Gao J., Sklar E.I., Parsons S., Towards practical object detection for weed spraying in precision agriculture, arXiv, (2021); Gallo I., Rehman A.U., Dehkord R.H., Landro N., La Grassa R., Boschetti M., Weed Detection by UAV 416a Image Dataset—Chicory Crop Weed, (2022); Lottes P., Khanna R., Pfeifer J., Siegwart R., Stachniss C., UAV-based crop and weed classification for smart farming, Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 3024-3031; Gao J., Liao W., Nuyttens D., Lootens P., Vangeyte J., Pizurica A., He Y., Pieters J.G., Fusion of pixel and object-based features for weed mapping using unmanned aerial vehicle imagery, Int. J. Appl. Earth Obs. Geoinf, 67, pp. 43-53, (2018); Lottes P., Behley J., Milioto A., Stachniss C., Fully convolutional networks with sequential information for robust crop and weed detection in precision farming, IEEE Robot. Autom. Lett, 3, pp. 2870-2877, (2018); Le V.N.T., Apopei B., Alameh K., Effective plant discrimination based on the combination of local binary pattern operators and multiclass support vector machine methods, Inf. Process. Agric, 6, pp. 116-131, (2019); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Sa I., Chen Z., Popovic M., Khanna R., Liebisch F., Nieto J., Siegwart R., weednet: Dense semantic weed classification using multispectral images and mav for smart farming, IEEE Robot. Autom. Lett, 3, pp. 588-595, (2017); Jin X., Che J., Chen Y., Weed identification using deep learning and image processing in vegetable plantation, IEEE Access, 9, pp. 10940-10950, (2021); Milioto A., Lottes P., Stachniss C., Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in CNNs, Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 2229-2235; Lottes P., Stachniss C., Semi-supervised online visual crop and weed classification in precision farming exploiting plant arrangement, Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5155-5161; Etienne A., Ahmad A., Aggarwal V., Saraswat D., Deep Learning-Based Object Detection System for Identifying Weeds Using UAS Imagery, Remote Sens, 13, (2021); Peteinatos G.G., Reichel P., Karouta J., Andujar D., Gerhards R., Weed identification in maize, sunflower, and potatoes with the aid of convolutional neural networks, Remote Sens, 12, (2020); Bah M.D., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens, 10, (2018); Di Cicco M., Potena C., Grisetti G., Pretto A., Automatic model based dataset generation for fast and accurate crop and weeds detection, Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5188-5195; Espejo-Garcia B., Mylonas N., Athanasakos L., Vali E., Fountas S., Combining generative adversarial networks and agricultural transfer learning for weeds identification, Biosyst. Eng, 204, pp. 79-89, (2021); Dwyer J., Quickly Label Training Data and Export To Any Format, (2020); Chien W., YOLOv7 Repositry with all Instruction, (2022); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Proceedings of the European Conference on Computer Vision, pp. 21-37; Chen Q., Wang Y., Yang T., Zhang X., Cheng J., Sun J., You only look one-level feature, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13039-13048; Jensen P.K., Survey of Weeds in Maize Crops in Europe, (2011)","I. Gallo; Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; email: ignazio.gallo@uninsubria.it","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146552598"
"Guo Z.-B.; Liu K.; Hu H.-T.; Li Y.-D.; Qu Z.-X.","Guo, Zi-Bo (57667747200); Liu, Kai (57223776888); Hu, Hang-Tian (57940446000); Li, Yi-Duo (57669070900); Qu, Ze-Xu (57940104300)","57667747200; 57223776888; 57940446000; 57669070900; 57940104300","An FPGA-Based Microinstruction Sequence Driven Spaceborne Convolution Neural Network Accelerator; [一种微指令序列调度数据流的星载卷积神经网络FPGA加速器]","2022","Jisuanji Xuebao/Chinese Journal of Computers","45","10","","2047","2064","17","1","10.11897/SP.J.1016.2022.02047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140451424&doi=10.11897%2fSP.J.1016.2022.02047&partnerID=40&md5=d8051ec7117e469eb71f703c0ed83c18","School of Computer Science and Technology, Xidian University, Xi'an, 710000, China; CAST-Xi'an Institute of Space Radio Technology, Xi'an, 710000, China","Guo Z.-B., School of Computer Science and Technology, Xidian University, Xi'an, 710000, China; Liu K., School of Computer Science and Technology, Xidian University, Xi'an, 710000, China; Hu H.-T., School of Computer Science and Technology, Xidian University, Xi'an, 710000, China; Li Y.-D., School of Computer Science and Technology, Xidian University, Xi'an, 710000, China; Qu Z.-X., CAST-Xi'an Institute of Space Radio Technology, Xi'an, 710000, China","Recently, with the evolvement of space remote sensing technology, the main earth observation device has been gradually transitioning from the single-satellite to a constellation composed of light and small satellites. A constellation of several high-resolution satellites collects hundreds of TBs(Terabytes) of RSI(Remote Sensing Image) data every day. The traditional satellite-to-ground data transmission mechanism has been unable to match the massive remote sensing data processing. In-orbit satellites need to improve their data processing capabilities to deal with increasingly complex observation missions. Meanwhile, in the field of RSI processing, deep learning algorithms based on CNN(Convolutional Neural Network) have become the mainstream method due to their excellent performance. However, the computation-intensive and memory-intensive features have brought many challenges to the deployment of CNN. Academia and industry propose many specific acceleration methods for the CNN domain to cope with the various application scenarios. Numerous FPGA(Field Programmable Gate Array) and ASIC(Application Specific Integrated Circuit) accelerators have been designed to accelerate CNN in edge and data center scenarios. Compared with ASIC, FPGA has higher flexibility and faster development iteration speed, making it very suitable for spaceborne scenarios. In this paper, we propose a microinstruction driven CNN Accelerator for RSI processing on FPGA. This accelerator is jointly designed by software and hardware, which mainly optimizes microinstruction coding, instruction-level parallelism(Coarse-Grained Parallelism) and operation-level parallelism(Fine-Grained Parallelism) under the constraints of limited storage bandwidth and computing resources on satellites. At software level, we propose an extensible microinstruction encoding format and the corresponding compilation method(Micro Assembler). A microinstruction code covers 14 instructions in 4 types, which can schedule the dataflow between different components of the accelerator. The micro assembler performs graph-level optimization on the CNN topology by convolutional loop tiling and operator fusion, and then generates micro-instruction sequences that can be executed by the accelerator. At hardware level, we design and implement an RTL(Register Transfer Level) CNN accelerator, which is mainly composed of micro controller and logic operator. The micro controller achieves the parallel execution of different types of instruction by a 5-stage coarse-grained pipeline(Data Load, Data Fetch, Compute, Post Process, Write Back). The logic operator is a computing array with DSP48E1 hard core resources cascaded, which can achieve parallel execution of convolution operations by a 32-stage fine-grained pipeline. When the pipeline is established, the logic operator can complete 32×32 MAC(Multiply-accumulate) operations in one clock cycle. The performance of our proposed accelerator is evaluated on the Xilinx VX690T FPGA chip commonly found on satellites. The designed power consumption is 10.68W. The RMT(Runtime Max Throughput) reaches 378.63GOP/s, and the ME(MAC Efficiency) reaches 91.5%. When our accelerator is used as a coprocessor to accelerate the CNN object detection algorithm YOLOV3Tiny, the average accuracy of the RSI data set reaches 0.9 and the detection speed reaches 102frames/s. The evaluation results show that our accelerator is 14 times more energy efficiency than the typical GPU acceleration method, and has more than 6.9% improvement in ME compared with other FPGA accelerators. © 2022, Science Press. All right reserved.","CNN; FPGA; Microinstruction sequences; Microprocessor design; Remote sensing object detection","Acceleration; Computation theory; Convolution; Convolutional neural networks; Data handling; Deep learning; Digital storage; Earth (planet); Field programmable gate arrays (FPGA); Integrated circuit design; Iterative methods; Network architecture; Orbits; Pipeline processing systems; Program processors; Remote sensing; Topology; Convolutional neural network; Field programmable gate array; Field programmables; Microinstruction sequence; Microinstructions; Microprocessor designs; Objects detection; Programmable gate array; Remote sensing object detection; Remote-sensing; Object detection","","","","","National Natural Science Foundation of China, NSFC, (201701B, 61850410523, 62171342)","This work has been supported in part by the National Natural Science Foundation of China(Grant Nos.62171342, 61850410523) and the Innovation Fund for Space TT&C Communications(Grant No.201701B).","Lecun Y, Bengio Y, Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Krizhevsky A, Et al., ImageNet classification with deep convolutional neural networks, Communications of the ACM, 60, 6, pp. 84-90, (2017); Szegedy C, Et al., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Jouppi N P, Young C, Patil N, Et al., In-datacenter performance analysis of a tensor processing unit, Proceedings of the Annual International Symposium on Computer Architecture, pp. 1-12, (2017); Chen Y, Luo T, Liu S, Et al., DaDianNao: A machine-learning supercomputer, Proceedings of the Annual IEEE/ACM International Symposium on Microarchitecture, pp. 609-622, (2014); Chen Y H, Yang T J, Emer J, Et al., Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices, IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 9, 2, pp. 292-308, (2019); Gu Yi-Kun, Ni Feng-Lei, Liu Hong, Fault-tolerance design of Xilinx FPGA with self-hosting configuration management, Journal of Astronautics, 33, 10, pp. 1519-1527, (2012); Wang Chao, Wang Teng, Ma Xiang, Zhou Xue-Hai, Research progress on FPGA-based machine learning hardware acceleration, Chinese Journal of Computers, 43, 6, pp. 1161-1182, (2020); Wu Yan-Xia, Liang Kai, Liu Ying, Cui Hui-Min, The progress and trends of FPGA-based accelerators in deep learning, Chinese Journal of Computers, 42, 11, pp. 2461-2480, (2019); Geng T, Wang T, Sanaullah A, Et al., A framework for acceleration of CNN training on deeply-pipelined FPGA clusters with work and weight load balancing, Proceedings of the International Conference on Field Programmable Logic and Applications, pp. 394-398, (2018); Yu J, Ge G, Hu Y, Et al., Instruction driven cross-layer CNN accelerator for fast detection on FPGA, ACM Transactions on Reconfigurable Technology and Systems, 11, 3, pp. 1-23, (2018); Ye Pei-Jian, Huang Jiang-Chuan, Sun Ze-Zhou, Et al., The process and experience in the development of Chinese lunar probe, SCIENTIA SINICA Technologica, 44, 6, pp. 543-558, (2014); Zhang Zhai, Liu Yan, Huang Li-Li, Self-repairing method for BRAM based on cold backup multi-mode redundancy structure, Acta Aeronautica et Astronautica Sinica, 42, 7, pp. 546-557, (2021); Li Tan, Shen Juan, Chen Sai-Qi, Tang Meng-Hui, Life constraints of the LEO remote sensing satellite platform, Bulletin of Surveying and Mapping, pp. 40-42, (2014); He Xiao-Jun, Li Zhu-Qiang, Qin Xiao-Bao, Et al., Technological innovation and application achievements of Jilin No.1 spectral satellite, Satellite Application, 3, pp. 18-26, (2020); Wei X, Liu W, Chen L, Et al., FPGA-based hybrid-type implementation of quantized neural networks for remote sensing applications, Sensors, 19, 4, (2019); Nong Yuan-Jun, Wang Jun-Jie, Real-time object detection in remote sensing images based on embedded system, Acta Optica Sinica, 41, 10, pp. 179-186, (2021); Wang Shi-Yu, Zhang Sheng-Bing, Huang Xiao-Ping, Chang Li-Bo, Single-chip multiprocessing architecture for spaceborne SAR imaging and intelligent processing, Journal of Northwestern Polytechnical University, 39, 3, pp. 510-520, (2021); Li H, Fan X, Jiao L, Et al., A high performance FPGA-based accelerator for large-scale convolutional neural networks, Proceedings of the International Conference on Field Programmable Logic and Applications, pp. 1-9, (2016); Liu Z, Dou Y, Jiang J, Et al., Automatic code generation of convolutional neural networks in FPGA implementation, Proceedings of the International Conference on Field-Programmable Technology, pp. 61-68, (2016); Nguyen D T, Nguyen T N, Kim H, Et al., A high-throughput and power-efficient FPGA implementation of YOLO CNN for object detection, IEEE Transactions on Very Large Scale Integration Systems, 27, 8, pp. 1861-1873, (2019); Redmon J, Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271, (2017); Farrukh F U D, Zhang C, Jiang Y, Et al., Power efficient tiny YOLO CNN using reduced hardware resources based on booth multiplier and WALLACE tree adders, IEEE Open Journal of Circuits and Systems, 1, pp. 76-87, (2020); Gokhale V, Zaidy A, Chang A X M, Et al., Snowflake: An efficient hardware accelerator for convolutional neural networks, Proceedings of the IEEE International Symposium on Circuits and Systems, pp. 1-4, (2017); Zhang C, Sun G, Fang Z, Et al., Caffeine: Toward uniformed representation and acceleration for deep convolutional neural networks, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 38, 11, pp. 2072-2085, (2018); Guo K, Sui L, Qiu J, Et al., Angel-Eye: A complete design flow for mapping CNN onto embedded FPGA, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 37, 1, pp. 35-47, (2017); Abdelfattah M S, Han D, Bitar A, Et al., DLA: Compiler and FPGA overlay for neural network inference acceleration, Proceedings of the International Conference on Field Programmable Logic and Applications, pp. 411-418, (2018); He K, Zhang X, Ren S, Et al., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Yu Y, Wu C, Zhao T, Et al., OPU: An FPGA-based overlay processor for convolutional neural networks, IEEE Transactions on Very Large Scale Integration Systems, 28, 1, pp. 35-47, (2019); Simonyan K, Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Redmon J, Divvala S, Girshick R, Et al., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Jia Y, Shelhamer E, Donahue J, Et al., Caffe: Convolutional architecture for fast feature embedding, Proceedings of the ACM International Conference on Multimedia, pp. 675-678, (2014); Ioffe S, Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, Proceedings of the International Conference on Machine Learning, pp. 448-456, (2015); Gysel P, Motamedi M, Ghiasi S., Hardware-oriented approximation of convolutional neural networks, (2016); Redmon J, Farhadi A., YOLOv3: An incremental improvement, (2018); Wang Y, Wang C, Zhang H, Et al., A SAR dataset of ship detection for deep learning under complex backgrounds, Remote Sensing, 11, 7, (2019)","K. Liu; School of Computer Science and Technology, Xidian University, Xi'an, 710000, China; email: kailiu@mail.xidian.edu.cn","","Science Press","","","","","","02544164","","JIXUD","","Chinese","Jisuanji Xuebao","Article","Final","","Scopus","2-s2.0-85140451424"
"Kim H.; Kim M.; Lee Y.","Kim, Hyungwoo (56981430700); Kim, Minho (57219041618); Lee, Yangwon (37030925300)","56981430700; 57219041618; 37030925300","Research Trend of the Remote Sensing Image Analysis Using Deep Learning","2022","Korean Journal of Remote Sensing","38","5-3","","819","834","15","3","10.7780/kjrs.2022.38.5.3.2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144506564&doi=10.7780%2fkjrs.2022.38.5.3.2&partnerID=40&md5=274870a59981818ed34bae8f93101e0b","Department of Spatial Information Engineering, Division of Earth Environmental System Science, Pukyong National University, Busan, South Korea; College of Space Environment, Sangmyung University, Seoul, South Korea","Kim H., Department of Spatial Information Engineering, Division of Earth Environmental System Science, Pukyong National University, Busan, South Korea; Kim M., College of Space Environment, Sangmyung University, Seoul, South Korea; Lee Y., Department of Spatial Information Engineering, Division of Earth Environmental System Science, Pukyong National University, Busan, South Korea","Artificial Intelligence (AI) techniques have been effectively used for image classification, object detection, and image segmentation. Along with the recent advancement of computing power, deep learning models can build deeper and thicker networks and achieve better performance by creating more appropriate feature maps based on effective activation functions and optimizer algorithms. This review paper examined technical and academic trends of Convolutional Neural Network (CNN) and Transformer models that are emerging techniques in remote sensing and suggested their utilization strategies and development directions. A timely supply of satellite images and real-time processing for deep learning to cope with disaster monitoring will be required for future work. In addition, a big data platform dedicated to satellite images should be developed and integrated with drone and Closed-circuit Television (CCTV) images. © 2022 Korean Society of Remote Sensing. All rights reserved.","Deep learning; Image analysis; Remote sensing","","","","","","","","Awada H., Prima D., Sirca S., Giadrossich C., Marras F., Spano S., Pirastru M., A remote sensing and modeling integrated approach for constructing continuous time series of daily actual evapotranspiration, Agricultural Water Management, 260, (2022); Abdollahi A., Pradhan B., Alamri A. M., An ensemble architecture of deep convolutional Segnet and Unet networks for building semantic segmentation from high-resolution aerial images, Geocarto International, 37, 12, pp. 3355-3370, (2020); Abid S.K., Sulaiman N., Chan S.W., Nazir U., Abid M., Han H., Ariza-Montes A., Vega-Munoz A., Toward an integrated disaster management approach: how artificial intelligence can boost disaster management, Sustainability, 13, 22, (2021); Alem A., Kumar S., Deep learning methods for land cover and land use classification in remote sensing: a review, Proc. of 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), pp. 903-908, (2020); Algiriyage N., Prasanna R., Stock K., Doyle E.E., Johnston D., Multi-source Multimodal Data and Deep Learning for Disaster Response: A Systematic Review, SN Computer Science, 3, 1, pp. 1-29, (2022); Alotaibi B., Alotaibi M., A hybrid deep ResNet and Inception model for hyperspectral image classification, PFG- Journal of Photogrammetry, Remote Sensing and Geoinformation Science, 88, pp. 463-476, (2020); Bai Y., Mas E., Koshimura S., Towards operational satellite-based damage-mapping using U-Net convolutional network: A case study of 2011 Tohoku earthquake-tsunami, Remote Sensing, 10, 10, (2018); Brand A.K., Manandhar A., Semantic segmentation of burned areas in satellite images using a U-Net-based convolutional neural network, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 43, pp. 47-53, (2021); Brodrick P.G., Davies A.B., Asner G.P., Uncovering ecological patterns with convolutional neural networks, Trends in Ecology & Evolution, 34, 8, pp. 734-745, (2019); Chen H., Qi Z., Shi Z., Remote sensing image change detection with transformers, IEEE Transactions on Geoscience and Remote Sensing, 60, pp. 1-14, (2021); Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., Uszkoreit J., Houlsby N., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, (2020); Fukushima K., Miyake S., Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition, Competition and Cooperation in Neural Nets, 45, pp. 267-285, (1982); Guo Y., Zhao Y., Rothfus T.A., Avalos A.S., A novel invasive plant detection approach using time series images from unmanned aerial systems based on convolutional and recurrent neural networks, Neural Computing and Applications, 34, pp. 20135-20147, (2022); Hashemi-Beni L., Gebrehiwot A., Deep learning for remote sensing image classification for agriculture applications, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 44, pp. 51-54, (2020); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Huang G., Liu Z., Van Der Maaten L., Weinberger K.Q., Densely connected convolutional networks, Proc. of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4700-4708, (2017); Jang K., A study on the deep learning-based tree species classification by using high-resolution orthophoto images, Journal of the Korean Association of Geographic Information Studies, 24, 3, pp. 1-9, (2021); Jiang H., Peng M., Zhong Y., Xie H., Hao Z., Lin J., Ma X., Hu X., A survey on deep learningbased change detection from high-resolution remote sensing images, Remote Sensing, 14, 7, (2022); Kang J., Kim K., Jeong Y., Kim S., Youn Y., Cho S., Lee Y., U-Net cloud detection for the SPARCS cloud dataset from Landsat 8 images, Korean Journal of Remote Sensing, 37, 5-1, pp. 1149-1161, (2021); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 24-49, (2021); Kaur S., Gupta S., Singh S., Hurricane damage detection using machine learning and deep learning techniques: A Review, Proc. of 1st International Conference on Computational Research and Data Analytics (ICCRDA 2020), 1022, (2021); Khelifi L., Mignotte M., Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis, IEEE Access, 8, pp. 126385-126400, (2020); Kurniawan J., Syahra S.G., Dewa C.K., Traffic congestion detection: learning from CCTV monitoring images using convolutional neural network, Procedia Computer Science, 144, pp. 291-297, (2018); Lake T.A., Briscoe Runquist R.D., Moeller D.A., Deep learning detects invasive plant species across complex landscapes using Worldview-2 and PlanetScope satellite imagery, Remote Sensing in Ecology and Conservation, (2022); LeCun Y., Boser B., Denker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation applied to handwritten zip code recognition, Neural Computation, 1, 4, pp. 541-551, (1989); Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo B., Swin Transformer: Hierarchical vision transformer using shifted windows, (2021); Mohan A., Singh A.K., Kumar B., Dwivedi R., Review on remote sensing methods for landslide detection using machine and deep learning, Transactions on Emerging Telecommunications Technologies, 32, 7, (2021); Natesan S., Armenakis C., Vepakomma U., ResNet-based tree species classification using UAV images, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 13, pp. 475-481, (2019); Natesan S., Armenakis C., Vepakomma U., Individual tree species identification using Dense Convolutional Network (DenseNet) on multitemporal RGB images from UAV, Journal of Unmanned Vehicle Systems, 8, 4, pp. 310-333, (2020); Park M., Kwak G., Park N., A convolutional neural network model with weighted combination of multi-scale spatial features for crop classification, Korean Journal of Remote Sensing, 35, 6-3, pp. 1273-1283, (2019); Park S., Ahn M., Li C., Kim J., Jeon H., Kim D., Evaluation of oil spill detection models by oil spill distribution characteristics and CNN architectures using Sentinel-1 SAR data, Korean Journal of Remote Sensing, 37, 5-3, pp. 1475-1490, (2021); Pasquali G., Iannelli G.C., Dell'Acqua F., Building footprint extraction from multispectral, spaceborne earth observation datasets using a structurally optimized U-Net convolutional neural network, Remote Sensing, 11, 23, (2019); Peppa M.V., Bell D., Komar T., Xiao W., Urban traffic flow analysis based on deep learning car detection from CCTV image Series, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, 4, pp. 499-506, (2018); Transformer neural network for weed and crop classification of high resolution UAV images, Remote Sensing, 14, 3, (2022); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015, 9351, pp. 234-241, (2015); Sapena M., Wurm M., Taubenbock H., Tuia D., Ruiz L.A., Estimating quality of life dimensions from urban spatial pattern metrics, Computers, Environment and Urban Systems, 85, (2021); Seong S., Choi J., Semantic segmentation of urban buildings using a high-resolution network (HRNet) with channel and spatial attention gates, Remote Sensing, 13, 16, (2021); Song D., Tan X., Wang B., Zhang L., Shan X., Cui J., Integration of super-pixel segmentation and deep-learning methods for evaluating earthquake-damaged buildings using single-phase remote sensing imagery, International Journal of Remote Sensing, 41, 3, pp. 1040-1066, (2020); Sun K., Xiao B., Liu D., Wang J., Deep highresolution representation learning for human pose estimation, Proc. of 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5693-5703, (2019); Sun K., Zhao Y., Jiang B., Cheng T., Xiao B., Liu D., Mu Y., Wang X., Liu W., Wang J., High-resolution representations for labeling pixels and regions, (2019); Takaya K., Sasaki Y., Ise T., Automatic detection of alien plant species in action camera images using the chopped picture method and the potential of citizen science, Breeding Science, 72, 1, pp. 96-106, (2022); Tao R., Zhang Y., Wang L., Liu Q., Wang J., U-High resolution network (U-HRNet): cloud detection with high-resolution representations for geostationary satellite imagery, International Journal of Remote Sensing, 42, 9, pp. 3511-3533, (2021); Tsagkatakis G., Aidini A., Fotiadou K., Giannopoulos M., Pentari A., Tsakalides P., Survey of deep-learning approaches for remote sensing observation enhancement, Sensors, 19, 18, (2019); Vali A., Comai S., Matteucci M., Deep learning for land use and land cover classification based on hyperspectral and multispectral earth observation data: A review, Remote Sensing, 12, 15, (2020); Wang D., Cao W., Zhang F., Li Z., Xu S., Wu X., A review of deep learning in multiscale agricultural sensing, Remote Sensing, 14, 3, (2022); Wang J., Zhang Z., Luo L., Zhu W., Chen J., Wang W., SwinGD: A Robust Grape Bunch Detection Model Based on Swin Transformer in Complex Vineyard Environment, Horticulturae, 7, 11, (2021); Wang G., Wu M., Wei X., Song H., Water identification from high-resolution remote sensing images based on multidimensional densely connected convolutional neural networks, Remote Sensing, 12, 5, (2020); Wei S., Zhang H., Wang C., Wang Y., Xu L., Multi-temporal SAR data large-scale crop mapping based on U-Net model, Remote Sensing, 11, 1, (2019); Xu X., Feng Z., Cao C., Li M., Wu J., Wu Z., Shang Y., Ye S., An improved Swin Transformerbased model for remote sensing object detection and instance segmentation, Remote Sensing, 13, 23, (2021); Xu Z., Zhang W., Zhang T., Li J., HRCNet: High-resolution context extraction network for semantic segmentation of remote sensing images, Remote Sensing, 13, 1, (2020); Yi Y., Zhang Z., Zhang W., Zhang C., Li W., Zhao T., Semantic segmentation of urban buildings from VHR remote sensing imagery using a deep convolutional neural network, Remote Sensing, 11, 15, (2019); Yuan Q., Shen H., Li T., Li Z., Li S., Jiang Y., Xu H., Tan W., Yang Q., Wang J., Gao J., Zhang L., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sensing of Environment, 241, (2020); Zhang H., Xu H., Tian X., Jiang J., Ma J., Image fusion meets deep learning: A survey and perspective, Information Fusion, 76, pp. 323-336, (2021); Zhang J., Zhao H., Li J., TRS: Transformers for remote sensing scene classification, Remote Sensing, 13, 20, (2021); Zhu M., He Y., He Q., A review of researches on deep learning in remote sensing application, International Journal of Geosciences, 10, 1, pp. 1-11, (2019)","Y. Lee; Department of Spatial Information Engineering, Division of Earth Environmental System Science, Pukyong National University, Busan, South Korea; email: modconfi@pknu.ac.kr","","Korean Society of Remote Sensing","","","","","","12256161","","","","Korean","Kor. J. Remote Sens.","Review","Final","","Scopus","2-s2.0-85144506564"
"Retallack A.; Finlayson G.; Ostendorf B.; Lewis M.","Retallack, Angus (57211057586); Finlayson, Graeme (7005300823); Ostendorf, Bertram (7003549233); Lewis, Megan (7404189050)","57211057586; 7005300823; 7003549233; 7404189050","Using deep learning to detect an indicator arid shrub in ultra-high-resolution UAV imagery","2022","Ecological Indicators","145","","109698","","","","1","10.1016/j.ecolind.2022.109698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142148895&doi=10.1016%2fj.ecolind.2022.109698&partnerID=40&md5=78c6558767389b595c1dc9dea2756430","Department of Ecology and Evolutionary Biology, The University of Adelaide, SA, Australia; Bush Heritage Australia, PO Box 329, Flinders Lane, Melbourne, 8009, VIC, Australia","Retallack A., Department of Ecology and Evolutionary Biology, The University of Adelaide, SA, Australia; Finlayson G., Department of Ecology and Evolutionary Biology, The University of Adelaide, SA, Australia, Bush Heritage Australia, PO Box 329, Flinders Lane, Melbourne, 8009, VIC, Australia; Ostendorf B., Department of Ecology and Evolutionary Biology, The University of Adelaide, SA, Australia; Lewis M., Department of Ecology and Evolutionary Biology, The University of Adelaide, SA, Australia","Effective monitoring of arid and semi-arid rangelands around the world is essential to understand and combat degradation caused by anthropogenic use and facilitate effective management practices. Remote sensing technologies provide ideal approaches for enhancing traditional on-ground monitoring. However, while broad-scale monitoring of vegetation in rangelands using satellites has been widely adopted, there has been far less uptake of remote sensing for measuring fine-scale indicators of ecosystem condition. This study demonstrates the feasibility of using ultra-high-resolution UAV (Uncrewed Aerial Vehicle) imagery and deep-learning-based object detection models to provide plant recognition and survey information relevant for operational monitoring programmes in arid and semi-arid ecosystems. Seven different object detectors using varying convolutional neural network (CNN) architectures are tested at three image resolutions to detect a widespread, dominant arid shrub species (pearl bluebush, Maireana sedifolia) that serves as a key indicator of overall site condition in southern Australian rangelands. To maximise the strength of statistical analysis, each method is trained on six different training datasets (each using 2,000 to 3,000 training samples) at six widely dispersed sites. This results in 90 trained models, each validated at two sites. To test model generalisability, training and validation data was always sourced from separate vegetation monitoring sites. The influence of variability between sites on detection accuracy is also considered. The best performing models achieved F1 scores (overall accuracy) of around 75% for pearl bluebush detection, a level of accuracy that provides useful monitoring information to land managers. Information extracted from UAV imagery using this approach relates directly to indicators of ecological condition measured in ground-based monitoring; including dominant plant species count, location and density. Continued development and eventual implementation of this method would provide objective conservation-relevant information at broad scales in a far reduced time and at a lower cost than is currently achievable using on-ground approaches. © 2022 The Authors","Arid ecology; Conservation; Deep learning; Rangelands; Remote sensing; UAV","Australia; Aircraft detection; Antennas; Convolution; Convolutional neural networks; Deep learning; Ecosystems; Image resolution; Information use; Object detection; Personnel training; Unmanned aerial vehicles (UAV); Vegetation; Anthropogenics; Arid ecology; Deep learning; Effective management; Management practises; Rangeland; Remote-sensing; Semi-arid rangeland; Ultrahigh resolution; Uncrewed aerial vehicles; arid environment; ground-based measurement; remote sensing; satellite imagery; shrub; unmanned vehicle; Remote sensing","","","","","Andrew Hennessey; Australian Institute of Machine Learning; Unmanned Research Aircraft Facility; Thyne Reid Foundation; University of Adelaide; Australian Plant Phenomics Facility, APPF","Funding text 1: Acknowledgements to Steven Humphris, Andrew Du (Australian Institute of Machine Learning) and Andrew Hennessey (The Plant Accelerator, Australian Plant Phenomics Facility) for sharing their knowledge of deep learning and providing strong expertise of the field and software for this research to be built on. To Kenneth Clarke (The Plant Accelerator, Australian Plant Phenomics Facility) and Andrew Du for providing their feedback on study design. To the Unmanned Research Aircraft Facility (URAF, The University of Adelaide) for supplying the UAV and ground control points used for data collection. To Ramesh Raja Segaran (URAF) and Dillon Campbell (URAF) for help designing a ground control and data collection protocol, and Dillon for assisting with orthomosaic processing. We acknowledge the support of Bush Heritage Australia staff and supporters for supporting this project. This project is being partially supported by the Thyne Reid Foundation. We acknowledge the Antakirinja Matu-Yankunytjatjara people as the Traditional Owners of Bon Bon.; Funding text 2: We acknowledge the support of Bush Heritage Australia staff and supporters for supporting this project. This project is being partially supported by the Thyne Reid Foundation. ","(2020); Anderson K., Gaston K.J., Lightweight unmanned aerial vehicles will revolutionize spatial ecology, Front. Ecol. Environ., 11, 3, pp. 138-146, (2013); Ayhan B., Kwan C., Budavari B., Kwan L., Lu Y., Perez D., Li J., Skarlatos D., Vlachos M., Vegetation detection using deep learning and conventional methods, Remote Sens., 12, 15, (2020); Baena S., Moat J., Whaley O., Boyd D.S., Identifying species from the air: UAVs and the very high resolution challenge for plant conservation, PLoS One, 12, 11, pp. 1-21, (2017); Bastin G., Management Committee A.C.R.I.S., Rangelands 2008 - Taking the pulse: Gawler bioregion, (2008); Borowiec M.L., Dikow R.B., Frandsen P.B., Mckeeken A., Valentini G., White A.E., Deep learning as a tool for ecology and evolution, Methods Ecol. Evol., 13, 8, pp. 1640-1660, (2022); (2010); Chavda A., Dsouza J., Badgujar S., Damani A., Multi-stage CNN architecture for face mask detection, 2021 6th international Conference for Convergence in Technology (I2CT). 2021–04-02, (2021); Christin S., Hervet E., Lecomte N., Applications for deep learning in ecology, Methods Ecol. Evol., 10, 10, pp. 1632-1644, (2019); Deng X., Tong Z., Lan Y., Huang Z., Detection and location of dead trees with pine wilt disease based on deep learning and UAV remote sensing, AgriEngineering, 2, 2, pp. 294-307, (2020); (2015); Du L., Sun Y., Chen S., Feng J., Zhao Y., Yan Z., Zhang X., Bian Y., A novel object detection model based on Faster R-CNN for Spodoptera frugiperda according to feeding trace of corn leaves, Agriculture, 12, 2, (2022); (2021); Franklin S.E., Ahmed O.S., Deciduous tree species classification using object-based analysis and machine learning with unmanned aerial vehicle multispectral data, Int. J. Remote Sens., 39, 15-16, pp. 5236-5245, (2018); Fraser R.H., Olthof I., Lantz T.C., Carla S., UAV photogrammetry for mapping vegetation in the low-Arctic, Arct. Sci., 2, 3, pp. 79-102, (2016); Gallacher D., Drone-based vegetation assessment in arid ecosystems, The Sabkha Ecosystems of the World, pp. 91-98, (2019); Guirado E., Tabik S., Alcaraz-Segura D., Cabello J., Herrera F.; Guirado E., Tabik S., Alcaraz-Segura D., Cabello J., Herrera F., Deep-learning versus OBIA for scattered shrub detection with Google Earth imagery: Ziziphus lotus as case study, Remote Sensing, 9, 12, (2017); Hay G.J., Castilla G., pp. 75-89, (2008); Held A., Phinn S., Sotto-Berelov M., Jones S., AusCover good practice guidelines: A technical handbook supporting calibration and validation activities of remotely sense data products, version 1.2, (2015); Horning N., Fleishman E., Ersts P.J., Fogarty F.A., Wohlfeil Zillig M., Mapping of land cover with open-source software and ultra-high-resolution imagery acquired with unmanned aerial vehicles, Remote Sens. Ecol. Conserv., 6, 4, pp. 487-497, (2020); Huang H., Lan Y., Yang A., Zhang Y., Wen S., Deng J., Deep learning versus Object-based Image Analysis (OBIA) in weed mapping of UAV imagery, Int. J. Remote Sens., 41, 9, pp. 3446-3479, (2020); James K., Bradshaw K., Detecting plant species in the field with deep learning and drone technology, Methods Ecol. Evol., 11, 11, pp. 1509-1519, (2020); James K., Bradshaw K., Shrub detection in high-resolution Imagery: A comparative study of two deep learning approaches, International Advanced Computing Conference, (2021); Karfs R., Bastin G., Chewings V., Bartolo J., Grant R.L., (2001); Kattenborn T., Eichel J., Wiser S., Burrows L., Fassnacht F.E., Schmidtlein S., Convolutional neural networks accurately predict cover fractions of plant species and communities in unmanned aerial vehicle imagery, Remote Sens. Ecol. Conserv., 6, 4, pp. 472-486, (2020); Kislov D.E., Korznikov K.A., Altman J., Vozmishcheva A.S., Krestov P.V., Extending deep learning approaches for forest disturbance segmentation on very high-resolution satellite images, Remote Sens. Ecol. Conserv., 7, 3, pp. 355-368, (2021); Laliberte A.S., Goforth M.A., Steele C.M., Rango A., Multispectral remote sensing from unmanned aircraft: Image processing workflows and applications for rangeland environments, Remote Sensing, 3, 11, pp. 2529-2551, (2011); Lamba A., Cassey P., Segaran R.R., Koh L.P., Deep learning for environmental conservation, Curr. Biol., 29, 19, pp. R977-R982, (2019); (2009); Lu B., He Y., Optimal spatial resolution of unmanned aerial vehicle (UAV)-acquired imagery for species classification in a heterogeneous grassland ecosystem, GIScience & Remote Sensing, 55, 2, pp. 205-220, (2018); Lu B., He Y., Species classification using Unmanned Aerial Vehicle (UAV)-acquired high spatial resolution imagery in a heterogeneous grassland, ISPRS J. Photogramm. Remote Sens., 128, pp. 73-85, (2017); Lu B., He Y., Liu H., Investigating species composition in a temperate grassland using unmanned aerial vehicle-acquired imagery, 2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA). IEEE, (2016); Lussem U., Schellberg J., Bareth G., Monitoring forage mass with low-cost UAV data: Case study at the rengen grassland experiment. PFG – Journal of Photogrammetry, Remote Sensing and Geoinformation, Science, 88, 5, pp. 407-422, (2020); McMichael A.J., Bolin B., Costanza R., Daily G.C., Folke C., Lindahl-Kiessling K., Lindgren E., Niklasson B., Globalization and the sustainability of human health, Bioscience, 49, 3, (1999); Messina G., Pratico S., Badagliacca G., Di Fazio S., Monti M., Modica G., Monitoring onion crop “Cipolla Rossa di Tropea Calabria IGP” growth and yield response to varying nitrogen fertilizer application rates using UAV imagery, Drones, 5, 3, (2021); Nezami S., Khoramshahi E., Nevalainen O., Polonen I., Honkavaara E., Tree species classification of drone hyperspectral and RGB imagery with deep learning convolutional neural networks, Remote Sensing, 12, 7, (2020); NLWRA, Rangelands - Tracking changes: Australian Collaborative Rangeland Information System, (2001); Noss R.F., Indicators for monitoring biodiversity: A hierarchical approach, Conserv. Biol., 4, 4, pp. 355-364, (1990); NVIS Technical Working Group, Australian Vegetation Attribute Manual: National Vegetation Information System, version 7.0.Canberra, (2017); Oldeland J., Revermann R., Luther-Mosebach J., Buttschardt T., Lehmann J.R.K., New tools for old problems — comparing drone- and field-based assessments of a problematic plant species, Environ. Monit. Assess., 193, 2, (2021); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Calvert B., Azghadi M.R., White R.D., DeepWeeds: A multiclass weed species image dataset for deep learning, Sci. Rep., 9, pp. 1-12, (2019); (2018); Reid R.S., Fernandez-Gimenez M.E., Galvin K.A., Dynamics and resilience of rangelands and pastoral peoples around the globe, Annu. Rev. Environ. Resour., 39, pp. 217-242, (2014); Sankey T.T., Leonard J.M., Moore M.M., Unmanned aerial vehicle-based rangeland monitoring: Examining a century of vegetation changes, Rangeland Ecol. Manage., 72, 5, pp. 858-863, (2019); Sasaki Y., (2007); Theau J., Lauzier-Hudon E., Aube L., Devillers N., Estimation of forage biomass and vegetation cover in grasslands using UAV imagery, PLoS ONE, 16, 1, pp. 1-18, (2021); Thomson E.R., Spiegel M.P., Althuizen I.H., Bass P., Chen S., Chmurzynski A., Halbritter A.H., Henn J.J., Jonsdottir I.S., Klanderud K., Multiscale mapping of plant functional groups and plant traits in the High Arctic using field spectroscopy, UAV imagery and Sentinel-2A data, Environ. Res. Lett., 16, 5, pp. 1-21, (2021); Tueller P.T., Remote sensing technology for rangeland management applications, J. Range Manag., 42, 6, pp. 442-453, (1989); (2011); (2020); Waddell P., Gardner A., Hennig P., An inventory and condition survey of the Western Australian part of the Nullarbor region.Perth, (2010); Wilson L., Van Dongen R., Cowen S., Robinson T.P., Mapping restoration activities on Dirk Hartog Island using remotely piloted aircraft imagery, Remote Sensing, 14, 6, (2022); Zhang C., Atkinson P.M., George C., Wen Z., Diazgranados M., Gerard F., Identifying and mapping individual plants in a highly diverse high-elevation ecosystem using UAV imagery and deep learning, ISPRS J. Photogramm. Remote Sens., 169, pp. 280-291, (2020); Zhang B., Carter J., FORAGE – An online system for generating and delivering property-scale decision support information for grazing land and environmental management, Comput. Electron. Agric., 150, pp. 302-311, (2018); Zhao Z.-Q., Zheng P., Xu S.-T., Wu X., Object detection with deep learning: A review, IEEE Trans. Neural Networks Learn. Syst., 30, 11, pp. 3212-3232, (2019); Zhu W., Braun B., Chiang L.H., Romagnoli J.A., Investigation of transfer learning for image classification and impact on training sample size, Chemometrics Intell. Labor. Syst., 211, (2021)","A. Retallack; Department of Ecology and Evolutionary Biology, The University of Adelaide, Adelaide, North Terrace Campus, 5005, Australia; email: angus.retallack@adelaide.edu.au","","Elsevier B.V.","","","","","","1470160X","","","","English","Ecol. Indic.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142148895"
"Tan Y.; Cai R.; Li J.; Chen P.; Wang M.","Tan, Yi (57193550394); Cai, Ruying (57227610300); Li, Jingru (26660256200); Chen, Penglu (57226812966); Wang, Mingzhu (57202642024)","57193550394; 57227610300; 26660256200; 57226812966; 57202642024","Automatic detection of sewer defects based on improved you only look once algorithm","2021","Automation in Construction","131","","103912","","","","26","10.1016/j.autcon.2021.103912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113313097&doi=10.1016%2fj.autcon.2021.103912&partnerID=40&md5=8eb01c7ac30a82547e7a91a1efab9d92","Key Laboratory for Resilient Infrastructures of Coastal Cities (Shenzhen University), Ministry of Education, China; Sino-Australia Joint Research Center in BIM and Smart Construction, Shenzhen University, Shenzhen, China; College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","Tan Y., Key Laboratory for Resilient Infrastructures of Coastal Cities (Shenzhen University), Ministry of Education, China, Sino-Australia Joint Research Center in BIM and Smart Construction, Shenzhen University, Shenzhen, China, College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Cai R., College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Li J., Key Laboratory for Resilient Infrastructures of Coastal Cities (Shenzhen University), Ministry of Education, China, Sino-Australia Joint Research Center in BIM and Smart Construction, Shenzhen University, Shenzhen, China, College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Chen P., College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Wang M., Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","The drainage system is an important part of civil infrastructure. However, the underground sewage pipe will gradually suffer from defects over time, such as tree roots, deposits, infiltrations and cracks, which heavily affect the performance of sewage pipes. Therefore, it is significant to timely inspect the condition of sewage pipes. Closed-circuit television (CCTV) inspection is a commonly employed underground infrastructure inspection technology requiring engineering experience that can be subjective and inefficient. Nowadays, object detection based on convolutional neural network (CNN) can automatically detect defects, showing high potential for improving inspection efficiency. This paper proposed an improved CNN-based You Only Look Once version 3 (YOLOv3) method for automatic detection of sewage pipe defects, where the improvements are mainly involved in loss function, data augmentation, bounding box prediction and network structure. Experiment results demonstrate that the improved model outperforms Faster R-CNN and YOLOv3, achieving a mean average precision (mAP) value of 92%, which is higher than the existing research on automatic detection of sewage pipe defects. © 2021","Automatic defect detection; CCTV; CNN; Computer vision; Deep learning; Object detection; Sewer defects; YOLO","Closed circuit television systems; Computer vision; Convolutional neural networks; Deep learning; Defects; Inspection; Object recognition; Sewage; Sewers; Automatic defect detections; Automatic Detection; Closed circuit television; Convolutional neural network; Deep learning; Objects detection; Pipe defects; Sewage pipes; Sewer defect; YOLO; Object detection","","","","","FDYT, (2020KQNCX060); Foundation for Distinguished Young Talents in Higher Education of Guangdong","This research is supported by Foundation for Distinguished Young Talents in Higher Education of Guangdong , China (FDYT), No. 2020KQNCX060 .","Harvey R.R., McBean E.A., Comparing the utility of decision trees and support vector machines when planning inspections of linear sewer infrastructure, J. Hydroinf., 16, 6, pp. 1265-1279, (2014); Elsawah H., Bakry I., Moselhi O., Decision support model for integrated risk assessment and prioritization of intervention plans of municipal infrastructure, J. Pipeline Syst. Eng. Pract., 7, 4, (2016); Cheng J.C.P., Wang M., Automated detection of sewer pipe defects in closed-circuit television images using deep learning techniques, Autom. Constr., 95, pp. 155-171, (2018); Kuliczkowska E., An analysis of road pavement collapses and traffic safety hazards resulting from leaky sewers, Baltic J. Road Bridge Eng., 11, 4, pp. 251-258, (2016); Tanaka Y., Ishihara S., I. Assoc Comp Machinery, Cooperative Video Data Transmission for Sewer Inspection Using Multiple Drifting Cameras. Adjunct Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing Networking and Services, pp. 195-200, (2016); Liu J., Yi Y., Wang X., Exploring factors influencing construction waste reduction: a structural equation modeling approach, J. Clean. Prod., 276, 4, (2020); Caradot N., Sonnenberg H., Kropp I., Ringe A., Denhez S., Hartmann A., Rouault P., The relevance of sewer deterioration modelling to support asset management strategies, Urban Water J., 14, 10, pp. 1007-1015, (2017); Ke Z., Qwa B., Lca B., Jy D., Zl B., Zyab C., Tao Y., Qin J.B., Ground observation-based analysis of soil moisture spatiotemporal variability across a humid to semi-humid transitional zone in China, J. Hydrol., 574, pp. 903-914, (2019); Halfawy M.R., Hengmeechai J., Efficient algorithm for crack detection in sewer images from closed-circuit television inspections, J. Infrastruct. Syst., 20, 2, (2014); Chao L., Zhang K., Li Z., Zhu Y., Wang J., Yu Z., Geographically weighted regression based methods for merging satellite and gauge precipitation, J. Hydrol., 558, pp. 275-289, (2018); Dong H., Zhao B., Deng Y., Instability phenomenon associated with two typical high speed railway vehicles, Int. J. Non-Linear Mechanics, 105, Oct, pp. 130-145, (2018); Kovalnogov V., Simos T., Tsitouras C., Runge–Kutta pairs suited for SIR-type epidemic models, Math. Meth. Appl. Sci., 44, (2020); Kovalnogov V.N., Simos T.E., Tsitouras C., Ninth-order, explicit, two-step methods for second-order inhomogeneous linear IVPs, Math. Meth. Appl. Sci., 168, (2020); Medvedeva M., Simos T., Tsitouras C., Katsikis V., Direct estimation of SIR model parameters through second-order finite differences, Math. Meth. Appl. Sci., 44, (2020); Zhong B.T., Wu H.T., Ding L.Y., Love P.E.D., Li H., Luo H.B., Jiao L., Mapping computer vision research in construction: developments, knowledge gaps and implications for research, Autom. Constr., 107, (2019); Xiong Z., Tang Z., Chen X., Zhang X.M., Ye C., Research on image retrieval algorithm based on combination of color and shape features, J. Signal Process. Syst., (2021); Walsh J., Mahony N.O., Campbell S., Carvalho A., Riordan D., Deep learning vs. traditional computer vision, Computer Vision Conference (CVC), 2019, (2019); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Comput. Therm. Sci., (2014); Deng L., Chu H.H., Shi P., Wang W., Kong X., Region-based CNN method with deformable modules for visually classifying concrete cracks, Appl. Sci., 10, 7, (2020); Wang M.Z., Cheng J.C.P., A unified convolutional neural network integrated with conditional random field for pipe defect segmentation, Computer-Aided Civil Infrastruct. Eng., 35, 2, pp. 162-177, (2020); Ren S.Q., He K.M., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: single shot multibox detector, European Conference on Computer Vision, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Li Y., Qi H.Z., Dai J., Ji X.Y., Wei Y.C., Fully convolutional instance-aware semantic segmentation, 30th IEEE Conference on Computer Vision and Pattern Recognition, pp. 4438-4446, (2017); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, (2017); Kumar S.S., Wang M.Z., Abraham D.M., Jahanshahi M.R., Iseley T., Cheng J.C.P., Deep learning-based automated detection of sewer defects in CCTV videos, J. Comput. Civ. Eng., 34, 1, (2020); Medvedeva M., Katsikis V., Mourtas S., Simos T., Randomized time-varying knapsack problems via binary beetle antennae search algorithm: emphasis on applications in portfolio insurance, Math. Meth. Appl. Sci., (2020); Makar J.M., Diagnostic techniques for sewer systems, J. Infrastruct. Syst., (1999); Shehab T., Moselhi O., Automated detection and classification of infiltration in sewer pipes, J. Infrastruct. Syst., 11, 3, pp. 165-171, (2005); Costello S.B., Chapman D.N., Rogers C.D.F., Metje N., Underground asset location and condition assessment technologies, Tunnel. Undergr. Space Technol., 22, 5-6, pp. 524-542, (2007); Li Q., Liu X., Novel approach to pavement image segmentation based on neighboring difference histogram method, Cisp 2008: First International Congress on Image and Signal Processing, Vol 2, Proceedings, pp. 792-796, (2008); Ayenu-Prah A., Attoh-Okine N., Evaluating pavement cracks with bidimensional empirical mode decomposition, Eurasip J. Adv. Signal Process., (2008); Yan M., Bo S., Xu K., He Y., Pavement crack detection and analysis for high-grade highway, Electronic Measurement and Instruments, 2007. ICEMI '07. 8th International Conference on, (2007); Zhou J., Huang P.S., Chiang F.P., Wavelet-based pavement distress detection and evaluation, Opt. Eng., (2006); Cevallos-Torres L.J., Gilces D.M., Guijarro-Rodriguez A., Barriga-Diaz R., Leyva-Vazquez M., Botto-Tobar M., BottoTobar M., Pizarro G., ZunigaPrieto M., Darmas M., Sanchez M.Z., An approach to the detection of post-seismic structural damage based on image segmentation methods, Technology Trends, pp. 644-658, (2019); Huynh P., Ross R., Martchenko A., Devlin J., Dou-edge evaluation algorithm for automatic thin crack detection in pipelines, 2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), (2015); Su T.C., Segmentation of crack and open joint in sewer pipelines based on CCTV inspection images, Proceedings of the 2015 Aasri International Conference on Circuits and Systems, pp. 263-266, (2015); Halfawy M.R., Hengmeechai J., Automated defect detection in sewer closed circuit television images using histograms of oriented gradients and support vector machine, Autom. Constr., 38, pp. 1-13, (2014); Ahrary A., Ishikawa M., Detecting pipe feature points for sewer pipe system based on image information - art. no. 604130, ICMIT 2005: Information Systems and Signal Processing, (2005); Saranya R., Daniel J., Abudhahir A., Chermakani N., Comparison of segmentation techniques for detection of defects in non-destructive testing images, 2014 International Conference on Electronics and Communication Systems, (2014); Su T.C., Yang M.D., Application of morphological segmentation to leaking defect detection in sewer pipelines, Sensors, 14, 5, pp. 8686-8704, (2014); Hou C., Simos T.E., Famelis I.T., Neural network solution of pantograph type differential equations, Math. Meth. Appl. Sci., (2020); Voulodimos A., Doulamis N., Doulamis A., Protopapadakis E., Deep learning for computer vision: a brief review, Comput. Intellig. Neurosci., (2018); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Commun. ACM, 60, 6, pp. 84-90, (2017); Szegedy C., Liu W., Jia Y., Sermanet P., Rabinovich A., Going deeper with convolutions, Comput. Soc., (2014); Kaiming H., Xiangyu Z., Shaoqing R., Jian S., Deep residual learning for image recognition, (2015); Kumar S.S., Abraham D.M., Jahanshahi M.R., Iseley T., Starr J., Automated defect classification in sewer closed circuit television inspections using deep convolutional neural networks, Autom. Constr., 91, pp. 273-283, (2018); Meijer D., Scholten L., Clemens F., Knobbe A., A defect classification methodology for sewer image sets with convolutional neural networks, Autom. Constr., 104, pp. 281-298, (2019); Hassan S.I., Dang L.M., Mehmood I., Im S., Choi C., Kang J., Park Y.S., Moon H., Underground sewer pipe condition assessment based on convolutional neural networks, Autom. Constr., 106, (2019); Zuo X., Dai B., Shan Y.W., Shen J.F., Hu C.L., Huang S.C., Classifying cracks at sub-class level in closed circuit television sewer inspection videos, Autom. Constr., 118, (2020); Modarres C., Astorga N., Droguett E.L., Meruane V., Convolutional neural networks for automated damage recognition and damage type identification, Struct. Control. Health Monit., 25, 10, (2018); Zhou S.L., Song W., Deep learning-based roadway crack classification using laser-scanned range images: a comparative study on hyperparameter selection, Autom. Constr., 114, (2020); Li B., Wang K.C.P., Zhang A., Yang E., Wang G., Automatic classification of pavement crack using deep convolutional neural network, Int. J. Pavement Eng., 21, 4, pp. 457-463, (2020); Cha Y.J., Choi W., Buyukozturk O., Deep learning-based crack damage detection using convolutional neural networks, Computer-Aided Civil Infrastruct. Eng., 32, 5, pp. 361-378, (2017); Kim B., Cho S., Image-based concrete crack assessment using mask and region-based convolutional neural network, Struct. Control. Health Monit., 26, 8, (2019); Wang M.Z., Cheng J.C.P., Development and improvement of deep learning based automated defect detection for sewer pipe inspection using faster R-CNN, Advanced Computing Strategies for Engineering, Pt Ii, pp. 171-192, (2018); Mondal T.G., Jahanshahi M.R., Wu R.T., Wu Z.Y., Deep learning-based multi-class damage detection for autonomous post-disaster reconnaissance, Struct. Control. Health Monit., 27, 4, (2020); Kim I.H., Jeon H., Baek S.C., Hong W.H., Jung H.J., Application of crack identification techniques for an aging concrete bridge inspection using an unmanned aerial vehicle, Sensors, 18, 6, (2018); Zhang C., Chang C.C., Jamshidi M., Concrete bridge surface damage detection using a single-stage detector, Computer-Aided Civil Infrastruct. Eng., 35, 4, pp. 389-409, (2019); Du Y., Pan N., Xu Z., Deng F., Shen Y., Kang H., Pavement distress detection and classification based on YOLO network, Int. J. Pavement Eng., pp. 1-14, (2020); Yin X.F., Chen Y., Bouferguene A., Zaman H., Al-Hussein M., Kurach L., A deep learning-based framework for an automated defect detection system for sewer pipes, Autom. Constr., 109, (2020); Fang W., Ding L., Love P.E.D., Luo H., Li H., Pena-Mora F., Zhong B., Zhou C., Computer vision applications in construction safety assurance, Autom. Constr., 110, (2020); Mi C., Huang Y., Fu C., Zhang Z., Postolache O., Vision-based measurement: actualities and developing trends in automated container terminals, IEEE Instrument. Meas. Magaz., 24, 4, pp. 65-76, (2021); Wei F.J., Yao G., Yang Y., Sun Y.J., Instance-level recognition and quantification for concrete surface bughole based on deep learning, Autom. Constr., 107, (2019); Augustauskas R., Lipnickas A., Improved pixel-level pavement-defect segmentation using a deep autoencoder, Sensors, 20, 9, (2020); Redmon J., Farhadi A., YOLO9000: better, faster, stronger, 30th IEEE Conference on Computer Vision and Pattern Recognition, pp. 6517-6525, (2017); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, pp. 1-6, (2018); Bochkovskiy A., Chien-Yao W., Liao H.Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020); Jocher G., Kwon Y., Guigarfr J.V.-M., Perry T., Marc G.B., Baltaci F., Suess D., Tailang C., Peiwen Y., idow09, Wanna Sea U., Xinyu W., Shead T.M., Havlik T., Skalski P., NirZarrabi L.A.I., Coce L., Hu J., Ovodov I., Wiki G., Reveriano F., Falak, Kendall D., ultralytics/yolov3: 43.1mAP@0.5:0.95 on COCO2014, (2020); Yap M.H., Hachiuma R., Alavi A., Brungel R., Cassidy B., Goyal M., Zhu H., Ruckert J., Olshansky M., Huang X., Saito H., Hassanpour S., Friedrich C.M., Ascher D.B., Song A., Kajita H., Gillespie D., Reeves N.D., Pappachan J.M., O'Shea C., Frank E., Deep learning in diabetic foot ulcers detection: a comprehensive evaluation, Comput. Biol. Med., 135, (2021); Ioffe S., Szegedy C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, (2015); Dubey A.K., Jain V., Comparative study of convolution neural network's Relu and leaky-Relu activation functions, Applications of Computing, Automation and Wireless Systems in Electrical Engineering. Proceedings of MARC 2018. Lecture Notes in Electrical Engineering, pp. 873-880, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, 30th IEEE Conference on Computer Vision and Pattern Recognition, pp. 936-944, (2017); Jocher G., Stoken A., Borovec J., NanoCode012 A.; Rezatofighi H., Tsoi N., Gwak J., Sadeghian A., Reid I., Savarese S., Generalized intersection over union: a metric and a loss for bounding box regression, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 658-666, (2019); Zhaohui Z., Ping W., Wei L., Jinze L., Rongguang Y., Dongwei R., Distance-IoU loss: faster and better learning for bounding box regression, (2019); Yun S., Han D., Oh S.J., Chun S., Choe J., Yoo Y., Choe J., CutMix: regularization strategy to train strong classifiers with localizable features, 2019 IEEE/CVF International Conference on Computer Vision, pp. 6022-6031, (2019); Wang C., Liao H.M., Wu Y., Chen P., Hsieh J., Yeh I., CSPNet: a new backbone that can enhance learning capability of CNN, IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), (2020); He K.M., Zhang X.Y., Ren S.Q., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 37, 9, pp. 1904-1916, (2015); Liu S., Qi L., Qin H., Shi J., Jia J., Path aggregation network for instance segmentation, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8759-8768, (2018); Tzutalin, LabelImg; Kingma D., Ba J., Adam: a method for stochastic optimization, Comput. Therm. Sci., (2014); Loshchilov I., Hutter F., SGDR: Stochastic Gradient Descent with Warm Restarts, (2016); Neubeck A., Gool L., Efficient non-maximum suppression, International Conference on Pattern Recognition, (2006); Li D.S., Cong A.R., Guo S., Sewer damage detection from imbalanced CCTV inspection data using deep convolutional neural networks with hierarchical classification, Autom. Constr., 101, pp. 199-208, (2019)","R. Cai; College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; email: ruycaicai@163.com","","Elsevier B.V.","","","","","","09265805","","AUCOE","","English","Autom Constr","Article","Final","","Scopus","2-s2.0-85113313097"
"Oveis A.H.; Giusti E.; Ghio S.; Martorella M.","Oveis, Amir Hosein (56898906700); Giusti, Elisa (55388839300); Ghio, Selenia (57203925381); Martorella, Marco (6603185380)","56898906700; 55388839300; 57203925381; 6603185380","A Survey on the Applications of Convolutional Neural Networks for Synthetic Aperture Radar: Recent Advances","2022","IEEE Aerospace and Electronic Systems Magazine","37","5","","18","42","24","18","10.1109/MAES.2021.3117369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122094962&doi=10.1109%2fMAES.2021.3117369&partnerID=40&md5=2318b5b7ce5fa973e9f0d9c052c203f7","National Inter-University Consortium For Telecommunications, Pisa, 56124, Italy; University Of Pisa, Pisa, 56124, Italy","Oveis A.H., National Inter-University Consortium For Telecommunications, Pisa, 56124, Italy; Giusti E., National Inter-University Consortium For Telecommunications, Pisa, 56124, Italy; Ghio S., National Inter-University Consortium For Telecommunications, Pisa, 56124, Italy; Martorella M., University Of Pisa, Pisa, 56124, Italy","In recent years, convolutional neural networks (CNNs) have drawn considerable attention for the analysis of synthetic aperture radar (SAR) data. In this study, major subareas of SAR data analysis that have been tackled by CNNs are systematically reviewed, such as automatic target recognition, land use and land cover classification, segmentation, change detection, object detection, and image denoising. Special emphasis has been given to practical techniques such as data augmentation and transfer learning. Complex-valued CNNs, which have been introduced to exploit phase information embedded in SAR complex images, have also been extensively reviewed. To conclude this review paper, open challenges and future research directions are highlighted. © 1986-2012 IEEE.","Automatic Target Recognition; Change Detection; Complex-Valued CNN; Convolutional Neural Network; Data Augmentation; Deep Learning; Land Use and Land Cover Classification; Object Detection; Segmentation; Synthetic Aperture Radar; Transfer Learning","Automatic target recognition; Complex networks; Convolution; Convolutional neural networks; Deep neural networks; Feature extraction; Image denoising; Image segmentation; Land use; Object detection; Object recognition; Radar imaging; Radar target recognition; Tracking radar; Change detection; Complex-valued; Complex-valued convolutional neural network; Convolutional neural network; Data augmentation; Deep learning; Features extraction; Land-use and land-cover classifications; Objects detection; Segmentation; Transfer learning; Synthetic aperture radar","","","","","","","Li Y., Zhang H., Xue X., Jiang Y., Shen Q., Deep learning for remote sensing image classification: A survey, Wiley Interdiscipl. Rev. Data Mining Knowl. Dis-cov., 8, 6, (2018); Alom M.Z., Et al., A state-of-the-art survey on deep learning theory and architectures, Electronics, 8, 3, (2019); Cun Y.L., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, 11, pp. 2278-2324, (1998); Vincent P., Larochelle H., Lajoie I., Bengio Y., Manzagol P., Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion, J. Mach. Learn. Res., 11, pp. 3371-3408, (2010); Hinton G.E., Salakhutdinov R.R., Reducing the dimensionality of data with neural networks, Science, 313, 5786, pp. 504-507, (2006); Hinton G., Osindero S., Teh Y.W., A fast learning algorithm for deep belief nets, Neural Comput, 18, 7, pp. 1527-1554, (2006); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177, (2019); The Air Force Moving And Stationary Target Recognition Database., (2014); Morgan D.A.E., Deep convolutional neural networks for ATR from SAR imagery, Algorithms Synth. Aperture Radar Imagery, 9475, (2015); Wilmanski M., Kreucher C., Lauer J., Modern approaches in deep learning for SAR ATR, Proc. SPIE, 9843, (2016); Masci J., Meier U., Ciresan D., Schmidhuber J., Stacked convolutional auto-encoders for hierarchical feature extraction, Proc. Int. Conf. Artif. Neural Netw., pp. 52-59, (2011); Shang R., Liu Y., Wang J., Jiao L., Stolkin R., Stacked auto-encoder for classification of polarimetric SAR images based on scattering energy, Int. J. Remote Sens., 40, 13, pp. 5094-5120, (2019); Zhang L., Ma W., Zhang D., Stacked sparse autoen-coder in PolSAR data classification using local spatial information, IEEE Geosci. Remote Sens. Lett., 13, 9, pp. 1359-1363, (2016); Guo J., H. Li N.J., W. Han Z.W., Zhou Z.S., Feature dimension reduction using stacked sparse auto-encoders for crop classification with multi-temporal, quad-pol SAR data, Remote Sens, 12, 2, (2020); Wang J., Hou B., Jiao L., Wang S., POL-SAR image classification based on modified stacked autoencoder network and data distribution, IEEE Trans. Geosci. Remote Sens., 58, 3, pp. 1678-1695, (2019); Hou B., Kou H., Jiao L., Classification of polarimet-ric SAR images using multilayer autoencoders and super-pixels, IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 9, 7, pp. 3072-3081, (2016); Planinsic P., Gleich D., Temporal change detection in SAR images using log cumulants and stacked autoencoder, IEEE Geosci. Remote Sens. Lett., 15, 2, pp. 297-301, (2018); Lv N., Chen C., Qiu T., Sangaiah A.K., Deep learning and superpixel feature extraction based on contractive autoen-coder for change detection in SAR images, IEEE Trans. Ind. Informat., 14, 12, pp. 5530-5538, (2018); Freund Y., Haussler D., Unsupervised Learning of Distributions on Binary Vectors Using Two Layer Networks, (1994); Zhao Z., Jiao L., Zhao J., Gu J., Zhao J., Discriminant deep belief network for high-resolution SAR image classification, Pattern Recognit, 61, pp. 686-701, (2017); Zhao Z., Guo L., Jia M., Wang L., The generalized gamma-DBN for high-resolution SAR image classification, Remote Sens, 10, 6, (2018); Liu F., Jiao L., Hou B., Yang S., POL-SAR image classification based on wishart DBN and local spatial information, IEEE Trans. Geosci. Remote Sens., 54, 6, pp. 3292-3308, (2016); Lv Q., Dou Y., Niu X., Xu J., Xu J., Xia F., Urban land use and land cover classification using remotely sensed SAR data through deep belief networks, J. Sensors, 2015, (2015); Geng J., Wang H., Fan J., Ma X., SAR image classification via deep recurrent encoding neural networks, IEEE Trans. Geosci. Remote Sens., 56, 4, pp. 2255-2269, (2018); Yonel B., Mason E., Yazici B., Deep learning for passive synthetic aperture radar, IEEE J. Sel. Top. Signal Process., 12, 1, pp. 90-103, (2018); Ndikumana E., Ho Tong Minh D., Baghdadi N., Coura-Ult D., Hossard L., Deep recurrent neural network for agricultural classification using multitemporal SAR senti-nel-1 for Camargue France, Remote Sens, 10, 8, (2018); Minh D.H.T., Et al., Deep recurrent neural networks for winter vegetation quality mapping via multitemporal sar sentinel-1, IEEE Geosci. Remote Sens. Lett., 15, 3, pp. 464-468, (2018); Krizhevsky A., Sutskever I., Hinton G., ImageNet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Process. Syst., pp. 1097-1105, (2012); Simonyan K., Zisserman A., Very Deep Convolutional Networks For Large-Scale Image Recognition, (2014); Szegedy C., Et al., Going deeper with convolutions, Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1-9, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 770-778, (2016); Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories tools and challenges for the community, J. Appl. Remote Sens., 11, 4, (2017); Ball J.E., Anderson D.T., Chan C.S., Feature and deep learning in remote sensing applications, J. Appl. Remote Sens., 11, 4, (2018); Song J., Gao S., Zhu Y., Ma C., A survey of remote sensing image classification based on CNNs, Big Earth Data, 3, 3, pp. 232-254, (2019); Gu Y., Wang Y., Li Y., A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection, Appl. Sci., 9, 10, (2019); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag., 4, 2, pp. 22-40, (2016); Zhu X.X., Et al., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geosci. Remote Sens. Mag., 5, 4, pp. 8-36, (2017); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1800-1807, (2017); Yu F., Koltun V., Multi-Scale Context Aggregation By Dilated Convolutions, (2015); Wang Y., Li Y., Song Y., Rong X., The influence of the activation function in a convolution neural network model of facial expression recognition, Appl. Sci., 10, 5, (2020); Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); Ding J., Chen B., Liu H., Huang M., Convolutional neural network with data augmentation for SAR target recognition, IEEE Geosci. Remote Sens. Lett., 13, 3, pp. 364-368, (2016); Chen S., Wang H., Xu F., Jin Y.-Q., Target classification using the deep convolutional networks for SAR images, IEEE Trans. Geosci. Remote Sens., 54, 8, pp. 4806-4817, (2016); Wagner S.A., SAR ATR by a combination of convolu-tional neural network and support vector machines, IEEE Trans. Aerosp. Electron. Syst., 52, 6, pp. 2861-2872, (2016); Gao F., Huang T., Sun J., Wang J., Hussain A., Yang E., A new algorithm of SAR image target recognition based on improved deep convolutional neural network, Cogn. Comput., 5, pp. 1-16, (2018); Zhang F., Wang Y., Ni J., Zhou Y., Hu W., SAR target small sample recognition based on CNN cascaded features and adaboost rotation forest, IEEE Geosci. Remote Sens. Lett, 17, 6, pp. 1008-1012, (2020); Zhou F., Wang L., Bai X., Hui Y., SAR ATR of ground vehicles based on LM-BN-CNN, IEEE Trans. Geosci. Remote Sens., 99, 99, pp. 1-12, (2018); Liu W., Wen Y., Yu Z., Yang M., Large-margin soft-max loss for convolutional neural networks, Proc. Int. Conf. Mach. Learn., pp. 1-1, (2016); Wang L., Bai X., Zhou F., SAR ATR of ground vehicles based on ESENet, Remote Sens, 11, 11, (2019); Fathullah Y., Zhang C., Woodland P.C., Improved large-margin softmax loss for speaker diarisation, Proc. IEEE Int. Conf. Acoust., Speech Signal Process, pp. 7104-7108, (2020); Liu Y., He L., Liu J., Large Margin Softmax Loss For Speaker Verification, (2019); Liu W., Wen Y., Yu Z., Yang M., Large-margin soft-max loss for convolutional neural networks, Proc. Int. Conf. Mach. Learn., pp. 507-516, (2016); Liu W., Wen Y., Yu Z., Li M., Raj B., Song L., SphereFace: Deep hypersphere embedding for face recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recog-nit, pp. 6738-6746, (2017); Shao J., Qu C., Li J., Peng S., A lightweight convolu-tional neural network based on visual attention for SAR image target classification, J. Sensors, 18, 9, (2018); Chen H., Zhang F., Tang B., Yin Q., Sun X., Slim and efficient neural network design for resource-constrained SAR target recognition, Remote Sens., 10, 10, (2018); Min R., Lan H., Cao Z., Cui Z., A gradually distilled CNN for SAR target recognition, IEEE Access, 7, pp. 42190-42200, (2019); Ba J., Caruana R., Do deep nets really need to be deep?, Proc. Adv. Neural Inf. Process. Syst., pp. 2654-2662, (2014); Zhang F., Liu Y., Zhou Y., Yin Q., Li H.C., A lossless lightweight CNN design for SAR target recognition, Remote Sens. Lett., 11, 5, pp. 485-494, (2020); Pei J., Huang Y., Huo W., Zhang Y., Yang J., Yeo T.-S., SAR automatic target recognition based on multi-view deep learning framework, IEEE Trans. Geosci. Remote Sens., 56, 4, pp. 2196-2210, (2018); Pei J., Huang Y., Sun Z., Zhang Y., Yang J., Yeo T.S., Multiview synthetic aperture radar automatic target recognition optimization: Modeling and implementation, IEEE Trans. Geosci. Remote Sens., 56, 11, pp. 6425-6439, (2018); Zhao P., Liu K., Zou H., Zhen X., Multi-stream con-volutional neural network for SAR automatic target recognition, Remote Sens, 10, 9, (2018); Wang N., Wang Y., Liu H., Zuo Q., He J., Feature-fused SAR target discrimination using multiple convolu-tional neural networks, IEEE Geosci. Remote Sens. Lett., 14, 10, pp. 1695-1699, (2017); Cho J.H., Park C.G., Multiple feature aggregation using convolutional neural networks for SAR image-based automatic target recognition, IEEE Geosci. Remote Sens. Lett., 15, 12, pp. 1882-1886, (2018); Bai X., Zhou X., Zhang F., Wang L., Xue R., Zhou F., Robust Pol-ISAR target recognition based on ST-MC-DCNN, IEEE Trans. Geosci. Remote Sens., 57, 12, pp. 9912-9927, (2019); Jaderberg M., Simonyan K., Zisserman A., Kavuk-Cuoglu K., Spatial transformer networks, Proc. Adv. Neural Inf. Process. Syst., pp. 2017-2025, (2015); Finnveden L., Jansson Y., Lindeberg T., Understanding when spatial transformer networks do not support invariance, and what to do about it, Proc. 25th IEEE Int. Conf. Pattern Recognit, pp. 3427-3434, (2020); Lin Z., Ji K., Kang M., Leng X., Zou H., Deep convo-lutional highway unit network for SAR target classification with limited labeled training data, IEEE Geosci. Remote Sens. Lett., 14, 7, pp. 1091-1095, (2017); Srivastava R.K., Greff K., Schmidhuber J., Training very deep networks, Proc. Adv. Neural Inf. Process. Syst., pp. 2377-2385, (2015); Yue Z., Et al., A novel semi-supervised convolutional neural network method for synthetic aperture radar image recognition, Cogn. Comput., 13, pp. 795-806, (2021); Huang L., OpenSARShip: A dataset dedicatedto sen-tinel-1 ship interpretation, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, 1, pp. 195-208, (2018); Li J., Qu C., Shao J., Ship detection in SAR images based on an improved faster R-CNN, Proc. SAR Big Data Era, Models, Methods Appl, pp. 1-6, (2017); Liu N., Cao Z., Cui Z., Pi Y., Dang S., Multi-scale proposal generation for ship detection in SAR images, Remote Sens, 11, 5, pp. 526-546, (2019); Ohki M., Shimada M., Large-area land use and land cover classification with quad compact and dual polarization SAR data by PALSAR-2, IEEE Trans. Geosci. Remote Sens., 56, 9, pp. 5550-5557, (2018); Xu Z., Wang R., Zhang H., Li N., Zhang L., Building extraction from high-resolution SAR imagery based on deep neural networks, Remote Sens. Lett., 8, 9, pp. 888-896, (2017); Shahzad M., Maurer M., Fraundorfer F., Wang Y., Zhu X.X., Buildings detection in VHR SAR images using fully convolution neural networks, IEEE Trans. Geosci. Remote Sens., 57, 2, pp. 1100-1116, (2019); Li Y., Martinis S., Wieland M., Urban flood mapping with an active self-learning convolutional neural network based on TerraSAR-X intensity and interferometric coherence, ISPRS J. Photogramm. Remote Sens., 152, pp. 178-191, (2019); Lin Y.N., Yun S.H., Bhardwaj A., Hill E.M., Urban flood detection with sentinel-1 multi-temporal synthetic aperture radar (SAR) observations in a bayesian framework: A case study for hurricane matthew, Remote Sens, 11, 15, (2019); Chen S.W., Tao C.S., PolSAR image classification using polarimetric-feature-driven deep convolutional neural network, IEEE Geosci. Remote Sens. Lett., 15, 4, pp. 627-631, (2018); Chen S.W., Wang X.S., Sato M., Uniform polarimet-ric matrix rotation theory and its applications, IEEE Trans. Geosci. Remote Sens., 52, 8, pp. 4756-4770, (2014); Zhou Y., Wang H., Xu F., Jin Y.-Q., Polarimetric SAR image classification using deep convolutional neural networks, IEEE Geosci. Remote Sens. Lett., 13, 12, pp. 1935-1939, (2016); Singh G., Mohanty S., Yamazaki Y., Yamaguchi Y., Physical scattering interpretation of POLSAR coherency matrix by using compound scattering phenomenon, IEEE Trans. Geosci. Remote Sens., 58, 4, pp. 2541-2556, (2019); Liu X., Jiao L., Tang X., Sun Q., Zhang D., Polarimetric convolutional network for PolSAR image classification, IEEE Trans. Geosci. Remote Sens., 57, 5, pp. 3040-3054, (2019); Wang L., Xu X., Dong H., Gui R., Pu F.L., Multi-pixel simultaneous classification of PolSAR image using convo-lutional neural networks, Sensors, 18, 3, (2018); Ahishali M., Kiranyaz S., Ince T., Gabbouj M., Dual and single polarized SAR image classification using compact convolutional neural networks, Remote Sens., 11, 11, (2019); Bi H., Sun J., Xu Z., A graph-based semisupervised deep learning model for PolSAR image classification, IEEE Trans. Geosci. Remote Sens., 57, 4, pp. 2116-2132, (2019); Bi H., Xu F., Wei Z., Xue Y., Xu Z., An active deep learning approach for minimally supervised PolSAR image classification, IEEE Trans. Geosci. Remote Sens., 57, 11, pp. 9378-9395, (2019); Xie W., Et al., PolSAR image classification via Wishart-AE model or wishart-CAE model, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 10, 8, pp. 3604-3615, (2017); Ren Z., Hou B., Wen Z., Jiao L., Patch-sorted deep feature learning for high resolution SAR image classification, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, 9, pp. 3113-3126, (2018); Wang L., Scott K.A., Xu L., Clausi D.A., Sea ice concentration estimation during melt from dual-pol SAR scenes using deep convolutional neural networks: A case study, IEEE Trans. Geosci. Remote Sens., 54, 8, pp. 4524-4533, (2016); Wang L., Scott K.A., Clausi D.A., Sea ice concentration estimation during freeze-up from SAR imagery using a convolutional neural network, Remote Sens, 9, 5, (2017); Cooke C.L., Scott K.A., Estimating sea ice concentration from SAR: Training convolutional neural networks with passive microwave data, IEEE Trans. Geosci. Remote Sens., 57, 7, pp. 4735-4747, (2019); Scarpa G., Gargiulo M., Mazza A., Gaetano R., A CNN-based fusion method for feature extraction from sentinel data, Remote Sens, 10, 2, (2018); Wang J., Lu C., Jiang W., Simultaneous ship detection and orientation estimation in SAR images based on attention module and angle regression, Sensors, 18, 9, (2018); Song T., Kuang L., Han L., Wang Y., Liu Q.H., Inversion of rough surface parameters from SAR images using simulation-trained convolutional neural networks, IEEE Geosci. Remote Sens. Lett., 15, 7, pp. 1130-1134, (2018); Wang P., Zhang H., Patel V.M., SAR image des-peckling using a convolutional neural network, IEEE Signal Process. Lett., 24, 12, pp. 1763-1767, (2017); Wang J., Zheng T., Lei P., Bai X., Ground target classification in noisy SAR images using convolutional neural networks, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, 11, pp. 4180-4192, (2018); Pan T., Peng D., Yang W., Li H.C., A filter for SAR image despeckling using pre-trained convolutional neural network model, Remote Sens., 11, 20, (2019); Zhang K., Zuo W., Zhang L., FFDNet: Toward a fast and flexible solution for CNN based image denoising, IEEE Trans. Image Process., 27, 9, pp. 4608-4622, (2018); Zhang Q., Yuan Q., Li J., Yang Z., Ma X., Learning a dilated residual network for SAR image despeckling, Remote Sens, 10, 2, (2018); Hamaguchi R., Fujita A., Nemoto K., Imaizumi T., Hikosaka S., Effective use of dilated convolutions for segmenting small object instances in remote sensing imagery, Proc. IEEE Winter Conf. Appl. Comput. Vis, pp. 1442-1450, (2018); Liu S., Et al., Convolutional neural network and guided filtering for SAR image denoising, Remote Sens, 11, 6, (2019); Mukherjee S., Zimmer A., Kottayil N.K., Sun X., Ghu-Man P., Cheng I., CNN-based InSAR denoising and coherence metric, Proc. IEEE Sensors, pp. 28-31, (2018); Lattari F., Leon B.G., Asaro F., Rucci A., Prati C., Matteucci M., Deep learning for SAR image despeckling, Remote Sens, 11, 13, (2019); Li J., Li Y., Xiao Y., Bai Y., HDRANet: Hybrid dilated residual attention network for SAR image despeckling, Remote Sens., 11, 24, (2019); Woo S., Park J., Lee J.-Y., Kweon I.S., CBAM: Convolutional block attention module, Proc. Eur. Conf. Comput. Vis, pp. 3-19, (2018); Romero A., Gatta C., Camps-Valls G., Unsupervised deep feature extraction for remote sensing image classification, IEEE Trans. Geosci. Remote Sens., 54, 3, pp. 1349-1362, (2016); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recog, pp. 3431-3440, (2015); Henry C., Azimi S.M., Merkle N., Road segmentation in SAR satellite images with deep fully convolutional neural networks, IEEE Geosci. Remote Sens. Lett., 15, 12, pp. 1867-1871, (2018); Chen L.C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-decoder with atrous separable convolution for semantic image segmentation, Proc. Eur. Conf. Comput. Vis., pp. 833-851, (2018); Zhang Z., Liu Q., Wang Y., Road extraction by deep residual U-net, IEEE Geosci. Remote Sens. Lett., 15, 5, pp. 749-753, (2018); Mohammadimanesh F., Salehi B., Mahdianpari M., Gill E., Molinier M., A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem, ISPRS J. Pho-togramm. Remote Sens., 151, pp. 223-236, (2019); Ronneberger O., Fischer P., Brox T., U-net: Convolu-tional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervention, pp. 234-241, (2015); Krestenitis M., Orfanidis G., Ioannidis K., Avgerinakis K., Vrochidis S., Kompatsiaris I., Oil spill identification from satellite images using deep neural networks, Remote Sens, 11, 15, (2019); Cantorna D., Dafonte C., Iglesias A., Arcay B., Oil spill segmentation in SAR images using convolutional neural networks. A comparative analysis with clustering and logistic regression algorithms, Appl. Soft Comput., 84, (2019); Duan Y., Liu F., Jiao L., Zhao P., Zhang L., SAR image segmentation based on convolutional-wavelet neural network and Markov random field, Pattern Recognit, 64, pp. 255-267, (2017); Martino G.D., Iodice A., Riccio D., Ruello G., A novel approach for disaster monitoring: Fractal models and tools, IEEE Trans. Geosci. Remote Sens., 45, 6, pp. 1559-1570, (2007); Hame T., Heiler I., Miguel-Ayanz J.S., An unsuper-vised change detection and recognition system for forestry, Int. J. Remote Sens., 19, 6, pp. 1079-1099, (1998); Tsai D.-M., Lai S.-C., Independent component analysis-based background subtraction for indoor surveillance, IEEE Trans. Image Process., 18, 1, pp. 158-167, (2009); Asokan A., Anitha J., Change detection techniques for remote sensing applications: A survey, Earth Sci. Inform., 12, 2, pp. 143-160, (2019); Liu J., Gong M., Qin K., Zhang P., A deep convolu-tional coupling network for change detection based on heterogeneous optical and radar images, IEEE Trans. Neural Netw. Learn. Syst., 29, 3, pp. 545-559, (2016); Gong M., Yang H., Zhang P., Feature learning and change feature classification based on deep learning for ternary change detection in SAR images, J. Photogramm. Remote Sens., 129, pp. 212-225, (2017); Iino S., Ito R., Doi K., Imaizumi T., Hikosaka S., CNN-based generation of high-accuracy urban distribution maps utilising SAR satellite imagery for short-term change monitoring, Int. J. Image Data Fusion, 9, 4, pp. 302-318, (2018); Cao X., Ji Y., Wang L., Ji B., Jiao L., Han J., SAR image change detection based on deep denoising and CNN, IET Image Process, 13, 9, pp. 1509-1515, (2019); Gao F., Wang X., Gao Y., Dong J., Wang S., Sea ice change detection in SAR images based on convolutional-wavelet neural networks, IEEE Geosci. Remote Sens. Lett., 16, 8, pp. 1240-1244, (2019); Gao Y., Gao F., Dong J., Wang S., Transferred deep learning for sea ice change detection from synthetic-aperture radar images, IEEE Geosci. Remote Sens. Lett., 16, 10, pp. 1655-1659, (2019); Gao Y., Gao F., Dong J., Wang S., Change detection from synthetic aperture radar images based on channel weighting-based deep cascade network, IEEE J. Select. Topics Appl. Earth Observ. Remote Sens., 12, 11, pp. 4517-4529, (2019); Liu F., Jiao L., Tang X., Yang S., Ma W., Hou B., Local restricted convolutional neural network for change detection inpolarimetric SAR images, IEEE Trans. Neural Netw. Learn. Syst., 30, 3, pp. 818-833, (2019); Dong H., Ma W., Wu Y., Gong M., Jiao L., Local descriptor learning for change detection in synthetic aperture radar images via convolutional neural networks, IEEE Access, 7, pp. 15389-15403, (2019); Bromley J., Et al., Signature verification using a 'Siamese' time delay neural network, Int. J. Pattern Recognit. Artif. Intell., 7, 4, pp. 669-688, (1993); Ruan W., Et al., Ticnet: A target-insight correlation network for object tracking, IEEE Trans. Cybern., early access, (2021); Pang H., Et al., Research on target tracking algorithm based on siamese neural network, Mobile Inf. Syst., 4, pp. 1-11, (2021); Hughes L.H., Schmitt M., Mou L., Wang Y., Zhu X.X., Identifying corresponding patches in SAR and optical images with a pseudo-siamese CNN, IEEE Geosci. Remote Sens. Lett., 15, 5, pp. 784-788, (2018); Chen L., Et al., A new deep learning algorithm for SAR scene classification based on spatial statistical modeling and features re-Calibration, Sensors, 19, 11, (2019); Kang M., Ji K., Leng X., Lin Z., Contextual region-based convolutional neural network with multilayer fusion for SAR ship detection, Remote Sens, 9, 8, (2017); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recog, pp. 580-587, (2014); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 779-788, (2016); Liu W., Et al., SSD: Single shot multibox detector, Proc. Eur. Conf. Comput. Vis., pp. 21-37, (2016); Alganci U., Soydas M., Sertel E., Comparative research on deep learning approaches for airplane detection from very high-resolution satellite images, Remote Sens., 12, 3, (2020); Kim J.A., Sung J.Y., Park S.H., Comparison of Faster-RCNN, YOLO, and SSD for real-time vehicle type recognition, Proc. IEEE Int. Conf. Consum. Electron.-Asia, pp. 1-4, (2020); Cui Z., Dang S., Cao Z., Wang S., Liu N., Sar target recognition in large scene images via region-based convo-lutional neural networks, Remote Sens, 10, 5, (2018); Zhao J., Zhang Z., Yu W., Truong T.-K., A cascade coupled convolutional neural network guided visual attention method for ship detection from SAR images, IEEE Access, 6, pp. 50693-50708, (2018); Bentes C., Velotto D., Tings B., Ship classification in terrasar-x images with convolutional neural networks, IEEE J. Ocean. Eng., 43, 1, pp. 258-266, (2018); Wang Y., Wang C., Zhang H., Ship classification in high-resolution SAR images using deep learning of small datasets, J. Sensors, 18, 9, (2018); Szegedy C., Vincent V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proc. IEEE Conf. Comput. Vis. Pattern Recog-nit, pp. 2818-2826, (2016); Lin Z., Ji K., Leng X., Kuang G., Squeeze and excitation rank faster R-CNN for ship detection in SAR images, IEEE Geosci. Remote Sens. Lett., 16, 5, pp. 751-755, (2019); Hu J., Shen L., Sun G., Squeeze-and-excitation networks, Proc. Conf. Comput. Vis. Pattern Recognit., pp. 7132-7141, (2018); Zhang T., Zhang X., High-speed ship detection in SAR images based on a grid convolutional neural network, Remote Sens., 11, 10, (2019); Ai J., Tian R., Luo Q., Jin J., Tang B., Multi-scale rotation-invariant Haar-like feature integrated CNN-based ship detection algorithm of multiple-target environment in SAR imagery, IEEE Trans. Geosci. Remote Sens., 57, 12, pp. 10070-10087, (2019); Lienhart R., Maydt J., An extended set of haar-like features for rapid object detection, Proc. IEEE Int. Conf. Image Process, 1, pp. 900-903, (2002); Deng Z., Sun H., Zhou S., Zhao J., Learning deep ship detector in SAR images from scratch, IEEE Trans. Geosci. Remote Sens., 57, 6, pp. 4021-4039, (2019); LabelImg. Git Code, (2015); Zhang T., Zhang X., Shi J., Wei S., Depthwise separable convolution neural network for high-speed SAR ship detection, Remote Sens., 11, 21, (2019); Ma M., Chen J., Liu W., Yang W., Ship classification and detection based on CNN using GF-3 SAR images, Remote Sens, 10, 12, (2018); An Q., Pan Z., You H., Ship detection in Gaofen-3 SAR images based on sea clutter distribution analysis and deep convolutional neural network, Sensors, 18, 2, (2018); Wang Y., Wang C., Zhang H., Dong Y., Wei S., Automatic ship detection based on retinanet using multi-resolution Gaofen-3 imagery, Remote Sens., 11, 5, pp. 531-544, (2019); Liu J., Qiu X., Hong W., Automated ortho-rectified SAR image of GF-3 satellite using reverse-range-Doppler method, Proc. IEEE Geosci. Remote Sens. Symp, pp. 4445-4448, (2015); Chen C., He C., Hu C., Pei H., Jiao L., MSARN: A deep neural network based on an adaptive recalibration mechanism for multiscale and arbitrary-oriented SAR ship detection, IEEE Access, 7, pp. 159262-159283, (2019); Zhang X., Et al., A lightweight feature optimizing network for ship detection in SAR image, IEEE Access, 7, pp. 141662-141678, (2019); Lu C., Li W., Ship classification in high-resolution SAR images via transfer learning with small training data-set, Sensors, 19, 1, (2019); Tan X., Li M., Zhang P., Wu Y., Song W., Complex-valued 3-D convolutional neural network for PolSAR image classification, IEEE Geosci. Remote Sens. Lett., 17, 6, pp. 1022-1026, (2020); Guberman N., On Complex Valued Convolutional Neural Networks, (2016); Tygert M., Bruna J., Chintala S., LeCun Y., Piantino S., Szlam A., A mathematical motivation for complex-valued convolutional networks, Neural Comput., 28, 5, pp. 815-825, (2016); Zhang Z., Wang H., Xu F., Jin Y.-Q., Complex-valued convolutional neural network and its application in polari-metric SAR image classification, IEEE Trans. Geosci. Remote Sens., 55, 12, pp. 7177-7188, (2017); Gao J., Deng B., Qin Y., Wang H., Li X., Enhanced radar imaging using a complex-valued convolutional neural network, IEEE Geosci. Remote Sens. Lett., 16, 1, pp. 35-39, (2019); Sunaga Y., Natsuaki R., Hirose A., Land form classification and similar land-shape discovery by using complex-valued convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 57, 10, pp. 24-38, (2019); Cao Y., Wu Y., Zhang P., Liang W., Li M., Pixel-wise PolSAR image classification via a novel complex-valued deep fully convolutional network, Remote Sens, 11, 22, (2019); Yu L., Hu Y., Xie X., Lin Y., Hong W., Complex-valued full convolutional neural network for SAR target classification, IEEE Geosci. Remote Sens. Lett., 16, 1, pp. 1752-1756, (2019); Wang X., Wang H., Forest height mapping using complex-valued convolutional neural network, IEEE Access, 7, pp. 126334-126343, (2019); Li L., Ma L., Jiao L., Liu F., Sun Q., Zhao J., Complex contourlet-CNN for polarimetric SAR image classification, Pattern Recognit., 100, (2020); Mu H., Zhang Y., Ding C., Jiang Y., Er M.H., Kot A.C., DeepImaging: A ground moving target imaging based on CNN for SAR-GMTI system, IEEE Geosci. Remote Sens. Lett., 18, 1, pp. 117-121, (2021); Kwak Y., Song W.-J., Kim S.-E., Speckle-noise-invariant convolutional neural network for SAR target recognition, IEEE Geosci. Remote Sens. Lett., 16, 4, pp. 549-553, (2019); Lee J.-S., Wen J.-H., Ainsworth T.L., Chen K.-S., Chen A.J., Improved sigma filter for speckle filtering of SAR imagery, IEEE Trans. Geosci. Remote Sens., 47, 1, pp. 202-213, (2009); Du K., Deng Y., Wang R., Zhao T., Li N., SAR ATR based on displacement-and rotation-insensitive CNN, Remote Sens. Lett., 7, 9, pp. 895-904, (2016); Lv J., Liu Y., Data augmentation based on attributed scattering centers to train robust CNN for SAR ATR, IEEE Access, 7, pp. 25459-25473, (2019); Potter L.C., Moses R.L., Attributed scattering centers for SAR ATR, IEEE Trans. Image Process., 6, 1, pp. 79-91, (1997); Kechagias-Stamatis O., Aouf N., Automatic target recognition on synthetic aperture radar imagery: A survey, IEEE Aerosp. Electron. Syst. Mag., 36, 3, pp. 56-81, (2021); Jiang C., Zhou Y., Hierarchical fusion of convolu-tional neural networks and attributed scattering centers with application to robust SAR ATR, Remote Sens., 10, 6, (2018); Wang K., Zhang G., Leng Y., Leung H., Synthetic aperture radar image generation with deep generative models, IEEE Geosci. Remote Sens. Lett., 16, 6, pp. 912-916, (2019); Tolstikhin I., Bousquet O., Gelly S., Schoelkopf B., Wasserstein Auto-Encoders, (2018); Lu X., Wang Y., Yuan Y., Sparse coding from a Bayesian perspective, IEEE Trans. Neural Netw. Learn. Syst., 24, 6, pp. 929-939, (2013); Kechagias-Stamatis O., Aouf N., Fusing deep learning and sparse coding for SAR ATR, IEEE Trans. Aerosp. Electron. Syst., 55, 2, pp. 785-797, (2019); Gao H., Peng S., Zeng W., Recognition of targets in SAR images using joint classification of deep features fused by multi-canonical correlation analysis, Remote Sens. Lett., 10, 9, pp. 883-892, (2019); Lee S.H., Choi S., Two-dimensional canonical correlation analysis, IEEE Signal Process. Lett., 14, 10, pp. 735-738, (2007); Zhou Y., Chen Y., Gao R., Feng J., Zhao P., Wang L., SAR target recognition via joint sparse representation of monogenic components with 2D canonical correlation analysis, IEEE Access, 7, pp. 25815-25826, (2019); Tropp J.A., Gilbert A.C., Strauss M.J., Algorithms for simultaneous sparse approximation. Part I: Greedy pursuit, J. Signal Process., 86, pp. 572-588, (2006); Zhang H., Nasrabadi N., Zhang Y., Huang T.S., Multi-view automatic target recognition using joint sparse representation, IEEE Trans. Aerosp. Electron. Syst., 48, 3, pp. 2481-2495, (2011); Lv J., Exploiting multi-level deep features via joint sparse representation with application to SAR target recognition, Int. J. Remote Sens., 41, 1, pp. 320-338, (2020); Li Y., Chen Y., Liu G., Jiao L., A novel deep fully convolutional network for PolSAR image classification, Remote Sens., 10, 12, (2018); Huang Z., Pan Z., Lei B., What, where, and how to transfer in SAR target recognition based on deep CNNs, IEEE Trans. Geosci. Remote Sens., 58, 4, pp. 1-13, (2019); Malmgren-Hansen D., Kusk A., Dall J., Nielsen A.A., Engholm R., Skriver H., Improving SAR automatic target recognition models with transfer learning from simulated data, IEEE Geosci. Remote Sens. Lett., 14, 9, pp. 1484-1488, (2017); Wang K., Zhang G., Leung H., SAR target recognition based on cross-domain and cross-task transfer learning, IEEE Access, 7, pp. 153391-153399, (2019); Tzeng E., Hoffman J., Saenko K., Darrell T., Adversarial discriminative domain adaptation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 7167-7176, (2017); Huang Z., Pan Z., Lei B., Transfer learning with deep convolutional neural network for SAR target classification with limited labeled data, Remote Sens., 9, 9, (2017); Cui Z., Tang C., Cao Z., Dang S., SAR unlabeled target recognition based on updating CNN with assistant decision, IEEE Geosci. Remote Sens. Lett., 15, 10, pp. 1585-1589, (2018)","","","Institute of Electrical and Electronics Engineers Inc.","","","","","","08858985","","IESME","","English","IEEE Aerosp Electron Syst Mag","Article","Final","","Scopus","2-s2.0-85122094962"
"Xu H.; Huang D.; He Q.; Du Y.; Qin X.","Xu, Huifang (57208439086); Huang, Dongmei (16303959000); He, Qi (57195506496); Du, Yanling (55350371200); Qin, Xuebiao (57374712300)","57208439086; 16303959000; 57195506496; 55350371200; 57374712300","Ocean front detection method based on improved Mask R-CNN; [改进Mask R-CNN模型的海洋锋检测]","2021","Journal of Image and Graphics","26","12","","2981","2990","9","0","10.11834/jig.200599","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121289611&doi=10.11834%2fjig.200599&partnerID=40&md5=284aecd6b53284bb788e25486f664e31","College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China; College of Information Technology, Shanghai Jian Qiao University, Shanghai, 201306, China; Shanghai University of Electric Power, Shanghai, 201306, China","Xu H., College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China, College of Information Technology, Shanghai Jian Qiao University, Shanghai, 201306, China; Huang D., College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China, Shanghai University of Electric Power, Shanghai, 201306, China; He Q., College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China; Du Y., College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China; Qin X., College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China","Objective: The efficient detection of ocean front is of great significance to study the efficient detection of ocean front for marine ecosystem, fishery resources assessment, fishery forecast and typhoon track prediction. Gradient threshold method and edge detection algorithm have been widely used in ocean front detection. Traditional gradient method mainly depends on the gradient threshold, the sea area with gradient value greater than the set threshold has been regarded as the existence of ocean front. However, the selection criteria of threshold cannot tailor the requirements of accurate detection of complex and diverse ocean fronts due to artificial setting dependence, so it is more suitable for the object detection with fixed edge (such as land). During to the weak edge information of ocean front, it is difficult to achieve the good effect through the traditional edge extraction algorithm. A new automatic detection method to detect the small data volume and weak marginal characteristics of ocean fronts has been considering. Based on the advantages of the Mask R-CNN (region convolutional neural network) for instance segmentation, an improved Mask R-CNN network has been applied to the detection of ocean fronts. The ocean front detection method based on the modified Mask R-CNN has evolved the establishment of ocean front detection standards and data preprocessing, such as data expansion, data enhancement and labeling operations. High-precision detection of ocean fronts has been realized based on multiple iterations of training and parameter correction. Method First, the remote sensing images have been performed expansion operations for the small amount of data and the weak edge characteristics, such as rotating, flipping and cropping. Total 2 100 images have been obtained including 800 original images, 500 rotation and flip processing images and 800 random cropping processing images. Meanwhile, sea surface temperatures (SST) remote sensing images have been enhanced based on deep closest point (DCP) and contrast limited adaptive histogram equalization (CLAHE) algorithms. Next, based on migration learning, using the general image classification network model trained on the common objects in context (COCO) dataset as the pre-training model, and using training datasets to train the pre-trained model. In order to meet the needs of ocean front detection, the residual network (ResNet) and feature pyramid network (FPN) model in Mask R-CNN have been optimized respectively. Limited training data leads to over fitting of the deep residual network and poor detection results. Considering the scarcity of ocean front data and the difficulty of constructing training set, the shallow ResNet-18 network has been used to detect ocean front. Multi-scale fusion feature maps have been predicted to enhance the detection effect of ocean front respectively through the full use of the high-resolution and high-level semantic information of low-level features. Result In order to verify the effectiveness of the method, three training datasets of grayscale image, RGB image and gradient image have been designed. Using LabelMe software to label the dataset, and then achieving high-precision detection of ocean fronts by multiple iterations of training and parameter correction. In addition, the weighted harmonic mean Micro-F1 and intersection over union (IoU) have both been used to evaluate the detection accuracy and target location accuracy of the model. In the experiment and analysis section, several groups of comparative experiments have been designed from experimental model. In order to evaluate the robustness and effectiveness of the method, images collection of global ocean fronts has been conducted to make three different datasets and perform multiple different iterations on the datasets. The results have shown that the training has effectively converged, and the detection accuracy of the model has also increased, reaching more than 0.85 after 25 000 iterations. In order to further verify the proposed ocean front detection results based on Mask R-CNN, the three training sets have been trained separately. The experimental results have shown that gradient images have been both higher than RGB images and grayscale images to detect ocean fronts, the positioning accuracy and detection accuracy. In order to highlight the advantages of this model for ocean front detection, this research has compared it with you only look once (YOLOv3) and Mask R-CNN models under three different datasets. The three training datasets have been trained for 30 000 times under different models. The results have demonstrated that the positioning accuracy IoU and accuracy F1 of the ocean front detection method proposed are improved. The detection accuracy of this method is 84.33%, and Micro-F1 is 86.57%. Compared with the YOLOv3 and Mask R-CNN algorithms, the Micro-F1 value has increased by 4.27% and 3.01% respectively. Rapid identification of ocean fronts is the key to practical fishery applications. The running time of the RGB image set under different network models and different iteration times has been presented. Under different iterations, the proposed model takes much less time than YOLOv3. At last, in order to evaluate the effectiveness of the method in the detection of strong and weak fronts, the strong and weak fronts in the three datasets have been screened and trained separately. The results have shown that the accuracy of the method in the detection of strong ocean fronts can be achieved all above 80% higher. Conclusion: A reasonable ocean front detection standard has been setup combined with the weak edge characteristics of ocean front. By designing some comparative experiments to verify the high-precision detection effect of the method proposed in this paper on ocean fronts. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","Deep learning; Image enhancement; Mask R-CNN; Ocean front detection; Weak edge","","","","","","National Natural Science Foundation of China, NSFC, (41671431, 41906179); 上海市教育发展基金项目, (AASH2004); 国家海洋局数字海洋科学技术重点实 验室开放基金项目, (B201801029); 国家自然科学基金项目, (41671431,41906179)","收稿日期:2020-10-22;修回日期:2020-12-25;预印本日期:2021-01-03 ∗通信作者:黄冬梅　 dmhuang@ shou. edu. cn 基金项目:国家自然科学基金项目(41671431,41906179);上海市教育发展基金项目(AASH2004);国家海洋局数字海洋科学技术重点实 验室开放基金项目(B201801029) Supported by:National Natural Science Foundation of China (41671431, 41906179)","Bost C A, Cotte C, Bailleul F, Cherel Y, Charrassin J B, Guinet C, Ainley D G, Weimerskirch H., The importance of oceanographic fronts to marine birds and mammals of the southern oceans, Journal of Marine Systems, 78, 3, pp. 363-376, (2009); Cao W D, Xie C, Han B, Dong J Y., Automatic fine recognition of ocean front fused with deep learning, Computer Engineering, 46, 10, pp. 266-274, (2020); Cayula J F, Cornillon P., Multi-image edge detections for SST images, Journal of Atmospheric and Oceanic Technology, 12, 4, pp. 821-829, (1995); Chang P, Yan P F., Wavelet scale space filtering on edge detection, Pattern Recognition and Artificial Intelligence, 9, 3, pp. 251-257, (1996); Davis L S., A survey of edge detection techniques, Computer Graphics and Image Processing, 4, 3, pp. 248-270, (1975); Deng L, Wang Y Q, Liu Y, Wang F, Li S K, Liu J., A CNN-based vortex identification method, Journal of Visualization, 22, 1, pp. 65-78, (2019); He K, Gkioxari G, Dollar P, Girshick R., Mask R-CNN, Proceedings of 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988, (2017); Hu Y, Shan Z L, Gao F., Ship detection based on faster-RCNN and multiresolution SAR, Radio Engineering, 48, 2, pp. 96-100, (2018); Kostianoy A G, Ginzburg A I, Frankignoulle M, Delille B., Fronts in the Southern Indian Ocean as inferred from satellite sea surface temperature data, Journal of Marine Systems, 45, pp. 55-73, (2004); Lima E, Sun X, Dong J Y, Wang H, Yang Y T, Liu L P., Learning and transferring convolutional neural network knowledge to ocean front recognition, IEEE Geoscience and Remote Sensing Letters, 14, 3, pp. 354-358, (2017); Li A Z, Zhou W F, Fan X M., Research progress of methods for the extraction of mesoscale ocean fronts and eddies based on remote sensing data, Chinese Journal of Image Graphics, 22, 6, pp. 709-718, (2017); Li Q Y, Zhong G Q, Xie C., Weak edge identification nets for ocean front detection [EB/OL], (2019); Lima E, Sun X, Yang Y T, Dong J Y., Application of deep convolutional neural networks for ocean front recognition, Journal of Applied Remote Sensing, 11, 4, (2017); Liu Z., The Spatio-Temporal Variability of Oceanic Fronts offshore China Seas and Analysis of Marine Observations, (2012); Nieto K, Demarcq H, McClatchie S., Mesoscale frontal structures in the Canary Upwelling System: new front and filament detection algorithms applied to spatial and temporal patterns, Remote Sensing of Environment, 123, pp. 339-346, (2012); Ou P, Lu K, Zhang Z, Liu Z Y., Target recognition and spatial location based on mask RCNN, Computer Measurement and Control, 27, 6, pp. 172-176, (2019); Pi Q L, Hu J Y., Analysis of sea surface temperature fronts in the Taiwan Strait and its adjacent area using an advanced edge detection method, Science China Earth Sciences, 53, 7, pp. 1008-1016, (2010); Ping B., Research on Oceanic Field Recovery and Frontal Detection, (2015); Ping B, Su F Z, Meng Y S, Fang S H, Du Y Y., A model of sea surface temperature front detection based on a threshold interval, Acta Oceanologica Sinica, 33, 7, pp. 65-71, (2014); Ren S H, Liu N, Wang H., Review of ocean front in Chinese marginal seas and frontal forecasting, Advances in Earth Science, 30, 5, pp. 552-563, (2015); Shao L J, Zhang H L, Zhang C H, Zhou X., A method for detecting the oceanic front using remotely sensed sea-surface temperature, Hydrographic Surveying and Charting, 35, 2, pp. 42-44, (2015); Sun X, Wang C G, Dong J Y, Lima E, Yang Y T., A multiscale deep framework for ocean fronts detection and fine-grained location, IEEE Geoscience and Remote Sensing Letters, 16, 2, pp. 178-182, (2019); Wang Z, He S X., An adaptive edge-detection method based on Canny algorithm, Journal of Image and Graphics, 9, 8, pp. 957-962, (2004); Xue C J, Su F Z, Zhou J Q., Extraction of ocean fronts based on wavelet analysis, Marine Science Bulletin, 26, 2, pp. 20-27, (2007); Zhang L Y., Characteristics and Influences of the Northwest Pacific Subtropical Sea Surface Temperature Front, (2018); Zhang W, Cao Y, Luo Y., An ocean front detection method based on the Canny operator and mathematical morphology, Marine Science Bulletin, 33, 2, pp. 199-203, (2014); Zhao H., Research on Image Edge Detection Based on Mathematical Morphology, (2010); Zhou L, Chen D K, Lei X T, Wang W, Wang G H, Han G J., Progress and perspective on interactions between ocean and typhoon, Chinese Science Bulletin, 64, 1, pp. 60-72, (2019)","D. Huang; College of Information Technology, Shanghai Ocean University, Shanghai, 201306, China; email: dmhuang@shou.edu.cn; D. Huang; Shanghai University of Electric Power, Shanghai, 201306, China; email: dmhuang@shou.edu.cn","","Editorial and Publishing Board of JIG","","","","","","10068961","","","","Chinese","J. Image and Graphics","Article","Final","","Scopus","2-s2.0-85121289611"
"Andersen R.E.; Nalpantidis L.; Ravn O.; Boukas E.","Andersen, Rasmus Eckholdt (57206184543); Nalpantidis, Lazaros (8624502900); Ravn, Ole (6701391383); Boukas, Evangelos (51261053200)","57206184543; 8624502900; 6701391383; 51261053200","Simultaneous regression-based spatial coverage estimation and object detection with deep learning","2021","Electronics Letters","57","16","","605","607","2","3","10.1049/ell2.12183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122072554&doi=10.1049%2fell2.12183&partnerID=40&md5=82612974f20448d2123c3014aaf8fc0f","Section of Automation and Control, Department of Electrical Engineering, Technical University of Denmark, Kongens, Lyngby, Denmark","Andersen R.E., Section of Automation and Control, Department of Electrical Engineering, Technical University of Denmark, Kongens, Lyngby, Denmark; Nalpantidis L., Section of Automation and Control, Department of Electrical Engineering, Technical University of Denmark, Kongens, Lyngby, Denmark; Ravn O., Section of Automation and Control, Department of Electrical Engineering, Technical University of Denmark, Kongens, Lyngby, Denmark; Boukas E., Section of Automation and Control, Department of Electrical Engineering, Technical University of Denmark, Kongens, Lyngby, Denmark","Object detection has been in the focus of researchers within varying applications propelled by the recent advances in deep learning and neural networks. Many applications require both detection of class instances as well as a quantification of the spatial coverage of the class instances. While the performance of deep learning approaches for these tasks has been extensively studied there has not been much effort into creating a unified network structure to achieve both goals. The purpose of this paper is to present a regressor to the faster R-CNN architecture that can help quantify the spatial coverage estimation of some detected object. The goal of the regressor is to provide a reproducible result of the spatial coverage. To demonstrate the developed architecture, an example use-case of land cover estimation is used. The experiments conducted in this paper show that the network does not sacrifice object detection accuracy, and indicate that the network is able to estimate the spatial coverage of six different types of land. © 2021 The Authors. Electronics Letters published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","","Deep learning; Network architecture; Object recognition; Coverage estimations; Land cover; Learning approach; Learning network; Network structures; Neural-networks; Performance; Spatial coverage; Object detection","","","","","","","Bampis L., Gasteratos A., Boukas E., Cnn-based novelty detection for terrestrial and extra-terrestrial autonomous exploration, (2021); Boukas E., Gasteratos A., Modeling regions of interest on orbital and rover imagery for planetary exploration missions, Cybernetics and Systems, 47, 3, pp. 180-205, (2016); Ren S., Et al., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 91-99, (2015); Liu L., Et al., CNN-based automatic coating inspection system, Advances in Science, Technology and Engineering Systems, 3, 6, pp. 469-478, (2018); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Andersen R., Et al., Investigating deep learning architectures towards autonomous inspection for marine classification, 2020 IEEE International Symposium on Safety, Security, and Rescue Robotics, pp. 197-204, (2020); Liu X., Et al., Oil Palm Tree Detection and Counting in Aerial Images Based on Faster R-CNN, Lecture Notes in Electrical Engineering vol. 632, pp. 475-482, (2020); Wang Y.-R., Li X.-M., Arctic sea ice cover data from spaceborne SAR by deep learning, Earth System Science Data Discussions, pp. 1-30, (2020); He K., Et al., Deep residual learning for image recognition, The IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Lin T.-Y., Et al., Feature pyramid networks for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 936-944, (2017); Demir I., Et al., Deepglobe 2018: A challenge to parse the earth through satellite images, The IEEE Conference on Computer Vision and Pattern Recognition, pp. 172-17209, (2018); Lin T.-Y., Et al., Microsoft COCO: Common objects in context, European Conference on Computer Vision, pp. 740-755, (2014)","R.E. Andersen; Section of Automation and Control, Department of Electrical Engineering, Technical University of Denmark, Kongens, Lyngby, Denmark; email: recan@elektro.dtu.dk","","John Wiley and Sons Inc","","","","","","00135194","","ELLEA","","English","Electron. Lett.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122072554"
"Ma X.; Xu J.; Wu P.; Kong P.","Ma, Xiaoshuang (55768476100); Xu, Jiangong (57223034014); Wu, Penghai (55644317800); Kong, Peng (56329578800)","55768476100; 57223034014; 55644317800; 56329578800","Oil Spill Detection Based on Deep Convolutional Neural Networks Using Polarimetric Scattering Information from Sentinel-1 SAR Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","21","10.1109/TGRS.2021.3126175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122286978&doi=10.1109%2fTGRS.2021.3126175&partnerID=40&md5=8ad76f793d2cfdc45babe4ee041c94d4","School of Resources and Environmental Engineering, Anhui Province Key Laboratory of Wetland Ecosystem Protection and Restoration, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, 230601, China; Institute of Spacecraft System Engineering, Beijing, 100094, China","Ma X., School of Resources and Environmental Engineering, Anhui Province Key Laboratory of Wetland Ecosystem Protection and Restoration, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, 230601, China; Xu J., School of Resources and Environmental Engineering, Anhui Province Key Laboratory of Wetland Ecosystem Protection and Restoration, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, 230601, China; Wu P., School of Resources and Environmental Engineering, Anhui Province Key Laboratory of Wetland Ecosystem Protection and Restoration, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, 230601, China; Kong P., Institute of Spacecraft System Engineering, Beijing, 100094, China","Oil spill accidents can cause severe ecological disasters; hence, the timely and effective detection of oil spills on the marine surface is of great significance. Synthetic aperture radar (SAR) is very suitable for large-scale oil spill monitoring. As a more advanced form of SAR, polarimetric SAR (PolSAR) can provide more scattering information of land objects, which can help to improve the accuracy of oil spill detection. However, the current studies of oil spill detection by SAR data have mainly focused on using SAR intensity or amplitude information, and the phase information and other polarimetric information have not been fully utilized. To solve this problem, using Sentinel-1 dual-polarimetric images as the data source, this article presents an intelligent oil spill detection architecture based on a deep convolutional neural network (DCNN), in which both the amplitude information and phase information are utilized. Furthermore, to improve the feature discrimination capability, the Cloude polarimetric decomposition parameters are also integrated into the proposed model. The results show that the improved DeepLabv3+ model, which takes ResNet-101 as the backbone network and group normalization (GN) as the normalization layer, can achieve superior performance than those traditional methods. Moreover, the model is better able to capture the fine details of oil spill instances and can achieve fine-scale segmentation.  © 1980-2012 IEEE.","Deep convolutional neural network (DCNN); Oil spill detection; Polarimetric decomposition; Polarimetric synthetic aperture radar (PolSAR) image; Sentinel-1","Convolution; Information use; Marine pollution; Oil spills; Polarimeters; Radar imaging; Synthetic aperture radar; Amplitude information; Deep convolutional neural network; Oil spill detection; Phase information; Polarimetric decomposition; Polarimetric synthetic aperture radar  image; Polarimetric synthetic aperture radars; Scattering information; Sentinel-1; Synthetic aperture radar images; artificial neural network; detection method; oil spill; remote sensing; scattering; Deep neural networks","","","","","Hefei Municipal Natural Science Foundation, (2021041); National Natural Science Foundation of China, NSFC, (41701390)","This work was supported in part by the National Natural Science Foundation of China under Grant 41701390 and in part by the Hefei Municipal Natural Science Foundation under Grant 2021041.","Zacharias D.C., Rezende K.F.O., Fornaro A., Offshore petroleum pollution compared numerically via algorithm tests and computation solutions, Ocean Eng., 151, pp. 191-198, (2018); Li P., Cai Q., Lin W., Chen B., Zhang B., Offshore oil spill response practices and emerging challenges, Mar. Pollut. Bull., 110, 1, pp. 6-27, (2016); Chen J., Zhang W., Wan Z., Li S., Huang T., Fei Y., Oil spills from global tankers: Status review and future governance, J. Cleaner Prod., 227, pp. 20-32, (2019); The International Tanker Owners Pollution Federation Limited Oil Tanker Spill Statistics, (2020); Alpers W., Holt B., Zeng K., Oil spill detection by imaging radars: Challenges and pitfalls, Remote Sens. Environ., 201, pp. 133-147, (2017); Krestenitis M., Et al., Oil spill identification from satellite images using deep neural networks, Remote Sens., 11, 15, pp. 1762-1783, (2019); Yekeen S.T., Balogun A.L., Advances in remote sensing technology, machine learning and deep learning for marine oil spill detection, prediction and vulnerability assessment, Remote Sens., 12, 20, pp. 3416-3446, (2020); Zheng H., Zhang J., Zhang Y., Khenchaf A., Wang Y., Theoretical study on microwave scattering mechanisms of sea surfaces covered with and without oil film for incidence angle smaller than 30, IEEE Trans. Geosci. Remote Sens., 59, 1, pp. 37-46, (2021); Topouzelis K., Psyllos A., Oil spill feature selection and classification using decision tree forest on SAR image data, ISPRS J. Photogramm. Remote Sens., 68, pp. 135-143, (2012); Raeisi A., Akbarizadeh G., Mahmoudi A., Combined method of an efficient cuckoo search algorithm and nonnegative matrix factorization of different Zernike moment features for discrimination between oil spills and lookalikes in SAR images, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, 11, pp. 4193-4205, (2018); Salberg A.-B., Rudjord O., Solberg A.H.S., Oil spill detection in hybrid-polarimetric SAR images, IEEE Trans. Geosci. Remote Sens., 52, 10, pp. 6521-6533, (2014); De Laurentiis L., Et al., Deep learning for mineral and biogenic oil slick classification with airborne synthetic aperture radar data, IEEE Trans. Geosci. Remote Sens., 51, 10, pp. 8455-8469, (2020); Migliaccio M., Gambardella A., Tranfaglia M., SAR polarimetry to observe oil spills, IEEE Trans. Geosci. Remote Sens., 45, 2, pp. 506-511, (2007); Nunziata F., Gambardella A., Migliaccio M., On the use of dualpolarized SAR data for oil spill observation, Proc. IEEE Int. Geosci. Remote Sens. Symp., pp. 225-228, (2008); Skrunes S., Brekke C., Eltoft T., An experimental study on oil spill characterization by multi-polarization SAR, Proc. 9th Eur. Conf. Synth. Aperture Radar, pp. 139-142, (2012); Shirvany R., Chabert M., Tourneret J.Y., Ship and oil-spill detection using the degree of polarization in linear and hybrid/compact dual-pol SAR, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 5, 3, pp. 885-892, (2012); Xu L., Li J., Brenning A., A comparative study of different classification techniques for marine oil spill identification using RADARSAT-1 imagery, Remote Sens. Environ., 141, pp. 14-23, (2014); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional neural networks for large-scale remote-sensing image classification, IEEE Trans. Geosci. Remote Sens., 55, 2, pp. 645-657, (2017); Zhou Y., Wang H., Xu F., Jin Y.-Q., Polarimetric SAR image classification using deep convolutional neural networks, IEEE Geosci. Remote Sens. Lett., 13, 12, pp. 1935-1939, (2016); Yang X., Et al., An attention-fused network for semantic segmentation of very-high-resolution remote sensing imagery, ISPRS J. Photogramm. Remote Sens., 177, pp. 238-262, (2021); Li X., Et al., Deep-learning-based information mining from ocean remote-sensing imagery, Nat. Sci. Rev., 7, 10, pp. 1584-1605, (2020); Chen G., Li Y., Sun G., Zhang Y., Application of deep networks to oil spill detection using polarimetric synthetic aperture radar images, Appl. Sci., 7, 10, (2017); Zeng K., Wang Y.X., A deep convolutional neural network for oil spill detection from spaceborne SAR images, Remote Sens., 12, 6, pp. 1015-1037, (2020); Bianchi F.M., Espeseth M.M., Borch N., Large-scale detection and categorization of oil spills from SAR images with deep learning, Remote Sens., 12, 14, pp. 2260-2286, (2020); Guo H., Wu D.N., An J.B., Discrimination of oil slicks and lookalikes in polarimetric SAR images using CNN, Sensors, 17, 8, pp. 1837-1856, (2017); Zhang J., Et al., Oil spill detection in quad-polarimetric SAR images using an advanced convolutional neural network based on SuperPixel model, Remote Sens., 12, 6, pp. 944-969, (2020); ESA's Radar Observatory Mission for GMES Operational Services, (2012); Chaturvedi S.K., Banerjee S., Lele S., An assessment of oil spill detection using sentinel 1 SAR-C images, J. Ocean Eng. Sci., 5, 2, pp. 116-135, (2020); El-Magd I.A., Et al., The potentiality of operational mapping of oil pollution in the mediterranean sea near the entrance of the Suez canal using sentinel-1 SAR data, Remote Sens., 12, 8, pp. 1352-1364, (2020); Konik M., Bradtke K., Object-oriented approach to oil spill detection using ENVISAT ASAR images, ISPRS J. Photogramm. Remote Sens., 118, pp. 37-52, (2016); Kudryavtsev V., On effect of wave breaking on short wind waves, Geophys. Res. Lett., 31, 20, pp. 1-5, (2004); Kudryavtsev V., A semiempirical model of the normalized radar crosssection of the sea surface 1. Background model, J. Geophys. Res., 108, 3, (2003); Skrunes S., Brekke C., Jones C.E., Holt B., A multisensor comparison of experimental oil spills in polarimetric SAR for high wind conditions, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 9, 11, pp. 4948-4961, (2016); Singha S., Ressel R., Velotto D., Lehner S., A combination of traditional and polarimetric features for oil spill detection using TerraSAR-X, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 9, 11, pp. 4979-4990, (2016); Girard-Ardhuin F., Mercier G., Collard F., Garello R., Operational oil-slick characterization by SAR imagery and synergistic data, IEEE J. Ocean. Eng., 30, 3, pp. 487-495, (2005); Brekke C., Solberg A.H.S., Oil spill detection by satellite remote sensing, Remote Sens. Environ., 95, 1, pp. 1-13, (2005); Li G., Et al., Marine oil slick detection using improved polarimetric feature parameters based on polarimetric synthetic aperture radar data, Remote Sens., 13, 9, pp. 1607-1629, (2021); Espeseth M.M., Brekke C., Jones C.E., Holt B., Freeman A., The impact of system noise in polarimetric SAR imagery on oil spill observations, IEEE Trans. Geosci. Remote Sens., 58, 6, pp. 4194-4214, (2020); Hansen M.W., Kudryavtsev V., Chapron B., Brekke C., Johannessen J.A., Wave breaking in slicks: Impacts on C-band quadpolarized SAR measurements, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 9, 11, pp. 4929-4940, (2016); Paquay M., Iriarte J.C., Ederra I., Gonzalo R., Maagt P.D., Thin AMC structure for radar cross-section reduction, IEEE Trans. Antennas Propag., 55, 12, pp. 3630-3638, (2007); Cloude S.R., Pottier E., An entropy based classification scheme for land applications of polarimetric SAR, IEEE Trans. Geosci. Remote Sens., 35, 1, pp. 68-78, (1997); Singha S., Johansson A.M., Doulgeris A.P., Robustness of SAR sea ice type classification across incidence angles and seasons at L-band, IEEE Trans. Geosci. Remote Sens., Early Access, (2020); Lopez-Martinez C., Pottier E., Statistical assessment of eigenvectorbased target decomposition theorems in radar polarimetry, Proc. IEEE Int. Geosci. Remote Sens. Symp., pp. 192-195, (2004); Lee J.-S., Pottier E., H/A/_ polarimetric decomposition theorem, Polarimetric Radar Imaging: From Basics to Applications, pp. 229-262, (2009); Lee J.S., Ainsworth T.L., Kelly J.P., Lopez-Martinez C., Evaluation and bias removal of multilook effect on entropy/alpha/anisotropy in polarimetric SAR decomposition, IEEE Trans. Geosci. Remote Sens., 46, 10, pp. 3039-3052, (2008); Yu H., Et al., Methods and datasets on semantic segmentation: A review, Neurocomputing, 304, pp. 82-103, (2018); Chen L.-C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoderdecoder with atrous separable convolution for semantic image segmentation, Proc. ECCV, pp. 833-851, (2018); Chen L.-C., Papandreou G., Schroff F., Adam H., Rethinking Atrous Convolution for Semantic Image Segmentation, (2017); Everingham M., Van Gool L., Williams C.K.I., The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results, (2012); Cordts M., Et al., The cityscapes dataset for semantic urban scene understanding, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3213-3223, (2016); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell., 40, 4, pp. 834-848, (2018); Yu F., Koltun V., Multi-scale Context Aggregation by Dilated Convolutions, (2015); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 37, 9, pp. 1904-1916, (2015); Wu Y., He K., Group Normalization, (2018); Ioffe S., Szegedy C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, (2015); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1800-1807, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 770-778, (2016); Sandler M., Et al., MobileNetV2: Inverted residuals and linear bottlenecks, Proc. 31st IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 4510-4520, (2018); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3431-3440, (2015); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent., pp. 234-241, (2015); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 318-327, (2017); Tantithamthavorn C., McIntosh S., Hassan A.E., Matsumoto K., An empirical comparison of model validation techniques for defect prediction models, IEEE Trans. Softw. Eng., 43, 1, pp. 1-18, (2017)","X. Ma; School of Resources and Environmental Engineering, Anhui Province Key Laboratory of Wetland Ecosystem Protection and Restoration, Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui University, Hefei, 230601, China; email: mxs.88@whu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85122286978"
"Chaithanya J.K.; Alisha M.; Sagar S.M.; Raghuram K.","Chaithanya, J. Krishna (57211869267); Alisha, Mohammand (57553415700); Sagar, S. Manish (57213675395); Raghuram, K. (57553178700)","57211869267; 57553415700; 57213675395; 57553178700","Development of Real-Time Violence Detection with Raspberry Pi","2022","Lecture Notes in Electrical Engineering","844","","","59","73","14","3","10.1007/978-981-16-8862-1_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127162874&doi=10.1007%2f978-981-16-8862-1_5&partnerID=40&md5=3adb49ab3497d50bb30a867b324416af","Department of ECE, Vardhaman College of Engineering, Hyderabad, India","Chaithanya J.K., Department of ECE, Vardhaman College of Engineering, Hyderabad, India; Alisha M., Department of ECE, Vardhaman College of Engineering, Hyderabad, India; Sagar S.M., Department of ECE, Vardhaman College of Engineering, Hyderabad, India; Raghuram K., Department of ECE, Vardhaman College of Engineering, Hyderabad, India","The detection of real-time object has drawn an increased interest in surveillance strategies, and it is one of the applications of CNNs. This project has focussed on the detection of fire and pistols in places that are tracked by cameras and fires in the home, business explosions, and wildfires are all major headaches that have negative implications for the environment. Mass shooting and violence caused by guns are also on the upward push in certain components of the sector. Such type of incidents is time touchy and may purpose a huge loss to lifestyles and assets. Hence, the proposed system is designed with YOLO v3 that detects perfectly by analysing the video or live feed frame to frame to discover such type of situations in real time and send an alert to the authorities through an email and mobile message. The model has been working well on data sets like IMFDB and Fire Net with accuracy more than 83%. Experimental output satisfies the aim of the proposed design is implemented on Raspberry Pi and that is tested with unique situations, and its detection is also very fast, and it can be installed indoor and outdoor. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Neural networks; Object tracking; Raspberry Pi; Surveillance system; Video processing; Violence; YOLO","Deep neural networks; Fires; Object detection; Security systems; Tracking (position); Deep learning; Neural-networks; Object Tracking; Raspberry pi; Real- time; Surveillance systems; Video processing; Violence; Violence detections; YOLO; Video signal processing","","","","","Department of Electronics and Communication Engineering","We sincerely thank the faculty of Department of Electronics and Communication Engineering, Vardhaman College of Engineering, Hyderabad for their help in reviewing of the manuscript and constructive suggestions made at various levels of the research work and thank the management for providing the facilities to carry out our research work at #3021 Lab physically/remotely (Centre of Excellence for IoT).","Celik T., Demirel H., Ozkaramanli H., Uyguroglu M., Fire detection in video sequences using statistical color model, IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, pp. II-II, (2006); Kotiyal S., Thapliyal H., Ranganathan N., Circuit for reversible quantum multiplier based on binary tree optimizing ancilla and garbage bits, 2014 27Th International Conference on VLSI Design and 2014 13Th International Conference on Embedded Systems, pp. 545-550, (2014); Toreyin B.U., Dedeoglu Y., Cetin A.E., Flame detection in video using hidden Markov models, IEEE International Conference on Image Processing. Genova, pp. II-1230, (2005); Li Z., Nadon S., Cihlar J., Satellite-based detection of Canadian boreal forest fires: Development and application of the algorithm, Int J Remote Sens, 21, 16, pp. 3057-3069, (2000); Celik T., Demirel H., Ozkaramanli H (2007) Fire and smoke detection without sensors: Image processing based approach, Proceedings of 15Th European Signal Processing Conference. Poland, 3–7, (2007); Kanehisa R., Neto A., Firearm detection using convolutional neural networks, Proceedings of the 11Th International Conference on Agents and Artificial Intelligence, 2, pp. 707-714, (2019); Grega M., Lach S., Sieradzki R., Automated recognition of firearms in surveillance video, 2013 IEEE International Multidisciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support (Cogsima). San Diego, CA, pp. 45-50, (2013); Sungheetha A., Sharma R., 3D image processing using machine learning based input processing for man-machine interaction, J Innovative Image Proc, 3, 1, pp. 1-6, (2021); Kayastha R., Preventing mass shooting through cooperation of mental health services, campus security, And Institutional Technology, (2016); Verma G.K., Dhillon A., A handheld gun detection using faster R-CNN deep learning, In: Proceedings of the 7Th International Conference on Computer and Communication technology— ICCCT2017, (2017); Valanarasu M.R., Comparative analysis for personality prediction by digital footprints in social media, J Inf Technol, 3, 2, pp. 77-91, (2021)","S.M. Sagar; Department of ECE, Vardhaman College of Engineering, Hyderabad, India; email: manishsagars13@gmail.com","Bindhu V.; Tavares J.M.; Du K.","Springer Science and Business Media Deutschland GmbH","","3rd International Conference on Communication, Computing and Electronics Systems, ICCCES 2021","28 October 2021 through 29 October 2021","Coimbatore","275269","18761100","978-981168861-4","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85127162874"
"Fei L.; Han B.","Fei, Lunlin (58203400700); Han, Bing (55726778600)","58203400700; 55726778600","Multi-Object Multi-Camera Tracking Based on Deep Learning for Intelligent Transportation: A Review","2023","Sensors","23","8","3852","","","","0","10.3390/s23083852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153940555&doi=10.3390%2fs23083852&partnerID=40&md5=42a71ff78b5856aebb5e304b4983e734","School of Traffic and Transportation, Beijing Jiaotong University, Beijing, 100044, China; Jiangxi Provincial Transportation Investment Group Co., Ltd, Nanchang, 330029, China; School of Civil Engineering, Beijing Jiaotong University, Beijing, 100044, China","Fei L., School of Traffic and Transportation, Beijing Jiaotong University, Beijing, 100044, China, Jiangxi Provincial Transportation Investment Group Co., Ltd, Nanchang, 330029, China; Han B., School of Civil Engineering, Beijing Jiaotong University, Beijing, 100044, China","Multi-Objective Multi-Camera Tracking (MOMCT) is aimed at locating and identifying multiple objects from video captured by multiple cameras. With the advancement of technology in recent years, it has received a lot of attention from researchers in applications such as intelligent transportation, public safety and self-driving driving technology. As a result, a large number of excellent research results have emerged in the field of MOMCT. To facilitate the rapid development of intelligent transportation, researchers need to keep abreast of the latest research and current challenges in related field. Therefore, this paper provide a comprehensive review of multi-object multi-camera tracking based on deep learning for intelligent transportation. Specifically, we first introduce the main object detectors for MOMCT in detail. Secondly, we give an in-depth analysis of deep learning based MOMCT and evaluate advanced methods through visualisation. Thirdly, we summarize the popular benchmark data sets and metrics to provide quantitative and comprehensive comparisons. Finally, we point out the challenges faced by MOMCT in intelligent transportation and present practical suggestions for the future direction. © 2023 by the authors.","deep neural network; intelligent transportation; multi-object multi-camera tracking; object detector","Cameras; Object detection; Intelligent transportation; Multi objective; Multi-camera tracking; Multi-object multi-camera tracking; Multiobject; Multiple cameras; Multiple objects; Object detectors; Public safety; Public selves; Deep neural networks","","","","","Jiangsu Provincial Department of Transport, (2022C0004, 2022C0005)","This research was funded by Science and Technology Project of Jiangxi Provincial Department of Transport (2022C0004, 2022C0005).","Wang Z., Zheng L., Liu Y., Li Y., Wang S., Towards real-time multi-object tracking, Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16, pp. 107-122, (2020); Tang Z., Naphade M., Liu M.Y., Yang X., Birchfield S., Wang S., Kumar R., Anastasiu D., Hwang J.N., Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8797-8806; Wang W., Wang L., Zhang C., Liu C., Sun L., Social interactions for autonomous driving: A review and perspectives, Found. Trends Robot, 10, pp. 198-376, (2022); Bendali-Braham M., Weber J., Forestier G., Idoumghar L., Muller P.A., Recent trends in crowd analysis: A review, Mach. Learn. Appl, 4, (2021); Cao J., Weng X., Khirodkar R., Pang J., Kitani K., Observation-centric sort: Rethinking sort for robust multi-object tracking, arXiv, (2022); Zhang Y., Wang Q., Zhao A., Ke Y., A multi-object posture coordination method with tolerance constraints for aircraft components assembly, Assem. Autom, 40, pp. 345-359, (2020); Liu Q., Chen D., Chu Q., Yuan L., Liu B., Zhang L., Yu N., Online multi-object tracking with unsupervised re-identification learning and occlusion estimation, Neurocomputing, 483, pp. 333-347, (2022); Parashar A., Shekhawat R.S., Ding W., Rida I., Intra-class variations with deep learning-based gait analysis: A comprehensive survey of covariates and methods, Neurocomputing, 505, pp. 315-338, (2022); Zhang Z., Wang S., Liu C., Xie R., Hu W., Zhou P., All-in-one two-dimensional retinomorphic hardware device for motion detection and recognition, Nat. Nanotechnol, 17, pp. 27-32, (2022); Jiang D., Li G., Tan C., Huang L., Sun Y., Kong J., Semantic segmentation for multiscale target based on object recognition using the improved Faster-RCNN model, Future Gener. Comput. Syst, 123, pp. 94-104, (2021); Li X., Zhao H., Yu L., Chen H., Deng W., Deng W., Feature extraction using parameterized multisynchrosqueezing transform, IEEE Sens. J, 22, pp. 14263-14272, (2022); Zaidi S.S.A., Ansari M.S., Aslam A., Kanwal N., Asghar M., Lee B., A survey of modern deep learning based object detection models, Digit. Signal Process, 126, (2022); Jimenez-Bravo D.M., Murciego A.L., Mendes A.S., San Blas H.S., Bajo J., Multi-object tracking in traffic environments: A systematic literature review, Neurocomputing, 494, pp. 43-55, (2022); Khan S.D., Ullah H., A survey of advances in vision-based vehicle re-identification, Comput. Vis. Image Underst, 182, pp. 50-63, (2019); Dong C., Zhou J., Wen W., Chen S., Deep Learning Based Multi-Target Multi-Camera Tracking System, Proceedings of the 8th International Conference on Computing and Artificial Intelligence, pp. 419-424; Luo R., Peng Z., Hu J., On Model Identification Based Optimal Control and It’s Applications to Multi-Agent Learning and Control, Mathematics, 11, (2023); Iguernaissi R., Merad D., Aziz K., Drap P., People tracking in multi-camera systems: A review, Multimed. Tools Appl, 78, pp. 10773-10793, (2019); Sufi F.B., Gazzano J.D.D., Calle F.R., Lopez J.C.L., Multi-camera tracking system applications based on reconfigurable devices: A review, Proceedings of the 2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2), pp. 1-5; Wang X., Intelligent multi-camera video surveillance: A review, Pattern Recognit. Lett, 34, pp. 3-19, (2013); Olagoke A.S., Ibrahim H., Teoh S.S., Literature survey on multi-camera system and its application, IEEE Access, 8, pp. 172892-172922, (2020); Bharati P., Pramanik A., Deep learning techniques—R-CNN to mask R-CNN: A survey, Computational Intelligence in Pattern Recognition: Proceedings of CIPR 2019, pp. 657-668, (2020); Girshick R., Fast r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448; Jiang M., Gu L., Li X., Gao F., Jiang T., Ship Contour Extraction from SAR images Based on Faster R-CNN and Chan-Vese model, IEEE Trans. Geosci. Remote Sens, 61, (2023); Felzenszwalb P.F., Girshick R.B., McAllester D., Ramanan D., Object detection with discriminatively trained part-based models, IEEE Trans. Pattern Anal. Mach. Intell, 32, pp. 1627-1645, (2009); Chen M., Yu L., Zhi C., Sun R., Zhu S., Gao Z., Ke Z., Zhu M., Zhang Y., Improved faster R-CNN for fabric defect detection based on Gabor filter with Genetic Algorithm optimization, Comput. Ind, 134, (2022); Maity M., Banerjee S., Chaudhuri S.S., Faster r-cnn and yolo based vehicle detection: A survey, Proceedings of the 2021 5th International Conference on Computing Methodologies and Communication (ICCMC), pp. 1442-1447; Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788; Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and pattern Recognition, pp. 7263-7271; Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14, pp. 21-37, (2016); Tiwari V., Singhal A., Dhankhar N., Detecting COVID-19 Opacity in X-ray Images Using YOLO and RetinaNet Ensemble, Proceedings of the 2022 IEEE Delhi Section Conference (DELCON), pp. 1-5; Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Peng X., Sun B., Ali K., Saenko K., Learning deep object detectors from 3d models, Proceedings of the IEEE International Conference on Computer Vision, pp. 1278-1286; Wang L., Chen T., Anklam C., Goldluecke B., High dimensional frustum pointnet for 3D object detection from camera, lidar, and radar, Proceedings of the 2020 IEEE Intelligent Vehicles Symposium (IV), pp. 1621-1628; Guo Y., Wang H., Hu Q., Liu H., Liu L., Bennamoun M., Deep learning for 3d point clouds: A survey, IEEE Trans. Pattern Anal. Mach. Intell, 43, pp. 4338-4364, (2020); Pan X., Xia Z., Song S., Li L.E., Huang G., 3D object detection with pointformer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7463-7472; Simon M., Milz S., Amende K., Gross H.M., Complex-yolo: Real-time 3d object detection on point clouds, arXiv, (2018); Wen L.H., Jo K.H., Fast and accurate 3D object detection for lidar-camera-based autonomous vehicles using one shared voxel-based backbone, IEEE Access, 9, pp. 22080-22089, (2021); Bochkovskiy A., Wang C.Y., Liao H.Y.M., Yolov4: Optimal speed and accuracy of object detection, arXiv, (2020); Wu W., Liu H., Li L., Long Y., Wang X., Wang Z., Li J., Chang Y., Application of local fully Convolutional Neural Network combined with YOLO v5 algorithm in small target detection of remote sensing image, PLoS ONE, 16, (2021); Ge Z., Liu S., Wang F., Li Z., Sun J., Yolox: Exceeding yolo series in 2021, arXiv, (2021); Wang C.Y., Yeh I.H., Liao H.Y.M., You only learn one representation: Unified network for multiple tasks, arXiv, (2021); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., LeCun Y., Overfeat: Integrated recognition, localization and detection using convolutional networks, arXiv, (2013); Hinton G.E., Krizhevsky A., Wang S.D., Transforming auto-encoders, Artificial Neural Networks and Machine Learning–ICANN 2011: 21st International Conference on Artificial Neural Networks, Espoo, Finland, June 14–17, 2011, Proceedings, Part I 21, pp. 44-51, (2011); Taylor G.W., Spiro I., Bregler C., Fergus R., Learning invariance through imitation, Proceedings of the CVPR 2011, pp. 2729-2736; Aliasghar O., Kanani Moghadam V., Selective search and new-to-market process innovation, J. Manuf. Technol. Manag, 33, pp. 1301-1318, (2022); Li Y., Shen Y., Zhang W., Zhang C., Cui B., VolcanoML: Speeding up end-to-end AutoML via scalable search space decomposition, VLDB J, 32, pp. 389-413, (2022); Daulton S., Eriksson D., Balandat M., Bakshy E., Multi-objective bayesian optimization over high-dimensional search spaces, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, pp. 507-517; Xie X., Cheng G., Wang J., Yao X., Han J., Oriented R-CNN for object detection, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3520-3529; Hong Q., Liu F., Li D., Liu J., Tian L., Shan Y., Dynamic sparse r-cnn, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4723-4732; Ali R., Chuah J.H., Talip M.S.A., Mokhtar N., Shoaib M.A., Structural crack detection using deep convolutional neural networks, Autom. Constr, 133, (2022); Wang X., Wang L., Zheng P., SC-dynamic R-CNN: A self-calibrated dynamic R-CNN model for lung cancer lesion detection, Comput. Math. Methods Med, 2022, (2022); Alsharekh M.F., Habib S., Dewi D.A., Albattah W., Islam M., Albahli S., Improving the Efficiency of Multistep Short-Term Electricity Load Forecasting via R-CNN with ML-LSTM, Sensors, 22, (2022); Ma W., Zhou T., Qin J., Zhou Q., Cai Z., Joint-attention feature fusion network and dual-adaptive NMS for object detection, Knowl.-Based Syst, 241, (2022); Zhang S., Yu Z., Liu L., Wang X., Zhou A., Chen K., Group R-CNN for weakly semi-supervised object detection with points, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9417-9426; Dai J., Li Y., He K., Sun J., R-fcn: Object detection via region-based fully convolutional networks, Adv. Neural Inf. Process. Syst, 29, pp. 1-9, (2016); Vijaya Kumar D., Mahammad Shafi R., A fast feature selection technique for real-time face detection using hybrid optimized region based convolutional neural network, Multimed. Tools Appl, 82, pp. 13719-13732, (2022); Zhang R., Song Y., Non-intrusive load identification method based on color encoding and improve R-FCN, Sustain. Energy Technol. Assess, 53, (2022); Roy A.M., Bhaduri J., Kumar T., Raj K., WilDect-YOLO: An efficient and robust computer vision-based accurate object localization model for automated endangered wildlife detection, Ecol. Inform, 75, (2023); Karaman A., Pacal I., Basturk A., Akay B., Nalbantoglu U., Coskun S., Sahin O., Karaboga D., Robust real-time polyp detection system design based on YOLO algorithms by optimizing activation functions and hyper-parameters with artificial bee colony (ABC), Expert Syst. Appl, 221, (2023); Xue Z., Xu R., Bai D., Lin H., YOLO-Tea: A Tea Disease Detection Model Improved by YOLOv5, Forests, 14, (2023); Mittal U., Chawla P., Tiwari R., EnsembleNet: A hybrid approach for vehicle detection and estimation of traffic density based on faster R-CNN and YOLO models, Neural Comput. Appl, 35, pp. 4755-4774, (2023); Han G., Huang S., Ma J., He Y., Chang S.F., Meta faster r-cnn: Towards accurate few-shot object detection with attentive feature alignment, Proc. AAAI Conf. Artif. Intell, 36, pp. 780-789, (2022); Cuomo S., Di Cola V.S., Giampaolo F., Rozza G., Raissi M., Piccialli F., Scientific machine learning through physics–informed neural networks: Where we are and what is next, J. Sci. Comput, 92, (2022); Jia D., Zhou J., Zhang C., Detection of cervical cells based on improved SSD network, Multimed. Tools Appl, 81, pp. 13371-13387, (2022); Chen Z., Guo H., Yang J., Jiao H., Feng Z., Chen L., Gao T., Fast vehicle detection algorithm in traffic scene based on improved SSD, Measurement, 201, (2022); Gao X., Xu J., Luo C., Zhou J., Huang P., Deng J., Detection of Lower Body for AGV Based on SSD Algorithm with ResNet, Sensors, 22, (2022); Ma R., Chen C., Yang B., Li D., Wang H., Cong Y., Hu Z., CG-SSD: Corner guided single stage 3D object detection from LiDAR point cloud, ISPRS J. Photogramm. Remote Sens, 191, pp. 33-48, (2022); Cheng L., Ji Y., Li C., Liu X., Fang G., Improved SSD network for fast concealed object detection and recognition in passive terahertz security images, Sci. Rep, 12, (2022); Cao H., Wang Y., Chen J., Jiang D., Zhang X., Tian Q., Wang M., Swin-unet: Unet-like pure transformer for medical image segmentation, Computer Vision–ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part III, pp. 205-218, (2023); Kim H., Jung W.K., Park Y.C., Lee J.W., Ahn S.H., Broken stitch detection method for sewing operation using CNN feature map and image-processing techniques, Expert Syst. Appl, 188, (2022); Chen H.C., Widodo A.M., Wisnujati A., Rahaman M., Lin J.C.W., Chen L., Weng C.E., AlexNet convolutional neural network for disease detection and classification of tomato leaf, Electronics, 11, (2022); Kim C., Li F., Ciptadi A., Rehg J.M., Multiple hypothesis tracking revisited, Proceedings of the IEEE International Conference on Computer Vision, pp. 4696-4704; Wang S., Sheng H., Yang D., Zhang Y., Wu Y., Wang S., Extendable multiple nodes recurrent tracking framework with RTU++, IEEE Trans. Image Process, 31, pp. 5257-5271, (2022); Wojke N., Bewley A., Paulus D., Simple online and realtime tracking with a deep association metric, Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP), pp. 3645-3649; Leal-Taixe L., Canton-Ferrer C., Schindler K., Learning by tracking: Siamese CNN for robust target association, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 33-40; Zhang J., Sun J., Wang J., Li Z., Chen X., An object tracking framework with recapture based on correlation filters and Siamese networks, Comput. Electr. Eng, 98, (2022); Su Q., Tang J., Zhai M., He D., An intelligent method for dairy goat tracking based on Siamese network, Comput. Electron. Agric, 193, (2022); Chen L., Ai H., Shang C., Zhuang Z., Bai B., Online multi-object tracking with convolutional neural networks, Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP), pp. 645-649; Theckedath D., Sedamkar R., Detecting affect states using VGG16, ResNet50 and SE-ResNet50 networks, SN Comput. Sci, 1, (2020); Chu Q., Ouyang W., Li H., Wang X., Liu B., Yu N., Online multi-object tracking using CNN-based single object tracker with spatial-temporal attention mechanism, Proceedings of the IEEE International Conference on Computer Vision, pp. 4836-4845; Liu M., Gu Q., Yang B., Yin Z., Liu S., Yin L., Zheng W., Kinematics Model Optimization Algorithm for Six Degrees of Freedom Parallel Platform, Appl. Sci, 13, (2023); Katz S.M., Corso A.L., Strong C.A., Kochenderfer M.J., Verification of image-based neural network controllers using generative models, J. Aerosp. Inf. Syst, 19, pp. 574-584, (2022); Lu J., Wan H., Li P., Zhao X., Ma N., Gao Y., Exploring High-order Spatio-temporal Correlations from Skeleton for Person Re-identification, IEEE Trans. Image Process, 32, pp. 949-963, (2023); Hasan M.R., Guest R., Deravi F., Presentation-Level Privacy Protection Techniques for Automated Face Recognition—A Survey, ACM Comput. Surv, (2023); Tang W., Chouzenoux E., Pesquet J.C., Krim H., Deep transform and metric learning network: Wedding deep dictionary learning and neural network, Neurocomputing, 509, pp. 244-256, (2022); Son J., Baek M., Cho M., Han B., Multi-object tracking with quadruplet convolutional neural networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5620-5629; Xiang J., Zhang G., Hou J., Sang N., Huang R., Multiple target tracking by learning feature representation and distance metric jointly, arXiv, (2018); Cheng D., Gong Y., Zhou S., Wang J., Zheng N., Person re-identification by multi-channel parts-based cnn with improved triplet loss function, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1335-1344; Aggarwal R., Singh N., An Approach to Learn Structural Similarity between Decision Trees Using Hungarian Algorithm, Proceedings of 3rd International Conference on Recent Trends in Machine Learning, IoT, Smart Cities and Applications: ICMISC 2022, pp. 185-199, (2023); Fang K., Xiang Y., Li X., Savarese S., Recurrent autoregressive networks for online multi-object tracking, Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 466-475; Fernando T., Denman S., Sridharan S., Fookes C., Tracking by prediction: A deep generative model for mutli-person localisation and tracking, Proceedings of the 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1122-1132; Ondruska P., Posner I., Deep tracking: Seeing beyond seeing using recurrent neural networks, Proc. AAAI Conf. Artif. Intell, 30, (2016); Milan A., Rezatofighi S.H., Dick A., Reid I., Schindler K., Online multi-target tracking using recurrent neural networks, Proc. AAAI Conf. Artif. Intell, 31, (2017); Sadeghian A., Alahi A., Savarese S., Tracking the untrackable: Learning to track multiple cues with long-term dependencies, Proceedings of the IEEE International Conference on Computer Vision, pp. 300-311; Li D., Ge S.S., Lee T.H., Fixed-time-synchronized consensus control of multiagent systems, IEEE Trans. Control Netw. Syst, 8, pp. 89-98, (2020); Kim C., Li F., Rehg J.M., Multi-object tracking with neural gating using bilinear lstm, Proceedings of the European Conference on Computer Vision (ECCV), pp. 200-215; Bashir R.M.S., Shahzad M., Fraz M., Vr-proud: Vehicle re-identification using progressive unsupervised deep architecture, Pattern Recognit, 90, pp. 52-65, (2019); Deng W., Zheng L., Ye Q., Kang G., Yang Y., Jiao J., Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 994-1003; Wang J., Zhu X., Gong S., Li W., Transferable joint attribute-identity deep learning for unsupervised person re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2275-2284; Shen F., Du X., Zhang L., Tang J., Triplet Contrastive Learning for Unsupervised Vehicle Re-identification, arXiv, (2023); Zhu W., Peng B., Manifold-based aggregation clustering for unsupervised vehicle re-identification, Knowl.-Based Syst, 235, (2022); Wang Y., Wei Y., Ma R., Wang L., Wang C., Unsupervised vehicle re-identification based on mixed sample contrastive learning, Signal Image Video Process, 16, pp. 2083-2091, (2022); Gao Z., Wu T., Lin L., Zhao J., Zhang A., Wu J., Eliminating domain deviation via synthetic data for vehicle re-identification, Proceedings of the International Conference on Computer, Artificial Intelligence, and Control Engineering (CAICE 2022), pp. 6-11; Chai X., Wang Y., Chen X., Gan Z., Zhang Y., TPE-GAN: Thumbnail preserving encryption based on GAN with key, IEEE Signal Process. Lett, 29, pp. 972-976, (2022); Zhou Z., Li Y., Li J., Yu K., Kou G., Wang M., Gupta B.B., Gan-siamese network for cross-domain vehicle re-identification in intelligent transport systems, IEEE Trans. Netw. Sci. Eng, 2022, (2022); Yan T., Li H., Sun B., Wang Z., Luo Z., Discriminative feature mining and enhancement network for low-resolution fine-grained image recognition, IEEE Trans. Circuits Syst. Video Technol, 32, pp. 5319-5330, (2022); Fayou S., Ngo H., Sek Y., Combining multi-feature regions for fine-grained image recognition, Int. J. Image Graph. Signal Process, 14, pp. 15-25, (2022); Ning X., Tian W., He F., Bai X., Sun L., Li W., Hyper-sausage coverage function neuron model and learning algorithm for image classification, Pattern Recognit, 136, (2023); Cenggoro T.W., Pardamean B., A systematic literature review of machine learning application in COVID-19 medical image classification, Procedia Comput. Sci, 216, pp. 749-756, (2023); Salaberria A., Azkune G., de Lacalle O.L., Soroa A., Agirre E., Image captioning for effective use of language models in knowledge-based visual question answering, Expert Syst. Appl, 212, (2023); Li Z., Wei J., Huang F., Ma H., Modeling graph-structured contexts for image captioning, Image Vis. Comput, 129, (2023); Zhu W., Wang Z., Wang X., Hu R., Liu H., Liu C., Wang C., Li D., A Dual Self-Attention mechanism for vehicle re-Identification, Pattern Recognit, 137, (2023); Lian J., Wang D., Zhu S., Wu Y., Li C., Transformer-based attention network for vehicle re-identification, Electronics, 11, (2022); Jiang G., Pang X., Tian X., Zheng Y., Meng Q., Global reference attention network for vehicle re-identification, Appl. Intell, pp. 1-16, (2022); Tian X., Pang X., Jiang G., Meng Q., Zheng Y., Vehicle Re-Identification Based on Global Relational Attention and Multi-Granularity Feature Learning, IEEE Access, 10, pp. 17674-17682, (2022); Li M., Wei M., He X., Shen F., Enhancing Part Features via Contrastive Attention Module for Vehicle Re-identification, Proceedings of the 2022 IEEE International Conference on Image Processing (ICIP), pp. 1816-1820; Song L., Zhou X., Chen Y., Global attention-assisted representation learning for vehicle re-identification, Signal Image Video Process, 16, pp. 807-815, (2022); Li H., Wang Y., Wei Y., Wang L., Li G., Discriminative-region attention and orthogonal-view generation model for vehicle re-identification, Appl. Intell, 53, pp. 186-203, (2023); Tang L., Wang Y., Chau L.P., Weakly-supervised Part-Attention and Mentored Networks for Vehicle Re-Identification, IEEE Trans. Circuits Syst. Video Technol, 32, pp. 8887-8898, (2022); Liu Y., Hu H., Chen D., Attentive Part-Based Alignment Network for Vehicle Re-Identification, Electronics, 11, (2022); Shen F., Xie Y., Zhu J., Zhu X., Zeng H., Git: Graph interactive transformer for vehicle re-identification, arXiv, (2021); Wang H., Peng J., Jiang G., Xu F., Fu X., Discriminative feature and dictionary learning with part-aware model for vehicle re-identification, Neurocomputing, 438, pp. 55-62, (2021); Rong L., Xu Y., Zhou X., Han L., Li L., Pan X., A vehicle re-identification framework based on the improved multi-branch feature fusion network, Sci. Rep, 11, (2021); Yang J., Xing D., Hu Z., Yao T., A two-branch network with pyramid-based local and spatial attention global feature learning for vehicle re-identification, CAAI Trans. Intell. Technol, 6, pp. 46-54, (2021); Fu X., Peng J., Jiang G., Wang H., Learning latent features with local channel drop network for vehicle re-identification, Eng. Appl. Artif. Intell, 107, (2022); Liu Y., Zhang X., Zhang B., Zhang X., Wang S., Xu J., Multi-camera vehicle tracking based on occlusion-aware and inter-vehicle information, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3257-3264; Hsu H.M., Wang Y., Cai J., Hwang J.N., Multi-Target Multi-Camera Tracking of Vehicles by Graph Auto-Encoder and Self-Supervised Camera Link Model, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 489-499; Hsu H.M., Wang Y., Hwang J.N., Traffic-aware multi-camera tracking of vehicles based on reid and camera link model, Proceedings of the 28th ACM International Conference on Multimedia, pp. 964-972; Li Y.J., Weng X., Xu Y., Kitani K.M., Visio-temporal attention for multi-camera multi-target association, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9834-9844; Liu C., Zhang Y., Chen W., Wang F., Li H., Shen Y.D., Adaptive Matching Strategy for Multi-Target Multi-Camera Tracking, Proceedings of the ICASSP 2022–2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2934-2938; Zhao J., Gao F., Jia W., Yuan W., Jin W., Integrated Sensing and Communications for UAV Communications with Jittering Effect, IEEE Wirel. Commun. Lett, 2023, (2023); Yang K.S., Chen Y.K., Chen T.S., Liu C.T., Chien S.Y., Tracklet-refined multi-camera tracking based on balanced cross-domain re-identification for vehicles, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3983-3992; Li Y.L., Chin Z.Y., Chang M.C., Chiang C.K., Multi-camera tracking by candidate intersection ratio tracklet matching, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4103-4111; Liang N.S.J., Srigrarom S., Multi-camera multi-target drone tracking systems with trajectory-based target matching and re-identification, Proceedings of the 2021 International Conference on Unmanned Aircraft Systems (ICUAS), pp. 1337-1344; He Y., Han J., Yu W., Hong X., Wei X., Gong Y., City-scale multi-camera vehicle tracking by semantic attribute parsing and cross-camera tracklet matching, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 576-577; Tran D.N.N., Pham L.H., Jeon H.J., Nguyen H.H., Jeon H.M., Tran T.H.P., Jeon J.W., A robust traffic-aware city-scale multi-camera vehicle tracking of vehicles, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3150-3159; Yu F., Chen H., Wang X., Xian W., Chen Y., Liu F., Madhavan V., Darrell T., Bdd100k: A diverse driving dataset for heterogeneous multitask learning, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2636-2645; Yao Y., Zheng L., Yang X., Naphade M., Gedeon T., Simulating content consistent vehicle datasets with attribute descent, Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16, pp. 775-791, (2020); Wen L., Du D., Cai Z., Lei Z., Chang M.C., Qi H., Lim J., Yang M.H., Lyu S., UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking, Comput. Vis. Image Underst, 193, (2020); Geiger A., Lenz P., Urtasun R., Are we ready for autonomous driving? the kitti vision benchmark suite, Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354-3361; Caesar H., Bankiti V., Lang A.H., Vora S., Liong V.E., Xu Q., Krishnan A., Pan Y., Baldan G., Beijbom O., nuscenes: A multimodal dataset for autonomous driving, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11621-11631; Bernardin K., Stiefelhagen R., Evaluating multiple object tracking performance: The clear mot metrics, EURASIP J. Image Video Process, 2008, (2008); Luna E., Miguel J.C.S., Martinez J.M., Escudero-Vinolo M., Graph Convolutional Network for Multi-Target Multi-Camera Vehicle Tracking, arXiv, (2022); Hsu H.M., Huang T.W., Wang G., Cai J., Lei Z., Hwang J.N., Multi-camera tracking of vehicles based on deep features re-id and trajectory-based camera link models, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 416-424; Hou Y., Du H., Zheng L., A locality aware city-scale multi-camera vehicle tracking system, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 167-174; He Z., Lei Y., Bai S., Wu W., Multi-Camera Vehicle Tracking with Powerful Visual Features and Spatial-Temporal Cue, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 203-212; Quach K.G., Nguyen P., Le H., Truong T.D., Duong C.N., Tran M.T., Luu K., Dyglip: A dynamic graph model with link prediction for accurate multi-camera multiple object tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13784-13793; Luna E., SanMiguel J.C., Martinez J.M., Escudero-Vinolo M., Online clustering-based multi-camera vehicle tracking in scenarios with overlapping FOVs, Multimed. Tools Appl, 81, pp. 7063-7083, (2022); Qian Y., Yu L., Liu W., Hauptmann A.G., Electricity: An efficient multi-camera vehicle tracking system for intelligent city, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 588-589; Chang M.C., Wei J., Zhu Z.A., Chen Y.M., Hu C.S., Jiang M.X., Chiang C.K., AI City Challenge 2019-City-Scale Video Analytics for Smart Transportation, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 99-108","L. Fei; School of Traffic and Transportation, Beijing Jiaotong University, Beijing, 100044, China; email: 20114085@bjtu.edu.cn","","MDPI","","","","","","14248220","","","37112193","English","Sensors","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85153940555"
"Sapkota B.B.; Hu C.; Bagavathiannan M.V.","Sapkota, Bishwa B. (57199153629); Hu, Chengsong (57224534070); Bagavathiannan, Muthukumar V. (24179346400)","57199153629; 57224534070; 24179346400","Evaluating Cross-Applicability of Weed Detection Models Across Different Crops in Similar Production Environments","2022","Frontiers in Plant Science","13","","837726","","","","9","10.3389/fpls.2022.837726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130203987&doi=10.3389%2ffpls.2022.837726&partnerID=40&md5=349c38d59916181c6f508656aac1ec92","Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States; Department of Biological and Agricultural Engineering, College Station, TX, United States","Sapkota B.B., Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States; Hu C., Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States, Department of Biological and Agricultural Engineering, College Station, TX, United States; Bagavathiannan M.V., Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States","Convolutional neural networks (CNNs) have revolutionized the weed detection process with tremendous improvements in precision and accuracy. However, training these models is time-consuming and computationally demanding; thus, training weed detection models for every crop-weed environment may not be feasible. It is imperative to evaluate how a CNN-based weed detection model trained for a specific crop may perform in other crops. In this study, a CNN model was trained to detect morningglories and grasses in cotton. Assessments were made to gauge the potential of the very model in detecting the same weed species in soybean and corn under two levels of detection complexity (levels 1 and 2). Two popular object detection frameworks, YOLOv4 and Faster R-CNN, were trained to detect weeds under two schemes: Detect_Weed (detecting at weed/crop level) and Detect_Species (detecting at weed species level). In addition, the main cotton dataset was supplemented with different amounts of non-cotton crop images to see if cross-crop applicability can be improved. Both frameworks achieved reasonably high accuracy levels for the cotton test datasets under both schemes (Average Precision-AP: 0.83–0.88 and Mean Average Precision-mAP: 0.65–0.79). The same models performed differently over other crops under both frameworks (AP: 0.33–0.83 and mAP: 0.40–0.85). In particular, relatively higher accuracies were observed for soybean than for corn, and also for complexity level 1 than for level 2. Significant improvements in cross-crop applicability were further observed when additional corn and soybean images were added to the model training. These findings provide valuable insights into improving global applicability of weed detection models. Copyright © 2022 Sapkota, Hu and Bagavathiannan.","CNNs; deep learning; digital technologies; precision agriculture; precision weed control; site-specific weed management","","","","","","NRCS-CIG, (213A750013G017); USDA-Natural Resources Conservation Service-Conservation; Cotton Incorporated, (20-739)","This study was funded in part by the USDA-Natural Resources Conservation Service-Conservation Innovation Grant (NRCS-CIG) program (award #NR213A750013G017) and Cotton Incorporated (award #20-739). ","Abdalla A., Cen H., Wan L., Rashid R., Weng H., Zhou W., Et al., Fine-tuning convolutional neural network with transfer learning for semantic segmentation of ground-level oilseed rape images in a field with high weed pressure, Comput. Electron. Agric, 167, (2019); Adhikari S.P., Yang H., Kim H., Learning semantic graphics using convolutional encoder–decoder network for autonomous weeding in paddy, Front. Plant Sci, 10, (2019); Ahmad F., Qiu B., Dong X., Ma J., Huang X., Ahmed S., Et al., Effect of operational parameters of UAV sprayer on spray deposition pattern in target and off-target zones during outer field weed control application, Comput. Electron. Agric, 172, (2020); Ahmed F., Al-Mamun H.A., Bari A.S.M.H., Hossain E., Kwan P., Classification of crops and weeds from digital images: a support vector machine approach, Crop Prot, 40, pp. 98-104, (2012); Alchanatis V., Ridel L., Hetzroni A., Yaroslavsky L., Weed detection in multi-spectral images of cotton fields, Comput. Electron. Agric, 47, pp. 243-260, (2005); Aravind R., Daman M., Kariyappa B.S., (2015); Bagavathiannan M.V., Davis A.S., An ecological perspective on managing weeds during the great selection for herbicide resistance, Pest Manag. Sci, 74, pp. 2277-2286, (2018); Beckie H.J., Ashworth M.B., Flower K.C., Herbicide resistance management: recent developments and trends, Plants, 8, (2019); Berge T.W., Goldberg S., Kaspersen K., Netland J., Towards machine vision based site-specific weed management in cereals, Comput. Electron. Agric, 81, pp. 79-86, (2012); Bochkovskiy A., Wang C.-Y., Liao H.Y.M., YOLOv4: optimal speed and accuracy of object detection, (2020); Buchanan G.A., Burns E.R., Influence of weed competition on cotton, Weed Sci, 18, pp. 149-154, (1970); Czymmek V., Harders L.O., Knoll F.J., Hussmann S., (2019); Dutta A., Zisserman A., (2019); Fawakherji M., Youssef A., Bloisi D., Pretto A., Nardi D., (2019); Gai J., Tang L., Steward B.L., Automated crop plant detection based on the fusion of color and depth images for robotic weed control, J. Field Robot, 37, pp. 35-52, (2020); Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters J.G., Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, (2020); Garcia-Santillan I.D., Pajares G., On-line crop/weed discrimination through the Mahalanobis distance from images in maize fields, Biosyst. Eng, 166, pp. 28-43, (2018); Girshick R., (2015); Hu C., Sapkota B.B., Thomasson J.A., Bagavathiannan M.V., Influence of image quality and light consistency on the performance of convolutional neural networks for weed mapping, Remote Sens, 13, (2021); Jiang H., Zhang C., Qiao Y., Zhang Z., Zhang W., Song C., CNN feature based graph convolutional network for weed and crop recognition in smart farming, Comput. Electron. Agric, 174, (2020); Kargar B.A.H., Shirzadifar A.M., (2013); Lamm R.D., Slaughter D.C., Giles D.K., Precision weed control system for cotton, Transact. ASAE, 45, (2002); Le V.N.T., Ahderom S., Alameh K., Performances of the LBP based algorithm over CNN models for detecting crops and weeds with similar morphologies, Sensors, 20, (2020); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Et al., SSD: single shot multibox detector, Computer Vision – ECCV 2016, Lecture Notes in Computer Science, pp. 21-37, (2016); Liu B., Bruch R., Weed detection for selective spraying: a review, Curr. Robot. Rep, 1, pp. 19-26, (2020); Lopez-Granados F., Weed detection for site-specific weed management: mapping and real-time approaches, Weed Res, 51, pp. 1-11, (2011); Lottes P., Behley J., Chebrolu N., Milioto A., Stachniss C., (2018); Lottes P., Behley J., Chebrolu N., Milioto A., Stachniss C., Robust joint stem detection and crop-weed classification using image sequences for plant-specific treatment in precision farming, J. Field Robot, 37, pp. 20-34, (2019); Ma X., Deng X., Qi L., Jiang Y., Li H., Wang Y., Et al., Fully convolutional network for rice seedling and weed image segmentation at the seedling stage in paddy fields, PLoS One, 14, (2019); Machleb J., Peteinatos G.G., Kollenda B.L., Andujar D., Gerhards R., Sensor-based mechanical weed control: present state and prospects, Comput. Electron. Agric, 176, (2020); Martin D., Singh V., Latheef M.A., Bagavathiannan M., Spray deposition on weeds (Palmer amaranth and Morningglory) from a remotely piloted aerial application system and packpack sprayer, Drones, 4, (2020); Nave W.R., Wax L.M., Effect of weeds on soybean yield and harvesting efficiency, Weed Sci, 19, pp. 533-535, (1971); Oquab M., Bottou L., Laptev I., Sivic J., (2014); Osorio K., Puerto A., Pedraza C., Jamaica D., Rodriguez L., A deep learning approach for weed detection in lettuce crops using multispectral images, AgriEngineering, 2, pp. 471-488, (2020); Partel V., Kakarla S.C., Ampatzidis Y., Development and evaluation of a low-cost and smart technology for precision weed management utilizing artificial intelligence, Comput. Electron. Agric, 157, pp. 339-350, (2019); Redmon J., Divvala S., Girshick R., Farhadi A., (2016); Redmon J., Farhadi A., YOLOv3: an incremental improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Rumpf T., Romer C., Weis M., Sokefeld M., Gerhards R., Plumer L., Sequential support vector machine classification for small-grain weed species discrimination with special regard to Cirsium arvense and Galium aparine, Comput. Electron. Agric, 80, pp. 89-96, (2012); Sabzi S., Abbaspour-Gilandeh Y., Garcia-Mateos G., A fast and accurate expert system for weed identification in potato crops using metaheuristic algorithms, Comput. Ind, 98, pp. 80-89, (2018); Sapkota B., Singh V., Neely C., Rajan N., Bagavathiannan M., Detection of Italian ryegrass in wheat and prediction of competitive interactions using remote-sensing and machine-learning techniques, Remote Sens, 12, (2020); Sharpe S.M., Schumann A.W., Boyd N.S., Goosegrass detection in strawberry and tomato using a convolutional neural network, Sci. Rep, 10, (2020); Suarez L.A., Apan A., Werth J., Detection of phenoxy herbicide dosage in cotton crops through the analysis of hyperspectral data, Int. J. Remote Sens, 38, pp. 6528-6553, (2017); Sujaritha M., Annadurai S., Satheeshkumar J., Kowshik Sharan S., Mahesh L., Weed detecting robot in sugarcane fields using fuzzy real time classifier, Comput. Electron. Agric, 134, pp. 160-171, (2017); Wu X., Xu W., Song Y., Cai M., A detection method of weed in wheat field on machine vision, Procedia Engin, 15, pp. 1998-2003, (2011); Xie S., Hu C., Bagavathiannan M., Song D., Toward robotic weed control: detection of nutsedge weed in bermudagrass turf using inaccurate and insufficient training data, IEEE Robot. Automat. Lett, 6, pp. 7365-7372, (2021); Yu J., Schumann A.W., Cao Z., Sharpe S.M., Boyd N.S., Weed detection in perennial ryegrass with deep learning convolutional neural network, Front. Plant Sci, 10, (2019)","M.V. Bagavathiannan; Department of Soil and Crop Sciences, Texas AM University, College Station, United States; email: muthu@tamu.edu","","Frontiers Media S.A.","","","","","","1664462X","","","","English","Front. Plant Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130203987"
"Zhao H.; Morgenroth J.; Pearse G.; Schindler J.","Zhao, Haotian (58169586300); Morgenroth, Justin (23991150200); Pearse, Grant (56940473500); Schindler, Jan (57219929281)","58169586300; 23991150200; 56940473500; 57219929281","A Systematic Review of Individual Tree Crown Detection and Delineation with Convolutional Neural Networks (CNN)","2023","Current Forestry Reports","9","3","","149","170","21","0","10.1007/s40725-023-00184-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151571890&doi=10.1007%2fs40725-023-00184-3&partnerID=40&md5=ee84152dddc835b185737212978c6747","New Zealand School of Forestry, University of Canterbury, Private Bag 4800, Christchurch, 8140, New Zealand; Data and Geospatial Intelligence, Scion, 49 Sala Street, Private Bag 3020, Rotorua, 3046, New Zealand; Informatics, Manaaki Whenua – Landcare Research, PO Box 10345, The Terrace, Wellington, 6143, New Zealand","Zhao H., New Zealand School of Forestry, University of Canterbury, Private Bag 4800, Christchurch, 8140, New Zealand; Morgenroth J., New Zealand School of Forestry, University of Canterbury, Private Bag 4800, Christchurch, 8140, New Zealand; Pearse G., Data and Geospatial Intelligence, Scion, 49 Sala Street, Private Bag 3020, Rotorua, 3046, New Zealand; Schindler J., Informatics, Manaaki Whenua – Landcare Research, PO Box 10345, The Terrace, Wellington, 6143, New Zealand","Purpose of Review: Crown detection and measurement at the individual tree level provide detailed information for accurate forest management. To efficiently acquire such information, approaches to conduct individual tree detection and crown delineation (ITDCD) using remotely sensed data have been proposed. In recent years, deep learning, specifically convolutional neural networks (CNN), has shown potential in this field. This article provides a systematic review of the studies that used CNN for ITDCD and identifies major trends and research gaps across six perspectives: accuracy assessment methods, data types, platforms and resolutions, forest environments, CNN models, and training strategies and techniques. Recent Findings: CNN models were mostly applied to high-resolution red–green–blue (RGB) images. When compared with other state-of-the-art approaches, CNN models showed significant improvements in accuracy. One study reported an increase in detection accuracy of over 11%, while two studies reported increases in F1-score of over 16%. However, model performance varied across different forest environments and data types. Several factors including data scarcity, model selection, and training approaches affected ITDCD results. Summary: Future studies could (1) explore data fusion approaches to take advantage of the characteristics of different types of remote sensing data, (2) further improve data efficiency with customised sample approaches and synthetic samples, (3) explore the potential of smaller CNN models and compare their learning efficiency with commonly used models, and (4) evaluate impacts of pre-training and parameter tunings. © 2023, The Author(s).","Crown delineation; Deep learning; Forestry; Instance segmentation; Object detection; Remote sensing; Tree detection","","","","","","University of Canterbury, UC; College of Engineering, University of Canterbury; Business Economics, University of Canterbury; Ngāi Tahu Research Centre, University of Canterbury, NTRC; Biomolecular Interaction Centre, University of Canterbury, BIC; Ministry of Business, Innovation and Employment, MBIE, (C09X1923); New Zealand Institute of Language Brain and Behaviour, University of Canterbury, NZILBB, UC","Haotian Zhao carried out the literature review and drafted the manuscript. Justin Morgenroth, Grant Pearse and Jan Schindler provided critical feedback during the process and helped to finalize the manuscript. Grant Pearse and Jan Schindler also assisted in seeking financial support for this research. ","Forests, biodiversity and people, The State of the World's Forests (SOFO)., (2020); Fujimoto A., Haga C., Matsui T., Machimura T., Hayashi K., Sugita S., Et al., An end to end process development for UAV-SfM based forest monitoring: Individual tree detection, species classification and carbon dynamics simulation, Forests, 10, 8, (2019); Saarinen N., Vastaranta M., Nasi R., Rosnell T., Hakala T., Honkavaara E., Et al., Assessing biodiversity in boreal forests with UAV-based photogrammetric point clouds and hyperspectral imaging, Remote Sens, 10, 2, (2018); Kimball L.L., Wiseman P.E., Day S.D., Munsell J.F., Use of urban tree canopy assessments by localities in the Chesapeake Bay Watershed, Cities and the Environment, 7, 2, (2014); Berland A., Shiflett S.A., Shuster W.D., Garmestani A.S., Goddard H.C., Herrmann D.L., Et al., The role of trees in urban stormwater management, Landscape and Urban Plan, 162, pp. 167-177, (2017); Brumelis G., Dauskane I., Elferts D., Strode L., Krama T., Krams I.J.F., Estimates of tree canopy closure and basal area as proxies for tree crown volume at a stand scale, Forests, 11, 11, (2020); Livesley S., McPherson E.G., Calfapietra C., The urban forest and ecosystem services: impact on urban water, heat, and pollution cycles at the tree, street, and city scale, J Environ Qual, 45, pp. 119-124, (2016); Shendryk I., Broich M., Tulbure M.G., McGrath A., Keith D., Alexandrov S.V.J.R.S.O.E., Mapping individual tree health using full-waveform airborne laser scans and imaging spectroscopy A case study for a floodplain eucalypt forest, Remote Sens Environ, 187, pp. 202-217, (2016); Wang L., Gong P., Biging G.S.J.P.E., Sensing R., Individual tree-crown delineation and treetop detection in high-spatial-resolution aerial imagery, Photogramm Eng Rem S, 70, 3, pp. 351-357, (2004); Alonzo M., Bookhagen B., Roberts D.A., Urban tree species mapping using hyperspectral and lidar data fusion, Remote Sens Environ, 148, pp. 70-83, (2014); Murtha P., Fournier R., Varying reflectance patterns influence photo interpretation of dead tree crowns, Can J Remote Sens, 18, 3, pp. 167-173, (1992); Roder M., Latifi H., Hill S., Wild J., Svoboda M., Bruna J., Et al., Application of optical unmanned aerial vehicle-based imagery for the inventory of natural regeneration and standing deadwood in post-disturbed spruce forests, Int J Remote Sens, 39, 15-16, pp. 5288-5309, (2018); St-Onge B., Grandin S., Estimating the height and basal area at individual tree and plot levels in Canadian subarctic lichen woodlands using stereo worldview-3 images, Remote Sens, 11, 3, (2019); Braga J.R.G., Peripato V., Dalagnol R., Ferreira M.P., Tarabalka Y., Aragao L.E.O.C., Tree crown delineation algorithm based on a convolutional neural network, Remote Sens., 12, 8, (2020); Pinz A., A computer vision system for the recognition of trees in aerial photographs, Nasa Conf P, 3099, pp. 111-124, (1991); Ke Y., Quackenbush L.J., A review of methods for automatic individual tree-crown detection and delineation from passive remote sensing, Int J Remote Sens, 32, 17, pp. 4725-4747, (2011); Yin D., Wang L., How to assess the accuracy of the individual tree-based forest inventory derived from remotely sensed data: a review, Int J Remote Sens, 37, 19, pp. 4521-4553, (2016); Zhen Z., Quackenbush L.J., Zhang L., Trends in automatic individual tree crown detection and delineation-evolution of LiDAR data, Remote Sens, 8, 4, (2016); Jing L., Hu B., Li J., Noland T., Guo H., Automated tree crown delineation from imagery based on morphological techniques, IOP C Ser Earth Env, 17, 1, (2014); Jing L., Hu B., Li J., Noland T., Automated delineation of individual tree crowns from lidar data by multi-scale analysis and segmentation, Photogramm Eng Rem S, 78, 12, pp. 1275-1284, (2012); Qiu L., Jing L., Hu B., Li H., Tang Y.J.R.S., A new individual tree crown delineation method for high resolution multispectral imagery, Remote Sens, 12, 3, (2020); Xu W., Deng S., Liang D., Cheng X., A crown morphology-based approach to individual tree detection in subtropical mixed broadleaf urban forests using UAV lidar data, Remote Sens, 13, 7, (2021); Hoeser T., Kuenzer C.J.R.S., Object detection and image segmentation with deep learning on Earth observation data: A review-part I: Evolution and recent trends, Remote Sens, 12, 10, (2020); Kattenborn T., Leitloff J., Schiefer F., Hinz S.J.I.J.O.P., Sensing R., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, ISPRS J Photogramm Remote Sens, 173, pp. 24-49, (2021); Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); Xiao C., Qin R., Huang X., Treetop detection using convolutional neural networks trained through automatically generated pseudo labels, Int J Remote Sens, 41, 8, pp. 3010-3030, (2020); Lou X., Huang Y., Fang L., Huang S., Gao H., Yang L., Et al., Measuring loblolly pine crowns with drone imagery through deep learning, J For Res, (2021); Chadwick A.J., Goodbody T.R.H., Coops N.C., Hervieux A., Bater C.W., Martens L.A., Et al., Automatic delineation and height measurement of regenerating conifer crowns under leaf-off conditions using uav imagery, Remote Sens, 12, 24, pp. 1-26, (2020); Wu J., Yang G., Yang H., Zhu Y., Li Z., Lei L., Et al., Extracting apple tree crown information from remote imagery using deep learning, Comput Electron Agric, 174, (2020); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual tree-crown detection in rgb imagery using semi-supervised deep learning neural networks, Remote Sens., 11, 11, (2019); Hoeser T., Bachofer F., Kuenzer C.J.R.S., Object detection and image segmentation with deep learning on earth observation data: a review—Part II: Applications, Remote Sens, 12, 18, (2020); Boogaard F.P., Rongen K.S.A.H., Kootstra G.W., Robust node detection and tracking in fruit-vegetable crops using deep learning and multi-view imaging, Biosyst Eng, 192, pp. 117-132, (2020); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: a meta-analysis and review, ISPRS J Photogramm Remote Sens, 152, pp. 166-177, (2019); Diez Y., Kentsch S., Fukuda M., Caceres M.L.L., Moritake K., Cabezas M., Deep learning in forestry using uav-acquired rgb data: a practical review, Remote Sens, 13, 14, (2021); Moher D., Liberati A., Tetzlaff J., Altman D.G., PRISMA Group* t. Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement, Ann Intern Med, 151, 4, pp. 264-269, (2009); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning–method overview and review of use for fruit detection and yield estimation, Comput Electron Agr, 162, pp. 219-234, (2019); Everingham M., Van Gool L., Williams C.K., Winn J., Zisserman A.J.I.J.O.C.V., The pascal visual object classes (voc) challenge, Int J Comput Vision, 88, 2, pp. 303-338, (2010); Chiang C.Y., Barnes C., Angelov P., Jiang R., Deep learning-based automated forest health diagnosis from aerial images, IEEE Access, 8, pp. 144064-144076, (2020); Culman M., Delalieux S., Van Tricht K., Individual palm tree detection using deep learning on RGB imagery to support tree inventory, Remote Sens, 12, 21, pp. 1-31, (2020); Ammar A., Koubaa A., Benjdira B., Deep-learning-based automated palm tree counting and geolocation in large farms from aerial geotagged images, Agronomy, 11, 8, (2021); Park H.G., Yun J.P., Kim M.Y., Jeong S.H., Multichannel object detection for detecting suspected trees with pine wilt disease using multispectral drone imagery, IEEE J Sel Top Appl Earth Obs Remote Sens, 14, pp. 8350-8358, (2021); Plesoianu A.I., Stupariu M.S., Sandric I., Patru-Stupariu I., Dragut L., Individual tree-crown detection and species classification in very high-resolution remote sensing imagery using a deep learning ensemble model, Remote Sens, 12, 15, (2020); Xi X., Xia K., Yang Y., Du X., Feng H., Evaluation of dimensionality reduction methods for individual tree crown delineation using instance segmentation network and UAV multispectral imagery in urban forest, Comput Electron Agric, 191, (2021); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Et al., Microsoft coco: Common objects in context, European Conference on Computer Vision: Springer, pp. 740-755, (2014); Oksuz K., Cam B.C., Akbas E., Kalkan S., Localization recall precision (LRP): A new performance metric for object detection, Proceedings of the European Conference on Computer Vision (ECCV), pp. 504-519, (2018); Xu C., Morgenroth J., Manley B., Integrating data from discrete return airborne LiDAR and optical sensors to enhance the accuracy of forest description: a review, Curr For Rep, 1, pp. 206-219, (2015); Pulido D., Salas J., Ros M., Puettmann K., Karaman S., Assessment of tree detection methods in multispectral aerial images, Remote Sens, 12, 15, (2020); Zheng J., Fu H., Li W., Wu W., Yu L., Yuan S., Et al., Growing status observation for oil palm trees using unmanned aerial vehicle (UAV) images, ISPRS J Photogramm Remote Sens, 173, pp. 95-121, (2021); Huang L., Wang Y., Xu Q., Liu Q., Recognition of abnormally discolored trees caused by pine wilt disease using YOLO algorithm and UAV images, Nongye Gongcheng Xuebao, 37, 14, pp. 197-203, (2021); Paul A., Bhattacharyya S., Chakraborty D., Estimation of shade tree density in tea garden using remote sensing images and deep convolutional neural network, J Spat Sci, 29, pp. 1-5, (2012); Emin M., Anwar E., Liu S., Emin B., Mamut M., Abdukeram A., Et al., Target detection-based tree recognition in a spruce forest area with a high tree density—implications for estimating tree numbers, Sustainability, 13, 6, (2021); Maschler J., Atzberger C., Immitzer M., Individual tree crown segmentation and classification of 13 tree species using Airborne hyperspectral data, Remote Sens, 10, 8, (2018); Naveed F., Hu B., Wang J., Hall G.B., Individual tree crown delineation using multispectral LiDAR data, Sensors, 19, 24, (2019); Ozdarici-Ok A., Automatic detection and delineation of citrus trees from VHR satellite imagery, Int J Remote Sens, 36, 17, pp. 4275-4296, (2015); Mo J., Lan Y., Yang D., Wen F., Qiu H., Chen X., Et al., Deep learning-based instance segmentation method of litchi canopy from uav-acquired images, Remote Sens, 13, 19, (2021); Safonova A., Guirado E., Maglinets Y., Alcaraz-Segura D., Tabik S., Olive tree biovolume from uav multi-resolution image segmentation with mask r-cnn, Sensors, 21, 5, pp. 1-17, (2021); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sens, 11, 4, (2019); Windrim L., Bryson M., Detection, segmentation, and model fitting of individual tree stems from airborne laser scanning of forests using deep learning, Remote Sens, 12, 9, (2020); Hao Z., Lin L., Post C.J., Mikhailova E.A., Li M., Chen Y., Et al., Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN), ISPRS J Photogramm Remote Sens, 178, pp. 112-123, (2021); Zheng X., Wu X., Huan L., He W., Zhang H., A Gather-to-guide network for remote sensing semantic segmentation of rgb and auxiliary image, IEEE Trans Geosci Remote Sens, 60, pp. 1-15, (2021); van Etten A., You only look twice: rapid multi-scale object detection in satellite imagery, Arxiv Preprint Arxiv:180509512, (2018); Fromm M., Schubert M., Castilla G., Linke J., McDermid G., Automated detection of conifer seedlings in drone imagery using convolutional neural networks, Remote Sens, 11, 21, (2019); Ocer N.E., Kaplan G., Erdem F., KucukMatci D., Avdan U., Tree extraction from multi-scale UAV images using Mask R-CNN with FPN, Remote Sens Lett, 11, 9, pp. 847-856, (2020); Weinstein B.G., Marconi S., Aubry-Kientz M., Vincent G., Senyondo H., White E.P., DeepForest: a Python package for RGB deep learning tree crown delineation, Methods Ecol Evol, 11, 12, pp. 1743-1751, (2020); Lee A.L., To C.C., Lee A.L., Li J.J., Chan R.C., Model architecture and tile size selection for convolutional neural network training for non-small cell lung cancer detection on whole slide images, Inform Med Unlocked, 28, (2022); Gomez Selvaraj M., Vergara A., Montenegro F., Alonso Ruiz H., Safari N., Raymaekers D., Et al., Detection of banana plants and their major diseases through aerial images and machine learning methods: a case study in DR Congo and Republic of Benin, ISPRS J Photogramm Remote Sens, 169, pp. 110-124, (2020); Ye Y., Shen B., Shen Y., Research on anti-shadow tree detection method based on generative adversarial network, Nongye Gongcheng Xuebao, 37, 10, pp. 118-126, (2021); Morgenroth J., Ostberg J., Van den Bosch C.K., Nielsen A.B., Hauer R., Sjoman H., Et al., Urban tree diversity—Taking stock and looking ahead, Urban For Urban Gree, 15, pp. 1-5, (2016); Zamboni P., Junior J.M., Silva J.A., Miyoshi G.T., Matsubara E.T., Nogueira K., Benchmarking anchor-based and anchor-free state-of-the-art deep learning methods for individual tree detection in rgb high-resolution images, Remote Sens., 13, 13, (2021); Xia K., Wang H., Yang Y., Du X., Feng H., Automatic detection and parameter estimation of Ginkgo biloba in urban environment based on RGB images, Journal of Sensors, 2021, (2021); Zhou Y., Liu W., Luo Y., Zong S., Small object detection for infected trees based on the deep learning method, Linye Kexue, 57, 3, pp. 98-107, (2021); Ferreira M.P., Almeida D.R.A.D., Papa D.D.A., Minervino J.B.S., Veras H.F.P., Formighieri A., Et al., Individual tree detection and species classification of Amazonian palms using UAV images and deep learning, For Ecol Manage, 475, (2020); Tong P., Han P., Li S., Li N., Bu S., Li Q., Et al., Counting trees with point-wise supervised segmentation network, Eng Appl Artif Intell, 100, (2021); Li F., Liu Z., Shen W., Wang Y., Wang Y., Ge C., Et al., A remote sensing and airborne edge-computing based detection system for pine wilt disease, IEEE Access, (2021); Dos Santos A.A., Marcato Junior J., Araujo M.S., Di Martini D.R., Tetila E.C., Siqueira H.L., Et al., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVS, Sensors, 19, 16, (2019); Weinstein B.G., Marconi S., Bohlman S.A., Zare A., White E.P., Cross-site learning in deep learning RGB tree crown detection, Ecol Informatics, 56, (2020); Zheng Y., Wu G., Single shot MultiBox detector for urban plantation single tree detection and location with high-resolution remote sensing imagery, Front Environ Sci, 9, (2021); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, . Arxiv Preprint Arxiv, 1409, (2014); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Et al., Going deeper with convolutions, Proceedings of IEEE Conf Comput Vis Pattern Recognit, pp. 1-9, (2015); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, Proceedings of IEEE Conf Comput Vis Pattern Recognit, pp. 779-788, (2016); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of IEEE Comput Soc Conf Comput Vis Pattern Recognit, pp. 770-778, (2016); Tan M., Le Q., Efficientnet: Rethinking model scaling for convolutional neural networks, International Conference on Machine Learning: PMLR; In: Proceedings of Int Conf Mach Learn., pp. 6105-6114, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, . In: Proceedings of IEEE Comput Soc Conf Comput Vis Pattern Recognit, pp. 2117-2125; Mahmud M.S., Zahid A., Das A.K., Muzammil M., Khan M.U., A systematic literature review on deep learning applications for precision cattle farming, Comput Electron Agric, 187, (2021); Zhao Z.Q., Zheng P., Xu S.T., Wu X., Object detection with deep learning: a review, IEEE T Neur Net Lear, 30, 11, pp. 3212-3232, (2019); Wang Y., Albrecht C.M., Braham N.A.A., Mou L., Zhu X.X., Self-supervised learning in remote sensing: A review, (2022); Chlap P., Min H., Vandenberg N., Dowling J., Holloway L., Haworth A., A review of medical image data augmentation techniques for deep learning applications, J Med Imag Radiat On, 65, 5, pp. 545-563, (2021); Xiao Y., Tian Z., Yu J., Zhang Y., Liu S., Du S., Et al., A review of object detection based on deep learning, Multimedia Tools Appl, 79, 33-34, pp. 23729-23791, (2020); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, IEEE Conf Comput Vis Pattern Recognit, pp. 248-255, (2009); Soviany P., Ionescu R.T., Optimizing the trade-off between single-stage and two-stage deep object detectors using image difficulty prediction, In: 2018 20Th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), pp. 209-214, (2018); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE I Conf Comp Vis, pp. 2980-2988, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Et al., Ssd: Single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Ge Z., Liu S., Wang F., Li Z., Sun J., Yolox: Exceeding yolo series in 2021, (2021); Wang C.Y., Bochkovskiy A., Liao H.Y.M., . Scaled-yolov4: Scaling cross stage partial network, . In: Proceedings 2021 IEEE Conf Comp Vis Pattern Recognit, IEEE Comput Soc., pp. 13024-13033; Ward D., Moghadam P., Scalable learning for bridging the species gap in image-based plant phenotyping, Comput Vision Image Understanding, pp. 197-198, (2020); Wang C., Liu B., Liu L., Zhu Y., Hou J., Liu P., Et al., A review of deep learning used in the hyperspectral image analysis for agriculture, Artif Intell Rev, 54, 7, pp. 5205-5253, (2021); Oksuz K., Cam B.C., Kalkan S., Akbas E., Imbalance problems in object detection: a review, IEEE Trans Pattern Anal Mach Intell, 43, 10, pp. 3388-3415, (2020); Shorten C., Khoshgoftaar T.M., A survey on image data augmentation for deep learning, Journal of big data, 6, 1, pp. 1-48, (2019); Ophoff T., van Beeck K., Goedeme T.J.S., Exploring RGB+ depth fusion for real-time object detection, 19, 4, (2019); Yeong D.J., Velasco-Hernandez G., Barry J., Walsh J.J.S., Sensor and sensor fusion technology in autonomous vehicles: a review, 21, 6, (2021); Johnson J.M., Khoshgoftaar T.M., Survey on deep learning with class imbalance, Journal of Big Data, 6, 1, (2019); Bissoto A., Valle E., Avila S., Gan-based data augmentation and anonymization for skin-lesion analysis: A critical review, In: Proceedings 2021 IEEE Conf Comp Vis Pattern Recognit, IEEE Comput Soc, pp. 1847-1856; Bi L., Hu G., Improving image-based plant disease classification with generative adversarial network under limited training set, Front Plant Sci, 11, (2020); Han C., Hayashi H., Rundo L., Araki R., Shimoda W., Muramatsu S., Et al., GAN-based synthetic brain MR image generation, 2018 IEEE 15Th International Symposium on Biomedical Imaging (ISBI 2018): IEEE, pp. 734-738, (2018); Frid-Adar M., Diamant I., Klang E., Amitai M., Goldberger J., Greenspan H.J.N., GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification, Neurocomputing, 321, pp. 321-331, (2018); Nguyen N.-D., Do T., Ngo T.D., Le D.-D., An evaluation of deep learning methods for small object detection, Journal of Electrical and Computer Engineering, 2020, (2020); Justus D., Brennan J., Bonner S., McGough A.S., Predicting the computational cost of deep learning models, IEEE Int Conf Big Data, pp. 3873-3882, (2018); Yu T., Zhu H., Hyper-parameter optimization: A review of algorithms and applications; Condesmanagement S.H.J.F.E., Derivation of compatible crown width equations for some important tree species of Spain, Forest Ecol Manag, 217, 2-3, pp. 203-218, (2005)","H. Zhao; New Zealand School of Forestry, University of Canterbury, Christchurch, Private Bag 4800, 8140, New Zealand; email: hzh159@uclive.ac.nz","","Springer Science and Business Media Deutschland GmbH","","","","","","21986436","","","","English","Curr. For. Rep.","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85151571890"
"Ghorbanzadeh O.; Gholamnia K.; Ghamisi P.","Ghorbanzadeh, Omid (57200006495); Gholamnia, Khalil (57205601418); Ghamisi, Pedram (53663404300)","57200006495; 57205601418; 53663404300","The application of ResU-net and OBIA for landslide detection from multi-temporal sentinel-2 images","2022","Big Earth Data","","","","","","","16","10.1080/20964471.2022.2031544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125285339&doi=10.1080%2f20964471.2022.2031544&partnerID=40&md5=2ecd91b3baad6814183371e2a82b2c98","Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria; Department of Remote Sensing and GIS, University of Tabriz, Tabriz, Iran; Machine Learning Group, Helmholtz Institute Freiberg for Resource Technology, Helmholtz-Zentrum Dresden-Rossendorf, Freiberg, Germany","Ghorbanzadeh O., Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria; Gholamnia K., Department of Remote Sensing and GIS, University of Tabriz, Tabriz, Iran; Ghamisi P., Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria, Machine Learning Group, Helmholtz Institute Freiberg for Resource Technology, Helmholtz-Zentrum Dresden-Rossendorf, Freiberg, Germany","Landslide detection is a hot topic in the remote sensing community, particularly with the current rapid growth in volume (and variety) of Earth observation data and the substantial progress of computer vision. Deep learning algorithms, especially fully convolutional networks (FCNs), and variations like the ResU-Net have been used recently as rapid and automatic landslide detection approaches. Although FCNs have shown cutting-edge results in automatic landslide detection, accuracy can be improved by adding prior knowledge through possible frameworks. This study evaluates a rule-based object-based image analysis (OBIA) approach built on probabilities resulting from the ResU-Net model for landslide detection. We train the ResU-Net model using a landslide dataset comprising landslide inventories from various geographic regions, including our study area and test the testing area not used for training. In the OBIA stage, we first calculate land cover and image difference indices for pre-and post-landslide multi-temporal images. Next, we use the generated indices and the resulting ResU-Net probabilities for image segmentation; the extracted landslide object candidates are then optimized using rule-based classification. In the result validation section, the landslide detection of the proposed integration of the ResU-Net with a rule-based classification of OBIA is compared with that of the ResU-Net alone. Our proposed approach improves the mean intersection-over-union of the resulting map from the ResU-Net by more than 22%. © 2022 The Author(s). Published by Taylor & Francis Group and Science Press on behalf of the International Society for Digital Earth, supported by the CASEarth Strategic Priority Research Programme.","Deep learning (DL); Eastern iburi; European Space Agency (ESA); Fully Convolutional Networks (FCNs); Japan; object-based image analysis (OBIA); rapid landslide mapping; ResU-net; Sentinel-2","Convolution; Convolutional neural networks; Deep learning; Image segmentation; Landslides; Learning algorithms; Remote sensing; Space optics; Statistical tests; Convolutional networks; Deep learning; Eastern iburi; European Space Agency; Fully convolutional network; Image-analysis; Japan; Landslide mapping; Object based; Object-based image analyse; Objects-based; Rapid landslide mapping; Resu-net; Sentinel-2; Image analysis","","","","","IARAI; Institute of Advanced Research in Artificial Intelligence","This research was funded by the Institute of Advanced Research in Artificial Intelligence (IARAI) GmbH. We are grateful to the European Space Agency (ESA) and Google Earth Engine (GEE) for providing Sentinel-2 MSI images and online computation. We acknowledge the use of ALOS PALSAR data provided by services from Alaska Satellite Facility (ASF), part of the NASA’s Earth Science Data and Information System (ESDIS). The authors are grateful to the anonymous referees for their useful comments/suggestions that have helped us to improve an earlier version of the manuscript. ","Aimaiti Y., Liu W., Yamazaki F., Maruyama Y., Earthquake-induced landslide mapping for the 2018 Hokkaido Eastern Iburi Earthquake Using PALSAR-2 data, Remote Sensing, 11, 20, (2019); Baatz M., Hoffmann C., Willhauck G., Progressing from object-based to object-oriented image analysis, Object-based image analysis, pp. 29-42, (2008); Bacha A.S., Van Der Werff H., Shafique M., Khan H., Transferability of object-based image analysis approaches for landslide detection in the Himalaya Mountains of northern Pakistan, International Journal of Remote Sensing, 41, 9, pp. 3390-3410, (2020); Benz U.C., Hofmann P., Willhauck G., Lingenfelder I., Heynen M., Multi-resolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information, ISPRS Journal of Photogrammetry and Remote Sensing, 58, 3-4, pp. 239-258, (2004); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Tiede D., Geographic object-based image analysis–towards a new paradigm, ISPRS Journal of Photogrammetry and Remote Sensing, 87, pp. 180-191, (2014); Blaschke T., Object based image analysis for remote sensing, ISPRS Journal of Photogrammetry and Remote Sensing, 65, 1, pp. 2-16, (2010); Chen T., Trinder J.C., Niu R., Object-oriented landslide mapping using ZY-3 satellite imagery, random forest and mathematical morphology, for the Three-Gorges Reservoir, China, Remote Sensing, 9, 4, (2017); Chen Y., Ming D., Lv X., Superpixel based land cover classification of VHR satellite image combining multi-scale CNN and scale parameter estimation, Earth Science Informatics, 123, pp. 341-363, (2019); Dabiri Z., Holbling D., Abad L., Helgason J.K., Saemundsson Th., Tiede D., Assessment of landslide-induced geomorphological changes in Hítardalur Valley, Iceland, using sentinel-1 and sentinel-2 data, Applied Sciences, 10, 17, (2020); Definiens A., Definiens professional 5 user guide, Definiens AG, Munich, (2006); Doshida S., Evaluation of secondary landslide susceptibility for the rescue activity using LiDAR UAV data, Workshop on world landslide forum, pp. 283-287, (2020); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of different machine learning methods and deep-learning convolutional neural networks for landslide detection, (In English), Remote Sens-Basel, 11, 2, (2019); Ghorbanzadeh O., Crivellari A., Ghamisi P., Shahabi H., Blaschke T., A comprehensive transferability evaluation of U-net and ResU-Net for landslide detection from Sentinel-2 data (case study areas from Taiwan, China, and Japan), Scientific Reports, 11, 1, pp. 1-20, (2021); Ghorbanzadeh O., Tiede D., Wendt L., Sudmanns M., Lang S., Transferable instance segmentation of dwellings in a refugee camp - integrating CNN and OBIA, European Journal of Remote Sensing, pp. 1-14, (2020); Guzzetti F., Carrara A., Cardinali M., Reichenbach P., Landslide hazard evaluation: A review of current techniques and their application in a multi-scale study, Central Italy, Geomo, 31, 1-4, pp. 181-216, (1999); Guzzetti F., Mondini A.C., Cardinali M., Fiorucci F., Santangelo M., Chang K.-T., Landslide inventory maps: New tools for an old problem, Earth-Science Reviews, 112, 1-2, pp. 42-66, (2012); Hay G., Blaschke T., Special issue on Geographic Object-Based Image Analysis (GEOBIA), PE&RS, Photogrammetric Engineering & Remote Sensing, 76, 2, pp. 99-202, (2010); Holbling D., Fureder P., Antolini F., Cigna F., Casagli N., Lang S., A semi-automated object-based approach for landslide detection validated by persistent scatterer interferometry measures and landslide inventories, Remote Sensing, 4, 5, pp. 1310-1336, (2012); Kawamura S., Kawajiri S., Hirose W., Watanabe T., Slope failures/landslides over a wide area in the 2018 Hokkaido Eastern Iburi earthquake, Soils and Foundations, 59, 6, pp. 2376-2395, (2019); Kingma D.P., Ba J., Adam: A method for stochastic optimization, arXiv Preprint arXiv:1412.6980, (2014); Lang S., Hay G.J., Baraldi A., Tiede D., Blaschke T., Geobia achievements and spatial opportunities in the era of big earth observation data, ISPRS International Journal of Geo-Information, 8, 11, (2019); Lei T., Xue D., Lv Z., Li S., Zhang Y., Nandi A.K., Unsupervised change detection using fast fuzzy clustering for landslide mapping from very high-resolution images, Remote Sens-Basel, 10, 9, (2018); Liu P., Wei Y., Wang Q., Chen Y., Xie J., Research on post-earthquake landslide extraction algorithm based on improved U-Net model, Remote Sensing, 12, 5, (2020); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, (2015); Lu H., Ma L., Fu X., Liu C., Wang Z., Tang M., Li N., Landslides information extraction using object-oriented image analysis paradigm based on deep learning and transfer learning, Remote Sensing, 12, 5, (2020); Ma L., Zhu X., Qiu C., Blaschke T., Li M., Advances of local climate zone mapping and its practice using object-based image analysis, Atmosphere, 12, 9, (2021); Melis M.T., Thermal remote sensing from UAVs: A review on methods in coastal cliffs prone to landslides, Remote Sens-Basel, 12, 12, (2020); Mondini A.C., Chang K.-T., Yin H.-Y., Combining multiple change detection indices for mapping landslides triggered by typhoons, Geomorphology, 134, 3-4, pp. 440-451, (2011); Mondini A.C., Guzzetti F., Chang K.-T., Monserrat O., Martha T.R., Manconi A., Landslide failures detection and mapping using synthetic aperture radar: Past, present and future, Earth-Science Reviews, (2021); Mondini A., Guzzetti F., Reichenbach P., Rossi M., Cardinali M., Ardizzone F., Semi-automatic recognition and mapping of rainfall induced shallow landslides using optical satellite images, Remote Sensing of Environment, 115, 7, pp. 1743-1757, (2011); Munyati C., Optimising multiresolution segmentation: Delineating savannah vegetation boundaries in the Kruger National Park, South Africa, using Sentinel 2 MSI imagery, International Journal of Remote Sensing, 39, 18, pp. 5997-6019, (2018); Naboureh A., Bian J., Lei G., Li A., A review of land use/land cover change mapping in the China-Central Asia-West Asia economic corridor countries, Big Earth Data, 5, 2, pp. 237-257, (2021); Nava L., Monserrat O., Catani F., Improving landslide detection on sar data through deep learning, arXiv Preprint arXiv:2105.00782, (2021); Nhu V.-H., Mohammadi A., Shahabi H., Ahmad B.B., Al-Ansari N., Shirzadi A., Nguyen H., Landslide detection and susceptibility modeling on Cameron Highlands (Malaysia): A comparison between random forest, logistic regression and logistic model tree algorithms, Forests, 11, 8, (2020); Osanai N., Yamada T., Hayashi S.-I., Kastura S., Furuichi T., Yanai S., Miyazaki M., Characteristics of landslides caused by the 2018 Hokkaido Eastern Iburi Earthquake, Landslides, 16, 8, pp. 1517-1528, (2019); Ozaki M., Taku K., 1: 200,000 land geological map in the Ishikari depression and its surrounding area with explanatory note, Seamless Geoinformation of Coastal Zone “Southern Coastal Zone of the Ishikari Depression”, Seamless Geological Map of Costal Zone S-4, Geological Survey of Japan ALST, (2014); Ozturk U., Saito H., Matsushi Y., Crisologo I., Schwanghart W., Can global rainfall estimates (satellite and reanalysis) aid landslide hindcasting?, Landslides, 189, pp. 3119-3133, (2021); Pourghasemi H.R., Rahmati O., Prediction of the landslide susceptibility: Which algorithm, which precision?, Catena, 162, pp. 177-192, (2018); Prakash N., Manconi A., Loew S., Hermankova B., Bubova K., Komarc M., Tomcik M., A new strategy to map landslides with a generalized convolutional neural network, Sci. Rep, 11, 1, pp. 1-15, (2021); Qi W., Wei M., Yang W., Xu C., Ma C., Automatic mapping of landslides by the ResU-Net, Remote Sens-Basel, 12, 15, (2020); Rahman M., Ningsheng C., Mahmud G.I., Islam M.M., Pourghasemi H.R., Ahmad H., Dewan A., Flooding and its relationship with land cover change, population growth, and road density, Geoscience Frontiers, 12, 6, (2021); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical image computing and computer-assisted intervention, pp. 234-241, (2015); Shahabi H., Jarihani B., Tavakkoli Piralilou S., Chittleborough D., Avand M., Ghorbanzadeh O., A semi-automated object-based gully networks detection using different machine learning models: A case study of bowen catchment, Queensland, Australia, Sensors, 19, 22, (2019); Soares L.P., Dias H.C., Grohmann C.H., Landslide segmentation with U-Net: Evaluating different sampling methods and patch sizes, arXiv Preprint arXiv:2007.06672, (2020); Su Z., Chow J.K., Tan P.S., Wu J., Ho Y.K., Wang Y.-H., Deep convolutional neural network–based pixel-wise landslide inventory mapping, Landslides, 184, pp. 1421-1443, (2020); Tavakkoli Piralilou S., Shahabi H., Jarihani B., Ghorbanzadeh O., Blaschke T., Gholamnia K., Aryal J., Landslide detection using multi-scale image segmentation and different machine learning models in the Higher Himalayas, Remote Sens-Basel, 11, 21, (2019); Tehrani F.S., Santinelli G., Herrera Herrera M., Multi-regional landslide detection using combined unsupervised and supervised machine learning, Geomatics, Natural Hazards and Risk, 12, 1, pp. 1015-1038, (2021); Xu Q., Ouyang C., Jiang T., Fan X., Cheng D., DFPENet-geology: A deep learning framework for high precision recognition and segmentation of co-seismic landslides, arXiv Preprint arXiv:1908.10907, (2019); Yamagishi H., Yamazaki F., Landslides by the 2018 Hokkaido Iburi-Tobu earthquake on september 6, Landslides, 15, 12, pp. 2521-2524, (2018); Ye C., Li Y., Cui P., Liang L., Pirasteh S., Marcato J., Li J., Landslide detection of hyperspectral remote sensing data based on deep learning with constrains, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12, 12, pp. 5047-5060, (2019); Yi Y., Zhang W., A new deep-learning-based approach for earthquake-triggered landslide detection from single-temporal rapideye satellite imagery, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13, pp. 6166-6176, (2020); Zhang C., Atkinson P.M., George C., Wen Z., Diazgranados M., Gerard F., Identifying and mapping individual plants in a highly diverse high-elevation ecosystem using UAV imagery and deep learning, ISPRS Journal of Photogrammetry and Remote Sensing, 169, pp. 280-291, (2020); Zhang S., Li R., Wang F., Iio A., Characteristics of landslides triggered by the 2018 Hokkaido Eastern Iburi earthquake, Northern Japan, Landslides, 16, 9, pp. 1691-1708, (2019); Zhang Z., Liu Q., Wang Y., Road extraction by deep residual u-net, IEEE Geoscience and Remote Sensing Letters, 15, 5, pp. 749-753, (2018); Zhu X.X., Tuia D., Mou L., Xia G.-S., Zhang L., Xu F., Fraundorfer F., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geoscience and Remote Sensing Magazine, 5, 4, pp. 8-36, (2017)","O. Ghorbanzadeh; Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Landstraßer Hauptstraße 5, 1030, Austria; email: omid.ghorbanzadeh@iarai.ac.at","","Taylor and Francis Ltd.","","","","","","20964471","","","","English","Big Earth Data","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85125285339"
"Prabowo Y.; Pradono K.A.; Amriyah Q.; Rasyidy F.H.; Carolita I.; Setiyoko A.; Candra D.S.; Musyarofah; Ulfa K.; Hestrio Y.F.","Prabowo, Yudhi (57406702100); Pradono, Kuncoro Adi (57205320164); Amriyah, Qonita (57747133400); Rasyidy, Fadillah Halim (57747133500); Carolita, Ita (56237384100); Setiyoko, Andie (56237045300); Candra, Danang Surya (56237082500); Musyarofah (57203240804); Ulfa, Kurnia (57196246436); Hestrio, Yohanes Fridolin (57203090895)","57406702100; 57205320164; 57747133400; 57747133500; 56237384100; 56237045300; 56237082500; 57203240804; 57196246436; 57203090895","Palm Trees Counting Using MobileNet Convolutional Neural Network in Very High-Resolution Satellite Images","2022","2022 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology: Understanding the Interaction of Land, Ocean, and Atmosphere: Smart City and Disaster Mitigation for Regional Resilience, AGERS 2022 - Proceeding","","","","79","83","4","0","10.1109/AGERS56232.2022.10093287","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156139745&doi=10.1109%2fAGERS56232.2022.10093287&partnerID=40&md5=50c4fea0dfb581090e3ebdc6ee48fb13","Research Center for Hydrodynamics Technology, National Research and Innovation Agency (BRIN), Yogyakarta, Indonesia; National Research and Innovation Agency (BRIN), Directorate of Laboratory Management, Research Facilities, and Science and Technology Park, Jakarta, Indonesia; Center for Data and Information, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia","Prabowo Y., Research Center for Hydrodynamics Technology, National Research and Innovation Agency (BRIN), Yogyakarta, Indonesia; Pradono K.A., National Research and Innovation Agency (BRIN), Directorate of Laboratory Management, Research Facilities, and Science and Technology Park, Jakarta, Indonesia; Amriyah Q., Center for Data and Information, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Rasyidy F.H., Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Carolita I., Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Setiyoko A., Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Candra D.S., Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Musyarofah, Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Ulfa K., Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia; Hestrio Y.F., Research Center for Remote Sensing, National Research and Innovation Agency (BRIN), Jakarta, Indonesia","Indonesia has a large area of oil palm plantation. Information related to the spatial distribution and number of palm trees is essential for oil palm plantation management and monitoring. The common standard of monitoring the number of oil palm trees has been either manually counting at the plantation itself or from the given aerial images. Manual counting requires many workers and has potential problems related to accuracy. This article presents an approach to the extraction and counting of oil palm trees using deep learning approach. We investigate the use of MobileNet-v1 to detect the individual palm trees from very high-resolution satellite images. MobileNet-v1 is a lightweight CNN architecture model that is usually used on smartphones or other devices with limited processing resources. The network was trained with the dataset that contains 3500 small images of size $25times 25$ pixels. The result shows that this method managed to detect oil palm trees with the precision, recall and F1 score more than 0.9. © 2022 IEEE.","convolutional neural network; image processing; MobileNet; object detection; oil palm tree; satellite image","Antennas; Convolution; Convolutional neural networks; Deep learning; Palm oil; Satellites; Convolutional neural network; Images processing; Indonesia; Mobilenet; Objects detection; Oil palm plantations; Oil palm tree; PaLM-tree; Satellite images; Very high resolution satellite images; Object detection","","","","","","","Li W., Fu H., Yu L., Cracknell A., Deep learning-based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens, 9, 1, (2017); Ke Y., Quackenbush L.J., A review of methods for automatic individual tree-crown detection and delineation from passive remote sensing, J Remote Sens, 32, 17, pp. 4725-4747, (2011); Shafri H.Z.M., Hamdan N., Saripan M.I., Semi-automatic detection and counting of oil palm trees from high spatial resolution airborne imagery, J Remote Sens, 32, 8, pp. 2095-2115, (2011); Xu K., Et al., A new machine learning approach in detecting the oil palm plantations using remote sensing data, Remote Sens, 13, 2, pp. 1-17, (2021); Nababan M., Et al., The diagnose of oil palm disease using naive bayes method based on expert system technology, Journal of Physics: Conference Series, 1007, 1, (2018); Wang Y., Zhu X., Wu B., Automatic detection of individual oil palm trees from UAV images using HOG features and an SVM classifier, J Remote Sens, 40, 19, pp. 7356-7370, (2019); Nevalainen O., Et al., Individual tree detection and classification with UAV-Based photogrammetric point clouds and hyperspectral imaging, Remote Sens, 9, 3, (2017); Waseem R., Zenghui W., Deep convolutional neural networks for image classification: A comprehensive review, Neural Comput, 29, 9, pp. 1-98, (2017); Fengyin X., Mengyun S., Zhenwei S., Jihao Y., Danpei Z., Multilevel cloud detection in remote sensing images based on deep learning, IEEE J Sel Top Appl Earth Obs Remote Sens, 10, 8, pp. 1-10, (2017); Almabdy S., Elrefaei L., Deep convolutional neural network-based approaches for face recognition, Applied Sciences, 9, 20, (2019); Prabowo Y., Candra D.S., Maulana R., Cloud detection for pleiades and spot 6/7 imageries using modified k-means and deep learning, J Adv Sci Eng Inf Tech, 11, 6, pp. 2459-2469, (2021); Lin Z., Guo W., Cotton stand counting from unmanned aerial system imagery using mobilenet and centernet deep learning models, Remote Sens, 13, 14, (2021); Howard A.G., Et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Cheuque C., Querales M., Leon R., Salas R., Torres R., An efficient multi-level convolutional neural network approach for white blood cells classification, Diagnostics, 12, 2, (2022); Prabowo Y., Et al., Deep learning dataset for estimating burned areas: Case study, Indonesia, Data, 7, 6, (2022)","","","Institute of Electrical and Electronics Engineers Inc.","Aerospace and Electronic Systems (AES); IEEE Geoscience and Remote Sensing Society (GRSS); IEEE Indonesia Section; Institute Technology Sepuluh Nopember (ITS); MAPIN","5th IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology, AGERS 2022","21 December 2022 through 22 December 2022","Surabaya","187906","","979-835039821-2","","","English","IEEE Asia-Pacific Conf. Geosci., Electron. Remote Sens. Technol.: Underst. Interact. Land, Ocean, Atmos.: Smart City Disaster Mitig. Reg. Resil., AGERS - Proceeding","Conference paper","Final","","Scopus","2-s2.0-85156139745"
"Golcarenarenji G.; Martinez-Alpiste I.; Wang Q.; Alcaraz-Calero J.M.","Golcarenarenji, Gelayol (57219550701); Martinez-Alpiste, Ignacio (57211669726); Wang, Qi (57091061800); Alcaraz-Calero, Jose Maria (25721805600)","57219550701; 57211669726; 57091061800; 25721805600","Illumination-aware image fusion for around-the-clock human detection in adverse environments from Unmanned Aerial Vehicle","2022","Expert Systems with Applications","204","","117413","","","","1","10.1016/j.eswa.2022.117413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130491591&doi=10.1016%2fj.eswa.2022.117413&partnerID=40&md5=1d1daba1f03fede5069049cfd206f19f","School of Computing, Engineering and Physical Sciences, University of the West of Scotland, United Kingdom","Golcarenarenji G., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, United Kingdom; Martinez-Alpiste I., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, United Kingdom; Wang Q., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, United Kingdom; Alcaraz-Calero J.M., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, United Kingdom","This study proposes a novel illumination-aware image fusion technique and a Convolutional Neural Network (CNN) called BlendNet to significantly enhance the robustness and real-time performance of small human objects detection from Unmanned Aerial Vehicles (UAVs) in harsh and adverse operation environments. The proposed solution is particular useful for mission-critical public safety applications such as search and rescue operations in rural areas. The operation environments of such missions are featured with poor illumination condition and complex background such as dense vegetation and undergrowth in diverse weather conditions, and the missions have to address the challenges of detecting humans from UAVs at high altitudes, with a moving platform and from various viewing angles. To overcome these challenges, the proposed solution register and fuse the images using Enhanced Correlation Coefficient (ECC) and arithmetic image addition with customised weights techniques. The result of this fusion is fuelled with our new BlendNet AI model achieving 95.01 % of accuracy with 42.2 Frames Per Second (FPS) on Titan X GPU with input size of 608 pixels. The effectiveness of the proposed fusion method has been evaluated and compared with other methods using the KAIST public dataset. The experimental results show competitive performance of BlendNet in terms of both visual quality as well as quantitative assessment of high detection accuracy at high speed. © 2022 Elsevier Ltd","Deep machine learning; Human detection; Image fusion; Image registration; UAV","Aircraft detection; Antennas; Convolutional neural networks; Deep learning; Image enhancement; Image fusion; Object detection; Adverse environment; Around the clock; Convolutional neural network; Human detection; Image fusion techniques; Images registration; Mission critical; Objects detection; Public Safety Applications; Real time performance; Unmanned aerial vehicles (UAV)","","","","","Autonomous Trust, Security and Privacy Management Framework, (101020259); EU Horizon 2020 ARCADIAN-IoT; Horizon 2020, (H2020-ICT-2020-2/section*101016941)","This work was in part funded by the Innovation Centre for Sensor and Imaging Systems (CENSIS) and in collaboration with Thales under the Grant number CAF-0680 , the EU Horizon 2020 5G-PPP 5G-INDUCE project (“Open cooperative 5G experimentation platforms for the industrial sector NetApps”) under the Grant number H2020-ICT-2020-2/section*101016941 , and the EU Horizon 2020 ARCADIAN-IoT project (“Autonomous Trust, Security and Privacy Management Framework for IoT”) under the Grant number 101020259 . The authors would like to thank all the partners in these projects for their support.","Alexey A.B., Darknet, (2020); Alexey A.B., Darknet, (2020); Bianco S., Cadene R., Celona L., Napoletano P., Benchmark analysis of representative deep neural network architectures, IEEE Access, 6, pp. 64270-64277, (2018); Bochkovskiy A., Wang C.-Y., Liao H., YOLOV4: Optimal speed and accuracy of object detection, (2020); Bradski G., The openCV library, Dr. Dobb's Journal: Software Tools for the Professional Programmer, 25, 11, pp. 120-123, (2000); Cao Y., Guan D., Huang W., Yang J., Cao Y., Qiao Y., Pedestrian detection with unsupervised multispectral feature learning using deep neural networks, Information Fusion, 46, pp. 206-217, (2019); Cao Z., Yang H., Zhao J., Guo S., Li L., Attention fusion for one-stage multispectral pedestrian detection, Sensors, 21, 12, (2021); Choi S., Kwon O.-J., Lee J., A method for fast multi-exposure image fusion, IEEE Access, 5, pp. 7371-7380, (2017); Dai J., Li Y., He K., Sun J., R-FCN: Object detection via region-based fully convolutional networks, NIPS’16: Proceedings of the 30th international conference on neural information processing systems, Vol. 29, pp. 379-387, (2016); Dandrifosse S., Carlier A., Dumont B., Mercatoris B., Registration and fusion of close-range multimodal wheat images in field conditions, Remote Sensing, 13, 7, (2021); Dawdi T.M., Abdalla N., Elkalyoubi Y.M., Soudan B., Locating victims in hot environments using combined thermal and optical imaging, Computers and Electrical Engineering, 85, (2020); Ding L., Wang Y., Laganiere R., Huang D., Luo X., Zhang H., A robust and fast multispectral pedestrian detection deep network, Knowledge-Based Systems, 227, (2021); Dollar P., Tu Z., Perona P., Belongie S., Integral channel features, Proceedings of the British machine vision conference, pp. 91.1-91.11, (2009); Duan K., Bai S., Xie L., Qi H., Huang Q., (2019); Dwibedi D., Misra I., (2017); Evangelidis G.D., Psarakis E.Z., Parametric image alignment using enhanced correlation coefficient maximization, IEEE Transactions on Pattern Analysis and Machine Intelligence, 30, 10, pp. 1858-1865, (2008); Felzenszwalb P.F., Girshick R.B., McAllester D.A., Cascade object detection with deformable part models, 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 2241-2248, (2010); Fu L., Gu W.-B., Ai Y.-B., Li W., Wang D., Adaptive spatial pixel-level feature fusion network for multispectral pedestrian detection, Infrared Physics & Technology, 116, (2021); (2015); Golcarenarenji G., Martinez-Alpiste I., Wang Q., Alcaraz-Calero J.M., Efficient real-time human detection using unmanned aerial vehicles optical imagery, International Journal of Remote Sensing, 42, 7, pp. 2440-2462, (2021); Golcarenarenji G., Martinez-Alpiste I., Wang Q., Alcaraz-Calero J.M., Machine-learning-based top-view safety monitoring of ground workforce on complex industrial sites, Neural Computing and Applications, 34, pp. 4207-4220, (2022); Guan D., Cao Y., Yang J., Cao Y., Yang M.Y., Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection, Information Fusion, 50, pp. 148-157, (2019); Hua C., Sun M., Zhu Y., Jiang Y., Yu J., Chen Y., Pedestrian detection network with multi-modal cross-guided learning, Digital Signal Processing, 122, (2022); Hwang S., Park J., Kim N., Choi Y., (2015); Hwooi S.K.W., Sabri A.Q.M., Enhanced correlation coefficient as a refinement of image registration, 2017 IEEE international conference on signal and image processing applications (ICSIPA), pp. 216-221, (2017); Jiang H., Wang S., Object detection and counting with low quality videos: Technical report, (2016); Jung A.B., Wada K., Crall J., Tanaka S., Graving J., Reinders C., Et al., Data augmentation, (2020); Kohavi R., A study of cross-validation and bootstrap for accuracy estimation and model selection, Proceedings of the fourteenth international joint conference on artificial intelligence, 1995, Vol. 2, pp. 1137-1143, (1995); Krishna H., Jawahar C., Improving small object detection, 2017 4th IAPR Asian conference on pattern recognition (ACPR), pp. 340-345, (2017); Li J., Peng Y., Jiang T., Embedded real-time infrared and visible image fusion for UAV surveillance, Journal of Real-Time Image Processing, 18, 6, pp. 2331-2345, (2021); Li C., Song D., Tong R., Tang M., Illumination-aware faster R-CNN for robust multispectral pedestrian detection, Pattern Recognition, 85, pp. 161-171, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Et al., Microsoft coco: Common objects in context, European conference on computer vision, pp. 740-755, (2014); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Et al., Ssd: Single shot multibox detector, Proceedings of the European conference on computer vision (ECCV), pp. 21-37, (2016); Liu S., Huang D., Et al., (2018); Liu S., Qi L., Qin H., Shi J., (2018); Liu J., Zhang S., Wang S., Metaxas D.N., Multispectral deep neural networks for pedestrian detection, Proceedings of the British machine vision conference (BMVC), pp. 73.1-73.13, (2016); Lopez A., Jurado J.M., Ogayar C.J., Feito F.R., A framework for registering UAV-based imagery for crop-tracking in precision agriculture, International Journal of Applied Earth Observation and Geoinformation, 97, (2021); Loshchilov I., Hutter F., Sgdr: Stochastic gradient descent with warm restarts, (2016); Martinez-Alpiste I., Casaseca-de-la-Higuera P., Alcaraz-Calero J., Grecos C., Wang Q., Benchmarking machine-learning-based object detection on a UAV and mobile platform, 2019 IEEE wireless communications and networking conference (WCNC), pp. 1-6, (2019); Martinez-Alpiste I., Casaseca-de-la-Higuera P., Alcaraz-Calero J.M., Grecos C., Wang Q., Smartphone-based object recognition with embedded machine learning intelligence for unmanned aerial vehicles, Journal of Field Robotics, 37, 3, pp. 404-420, (2020); Martinez-Alpiste I., Golcarenarenji G., Wang Q., Alcaraz-Calero J.M., Altitude-adaptive and cost-effective object recognition in an integrated smartphone and UAV system, 2020 European conference on networks and communications (EuCNC), pp. 316-320, (2020); Martinez-Alpiste I., Golcarenarenji G., Wang Q., Alcaraz-Calero J.; Martinez-Alpiste I., Golcarenarenji G., Wang Q., Alcaraz-Calero J.M., Search and rescue operation using UAVs: a case study, Expert Systems with Applications, 178, (2021); Meng L., Zhou J., Liu S., Ding L., Zhang J., Wang S., Et al., Investigation and evaluation of algorithms for unmanned aerial vehicle multispectral image registration, International Journal of Applied Earth Observation and Geoinformation, 102, (2021); Pei D., Jing M., Liu H., Sun F., Jiang L., A fast RetinaNet fusion framework for multi-spectral pedestrian detection, Infrared Physics & Technology, 105, (2020); Perez L., Wang J., The effectiveness of data augmentation in image classification using deep learning, (2017); Piao J., Chen Y., Shin H., A new deep learning based multi-spectral image fusion method, Entropy, 21, 6, (2019); Raudonis V., Paulauskaite-Taraseviciene A., Sutiene K., Fast multi-focus fusion based on deep learning for early-stage embryo image enhancement, Sensors, 21, 3, (2021); Redmon J., Divvala S., Girshick R., (2016); Redmon J., (2017); Redmon J., Farhadi A., Yolov3: An incremental improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Advances in neural information processing systems, Vol. 28, pp. 91-99, (2015); Roszyk K., Nowicki M.R., Skrzypczynski P., Adopting the YOLOv4 architecture for low-latency multispectral pedestrian detection in autonomous driving, Sensors, 22, 3, (2022); Rudol P., Doherty P., Human body detection and geolocalization for UAV search and rescue missions using color and thermal imagery, 2008 IEEE aerospace conference, pp. 1-8, (2008); Song X., Gao S., Chen C., A multispectral feature fusion network for robust pedestrian detection, Alexandria Engineering Journal, 60, 1, pp. 73-85, (2021); Surasak T., Takahiro I., Cheng C.-H., Wang C.-E., Sheng P.-Y., Histogram of oriented gradients for human detection in video, 2018 5th international conference on business and industrial research (ICBIR), pp. 172-176, (2018); Teutsch M., Kruger W., Detection, segmentation, and tracking of moving objects in UAV videos, 2012 IEEE ninth international conference on advanced video and signal-based surveillance, pp. 313-318, (2012); Van Etten A., You only look twice: Rapid multi-scale object detection in satellite imagery, (2018); Vandersteegen M., Beeck K.V., Goedeme T., Real-time multispectral pedestrian detection with a single-pass deep neural network, International conference image analysis and recognition, pp. 419-426, (2018); Wang C.-Y., Liao H.-Y.M., Wu Y.-H., Chen P.-Y., Hsieh J.-W., (2020); Wei X., Zhang H., Liu S., Lu Y., Pedestrian detection in underground mines via parallel feature transfer network, Pattern Recognition, 103, (2020); Wu Z., Wu X., Zhu Y., Zhai J., Yang H., Yang Z., Et al., Research on multimodal image fusion target detection algorithm based on generative adversarial network, Wireless Communications and Mobile Computing, 2022, (2022); Xue Y., Ju Z., Li Y., Zhang W., MAF-YOLO: MUlti-modal attention fusion based YOLO for pedestrian detection, Infrared Physics & Technology, 118, (2021); Yu X., Gong Y., Jiang N., Ye Q., Han Z., Scale match for tiny person detection, The IEEE winter conference on applications of computer vision, pp. 1257-1265, (2020); Yu K., Ma J., Hu F., Ma T., Quan S., Fang B., A grayscale weight with window algorithm for infrared and visible image registration, Infrared Physics & Technology, 99, pp. 178-186, (2019); Zhang S., Wen L., Bian X., Lei Z., Li S., (2018); Zhao Q., Sheng T., Wang Y., Tang Z., Chen Y., Cai L., Et al., (2019)","G. Golcarenarenji; School of Computing, Engineering and Physical Sciences, University of the West of Scotland, United Kingdom; email: g.golcarenarenji@uws.ac.uk","","Elsevier Ltd","","","","","","09574174","","ESAPE","","English","Expert Sys Appl","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85130491591"
"Constantinou G.; You S.; Shahabi C.","Constantinou, George (56906349900); You, Suya (7201517015); Shahabi, Cyrus (7004263107)","56906349900; 7201517015; 7004263107","Towards Scalable and Efficient Client Selection for Federated Object Detection","2022","Proceedings - International Conference on Pattern Recognition","2022-August","","","5140","5146","6","0","10.1109/ICPR56361.2022.9956464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143625202&doi=10.1109%2fICPR56361.2022.9956464&partnerID=40&md5=1b1d4d45048eadae5271fb13e291d9d8","University of Southern California, Integrated Media Systems Center, Los Angeles, 90089, CA, United States; Army Research Lab, Adelphi, MD, United States","Constantinou G., University of Southern California, Integrated Media Systems Center, Los Angeles, 90089, CA, United States; You S., Army Research Lab, Adelphi, MD, United States; Shahabi C., University of Southern California, Integrated Media Systems Center, Los Angeles, 90089, CA, United States","Various computer vision techniques based on deep neural networks have been proposed to detect objects accurately and fast. However, due to the privacy, security and communication bandwidth restrictions of diverse participating parties, it is sometimes prohibitive to train such models on a centralized machine. Federated Learning (FL) provides a promising solution to learn a model from decentralized data. Despite the advances in FL, the diversity of client regions in which they operate and the Non-IID nature of the crowdsourced datasets reduces the accuracy of object detection models significantly. In this paper, we introduce a novel FL object detection system to efficiently train models with heterogeneous client datasets. We propose lightweight client selection methods to learn object detection models faster. Our client selection methods based on the object data distribution at clients achieves up to 74% reduction in required federated rounds compared to conventional approaches. We further extend this method by leveraging the metadata of the training images (e.g., location, direction, depth), to select clients which maximize the coverage of diverse geographical regions. We report on extensive experiments with real datasets. © 2022 IEEE.","","Deep neural networks; Geographical regions; Object recognition; Bandwidth restrictions; Centralised; Communication bandwidth; Computer vision techniques; Decentralised; Detection models; Learn+; Learning objects; Objects detection; Selection methods; Object detection","","","","","","","McMahan B., Moore E., Ramage D., Hampson S., Arcas B.A.Y., Communication-efficient learning of deep networks from decentralized data, Artificial Intelligence and Statistics, pp. 1273-1282, (2017); Kairouz P., McMahan H.B., Avent B., Bellet A., Bennis M., Bhagoji A.N., Bonawitz K.A., Charles Z., Cormode G., Cummings R., D'Oliveira R.G., Rouayheb S.E., Evans D., Gardner J., Garrett Z., Gascon A., Ghazi B., Gibbons P.B., Gruteser M., Harchaoui Z., He C., He L., Huo Z., Hutchinson B., Hsu J., Jaggi M., Javidi T., Joshi G., Khodak M., Konecny J., Korolova A., Koushanfar F., Koyejo S., Lepoint T., Liu Y., Mittal P., Mohri M., Nock R., Ozgur A., Pagh R., Raykova M., Qi H., Ramage D., Raskar R., Song D., Song W., Stich S.U., Sun Z., Suresh A.T., Tramer F., Vepakomma P., Wang J., Xiong L., Xu Z., Yang Q., Yu F.X., Yu H., Zhao S., Advancements and open problems in federated learning, (2019); Nishio T., Yonetani R., Client selection for federated learning with heterogeneous resources in mobile edge, ICC 2019-2019 IEEE International Conference on Communications (ICC), pp. 1-7, (2019); Ren S., He K., Girshick R., Sun J., Faster rcnn: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, 28, pp. 91-99, (2015); Lin T., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2999-3007, (2017); Tan M., Pang R., Le Q.V., Efficientdet: Scalable and efficient object detection, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10778-10787, (2020); Konecny J., McMahan H.B., Yu F.X., Richtarik P., Suresh A.T., Bacon D., Federated learning: Strategies for improving communication efficiency, NIPS Workshop on Private Multi-Party Machine Learning, (2016); Smith V., Chiang C.-K., Sanjabi M., Talwalkar A.S., Federated multi-task learning, Advances in Neural Information Processing Systems, 30, pp. 4424-4434, (2017); Zhao Y., Li M., Lai L., Suda N., Civin D., Chandra V., Federated learning with non-iid data, (2018); Bonawitz K., Eichner H., Grieskamp W., Huba D., Ingerman A., Ivanov V., Kiddon C., Konecny J., Mazzocchi S., McMahan H.B., Et al., Towards federated learning at scale: System design, (2019); Wang S., Tuor T., Salonidis T., Leung K.K., Makaya C., He T., Chan K., Adaptive federated learning in resource constrained edge computing systems, IEEE Journal on Selected Areas in Communications, 37, 6, pp. 1205-1221, (2019); Mohri M., Sivek G., Suresh A.T., Agnostic federated learning, ICML, (2019); Li T., Sanjabi M., Beirami A., Smith V., Fair resource allocation in federated learning, International Conference on Learning Representations, (2020); Imteaj A., Thakker U., Wang S., Li J., Amini M.H., Federated learning for resource-constrained iot devices: Panoramas and state-ofthe-art, (2020); Redmon J., Farhadi A., Yolov3: An incremental improvement, (2018); Liu Y., Huang A., Luo Y., Huang H., Liu Y., Chen Y., Feng L., Chen T., Yu H., Yang Q., Fedvision: An online visual object detection platform powered by federated learning, AAAI, pp. 13172-13179, (2020); Luo J., Wu X., Luo Y., Huang A., Huang Y., Liu Y., Yang Q., Real-world image datasets for federated learning, (2019); Yu P., Liu Y., Federated object detection: Optimizing object detection model with federated learning, Proceedings of the 3rd International Conference on Vision, Image and Signal Processing, ser. ICVISP 2019, (2019); Li X., Huang K., Yang W., Wang S., Zhang Z., On the convergence of fedavg on non-iid data, (2019); Wang J., Liu Q., Liang H., Joshi G., Poor H.V., Tackling the objective inconsistency problem in heterogeneous federated optimization, (2020); Karimireddy S.P., Kale S., Mohri M., Reddi S., Stich S., Suresh A.T., SCAFFOLD: Stochastic controlled averaging for federated learning, Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, 119, pp. 5132-5143, (2020); Pan C.-S., Sui S.-L., Ling Liu C., Shi Y.-X., Proportional fair scheduling algorithm based on traffic in satellite communication system, Fourth Seminar on Novel Optoelectronic Detection Technology and Application, pp. 1330-1336, (2018); Ay S.A., Zimmermann R., Kim S.H., Viewable scene modeling for geospatial video search, Proceedings of the 16th ACM International Conference on Multimedia, ser. MM '08, pp. 309-318, (2008); Vincenty T., Direct and inverse solutions of geodesics on the ellipsoid with application of nested equations, Survey Review, 23, 176, pp. 88-93, (1975); Constantinou G., Shahabi C., Kim S.H., Spatial keyframe extraction of mobile videos for efficient object detection at the edge, Proceedings of the 27th IEEE International Conference on Image Processing, ser. ICIP '20, (2020); Tensorflow federated: Machine learning on decentralized data; Tensorflow object detection api; Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, Computer Vision-ECCV 2014, pp. 740-755, (2014); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The pascal visual object classes (voc) challenge, International Journal of Computer Vision, 88, 2, pp. 303-338, (2010); Geiger A., Lenz P., Stiller C., Urtasun R., Vision meets robotics: The kitti dataset, International Journal of Robotics Research (IJRR), (2013)","","","Institute of Electrical and Electronics Engineers Inc.","International Association for Pattern Recognition (IAPR)","26th International Conference on Pattern Recognition, ICPR 2022","21 August 2022 through 25 August 2022","Montreal","184755","10514651","978-166549062-7","PICRE","","English","Proc. Int. Conf. Pattern Recognit.","Conference paper","Final","","Scopus","2-s2.0-85143625202"
"Ammous D.; Chabbouh A.; Edhib A.; Chaari A.; Kammoun F.; Masmoudi N.","Ammous, Donia (57208340395); Chabbouh, Achraf (57209400574); Edhib, Awatef (58134473900); Chaari, Ahmed (57223112392); Kammoun, Fahmi (9337532900); Masmoudi, Nouri (56274857900)","57208340395; 57209400574; 58134473900; 57223112392; 9337532900; 56274857900","Improved YOLOv3-tiny for Silhouette Detection Using Regularisation Techniques","2023","International Arab Journal of Information Technology","20","2","","270","281","11","1","10.34028/iajit/20/2/14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149643256&doi=10.34028%2fiajit%2f20%2f2%2f14&partnerID=40&md5=31f24806262fc82cc32ee87e7a4d143a","National School of Engineers of Sfax, University of Sfax, Sfax, Tunisia; Anavid France, Road Penthièvre 10, France; Sogimel: a Consulting Company in Computer Engineering and Video Surveillance, Technopole, Sfax, Tunisia","Ammous D., National School of Engineers of Sfax, University of Sfax, Sfax, Tunisia; Chabbouh A., Anavid France, Road Penthièvre 10, France; Edhib A., Sogimel: a Consulting Company in Computer Engineering and Video Surveillance, Technopole, Sfax, Tunisia; Chaari A., Anavid France, Road Penthièvre 10, France; Kammoun F., National School of Engineers of Sfax, University of Sfax, Sfax, Tunisia; Masmoudi N., National School of Engineers of Sfax, University of Sfax, Sfax, Tunisia","Although recent advances in Deep Learning (DL) algorithms have been developed in many Computer Vision (CV) tasks with a high accuracy level, detecting humans in video streams is still a challenging problem. Several studies have, therefore, focused on the regularisation techniques to prevent the overfitting problem which is one of the most fundamental issues in the Machine Learning (ML) area. Likewise, this paper thoroughly examines these techniques, suggesting an improved you Only Look Once (YOLO) v3-tiny based on a modified neural network and an adjusted hyperparameters file configuration. The obtained experimental results, which are validated on two experimental tests, show that the proposed method is more effective than the YOLOv3-tiny predecessor model. The first test which includes only the data augmentation techniques indicates that the proposed approach reaches higher accuracy rates than the original YOLOv3-tiny model. Indeed, Visual Object Classes (VOC) test dataset accuracy rate increases by 32.54 % compared to the initial model. The second test which combines the three tasks reveals that the adopted combined method wins a gain over the existing model. For instance, the labelled crowd_human test dataset accuracy percentage rises by 22.7 % compared to the data augmentation model. © 2023, Zarka Private University. All rights reserved.","convolutional neural network; GPU; loss function; Silhouette/person detection; YOLOv3-tiny","","","","","","ANAVID; ANAVID Company; LETI; Ministère de l’Enseignement Supérieur et de la Recherche Scientifique, MESRS","This work was performed under the MOBIDOC PROMESSE fund from the Tunisian ministry of higher education and scientific research. This research publication is supported by ANAVID France. I would like to express my sincere gratitude to ANAVID Company and LETI research laboratory. I am grateful for the constructive feedback from the reviewers.","Ayadi S., Ben Said A., Jabbar R., Aloulou C., Chabbouh A., Achballah A., Dairy Cow Rumination Detection: A Deep Learning Approach, Proceedings of International Workshop on Distributed Computing for Emerging Smart Networks, pp. 123-139, (2020); Al-Sa'd M., Al-Ali A., Mohamed A., Khattab T., Erbad A., RF-Based Drone Detection And Identification Using Deep Learning Approaches: An Initiative Towards A Large Open Source Drone Database, Future Generation Computer Systems, 100, pp. 86-97, (2019); Ammous D., kallel A., Kammoun F., Masmoudi N., Analysis of Coding and Transfer of Arien Video Sequences from H. 264 Standard,  International Conference on Advanced Technologies for Signal and Image Processing, pp. 1-5, (2020); David B., Rangasamy D., Spatial-Contextual Texture and Edge Analysis Approach for Unsupervised Change Detection of Faces in Counterfeit Images, International Journal of Computers and Applications, 37, 3-4, pp. 143-159, (2015); Dong X., Han Y., Li W., Li B., Pedestrian Detection in Metro Station Based on Improved SSD,  International Conference on Intelligent Systems and Knowledge Engineering, pp. 936-939, (2019); Everingham M., Van Gool L., Williams C., Winn J., Zisserman A., The Pascal Visual Object Classes (Voc) Challenge, International Journal of Computer Vision, 88, pp. 303-338, (2010); Ghalleb A., Boumaiza S., Amara N., Demographic Face Profiling Based on Age, Gender and Race,  International Conference on Advanced Technologies for Signal and Image Processing, pp. 1-6, (2020); Gollapudi S., Object Detection and Recognition, Proceedings of Learn Computer Vision Using OpenCV, pp. 97-117, (2019); Girshick R., Donahue J., Darrell T., Malik J., Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Girshick R., Fast R-Cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Felzenszwalb P., McAllester D., Ramanan D., A Discriminatively Trained, Multiscale, Deformable Part Model, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8, (2008); Huang M., Wu Y., GCS-YOLOV4-Tiny: A Lightweight Group Convolution Network for Multi-Stage Fruit Detection, Mathematical Biosciences and Engineering, 20, 1, pp. 241-268, (2022); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings IEEE Conference on Computer Vision Pattern Recognition, pp. 770-778, (2016); Jiang Z., Zhao L., Li S., Jia Y., Real-Time Object Detection Method for Embedded Devices, Computer Vision and Pattern Recognition, 3, pp. 1-11, (2020); Jamiya S., Rani E., LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm, Optik, 225, (2021); Kessentini Y., Besbes M., Ammar S., Chabbouh A., A Two-Stage Deep Neural Network for Multi-Norm License Plate Detection and Recognition, Expert Systems with Applications, 136, pp. 159-170, (2019); Kong W., Hong J., Jia M., Yao J., Cong W., Hu H., Zhang H., YOLOv3-DPFIN: A Dual-Path Feature Fusion Neural Network for Robust Real-Time Sonar Target Detection, IEEE Sensors Journal, 20, 7, pp. 3745-3756, (2019); Lin Y., Cai R., Lin P., Cheng S., A Detection Approach for Bundled Log Ends Using K-Median Clustering and Improved Yolov4-Tiny Network, Computers and Electronics in Agriculture, 194, (2022); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C., Berg A., Ssd: Single Shot Multibox Detector, Proceedings of European Conference on Computer Vision, pp. 21-37, (2016); Lin T., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C., Microsoft Coco: Common Objects Incontext, Proceedings of Computer Vision-ECCV, pp. 740-755, (2014); Mzoughi H., Njeh I., Wali A., Slima M., BenHamida A., Mhiri C., Mahfoudhe K., Deep Multi-Scale 3D Convolutional Neural Network (CNN) For MRI Gliomas Brain Tumor Classification, Journal of Digital Imaging, 33, 4, pp. 903-915, (2020); Nasri M., Hmani M., Mtibaa A., Petrovska-Delacretaz D., Slima M., Hamida A., Face Emotion Recognition From Static Image Based on Convolution Neural Networks,  International Conference on Advanced Technologies for Signal and Image Processing, pp. 1-6, (2020); Niu J., Chen Y., Yu X., Li Z., Gao H., Data Augmentation on Defect Detection of Sanitary Ceramics,  Annual Conference of the IEEE Industrial Electronics Society, pp. 5317-5322, (2020); Ogundoyin S., An Autonomous Lightweight Conditional Privacy-Preserving Authentication Scheme with Provable Security for Vehicular Ad-Hoc Networks, International Journal of Computers and Applications, 42, 2, pp. 1-16, (2018); Pokkuluri K., Nedunuri S., Crop Disease Prediction with Convolution Neural Network (CNN) Augmented with Cellular Automata, The International Arab Journal of Information Technology, 19, 5, pp. 765-773, (2022); Felzenszwalb P., Girshick R., McAllester D., Ramanan D., Object Detection with Discriminatively Trained Part-Based Models, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32, 9, pp. 1627-1645, (2009); Prasetyo E., Suciati N., Fatichah C., Yolov4-Tiny and Spatial Pyramid Pooling for Detecting Head and Tail of Fish, Proceedings of International Conference on Artificial Intelligence and Computer Science Technology, pp. 157-161, (2021); Piotrowski A., Napiorkowski J., A Comparison of Methods to Avoid Overfitting in Neural Networks Training in The Case of Catchment Runoff Modelling, Journal of Hydrology, 476, pp. 97-111, (2013); Qi H., Xu T., Wang G., Cheng Y., Chen C., MYOLOv3-Tiny: “A New Convolutional Neural Network Architecture for Real-Time Detection of Track Fasteners, Computers in Industry, 123, (2020); Ren S., He K., Girshick R., Sun J., Faster r-Cnn: Towards Real-Time Object Detection with Region Proposal Networks, Advances in Neural Information Processing Systems, 28, (2015); Ren C., Kim D., Jeong D., A Survey of Deep Learning in Agriculture: Techniques and Their Applications, Journal of Information Processing Systems, 16, 5, pp. 1015-1033, (2020); Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A, YOLO9000: Better, Faster, Stronger, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 6517-6525, (2017); Redmon J., Farhadi A, YOLOv3: An incremental improvement, (2018); Ranjbar M., Mori G., Wang Y., Optimizing Complex Loss Functions in Structured Prediction, Proceedings of European Conference on Computer Vision, pp. 580-593, (2010); Redmon J.; Srivastava N., Hinton G., Krizhevsky A., Sutskever I., Salakhutdinov R., Dropout:A Simple Way to Prevent Neural Networks From Overfitting, The Journal of Machine Learning Research, 15, 1, pp. 1929-1958, (2014); Shao S., Zhao Z., Li B., Xiao T., Yu G., Zhang X., Sun J., Crowdhuman: A Benchmark for Detecting Human in A Crowd, (2018); Wang Y., Jia K., Liu P., Impolite Pedestrian Detection by Using Enhanced Yolov3-Tiny, Journal of Artificial Intelligence, 2, 3, pp. 113-124, (2020); Wang L., Shi J., Song G., Shen I., Object Detection Combining Recognition and Segmentation, Proceedings of Asian Conference on Computer Vision, pp. 189-199, (2007); Xun Z., Wang L., Liu Y., Improved Face Detection Algorithm Based on Multitask Convolutional Neural Network for Unmanned Aerial Vehicles View, Journal of Electronic Imaging, 31, 6, (2022); Yang Z., Xu W., Wang Z., He X., Yang F., Yin Z., Combining YOLOV3-Tiny Model with Dropblock for Tiny-Face Detection,  International Conference on Communication Technology, pp. 1673-1677, (2019); Yolo: Open Source Neural Networks in C, (2021); Ying X., An Overview of Overfitting and Its Solutions, Journal of Physics: Conference Series, 1168, 2, (2019); Yi Z., Yongliang S., Jun Z., An Improved Tiny-Yolov3 Pedestrian Detection Algorithm, Optik, 183, pp. 17-23, (2019); Zhang P., Zhong Y., Li X., SlimYOLOv3: Narrower, Faster And Better for Real-Time UAV Applications, Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, (2019); Zhang S., Wen L., Bian X., Lei Z., Li S., Single-shot Refinement Neural Network For Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4203-4212, (2018)","","","Zarka Private University","","","","","","16833198","","","","English","Int. Arab J. of Info. Tech.","Article","Final","","Scopus","2-s2.0-85149643256"
"James K.; Bradshaw K.","James, Katherine (57211254326); Bradshaw, Karen (36650469800)","57211254326; 36650469800","Shrub Detection in High-Resolution Imagery: A Comparative Study of Two Deep Learning Approaches","2022","Communications in Computer and Information Science","1528 CCIS","","","545","561","16","0","10.1007/978-3-030-95502-1_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125226172&doi=10.1007%2f978-3-030-95502-1_41&partnerID=40&md5=3761f1e0d6fc5bdaadbe3bd11f24eb95","Rhodes University, Grahamstown, South Africa","James K., Rhodes University, Grahamstown, South Africa; Bradshaw K., Rhodes University, Grahamstown, South Africa","A common task in high-resolution remotely-sensed aerial imagery is the detection of particular target plant species for various ecological and agricultural applications. Although traditionally object-based image analysis approaches have been the most popular method for this task, deep learning approaches such as image patch-based convolutional neural networks (CNNs) have been seen to outperform these older approaches. To a lesser extent, fully convolutional networks (FCNs) that allow for semantic segmentation of images, have also begun to be used in the broader literature. This study investigates patch-based CNNs and FCN-based segmentation for shrub detection, targeting a particular invasive shrub genus. The results show that while a patch-based CNN demonstrates strong performance on ideal image patches, the FCN outperforms this approach on real-world proposed image patches with a 52% higher object-level precision and comparable recall. This indicates that FCN-based segmentation approaches are a promising alternative to patch-based approaches, with the added advantage of not requiring any hand-tuning of a patch proposal algorithm. © 2022, Springer Nature Switzerland AG.","Classification; Convolutional neural networks; Plant species detection; Segmentation","Aerial photography; Antennas; Convolution; Convolutional neural networks; Deep learning; Image classification; Semantic Segmentation; Convolutional networks; Convolutional neural network; High resolution imagery; Image patches; Learning approach; Network-based; Patch based; Plant species; Plant species detection; Segmentation; Semantics","","","","","Telkom Centre of Excellence; Rhodes University","Acknowledgements. The authors acknowledge funding for this research from Rhodes University and the Telkom Centre of Excellence.","Baron J., Hill D., Elmiligi H., Combining image processing and machine learning to identify invasive plants in high-resolution images, International Journal of Remote Sensing, 39, 15-16, pp. 5099-5118, (2018); Blaschke T., Object based image analysis for remote sensing, ISPRS Journal of Photogrammetry and Remote Sensing, 65, 1, pp. 2-16, (2010); Chollet F., Xception: Deep learning with depthwise separable convolutions, 2017 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1800-1807, (2017); Flood N., Watson F., Collett L., Using a U-Net convolutional neural network to map woody vegetation extent from high resolution satellite imagery across Queensland, Australia, Inte. J. Appl. Earth Obs. Geoinf., 82, (2019); Gillespie A.R., Kahle A.B., Walker R.E., Color enhancement of highly correlated images. II. Channel ratio and “chromaticity” transformation techniques, Remote Sens. Environ., 22, 3, pp. 343-365, (1987); Gitelson A.A., Kaufman Y.J., Stark R., Rundquist D., Novel algorithms for remote estimation of vegetation fraction, Remote Sensing of Environment, 80, 1, pp. 76-87, (2002); Guirado E., Tabik S., Alcaraz-Segura D., Cabello J., Herrera F., Deep-learning versus OBIA for scattered shrub detection with Google Earth imagery: Ziziphus Lotus as case study, Remote Sens, 9, 12, pp. 1-22, (2017); Hamuda E., Glavin M., Jones E., A survey of image processing techniques for plant extraction and segmentation in the field, Computers and Electronics in Agriculture, 125, pp. 184-199, (2016); Howard A.G., Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications. Arxiv Preprint Arxiv, 1704, (2017); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Computers and Electronics in Agriculture, 147, pp. 70-90, (2018); Kluge R., Neser S., Biological control of Hakea sericea (Proteaceae) in South Africa, Agriculture, Ecosystems & Environment, 37, 1-3, pp. 91-113, (1991); Lin T., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, 2017 IEEE International Conference on Computer Vision, Pp. 2999–, (2017); Liu T., Abd-Elrahman A., Jon M., Wilhelm V., Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system, Giscience & Remote Sensing, 55, 2, pp. 243-264, (2018); Mboga N., Georganos S., Grippa T., Lennert M., Vanhuysse S., Wolff E., Fully convolutional networks for the classification of aerial VHR imagery, Proceedings of the 7Th Geographic Object-Based Image Analysis Conference, pp. 1-12, (2018); Milioto A., Lottes P., Stachniss C., Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in CNNS, IEEE International Conference on Robotics and Automation (ICRA), Pp. 2229– 2235 (05, (2018); Milletari F., Navab N., Ahmadi S.A., V-Net: Fully convolutional neural networks for volumetric medical image segmentation, 2016 Fourth International Conference on 3D Vision, pp. 565-571, (2016); Rahman M.A., Wang Y., Optimizing intersection-over-union in deep neural networks for image segmentation, International Symposium on Visual Computing, pp. 234-244, (2016); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Sa I., Et al., Weednet: Dense semantic weed classification using multispectral images and MAV for smart farming, IEEE Rob. Autom. Lett., 20, pp. 588-595, (2017); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826, (2016); Woebbecke D.M., Meyer G.E., von Bargen K., Mortensen D., Color indices for weed identification under various soil, residue, and lighting conditions, Trans. Am. Soc. Agric. Eng., 38, 1, pp. 259-269, (1995); Yosinski J., Clune J., Bengio Y., Lipson H., How transferable are features in deep neural networks?, Proceedings of the 27Th International Conference on Neural Information Processing Systems, NIPS 2014, Vol. 2, pp. 3320-3328, (2014); Zhao T., Yang Y., Niu H., Wang D., Chen Y., Comparing U-net convolutional network with mask R-CNN in the performances of pomegranate tree canopy segmentation, Multispectral, Hyperspectral, and Ultraspectral Remote Sensing Technology, Techniques and Applications VII, Vol. 10780, pp. 210-218, (2018)","K. Bradshaw; Rhodes University, Grahamstown, South Africa; email: k.bradshaw@ru.ac.za","Garg D.; Jagannathan S.; Gupta A.; Garg L.; Gupta S.","Springer Science and Business Media Deutschland GmbH","","11th International Advanced Computing Conference, IACC 2021","18 December 2021 through 19 December 2021","Msida","272079","18650929","978-303095501-4","","","English","Commun. Comput. Info. Sci.","Conference paper","Final","","Scopus","2-s2.0-85125226172"
"Tugrul B.; Elfatimi E.; Eryigit R.","Tugrul, Bulent (55258504400); Elfatimi, Elhoucine (57422481000); Eryigit, Recep (6602419330)","55258504400; 57422481000; 6602419330","Convolutional Neural Networks in Detection of Plant Leaf Diseases: A Review","2022","Agriculture (Switzerland)","12","8","1192","","","","21","10.3390/agriculture12081192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140456325&doi=10.3390%2fagriculture12081192&partnerID=40&md5=700480525007781baa7ca6033ddaecd3","Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey","Tugrul B., Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey; Elfatimi E., Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey; Eryigit R., Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey","Rapid improvements in deep learning (DL) techniques have made it possible to detect and recognize objects from images. DL approaches have recently entered various agricultural and farming applications after being successfully employed in various fields. Automatic identification of plant diseases can help farmers manage their crops more effectively, resulting in higher yields. Detecting plant disease in crops using images is an intrinsically difficult task. In addition to their detection, individual species identification is necessary for applying tailored control methods. A survey of research initiatives that use convolutional neural networks (CNN), a type of DL, to address various plant disease detection concerns was undertaken in the current publication. In this work, we have reviewed 100 of the most relevant CNN articles on detecting various plant leaf diseases over the last five years. In addition, we identified and summarized several problems and solutions corresponding to the CNN used in plant leaf disease detection. Moreover, Deep convolutional neural networks (DCNN) trained on image data were the most effective method for detecting early disease detection. We expressed the benefits and drawbacks of utilizing CNN in agriculture, and we discussed the direction of future developments in plant disease detection. © 2022 by the authors.","deep learning; machine learning; plant leaf diseases","","","","","","","","Altieri M.A., Agroecology: The Science of Sustainable Agriculture, (2018); Gebbers R., Adamchuk V.I., Precision agriculture and food security, Science, 327, pp. 828-831, (2010); Carvalho F.P., Agriculture, pesticides, food security and food safety, Environ. Sci. Policy, 9, pp. 685-692, (2006); Mohanty S.P., Hughes D.P., Salathe M., Using deep learning for image-based plant disease detection, Front. Plant Sci, 7, (2016); Miller S.A., Beed F.D., Harmon C.L., Plant disease diagnostic capabilities and networks, Annu. Rev. Phytopathol, 47, pp. 15-38, (2009); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Najafabadi M.M., Villanustre F., Khoshgoftaar T.M., Seliya N., Wald R., Muharemagic E., Deep learning applications and challenges in big data analytics, J. Big Data, 2, pp. 1-21, (2015); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Abade A., Ferreira P.A., de Barros Vidal F., Plant diseases recognition on images using convolutional neural networks: A systematic review, Comput. Electron. Agric, 185, (2021); Dhaka V.S., Meena S.V., Rani G., Sinwar D., Ijaz M.F., Wozniak M., A survey of deep convolutional neural networks applied for prediction of plant leaf diseases, Sensors, 21, (2021); Nagaraju M., Chawla P., Systematic review of deep learning techniques in plant disease detection, Int. J. Syst. Assur. Eng. Manag, 11, pp. 547-560, (2020); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Fernandez-Quintanilla C., Pena J., Andujar D., Dorado J., Ribeiro A., Lopez-Granados F., Is the current state of the art of weed monitoring suitable for site-specific weed management in arable crops?, Weed Res, 58, pp. 259-272, (2018); Lu J., Tan L., Jiang H., Review on convolutional neural network (CNN) applied to plant leaf disease classification, Agriculture, 11, (2021); Golhani K., Balasundram S.K., Vadamalai G., Pradhan B., A review of neural networks in plant disease detection using hyperspectral data, Inf. Process. Agric, 5, pp. 354-371, (2018); Bangari S., Rachana P., Gupta N., Sudi P.S., Baniya K.K., A Survey on Disease Detection of a potato Leaf Using CNN, Proceedings of the 2nd IEEE International Conference on Artificial Intelligence and Smart Energy (ICAIS), pp. 144-149; LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, pp. 2278-2324, (1998); Le Cun Y., Jackel L.D., Boser B., Denker J.S., Graf H.P., Guyon I., Henderson D., Howard R.E., Hubbard W., Handwritten digit recognition: Applications of neural network chips and automatic learning, IEEE Commun. Mag, 27, pp. 41-46, (1989); Abdel-Hamid O., Mohamed A.R., Jiang H., Deng L., Penn G., Yu D., Convolutional neural networks for speech recognition, IEEE/ACM Trans. Audio Speech Lang. Process, 22, pp. 1533-1545, (2014); Kamilaris A., Prenafeta-Boldu F.X., Disaster monitoring using unmanned aerial vehicles and deep learning, arXiv, (2018); Canziani A., Paszke A., Culurciello E., An analysis of deep neural network models for practical applications, arXiv, (2016); Schmidhuber J., Deep learning in neural networks: An overview, Neural Netw, 61, pp. 85-117, (2015); Alom M.Z., Taha T.M., Yakopcic C., Westberg S., Sidike P., Nasrin M.S., Hasan M., Van Essen B.C., Awwal A.A., Asari V.K., A state-of-the-art survey on deep learning theory and architectures, Electronics, 8, (2019); Bahrampour S., Ramakrishnan N., Schott L., Shah M., Comparative study of deep learning software frameworks, arXiv, (2015); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258; Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., Darrell T., Caffe: Convolutional architecture for fast feature embedding, Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675-678; Ferentinos K.P., Deep learning models for plant disease detection and diagnosis, Comput. Electron. Agric, 145, pp. 311-318, (2018); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Et al., {TensorFlow}: A system for {Large-Scale} machine learning, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265-283; Bastien F., Lamblin P., Pascanu R., Bergstra J., Goodfellow I., Bergeron A., Bouchard N., Warde-Farley D., Bengio Y., Theano: New features and speed improvements, arXiv, (2012); Kim P., Matlab Deep Learning: With Machine Learning, Neural Networks and Artificial Intelligence, (2017); Amara J., Bouaziz B., Algergawy A., A deep learning-based approach for banana leaf diseases classification, Proceedings of the Datenbanksysteme für Business, Technologie und Web (BTW 2017)—Workshopband; Naik B.N., Ramanathan M., Palanisamy P., Detection and classification of chilli leaf disease using a squeeze-and-excitation-based CNN model, Ecol. Inform, 69, (2022); Partel V., Kim J., Costa L., Pardalos P.M., Ampatzidis Y., Smart Sprayer for Precision Weed Control Using Artificial Intelligence: Comparison of Deep Learning Frameworks, Proceedings of the International Symposium on Artificial Intelligence and Mathematics, ISAIM 2020; Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Proceedings of the Advances in Neural Information Processing Systems 28 (NIPS 2015); Redmon J., Darknet: Open Source Neural Networks in C, (2013); Binguitcha-Fare A.A., Sharma P., Crops and weeds classification using convolutional neural networks via optimization of transfer learning parameters, Int. J. Eng. Adv. Technol. (IJEAT), 8, pp. 2249-8958, (2019); Sahu P., Chug A., Singh A.P., Singh D., Singh R.P., Deep Learning Models for Beans Crop Diseases: Classification and Visualization Techniques, Int. J. Mod. Agric, 10, pp. 796-812, (2021); Mukti I.Z., Biswas D., Transfer learning based plant diseases detection using ResNet50, Proceedings of the 4th IEEE International Conference on Electrical Information and Communication Technology (EICT), pp. 1-6; Arya S., Singh R., A Comparative Study of CNN and AlexNet for Detection of Disease in Potato and Mango leaf, Proceedings of the IEEE International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT), 1, pp. 1-6; Milioto A., Lottes P., Stachniss C., Real-Time Blob-Wise Sugar Beets VS Weeds Classification for Monitoring Fields Using Convolutional Neural Networks, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 4, pp. 41-48, (2017); Lu Y., Yi S., Zeng N., Liu Y., Zhang Y., Identification of rice diseases using deep convolutional neural networks, Neurocomputing, 267, pp. 378-384, (2017); Zhang W., Hansen M.F., Volonakis T.N., Smith M., Smith L., Wilson J., Ralston G., Broadbent L., Wright G., Broad-leaf weed detection in pasture, Proceedings of the 3rd IEEE International Conference on Image, Vision and Computing (ICIVC), pp. 101-105; Liang W.C., Yang Y.J., Chao C.M., Low-cost weed identification system using drones, Proceedings of the 7th IEEE International Symposium on Computing and Networking Workshops (CANDARW), pp. 260-263; Dyrmann M., Karstoft H., Midtiby H.S., Plant species classification using deep convolutional neural network, Biosyst. Eng, 151, pp. 72-80, (2016); Chen J., Liu Q., Gao L., Visual tea leaf disease recognition using a convolutional neural network model, Symmetry, 11, (2019); Nkemelu D.K., Omeiza D., Lubalo N., Deep convolutional neural network for plant seedlings classification, arXiv, (2018); Pearlstein L., Kim M., Seto W., Convolutional neural network application to plant detection, based on synthetic imagery, Proceedings of the 2016 IEEE Applied Imagery Pattern Recognition Workshop (AIPR), pp. 1-4; Jiang Z., Dong Z., Jiang W., Yang Y., Recognition of rice leaf diseases and wheat leaf diseases based on multi-task deep transfer learning, Comput. Electron. Agric, 186, (2021); Sravan V., Swaraj K., Meenakshi K., Kora P., A deep learning based crop disease classification using transfer learning, Mater. Today Proc, (2021); Shin J., Chang Y.K., Heung B., Nguyen-Quang T., Price G.W., Al-Mallahi A., A deep learning approach for RGB image-based powdery mildew disease detection on strawberry leaves, Comput. Electron. Agric, 183, (2021); Brahimi M., Boukhalfa K., Moussaoui A., Deep learning for tomato diseases: Classification and symptoms visualization, Appl. Artif. Intell, 31, pp. 299-315, (2017); Darwish A., Ezzat D., Hassanien A.E., An optimized model based on convolutional neural networks and orthogonal learning particle swarm optimization algorithm for plant diseases diagnosis, Swarm Evol. Comput, 52, (2020); DeChant C., Wiesner-Hanks T., Chen S., Stewart E.L., Yosinski J., Gore M.A., Nelson R.J., Lipson H., Automated identification of northern leaf blight-infected maize plants from field imagery using deep learning, Phytopathology, 107, pp. 1426-1432, (2017); Sibiya M., Sumbwanyambe M., A computational procedure for the recognition and classification of maize leaf diseases out of healthy leaves using convolutional neural networks, AgriEngineering, 1, pp. 119-131, (2019); Liu B., Zhang Y., He D., Li Y., Identification of apple leaf diseases based on deep convolutional neural networks, Symmetry, 10, (2017); Nachtigall L.G., Araujo R.M., Nachtigall G.R., Classification of apple tree disorders using convolutional neural networks, Proceedings of the 28th IEEE International Conference on Tools with Artificial Intelligence (ICTAI), pp. 472-476; Yuwana R.S., Suryawati E., Zilvan V., Ramdan A., Pardede H.F., Fauziah F., Multi-condition training on deep convolutional neural networks for robust plant diseases detection, Proceedings of the 2019 IEEE International Conference on Computer, Control, Informatics and Its Applications (IC3INA), pp. 30-35; Kawasaki Y., Uga H., Kagiwada S., Iyatomi H., Basic study of automated diagnosis of viral plant diseases using convolutional neural networks, Proceedings of the International Symposium on Visual Computing, pp. 638-645, (2015); Cruz A.C., Luvisi A., De Bellis L., Ampatzidis Y., X-FIDO: An effective application for detecting olive quick decline syndrome with deep learning and data fusion, Front. Plant Sci, 8, (2017); Ha J.G., Moon H., Kwak J.T., Hassan S.I., Dang M., Lee O.N., Park H.Y., Deep convolutional neural network for classifying Fusarium wilt of radish from unmanned aerial vehicles, J. Appl. Remote Sens, 11, (2017); Dang L.M., Syed I.H., Suhyeon I., Drone agriculture imagery system for radish wilt, J. Appl. Remote Sens, 11, (2017); Liang W., Zhang H., Zhang G.F., Cao H., Rice blast disease recognition using a deep convolutional neural network, Sci. Rep, 9, pp. 1-10, (2019); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, Proceedings of the International Conference on Machine Learning, pp. 448-456; Lu J., Hu J., Zhao G., Mei F., Zhang C., An In-field Automatic Wheat Disease Diagnosis System, Comput. Electron. Agric, 142, pp. 369-379, (2017); Ramcharan A., Baranowski K., McCloskey P., Ahmed B., Legg J., Hughes D.P., Deep learning for image-based cassava disease detection, Front. Plant Sci, 8, (2017); Wang G., Sun Y., Wang J., Automatic image-based plant disease severity estimation using deep learning, Comput. Intell. Neurosci, 2017, (2017); Oppenheim D., Shani G., Potato disease classification using convolution neural networks, Adv. Anim. Biosci, 8, pp. 244-249, (2017); Durmus H., Gunes E.O., Kirci M., Disease detection on the leaves of the tomato plants by using deep learning, Proceedings of the 6th International Conference on Agro-Geoinformatics, pp. 1-5; Arivazhagan S., Ligi S.V., Mango leaf diseases identification using convolutional neural network, Int. J. Pure Appl. Math, 120, pp. 11067-11079, (2018); Rangarajan A.K., Purushothaman R., Ramesh A., Tomato crop disease classification using pre-trained deep learning algorithm, Procedia Comput. Sci, 133, pp. 1040-1047, (2018); Nandhini M., Kala K.U., Thangadarshini M., Verma S.M., Deep Learning model of sequential image classifier for crop disease detection in plantain tree cultivation, Comput. Electron. Agric, 197, (2022); Picon A., Alvarez-Gila A., Seitz M., Ortiz-Barredo A., Echazarra J., Johannes A., Deep convolutional neural networks for mobile capture device-based crop disease classification in the wild, Comput. Electron. Agric, 161, pp. 280-290, (2019); Howlader M.R., Habiba U., Faisal R.H., Rahman M.M., Automatic Recognition of Guava Leaf Diseases using Deep Convolution Neural Network, Proceedings of the 2019 International Conference on Electrical, Computer and Communication Engineering (ECCE), pp. 1-5; Singh U.P., Chouhan S.S., Jain S., Jain S., Multilayer Convolution Neural Network for the Classification of Mango Leaves Infected by Anthracnose Disease, IEEE Access, 7, pp. 43721-43729, (2019); GeethaRamani R., ArunPandian J., Identification of plant leaf diseases using a nine-layer deep convolutional neural network, Comput. Electr. Eng, 76, pp. 323-338, (2019); Fang T., Chen P., Zhang J., Wang B., Crop leaf disease grade identification based on an improved convolutional neural network, J. Electron. Imaging, 29, (2020); Mishra S., Sachan R., Rajpal D., Deep convolutional neural network based detection system for real-time corn plant disease recognition, Procedia Comput. Sci, 167, pp. 2003-2010, (2020); Mkonyi L., Rubanga D., Richard M., Zekeya N., Sawahiko S., Maiseli B., Machuve D., Early identification of Tuta absoluta in tomato plants using deep learning, Sci. Afr, 10, (2020); Karlekar A., Seal A., SoyNet: Soybean leaf diseases classification, Comput. Electron. Agric, 172, (2020); Liu B., Ding Z., Tian L., He D., Li S., Wang H., Grape Leaf Disease Identification Using Improved Deep Convolutional Neural Networks, Front. Plant Sci, 11, (2020); Ahmad J., Jan B., Farman H., Ahmad W., Ullah A., Disease Detection in Plum Using Convolutional Neural Network under True Field Conditions, Sensors, 20, (2020); Rangarajan A.K., Purushothaman R., Disease Classification in Eggplant Using Pre-trained VGG16 and MSVM, Sci. Rep, 10, (2020); Yin H., Gu Y.H., Park C.J., Park J.H., Yoo S.J., Transfer Learning-Based Search Model for Hot Pepper Diseases and Pests, Agriculture, 10, (2020); Wang F., Rao Y.L., Luo Q., Jin X., Jiang Z.H., Zhang W., Li S., Practical cucumber leaf disease recognition using improved Swin Transformer and small sample size, Comput. Electron. Agric, 199, (2022); Subetha T., Khilar R., Christo M.S., A comparative analysis on plant pathology classification using deep learning architecture—Resnet and VGG19, Mater. Today Proc, (2021); Indu V.T., Priyadharsini S.S., Crossover-based wind-driven optimized convolutional neural network model for tomato leaf disease classification, J. Plant Dis. Prot, 129, pp. 559-578, (2021); Vallabhajosyula S., Sistla V., Kolli V.K.K., Transfer learning-based deep ensemble neural network for plant leaf disease detection, J. Plant Dis. Prot, 129, pp. 545-558, (2021); Hassan S.M., Maji A.K., Jasinski M.F., Leonowicz Z., Jasinska E., Identification of Plant-Leaf Diseases Using CNN and Transfer-Learning Approach, Electronics, 10, (2021); Yadav S., Sengar N., Singh A., Singh A., Dutta M.K., Identification of disease using deep learning and evaluation of bacteriosis in peach leaf, Ecol. Inform, 61, (2021); Atila U., Ucar M., Akyol K., Ucar E., Plant leaf disease classification using EfficientNet deep learning model, Ecol. Inform, 61, (2021); Ahmad I., Hamid M., Yousaf S., Shah S.T., Ahmad M.O., Optimizing Pretrained Convolutional Neural Networks for Tomato Leaf Disease Detection, Complexity, 2020, pp. 8812019:1-8812019:6, (2020); Zhang K., Wu Q., Chen Y., Detecting soybean leaf disease from synthetic image using multi-feature fusion faster R-CNN, Comput. Electron. Agric, 183, (2021); Pandey A., Jain K., A robust deep attention dense convolutional neural network for plant leaf disease identification and classification from smart phone captured real world images, Ecol. Inform, 70, (2022); Jin H., Li Y., Qi J., Feng J., Tian D., Mu W., GrapeGAN: Unsupervised image enhancement for improved grape leaf disease recognition, Comput. Electron. Agric, 198, (2022); Javidan S.M., Banakar A., Vakilian K.A., Ampatzidis Y., Diagnosis of Grape Leaf Diseases Using Automatic K-Means Clustering and Machine Learning, SSRN Electron. J, 3, (2022); Zeng W., Li H., Hu G., Liang D., Lightweight dense-scale network (LDSNet) for corn leaf disease identification, Comput. Electron. Agric, 197, (2022); Yu H., Cheng X., Chen C., Heidari A.A., Liu J., Cai Z., Chen H., Apple leaf disease recognition method with improved residual network, Multimed. Tools Appl, 81, pp. 7759-7782, (2022); Wei K., Chen B., Zhang J., Fan S., Wu K., Liu G., Chen D., Explainable Deep Learning Study for Leaf Disease Classification, Agronomy, 12, (2022); Hanh B.T., Manh H.V., Nguyen N.V., Enhancing the performance of transferred efficientnet models in leaf image-based plant disease classification, J. Plant Dis. Prot, 129, pp. 623-634, (2022); Ravi V., Acharya V., Pham T.D., Attention deep learning-based large-scale learning classifier for Cassava leaf disease classification, Expert Syst, 39, (2022); Li X., Li S., Transformer Help CNN See Better: A Lightweight Hybrid Apple Disease Identification Model Based on Transformers, Agriculture, 12, (2022); Sun X., Li G., Qu P., Xie X., Pan X., Zhang W., Research on plant disease identification based on CNN, Cogn. Robot, 2, pp. 155-163, (2022); Jiang J., Liu H., Zhao C., He C., Ma J., Cheng T., Zhu Y., Cao W., Yao X., Evaluation of Diverse Convolutional Neural Networks and Training Strategies for Wheat Leaf Disease Identification with Field-Acquired Photographs, Remote Sens, 14, (2022); Memon M.S., Kumar P., Iqbal R., Meta Deep Learn Leaf Disease Identification Model for Cotton Crop, Computers, 11, (2022); Chen Y., Xu K., Zhou P., Ban X., He D., Improved cross entropy loss for noisy labels in vision leaf disease classification, IET Image Process, 16, pp. 1511-1519, (2022); Russel N.S., Selvaraj A., Leaf species and disease classification using multiscale parallel deep CNN architecture, Neural Comput. Appl, (2022); Gaikwad S.S., Rumma S.S., Hangarge M., Fungi affected fruit leaf disease classification using deep CNN architecture, Int. J. Inf. Technol, (2022); Prabu M., Chelliah B.J., Mango leaf disease identification and classification using a CNN architecture optimized by crossover-based levy flight distribution algorithm, Neural Comput. Appl, 34, pp. 7311-7324, (2022); Kurmi Y., Saxena P., Kirar B.S., Gangwar S., Chaurasia V., Goel A., Deep CNN model for crops’ diseases detection using leaf images, Multidimens. Syst. Signal Process, 33, pp. 981-1000, (2022); Nagi R., Tripathy S.S., Deep convolutional neural network based disease identification in grapevine leaf images, Multimed. Tools Appl, 81, pp. 24995-25006, (2022); Subramanian M., Shanmugavadivel K., Nandhini P.S., On fine-tuning deep learning models using transfer learning and hyper-parameters optimization for disease identification in maize leaves, Neural Comput. Appl, 34, pp. 13951-13968, (2022); Gajjar R., Gajjar N.P., Thakor V.J., Patel N.P., Ruparelia S., Real-time detection and identification of plant leaf diseases using convolutional neural networks on an embedded platform, Vis. Comput, 38, pp. 2923-2938, (2022); Xu Y., Kong S., Gao Z., Chen Q., Jiao Y.B., Li C., HLNet Model and Application in Crop Leaf Diseases Identification, Sustainability, 14, (2022); Singh R.K., Tiwari A., Gupta R.K., Deep transfer modeling for classification of Maize Plant Leaf Disease, Multimed. Tools Appl, 81, pp. 6051-6067, (2022); Ruth J.A., Uma R., Meenakshi A., Ramkumar P., Meta-Heuristic Based Deep Learning Model for Leaf Diseases Detection, Neural Process. Lett, (2022); Pandian J.A., Kanchanadevi K., Kumar V.D., Jasinska E., Gono R., Leonowicz Z., Jasinski M.L., A Five Convolutional Layer Deep Convolutional Neural Network for Plant Leaf Disease Detection, Electronics, 11, (2022); Pandian J.A., Kumar V.D., Geman O., Hnatiuc M., Arif M., Kanchanadevi K., Plant Disease Detection Using Deep Convolutional Neural Network, Appl. Sci, 12, (2022); Borhani Y., Khoramdel J., Najafi E., A deep learning based approach for automated plant disease classification using vision transformer, Sci. Rep, 12, (2022); Yakkundimath R., Saunshi G., Anami B.S., Palaiah S., Classification of Rice Diseases using Convolutional Neural Network Models, J. Inst. Eng. Ser. B, 103, pp. 1047-1059, (2022); Chen W., Chen J., Zeb A., Yang S., Zhang D., Mobile convolution neural network for the recognition of potato leaf disease images, Multimed. Tools Appl, 81, pp. 20797-20816, (2022); Yogeswararao G., Naresh V., Malmathanraj R., Palanisamy P., An efficient densely connected convolutional neural network for identification of plant diseases, Multimed. Tools Appl, (2022); Paymode A.S., Malode V.B., Transfer learning for multi-crop leaf disease image classification using convolutional neural networks VGG, Artif. Intell. Agric, 6, pp. 23-33, (2022); Thakur P.S., Sheorey T., Ojha A., VGG-ICNN: A Lightweight CNN model for crop disease identification, Multimed. Tools Appl, (2022); Math R.M., Dharwadkar D.N.V., Early detection and identification of grape diseases using convolutional neural networks, J. Plant Dis. Prot, 129, pp. 521-532, (2022); Kamal K., Yin Z., Wu M., Wu Z., Depthwise separable convolution architectures for plant disease classification, Comput. Electron. Agric, 165, (2019); Elfatimi E., Eryigit R., Elfatimi L., Beans Leaf Diseases Classification Using MobileNet Models, IEEE Access, 10, pp. 9471-9482, (2022); Lim S., Kim I., Kim T., Kim C., Kim S., Fast autoaugment, Adv. Neural Inf. Process. Syst, 32, pp. 6665-6675, (2019); Hendrycks D., Mu N., Cubuk E.D., Zoph B., Gilmer J., Lakshminarayanan B., Augmix: A simple data processing method to improve robustness and uncertainty, arXiv, (2019); Cubuk E.D., Zoph B., Shlens J., Le Q.V., Randaugment: Practical automated data augmentation with a reduced search space, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 702-703; Ho D., Liang E., Chen X., Stoica I., Abbeel P., Population based augmentation: Efficient learning of augmentation policy schedules, Proceedings of the International Conference on Machine Learning, pp. 2731-2741; Sladojevic S., Arsenovic M., Anderla A., Culibrk D., Stefanovic D., Deep neural networks based recognition of plant diseases by leaf image classification, Comput. Intell. Neurosci, 2016, (2016); Barbedo J.G.A., Plant disease identification from individual lesions and spots using deep learning, Biosyst. Eng, 180, pp. 96-107, (2019); Coulibaly S., Kamsu-Foguem B., Kamissoko D., Traore D., Deep neural networks with transfer learning in millet crop images, Comput. Ind, 108, pp. 115-120, (2019); Barbedo J.G.A., A review on the main challenges in automatic plant disease identification based on visible range images, Biosyst. Eng, 144, pp. 52-60, (2016); Zhang H., Tang Z., Xie Y., Gao X., Chen Q., A watershed segmentation algorithm based on an optimal marker for bubble size measurement, Measurement, 138, pp. 182-193, (2019); Barbedo J.G., Factors influencing the use of deep learning for plant disease recognition, Biosyst. Eng, 172, pp. 84-91, (2018); Taherkhani A., Cosma G., McGinnity T.M., AdaBoost-CNN: An adaptive boosting algorithm for convolutional neural networks to classify multi-class imbalanced datasets using transfer learning, Neurocomputing, 404, pp. 351-366, (2020); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proceedings of the 26th Annual Conference on Neural Information Processing Systems 25; He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, arXiv, (2014); LeCun Y., Boser B., Denker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation applied to handwritten zip code recognition, Neural Comput, 1, pp. 541-551, (1989)","R. Eryigit; Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey; email: reryigit@eng.ankara.edu.tr","","MDPI","","","","","","20770472","","","","English","Agric.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140456325"
"Peng B.; Liu X.; Meng Z.; Huang Q.","Peng, Bo (57213932049); Liu, Xinyi (57203226462); Meng, Zonglin (57211623887); Huang, Qunying (56648671800)","57213932049; 57203226462; 57211623887; 56648671800","Urban flood mapping with residual patch similarity learning","2019","Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, GeoAI 2019","","","","40","47","7","4","10.1145/3356471.3365235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075583171&doi=10.1145%2f3356471.3365235&partnerID=40&md5=8b15074ca5b8fa5ea7865d593df607b3","Department of Geography, University of Wisconsin-Madison, Madison, WI, United States; Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI, United States","Peng B., Department of Geography, University of Wisconsin-Madison, Madison, WI, United States; Liu X., Department of Geography, University of Wisconsin-Madison, Madison, WI, United States; Meng Z., Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI, United States; Huang Q., Department of Geography, University of Wisconsin-Madison, Madison, WI, United States","Urban flood mapping is essential for disaster rescue and relief missions, reconstruction efforts, and financial loss evaluation. Much progress has been made to map the extent of flooding with multisource remote sensing imagery and pattern recognition algorithms. However, urban flood mapping at high spatial resolution remains a major challenge due to three main reasons: (1) the very high resolution (VHR) optical remote sensing imagery often has heterogeneous background involving various ground objects (e.g., vehicles, buildings, roads, and trees), making traditional classification algorithms fail to capture the underlying spatial correlation between neighboring pixels within the flood hazard area; (2) traditional flood mapping methods with handcrafted features as input cannot fully leverage massive available data, which requires robust and scalable algorithms; and (3) due to inconsistent weather conditions at different time of data acquisition, pixels of the same objects in VHR optical imagery could have very different pixel values, leading to the poor generalization capability of classical flood mapping methods. To address this challenge, this paper proposed a residual patch similarity convolutional neural network (ResPSNet) to map urban flood hazard zones using bi-temporal high resolution (3m) pre- and post-flooding multispectral surface reflectance satellite imagery. Besides, remote sensing specific data augmentation was also developed to remove the impact of varying illuminations due to different data acquisition conditions, which in turn further improves the performance of the proposed model. Experiments using the high resolution imagery before and after the 2017 Hurricane Harvey flood in Houston, Texas, showed that the developed ResPSNet model, along with associated remote sensing specific data augmentation method, can robustly produce flood maps over urban areas with high precision (0.9002), recall (0.9302), F1 score (0.9128), and overall accuracy (0.9497). The research sheds light on multi-temporal image fusion for high precision image change detection, which in turn can be used for monitoring natural hazards. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Deep learning; Flood extent estimation; Flood mapping; Patch similarity; Residual learning","Data acquisition; Deep learning; Hazards; Image fusion; Losses; Mapping; Neural networks; Optical correlation; Pattern recognition; Pixels; Remote sensing; Satellite imagery; Trees (mathematics); Classification algorithm; Convolutional neural network; Flood mapping; Generalization capability; Optical remote-sensing imagery; Patch similarity; Pattern recognition algorithms; Residual learning; Floods","","","","","National Science Foundation, NSF, (1940091)","The authors gratefully acknowledge support from the National Science Foundation with project ID: 1940091.","Cian F., Marconcini M., Ceccato P., Normalized Difference Flood Index for rapid flood mapping: Taking advantage of EO big data, Remote Sensing of Environment, 209, pp. 712-730, (2018); Open Data Program; Doshi J., Basu S., Pang G., From satellite imagery to disaster insights, 32nd Conference on Neural Information Processing Systems Workshop, (2018); Federal Emergency Management Agency Flood Mapping Products; Feng Q., Liu J., Gong J., Urban flood mapping based on unmanned aerial vehicle remote sensing and random forest classifier a case of Yuyao, China, Water, 7, 12, pp. 1437-1455, (2015); Gao B.-C., NDWIâĂŤA normalized difference water index for remote sensing of vegetation liquid water from space, Remote Sensing of Environment, 58, 3, pp. 257-266, (1996); Gebrehiwot A., Hashemi-Beni L., Thompson G., Kord-Jamshidi P., Langan T., Deep convolutional neural network for flood extent mapping using unmanned aerial vehicles data, Sensors, 19, 7, (2019); Giustarini L., Hostache R., Kavetski D., Chini M., Corato G., Schlaffer S., Matgen P., Probabilistic flood mapping using synthetic aperture radar data, IEEE Transactions on Geoscience and Remote Sensing, 54, 12, pp. 6958-6969, (2016); Hallegatte S., Green C., Nicholls R.J., Corfee-Morlot J., Future flood losses in major coastal cities, Nature Climate Change, 3, 9, (2013); He K., Zhang X., Ren S., Sun J., Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, Proceedings of the IEEE International Conference on Computer Vision, pp. 1026-1034, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Kingma D.P., Ba J., Adam: A Method for Stochastic Optimization, (2014); Li L., Chen Y., Yu X., Liu R., Huang C., Sub-pixel flood inundation mapping from multispectral remotely sensed images based on discrete particle swarm optimization, ISPRS Journal of Photogrammetry and Remote Sensing, 101, pp. 10-21, (2015); Li Y., Martinis S., Wieland M., Urban flood mapping with an active self-learning convolutional neural network based on TerraSAR-X intensity and interferometric coherence, ISPRS Journal of Photogrammetry and Remote Sensing, 152, pp. 178-191, (2019); Lim B., Son S., Kim H., Nah S., Lee K.M., Enhanced deep residual networks for single image super-resolution, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 136-144, (2017); Malinowski R., Groom G., Schwanghart W., Heck-Rath G., Detection and delineation of localized flooding from WorldView-2 multispectral data, Remote Sensing, 7, 11, pp. 14853-14875, (2015); Masi G., Cozzolino D., Verdoliva L., Scarpa G., Pansharpening by Convolutional Neural Networks, 8, 7, (2016); Data and Imagery from NOAA National Geodetic Survey; Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cour-Napeau D., Brucher M., Perrot M., Duchesnay E., Scikit-learn: Machine learning in python, Journal of Machine Learning Research, 12, pp. 2825-2830, (2011); Planet Application Program Interface: In Space for Life on Earth, (2018); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Medical Image Computing and Computer-Assisted Intervention âĂŞ MICCAI 2015, pp. 234-241, (2015); Rudner T.G.J., Ruswurm M., Fil J., Pelich R., Bischke B., Kopackova V., Bilinski P., Net: Segmenting Flooded Buildings Via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery, (2019); Shen X., Anagnostou E.N., Allen G.H., Brakenridge G.R., Kettner A.J., Near-real-time non-obstructed flood inundation mapping using synthetic aperture radar, Remote Sensing of Environment, 221, pp. 302-315, (2019); The Sustainable Development Goals Report, (2018); Wang P., Zhang G., Leung H., Improving super-resolution flood inundation mapping for multispectral remote sensing image by supplying more spectral information, IEEE Geoscience and Remote Sensing Letters, (2018); Xie M., Jiang Z., Sainju A.M., Geographical hidden markov tree for flood extent mapping, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2545-2554, (2018); Zbontar J., LeCun Y., Stereo matching by training a convolutional neural network to compare image patches, Journal of Machine Learning Research, 17, pp. 1-32, (2016)","Q. Huang; Department of Geography, University of Wisconsin-Madison, Madison, United States; email: qhuang46@wisc.edu","Gao S.; Newsam S.; Zhao L.; Lunga D.; Hu Y.; Martins B.; Zhou X.; Chen F.","Association for Computing Machinery, Inc","DiDi; esri; et al.; here; Microsoft; ORACLE","3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, GeoAI 2019","5 November 2019","Chicago","154252","","978-145036957-2","","","English","Proc. ACM SIGSPATIAL Int. Workshop AI Geogr. Knowl. Discov., GeoAI","Conference paper","Final","","Scopus","2-s2.0-85075583171"
"Zhao T.; Wang Z.; Yang Q.; Chen Y.Q.","Zhao, Tiebiao (56447672000); Wang, Zhongdao (57214452256); Yang, Qi (53065054400); Chen, Yang Quan (7601439185)","56447672000; 57214452256; 53065054400; 7601439185","Melon yield prediction using small unmanned aerial vehicles","2017","Proceedings of SPIE - The International Society for Optical Engineering","10218","","1021808","","","","20","10.1117/12.2262412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021816226&doi=10.1117%2f12.2262412&partnerID=40&md5=996a6316a7c6b603a48ad69d7de15625","University of California, Merced, 5200 Lake Rd, Merced, CA, United States; Tsinghua University, 30 Shuangqing Rd, Haidian Qu, Beijing, China; Shenyang Ligong University, 6 Nanping Middle Rd, Hunnan Qu, Shenyang, Liaoning, China","Zhao T., University of California, Merced, 5200 Lake Rd, Merced, CA, United States; Wang Z., Tsinghua University, 30 Shuangqing Rd, Haidian Qu, Beijing, China; Yang Q., Shenyang Ligong University, 6 Nanping Middle Rd, Hunnan Qu, Shenyang, Liaoning, China; Chen Y.Q., University of California, Merced, 5200 Lake Rd, Merced, CA, United States","Thanks to the development of camera technologies, small unmanned aerial systems (sUAS), it is possible to collect aerial images of field with more flexible visit, higher resolution and much lower cost. Furthermore, the performance of objection detection based on deeply trained convolutional neural networks (CNNs) has been improved significantly. In this study, we applied these technologies in the melon production, where high-resolution aerial images were used to count melons in the field and predict the yield. CNN-based object detection framework-Faster R-CNN is applied in the melon classification. Our results showed that sUAS plus CNNs were able to detect melons accurately in the late harvest season. © 2017 SPIE.","Faster R-CNN; melon detection; SUAS; yield","Agriculture; Deep neural networks; Neural networks; Unmanned aerial vehicles (UAV); Convolutional neural network; Detection framework; Faster R-CNN; High-resolution aerial images; Small unmanned aerial vehicles; SUAS; Unmanned aerial systems; yield; Object detection","","","","","","","A National Information Resource for Value-Added Agriculture; The Fruit and Tree Nuts Year-book; The Fruit and Tree Nuts Trade; Shrestha R., Di L., Eugene G.Y., Kang L., Li L., Rahman M.S., Deng M., Yang Z., Regression based corn yield assessment using modis based daily ndvi in Iowa state,"" in, Agro-Geoinformatics (Agro-Geoinformatics), 2016 Fifth International Conference On], pp. 1-5, (2016); Pantazi X.E., Moshou D., Alexandridis T., Whetton R., Mouazen A.M., Wheat yield prediction using machine learning and advanced sensing techniques, Computers and Electronics in Agriculture, 121, pp. 57-65, (2016); Ali I., Cawkwell F., Dwyer E., Green S., Modeling managed grassland biomass estimation by using multitemporal remote sensing dataa machine learning approach, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, (2016); Akhand K., Nizamuddin M., Roytman L., Kogan F., Using remote sensing satellite data and artificial neural network for prediction of potato yield in Bangladesh, SPIE Optical Engineering+ Applications], (2016); Maresma A., Ariza M., Martinez E., Lloveras J., Martinez-Casasnovas J.A., Analysis of vegetation indices to determine nitrogen application and yield prediction in maize (zea mays l.) from a standard uav service, Remote Sensing, 8, 12, (2016); Sa I., Ge Z., Dayoub F., Upcroft B., Perez T., McCool C., Deepfruits: A fruit detection system using deep neural networks, Sensors, 16, 8, (2016); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems (NIPS)], (2015); Uijlings J.R., Van De Sande K.E., Gevers T., Smeulders A.W., Selective search for object recognition, International Journal of Computer Vision, 104, 2, pp. 154-171, (2013); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detec-tion and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition], pp. 580-587, (2014); Girshick R., Fast r-cnn, Proceedings of the IEEE International Conference on Computer Vision], pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems], pp. 91-99, (2015); Zeiler M.D., Fergus R., Visualizing and understanding convolutional networks, European Con-Ference on Computer Vision], pp. 818-833, (2014); Tiebiao Z., Jose G., Jozef F., Qi Y., YangQuan C., Melon classification and segmentation using low cost remote sensing data drones, 13th International Conference on Precision Agriculture] the International Society of Precision Agriculture, (2016); Yang Q., Chen D., Zhao T., Chen Y., Fractional calculus in image processing: A review, Fractional Calculus and Applied Analysis, 19, 5, pp. 1222-1249, (2016)","Y.Q. Chen; University of California, Merced, Merced, 5200 Lake Rd, United States; email: ychen53@ucmerced.edu","Moorhead R.J.; Thomasson J.A.; McKee M.","SPIE","Monsanto Company; The Society of Photo-Optical Instrumentation Engineers (SPIE)","Autonomous Air and Ground Sensing Systems for Agricultural Optimization and Phenotyping II 2017","10 April 2017 through 11 April 2017","Anaheim","128437","0277786X","978-151060937-2","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85021816226"
"Dahmane M.; Foucher S.; Beaulieu M.; Riendeau F.; Bouroubi Y.; Benoit M.","Dahmane, M. (21742057400); Foucher, S. (6701728686); Beaulieu, M. (57198137843); Riendeau, F. (57192706076); Bouroubi, Y. (42961054400); Benoit, M. (57201545318)","21742057400; 6701728686; 57198137843; 57192706076; 42961054400; 57201545318","Object detection in pleiades images using deep features","2016","International Geoscience and Remote Sensing Symposium (IGARSS)","2016-November","","7729396","1552","1555","3","12","10.1109/IGARSS.2016.7729396","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007492933&doi=10.1109%2fIGARSS.2016.7729396&partnerID=40&md5=ebf11f2b2e3e8b2bf011120780e4d04e","Computer Research Institute of Montreal, Vision and Imaging Team, Canada; Effigis Géo Solutions, Montreal, Canada","Dahmane M., Computer Research Institute of Montreal, Vision and Imaging Team, Canada; Foucher S., Computer Research Institute of Montreal, Vision and Imaging Team, Canada; Beaulieu M., Computer Research Institute of Montreal, Vision and Imaging Team, Canada; Riendeau F., Effigis Géo Solutions, Montreal, Canada; Bouroubi Y., Effigis Géo Solutions, Montreal, Canada; Benoit M., Effigis Géo Solutions, Montreal, Canada","Extracting and identifying objects in very high resolution imagery has been a popular research topic in remote sensing. Since the beginning of this decade, deep learning techniques have revolutionized computer vision providing significant performance gains compared to traditional 'shallow' techniques in various challenging vision problems. The training of deep neural networks usually requires very large training datasets. The advantage of using deep features is to exploit already trained Convolutional Neural Networks (CNN) in order to produce high level features without the burden of having to train a CNN from scratch. In this paper, we are investigating the use of deep features for the detection of small objects (cars and individual trees) in high resolution Pleiades imagery. Preliminary results show good detection performance and are very encouraging for future applications. © 2016 IEEE.","deep learning; Object detection; very high resolution","","","","","","","","Moranduzzo T., Melgani F., Detecting cars in uav images with a catalog-based approach, IEEE Trans. on Rem. Sens. And Geo, 52, 10, pp. 6356-6367, (2014); Tuermer S., Kurz F., Reinartz P., Stilla U., Airborne vehicle detection in dense urban areas using HOG features and disparity maps, IEEE Sel. Topics Appl. Earth Obs. Rem. Sens, 6, 6, pp. 2327-2337, (2013); Kluckner S., Pacher G., Grabner H., Bischof H., Bauer J., A 3D teacher for car detection in aerial images, ICCV 2007, pp. 1-8, (2007); Tsai L.-W., Hsieh J.-W., Fan K.-C., Vehicle detection using normalized color and edge map, IEEE Trans. on Image Proc, 16, 3, pp. 850-864, (2007); Chen X., Xiang S., Liu C.-L., Pan C.-H., Vehicle detection in satellite images by hybrid deep convolutional neural networks, Geo. And Remote Sens. Letters, 11, 10, pp. 1797-1801, (2014); Sivic J., Zisserman A., Efficient visual search of videos cast as text retrieval, IEEE Trans. on PAMI, 31, 4, pp. 591-606, (2009); Sun H., Sun X., Wang H., Li Y., Li X., Automatic target detection in high-resolution remote sensing images using spatial sparse coding bag-of-words model, IEEE Geo. And Rem. Sens. Let, 9, 1, pp. 109-113, (2012); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, ACM SIGSPATIAL, pp. 270-279, (2010); Perronnin F., Sanchez J., Mensink T., Improving the fisher kernel for large-scale image classification, Proceedings of the 11th European Conference on Computer Vision: Part IV, pp. 143-156, (2010); Negrel R., Picard D., Gosselin P.-H., Evaluation of second-order visual features for land-use classification, CBMI, pp. 1-5, (2014); Bai X., Zhang H., Zhou J., VHR object detection based on structural feature extraction and query expansion, IEEE Trans. on Rem. Sens. And Geo, 52, 10, pp. 6508-6520, (2014); Sandham W., Leggett M., Aminzadeh F., Geophysical Applications of Artificial Neural Networks and Fuzzy Logic, (2004); Midhun M.E., Nair S.R., Nidhin Prabhakar V.T., Sachin Kumar S., Deep model for classification of hyperspectral image using restricted boltzmann machine, ICONIAAC., (2014); Tuia D., Flamary R., Courty N., Multiclass feature learning for hyperspectral image classification: Sparse and hierarchical solutions, ISPRS Journal of Photo. And Rem. Sens, 105, pp. 272-285, (2015); Firat O., Can G., Yarman Vural F., Representation learning for contextual object and region detection in remote sensing, ICPR, pp. 3708-3713, (2014); Chen Y., Lin Z., Zhao X., Wang G., Gu Y., Deep learning-based classification of hyperspectral data, IEEE Sel. Topics Appl. Earth Obs. Rem. Sens, 7, 6, pp. 2094-2108, (2014); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., LeCun Y., OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks, (2013); Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., Darrell T., Caffe: Convolutional Architecture for Fast Feature Embedding, (2014); Razavian A.-S., Azizpour H., Sullivan J., Carlsson S., CNN Features Off-the-shelf: An Astounding Baseline for Recognition, (2014); Penatti O.A.B., Nogueira K., Dos Santos J.A., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains, CVPRW, pp. 44-51, (2015); Li T., Zhang J., Zhang Y., Classification of hyperspectral image based on deep belief networks, ICIP, pp. 5132-5136, (2014); Castelluccio M., Poggi G., Sansone C., Verdoliva L., Land Use Classification in Remote Sensing Images by Convolutional Neural Networks, (2015); Szegedy C., Et al., Going deeper with convolutions, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, (2015); Russakovsky O., Et al., Imagenet large scale visual recognition challenge, International Journal of Computer Vision (IJCV), 115, 3, pp. 211-252, (2015)","","","Institute of Electrical and Electronics Engineers Inc.","The Institute of Electrical and Electronics Engineers, Geoscience and Remote Sensing Society (GRSS)","36th IEEE International Geoscience and Remote Sensing Symposium, IGARSS 2016","10 July 2016 through 15 July 2016","Beijing","124694","","978-150903332-4","IGRSE","","English","Dig Int Geosci Remote Sens Symp (IGARSS)","Conference paper","Final","","Scopus","2-s2.0-85007492933"
"Daneshtalab S.; Rastiveis H.; Hosseiny B.","Daneshtalab, S. (57196248456); Rastiveis, H. (55609081400); Hosseiny, B. (57216336978)","57196248456; 55609081400; 57216336978","CNN-based feature-level fusion of very high resolution aerial imagery and lidar data","2019","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","4/W18","","279","284","5","2","10.5194/isprs-archives-XLII-4-W18-279-2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083224798&doi=10.5194%2fisprs-archives-XLII-4-W18-279-2019&partnerID=40&md5=ce71e9bdb64f09fb7657204091528d5d","School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran","Daneshtalab S., School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; Rastiveis H., School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; Hosseiny B., School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran","Land-cover classification of Remote Sensing (RS) data in urban area has always been a challenging task due to the complicated relations between different objects. Recently, fusion of aerial imagery and light detection and ranging (LiDAR) data has obtained a great attention in RS communities. Meanwhile, convolutional neural network (CNN) has proven its power in extracting high-level (deep) descriptors to improve RS data classification. In this paper, a CNN-based feature-level framework is proposed to integrate LiDAR data and aerial imagery for object classification in urban area. In our method, after generating low-level descriptors and fusing them in a feature-level fusion by layer-stacking, the proposed framework employs a novel CNN to extract the spectral-spatial features for classification process, which is performed using a fully connected multilayer perceptron network (MLP). The experimental results revealed that the proposed deep fusion model provides about 10% improvement in overall accuracy (OA) in comparison with other conventional feature-level fusion techniques. © 2019 S. Daneshtalab et al.","Aerial Imagery; Convolutional Neural Network (CNN); Deep Learning; Feature Extraction; Feature Fusion; LiDAR","Aerial photography; Antennas; Convolutional neural networks; Optical radar; Remote sensing; Classification process; Feature level fusion; Land cover classification; Light detection and ranging; Low level descriptors; Multilayer perceptron network (MLP); Object classification; Very high resolution; Classification (of information)","","","","","","","Bigdeli B., Amini Amirkolaee H., Pahlavani P., Deep feature learning versus shallow feature learning systems for joint use of airborne thermal hyperspectral and visible remote sensing data, International Journal of Remote Sensing, 40, pp. 7048-7070, (2019); Chen Y., Li C., Ghamisi P., Jia X., Gu Y., Deep fusion of remote sensing data for accurate classification, IEEE Geoscience and Remote Sensing Letters, 14, pp. 1253-1257, (2017); Daneshtalab S., Rastiveis H., Decision level fusion of orthophoto and lidar data using confusion matrix information for land cover classification, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, (2017); Feng Q., Zhu D., Yang J., Li B., Multisource hyperspectral and lidar data fusion for urban land-use mapping based on a modified two-branch convolutional neural network, ISPRS International Journal of Geo-Information, 8, (2019); Goshtasby A., Nikolov S., Guest editorial: Image fusion: Advances in the state of the art, Information Fusion, 8, pp. 114-118, (2007); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a worldview-2/3 and lidar data fusion approach and deep learning, Sensors, 19, (2019); Langkvist M., Kiselev A., Alirezaie M., Loutfi A., Classification and segmentation of satellite orthoimagery using convolutional neural networks, Remote Sensing, 8, (2016); Li H., Gu H., Han Y., Yang J., Fusion of high-resolution aerial imagery and lidar data for object-oriented urban land-cover classification based on svm, ISPRS Workshop on Updating Geo-spatial Databases with Imagery & the 5th ISPRS Workshop on DMGISs, (2007); Li W., Chen C., Su H., Du Q., Local binary patterns and extreme learning machine for hyperspectral imagery classification, IEEE Transactions on Geoscience and Remote Sensing, 53, pp. 3681-3693, (2015); Makarau A., Palubinskas G., Reinartz P., Multi-sensor data fusion for urban area classification, 2011 Joint Urban Remote Sensing Event, pp. 21-24, (2011); Morsy S., Shaker A., El-Rabbany A., Multispectral lidar data for land cover classification of urban areas, Sensors, 17, (2017); Nahhas F.H., Shafri H.Z., Sameen M.I., Pradhan B., Mansor S., Deep learning approach for building detection using lidar-orthophoto fusion, Journal of Sensors, 2018, (2018); Rastiveis H., Decision level fusion of lidar data and aerial color imagery based on bayesian theory for urban area classification, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 40, (2015); Santos A.A.D., Marcato Junior J., Araujo M.S., Martini D., Robledo D., Tetila E.C., Siqueira H.L., Aoki C., Eltner A., Matsubara E.T., Assessment of cnn-based methods for individual tree detection on images captured by rgb cameras attached to uavs, Sensors, 19, (2019); Schmitt M., Zhu X.X., Data fusion and remote sensing: An ever-growing relationship, IEEE Geoscience and Remote Sensing Magazine, 4, pp. 6-23, (2016); Wu M., Zhao X., Sun Z., Guo H., A hierarchical multiscale super-pixel-based classification method for extracting urban impervious surface using deep residual network from worldview-2 and lidar data, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12, pp. 210-222, (2019); Xia Y., D'Angelo P., Tian J., Fraundorfer F., Reinartz P., Self-supervised convolutional neural networks for plant reconstruction using stereo imagery, Photogrammetric Engineering & Remote Sensing, 85, pp. 389-399, (2019); Xu X., Li W., Ran Q., Du Q., Gao L., Zhang B., Multisource remote sensing data classification based on convolutional neural network, IEEE Transactions on Geoscience and Remote Sensing, 56, pp. 937-949, (2017); Xu X., Li W., Ran Q., Du Q., Gao L., Zhang B., Multisource remote sensing data classification based on convolutional neural network, IEEE Transactions on Geoscience and Remote Sensing, 56, pp. 937-949, (2018); Xu Y., Du B., Zhang L., Cerra D., Pato M., Carmona E., Prasad S., Yokoya N., Hansch R., Saux B.L., Advanced multi-sensor optical remote sensing for urban land use and land cover classification: Outcome of the 2018 ieee grss data fusion contest, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12, pp. 1709-1724, (2019); Zhang L., Zhang L., Du B.J.I.G., Magazine R.S., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art., 4, pp. 22-40, (2016)","H. Rastiveis; School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; email: hrasti@ut.ac.ir","Arefi H.; Saadat Seresht M.","International Society for Photogrammetry and Remote Sensing","","ISPRS International GeoSpatial Conference 2019, Joint Conferences of 5th Sensors and Models in Photogrammetry and Remote Sensing, SMPR 2019 and 3rd Geospatial Information Research, GI Research 2019","12 October 2019 through 14 October 2019","Karaj","158824","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083224798"
"Timilsina S.; Aryal J.; Kirkpatrick J.B.","Timilsina, Shirisa (57216852299); Aryal, Jagannath (16315027400); Kirkpatrick, Jamie B. (7203081724)","57216852299; 16315027400; 7203081724","Mapping urban tree cover changes using object-based convolution neural network (OB-CNN)","2020","Remote Sensing","12","18","3017","","","","34","10.3390/RS12183017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092112272&doi=10.3390%2fRS12183017&partnerID=40&md5=4f1365f720006efbe141ff8f61c2250e","School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia; Melbourne School of Engineering, University of Melbourne, Parkville, 3010, VIC, Australia","Timilsina S., School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia; Aryal J., School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia, Melbourne School of Engineering, University of Melbourne, Parkville, 3010, VIC, Australia; Kirkpatrick J.B., School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia","Urban trees provide social, economic, environmental and ecosystem services benefits that improve the liveability of cities and contribute to individual and community wellbeing. There is thus a need for effective mapping, monitoring and maintenance of urban trees. Remote sensing technologies can effectively map and monitor urban tree coverage and changes over time as an efficient and low-cost alternative to field-based measurements, which are time consuming and costly. Automatic extraction of urban land cover features with high accuracy is a challenging task, and it demands object based artificial intelligence workflows for efficiency and thematic accuracy. The aim of this research is to effectively map urban tree cover changes and model the relationship of such changes with socioeconomic variables. The object-based convolutional neural network (CNN) method is illustrated by mapping urban tree cover changes between 2005 and 2015/16 using satellite, Google Earth imageries and Light Detection and Ranging (LiDAR) datasets. The training sample for CNN model was generated by Object Based Image Analysis (OBIA) using thresholds in a Canopy Height Model (CHM) and the Normalised Difference Vegetation Index (NDVI). The tree heatmap produced from the CNN model was further refined using OBIA. Tree cover loss, gain and persistence was extracted, and multiple regression analysis was applied to model the relationship with socioeconomic variables. The overall accuracy and kappa coefficient of tree cover extraction was 96% and 0.77 for 2005 images and 98% and 0.93 for 2015/16 images, indicating that the object-based CNN technique can be effectively implemented for urban tree coverage mapping and monitoring. There was a decline in tree coverage in all suburbs. Mean parcel size and median household income were significantly related to tree cover loss (R2 = 58.5%). Tree cover gain and persistence had positive relationship with tertiary education, parcel size and ownership change (gain: R2 = 67.8% and persistence: R2 = 75.3%). The research findings demonstrated that remote sensing data with intelligent processing can contribute to the development of policy input for management of tree coverage in cities. © 2020 by the authors.","Convolution neural networks (CNNs); Deep learning; GEOBIA; Object-based CNN; Socioeconomic predictor variables; Urban tree mapping","Convolution; Convolutional neural networks; Data handling; Ecosystems; Extraction; Heating; Lithium compounds; Mapping; Optical radar; Regression analysis; Remote sensing; Research and development management; Trees (mathematics); Convolution neural network; Field-based measurements; Intelligent processing; Light detection and ranging; Multiple regression analysis; Normalised difference vegetation index; Object based image analysis (OBIA); Remote sensing technology; Forestry","","","","","Google Earth and Land Information System Tasmania; Kingborough Council; LiDAR point cloud and cadastral parcel datasets; University of Tasmania, UTAS; AVL List, AVL","This research received no external funding. The authors wish to thank the Kingborough Council, Tasmania for providing the aerial imagery of the study area. We are grateful to the University of Tasmania for providing research facilities. We also thank Google Earth and Land Information System Tasmania (LIST) for providing imagery and LiDAR point cloud and cadastral parcel datasets.","Bolund P., Hunhammar S., Ecosystem services in urban areas., Ecol. Econ., 29, pp. 293-301, (1999); Lohr V., earson-Mi S.C., Tarnai J., Dillman D., How Urb n Residents Rate and Rank the Benefits and Problems Associated with Trees in Cities., J. Arboric., 1, pp. 28-35, (2004); Shackleton S., Chinyimba A., Hebinck P., Shackleton C., Kaoma H., Multiple benefits and values of trees in urban landscapes in two towns in northern South Africa., Landsc. Urban Plan., 136, pp. 76-86, (2015); Solecki W.D., Welchb J.M., Urban parks: Green spaces or green walls? Landsc., Urban Plan., 32, pp. 93-106, (1995); Tyrvainen L., Silvennoinen H., Kolehmainen O., Ecological and aesthetic values in urban forest management., Urban For. Urban Green., 1, pp. 15-149, (2003); Erker T., Wang L., Lorentz L., Stoltman A., Townsend P.A., A statewide urban tree canopy mapping ethod., Remote Sens. Environ., 229, pp. 148-158, (2019); Guo T., Morgenroth J., Conway T., Xu C., City- ide canopy cover decline due to residential property redevelopment in Christchurch, New Zealand., Sci. Total Environ., 681, pp. 202-210, (2019); Nowak D.J., Rowntree R.A., McPherson E.G., Sisinni S.M., Kerkmann E.R., Stevens J.C., Measuring and analyzing urban tree cover., Landsc. Urban Plan., 36, pp. 49-57, (1996); Schneider A., Monitoring land cover change in urban and peri-urban areas using dense time stacks of Landsat satellite data and a data mining approach., Remote Sens. Environ., 124, pp. 689-704, (2012); Stave J., Oba G., Stenseth N.C., Temporal changes in woody-plant use and the ekwar indigenous tree management system along the Turkwel River, Kenya., Environ. Conserv., 28, pp. 150-159, (2001); Tucker Lima J.M., Staudhammer C.L., Brandeis T.J., Escobedo F.J., Zipperer W., Temporal dynamics of a subtropical urban forest in San Juan, Puerto Rico, 2001-2010., Landsc. Urban Plan., 120, pp. 96-106, (2013); Bowden L.W., Urban environments: Inventory and analysis., Man. Remote Sens., 12, pp. 1815-1880, (1975); Grove J.M., Troy A.R., O'Neil-Dunne J.P.M., Burch W.R., Cadenasso M.L., Pickett S.T.A., Characterization of households and its implications for the vegetation of urban ecosystems., Ecosystems, 9, pp. 578-597, (2006); Iverson L.R., Cook E.A., Urban forest cover of the Chicago region and its relation to household density and income., Urban Ecosyst., 4, pp. 105-124, (2000); Kirkpatrick J.B., Daniels G.D., Zagorski T., Explaining variation in front gardens between suburbs of Hobart, Tasmania, Australia., Landsc. Urban Plan., 79, pp. 314-322, (2007); Kirkpatrick J.B., Daniels G.D., Davison A., Temporal and spatial variation in garden and street trees in six eastern Australian cities., Landsc. Urban Plan., 101, pp. 244-252, (2011); Martin C.A., Paige S.W., Kinzig A.P., Neighbourhood socioeconomic status is a useful predictor of perennial landscape vegetation in residential neighbourhoods and embedded small parks of Phoenix, AZ., Landsc. Urban Plan., 69, pp. 355-368, (2004); Talarchek G.M., The Urban forest of New Orleans: An exploratory analysis of relationship., Urban Geogr., 11, pp. 65-86, (1990); Moskal L.M., Styers D.M., Halabisky M., Monitoring urban tree cover using object-based image analysis and public domain remotely sensed data., Remote Sens., 3, pp. 2243-2262, (2011); Ehlers M., Gahler M., Janowsky R., Automated analysis of ultra high resolution remote sensing data for biotope type mapping: New possibilities and challenges., ISPRS J. Photogramm. Remote Sens., 57, pp. 315-326, (2003); Mikita T., Janata P., Surovy P., Forest stand inventory based on combined aerial and terrestrial close-range photogrammetry., Forests, 7, (2016); Ke Y., Quackenbush L.J., A review of methods for automatic individual tree-crown detection and delineation from passive remote sensing., Internatl. J. Remote Sens., 32, pp. 4725-4747, (2011); Xiao Q., McPherson E.G., Tree health mapping with multispectral remote sensing data at UC Davis, California., Urban Ecosyst., 8, pp. 349-361, (2005); Anees A., Aryal J., A Statistical Framework for Near-Real Time Detection of Beetle Infestation in Pine Forests Using MODIS Data., IEEE Geosci. Remote Sens. Lett., 11, pp. 1717-1721, (2014); Anees A., Aryal J., Near-real time detection of beetle infestation in pine forests using MODIS data., IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7, pp. 3713-3723, (2014); Anees A., Aryal J., O'Reilly M.M., Gale T.J., A Relative Density Ratio-Based Framework for Detection of Land Cover Changes in MODIS NDVI Time Series., IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 3359-3371, (2016); Rogan J., Chen D.M., Remote sensing technology for mapping and monitoring land-cover and land-use change., Prog. Plann., 61, pp. 301-325, (2004); Ardila J.P., Bijker W., Tolpekin V.A., Stein A., Context-sensitive extraction of tree crown objects in urban areas using VHR satellite images., Int. J. Appl. Earth Obs. Geoinf., 15, pp. 57-69, (2012); O'Neil-Dunne J., MacFaden S., Royar A., A versatile, production-oriented approach to high-resolution tree-canopy mapping in urban and suburban landscapes using GEOBIA and data fusion., Remote Sens., 6, pp. 12837-12865, (2014); Walker J.S., Briggs J.M., An Object-oriented Approach to Urban Forest Mapping in Phoenix., Photogramm. Eng. Remote Sens., 73, pp. 577-583, (2007); Zhou W., Troy A., Grove M., Object-based Land Cover Classification and Change Analysis in the Baltimore Metropolitan Area Using Multitemporal High Resolution Remote Sensing Data., Sensors, 8, pp. 1613-1636, (2008); Blaschke T., Object based image analysis for remote sensing., ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16, (2010); Walker J.S., Blaschke T., Object-based land-cover classification for the Phoenix metropolitan area: Optimization vs. transportability., Int. J. Remote Sens., 29, pp. 2021-2040, (2008); Zhou J., Yu B., Qin J., Multi-level spatial analysis for change detection of urban vegetation at individual tree scale., Remote Sens., 6, pp. 9086-9103, (2014); Banzhaf E., Kollai H., Monitoring the urban tree cover for urban ecosystem services-The case of Leipzig, Germany., In Proceedings of the 36th International Symposium on Remote Sensing of Environment, 40, pp. 301-305, (2015); Ejares J.A., Violanda R.R., Diola A.G., Dy D.T., Otadoy J.B., Otadoy R.E.S., Tree canopy cover mapping using LiDAR in urban barangays of Cebu City, central Philippines., In Proceedings of the XXIII ISPRS Congress, The International Archives of the Photogrammetry, 41, pp. 611-615, (2016); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., Van der Meer F., Van der Werff H., Van Coillie F., Et al., Geographic Object-Based Image Analysis-Towards a new paradigm., ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191, (2014); Belgiu M., Dragut L., Random forest in remote sensing: A review of applications and future directions., ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31, (2016); Dragut L., Tiede D., Levick S.R., ESP: A tool to estimate scale parameter for multiresolution image segmentation of remotely sensed data., Int. J. Geogr. Inf. Sci., 24, pp. 859-871, (2010); Jin B., Ye P., Zhang X., Song W., Li S., Object-Oriented Method Combined with Deep Convolutional Neural Networks for Land-Use-Type Classification of Remote Sensing Images., J. Indian Soc. Remote Sens., 47, pp. 951-965, (2019); Ming D., Li J., Wang J., Zhang M., Scale parameter selection by spatial statistics for GeOBIA: Using mean-shift based multi-scale segmentation as an example., ISPRS J. Photogramm. Remote Sens., 106, pp. 28-41, (2015); Du S., Shy M., Wang Q., Modelling relational contexts in GEOBIA framework for improving urban land-cover mapping., GISci. Remote Sens., 56, pp. 184-209, (2019); Belgiu M., Tomljenovic I., Lampoltshammer T.J., Blaschke T., Hofle B., Ontology-based classification of building types detected from airborne laser scanning data., Remote Sens., 6, pp. 1347-1366, (2014); Duro D.C., Franklin S.E., Dube M.G., A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery., Remote Sens. Environ., 118, pp. 259-272, (2012); Heumann B.W., An object-based classification of mangroves using a hybrid decision tree-support vector machine approach., Remote Sens., 3, pp. 2440-2460, (2011); Fukushima K., Neocognitron: A hierarchical neural network capable of visual pattern recognition., Neural Netw., 1, pp. 119-130, (1988); Fu T., Ma L., Li M., Johnson B.A., Using convolutional neural network to identify irregular segmentation objects from very high-resolution remote sensing imagery., J. Appl. Remote Sens., 12, (2018); Zhang Q., Wang Y., Liu Q., Liu X., Wang W., CNN based suburban building detection using monocular high resolution Google Earth images., In Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 661-664, (2016); Zhu X.X., Tuia D., Mou L., Xia G.S., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources., IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36, (2017); Alom M.Z., Taha T.M., Yakopcic C., Westberg S., Sidike P., Nasrin M.S., Van Esesn B.C., Awwal A.A.S., Asari V.K., The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches., (2018); Zhou W., Newsam S., Li C., Shao Z., Learning low dimensional convolutional neural networks for high-resolution remote sensing image retrieval., Remote Sens., 9, (2017); Chen S.W., Shivakumar S.S., Dcunha S., Das J., Okon E., Qu C., Taylor C.J., Kumar V., Counting Apples and Oranges with Deep Learning: A Data-Driven Approach., IEEE Robot. Autom. Lett., 2, pp. 781-788, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks., Drones, 2, (2018); Sa I., Ge Z., Dayoub F., Upcroft B., Perez T., McCool C., Deepfruits: A fruit detection system using deep neural networks., Sensors, 16, (2016); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks., Remote Sens., 11, (2019); Wang Z., Underwood J., Walsh K.B., Machine vision assessment of mango orchard flowering., Comput. Electron. Agric., 151, pp. 501-511, (2018); Timilsina S., Sharma S.K., Aryal J., Mapping Urban Trees Within Cadastral Parcels Using an Object-based Convolutional Neural Network., In Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 111-117, (2019); Fan C., Johnston M., Darling L., Scott L., Liao F.H., Land use and socio-economic determinants of urban forest structure and diversity., Landsc. Urban. Plan., 181, pp. 10-21, (2019); Steenberg J.W.N., Robinson P.J., Duinker P.N., A spatio-temporal analysis of the relationship between housing renovation, socioeconomic status, and urban forest ecosystems., Environ. Plan. B Urban. Anal. City Sci., 46, pp. 1115-1131, (2018); Grove J.M., Burch W.R.J., A social ecosystem approach and applications of urban ecosystem and landscape analyses: A case study of Baltimore, Maryland., Urban Ecosyst., 1, pp. 259-275, (1997); Kirkpatrick J.B., Davison A., Daniels G.D., Resident attitudes towards trees influence the planting and removal of different types of trees in eastern Australian cities., Landsc. Urban Plan., 107, pp. 147-158, (2012); Kirkpatrick J.B., Davison A., Daniels G.D., Sinners, scapegoats or fashion victims? Understanding the deaths of trees in the green city., Geoforum, 48, pp. 165-176, (2013); (2019); (2019); Bolstad P., GIS Fundamentals: A First Text on Geographic Information Systems, (2012); Yang C., A high-resolution airborne four-camera imaging system for agricultural remote sensing., Comput. Electron. Agric., 88, pp. 13-24, (2012); Bannari A., Morin D., Bonn F., A Review of Vegetation Indices., Remote Sens. Rev., 13, pp. 95-120, (1995); Dubayah R.O., Drake J.B., Lidar Remote Sensing for Forestry., J. For., 98, pp. 44-46, (2000); (2019); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of Different Machine Learning Methods and Deep-Learning Convolutional Neural Networks for Landslide Detection., Remote Sens., 11, (2019); Chen L.-C., Barron J.T., Papandreou G., Murphy K., Yuille A.L., Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform., In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 4545-4554, (2016); User's Guide: Data Analysis and Quality Tools, (1998); Ellis E.A., Mathews A.J., Object-based delineation of urban tree canopy: Assessing change in Oklahoma City, 2006-2013., Comput. Environ. Urban. Syst., 73, pp. 85-94, (2019); Branson S., Wegner J.D., Hall D., Lang N., Schindler K., Perona P., From Google Maps to a fine-grained catalog of street trees., ISPRS J. Photogramm. Remote Sens., 135, pp. 13-30, (2018); Ballantyne M., Pickering C.M., Differences in the impacts of formal and informal recreational trails on urban forest loss and tree structure., J. Environ. Manag., 159, pp. 94-105, (2015); Brunner J., Cozens P., Where Have All the Trees Gone? Urban Consolidation and the Demise of Urban Vegetation: A Case Study from Western Australia., Plan. Pract. Res., 28, pp. 231-255, (2013); Kaspar J., Kendal D., Sore R., Livesley S.J., Urban Forestry & Urban Greening Random point sampling to detect gain and loss in tree canopy cover in response to urban densification., Urban For. Urban Green., 24, pp. 26-34, (2017); Lin B., Meyers J., Barnett G., Understanding the potential loss and inequities of green space distribution with urban densification., Urban For. Urban Green., 14, pp. 952-958, (2015); Ossola A., Hopton M.E., Measuring urban tree loss dynamics across residential landscapes., Sci. Total Environ., 612, pp. 940-949, (2018); Pauleit S., Ennos R., Golding Y., Modeling the environmental impacts of urban land use and land cover change-A study in Merseyside, UK., Landscap. Urban Plan., 71, pp. 295-310, (2005); Potapov P.V., Turubanova S.A., Hansen M.C., Adusei B., Broich M., Altstatt A., Mane L., Justice C.O., Quantifying forest cover loss in Democratic Republic of the Congo, 2000-2010, with Landsat ETM+ data., Remote Sens. Environ., 122, pp. 106-116, (2012)","J. Aryal; Melbourne School of Engineering, University of Melbourne, Parkville, 3010, Australia; email: Jagannath.aryal@unimelb.edu.au","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092112272"
"Guirado E.; Tabik S.; Alcaraz-Segura D.; Cabello J.; Herrera F.","Guirado, Emilio (56607108400); Tabik, Siham (55884151200); Alcaraz-Segura, Domingo (24463232500); Cabello, Javier (23567460300); Herrera, Francisco (7102347190)","56607108400; 55884151200; 24463232500; 23567460300; 7102347190","Deep-learning Versus OBIA for scattered shrub detection with Google Earth Imagery: Ziziphus lotus as case study","2017","Remote Sensing","9","12","1220","","","","105","10.3390/rs9121220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038210007&doi=10.3390%2frs9121220&partnerID=40&md5=3fd539e1f4720629c9fc5103786e2486","Andalusian Center for Assessment and Monitoring of Global Change (CAESCG), University of Almería, Almería, 04120, Spain; Soft Computing and Intelligent Information System Research Group, University of Granada, Granada, 18071, Spain; Department of Botany, Faculty of Science, University of Granada, Granada, 18071, Spain; Iecolab., Interuniversitary Institute for Earth System Research in Andalusia (IISTA), University of Granada, Granada, 18006, Spain; Department of Biology and Geology, University of Almería, La Cañada, Almería, 04120, Spain","Guirado E., Andalusian Center for Assessment and Monitoring of Global Change (CAESCG), University of Almería, Almería, 04120, Spain; Tabik S., Soft Computing and Intelligent Information System Research Group, University of Granada, Granada, 18071, Spain; Alcaraz-Segura D., Andalusian Center for Assessment and Monitoring of Global Change (CAESCG), University of Almería, Almería, 04120, Spain, Department of Botany, Faculty of Science, University of Granada, Granada, 18071, Spain, Iecolab., Interuniversitary Institute for Earth System Research in Andalusia (IISTA), University of Granada, Granada, 18006, Spain; Cabello J., Andalusian Center for Assessment and Monitoring of Global Change (CAESCG), University of Almería, Almería, 04120, Spain, Department of Biology and Geology, University of Almería, La Cañada, Almería, 04120, Spain; Herrera F., Soft Computing and Intelligent Information System Research Group, University of Granada, Granada, 18071, Spain","There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been traditionally performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image often cannot be extrapolated to a different image. Recently, deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in computer vision and are offering promising results in land cover mapping. This paper analyzes the potential of CNN-based methods for detection of plant species of conservation concern using free high-resolution Google EarthTM images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. Compared to the best performing OBIA-method, the best CNN-detector achieved up to 12% better precision, up to 30% better recall and up to 20% better balance between precision and recall. Besides, the knowledge that CNNs acquired in the first image can be re-utilized in other regions, which makes the detection process very fast. A natural conclusion of this work is that including CNN-models as classifiers, e.g., ResNet-classifier, could further improve OBIA methods. The provided methodology can be systematically reproduced for other species detection using our codes available through (https://github.com/EGuirado/CNN-remotesensing). © 2017 by the author.","Convolutional Neural Networks (CNNs); Land cover mapping; Object-Based Image Analysis (OBIA); Plant species detection; Remote sensing; Ziziphus lotus","Biodiversity; Conservation; Convolution; Deep learning; Deep neural networks; Ecosystems; Image analysis; Land use; Neural networks; Object recognition; Plants (botany); Remote sensing; Convolutional neural network; Land cover mapping; Object based image analysis (OBIA); Plant species; Ziziphus lotus; Mapping","","","","","Andalusian Government, (P09-RNM-5048, P11-TIC-7765, RNM-7033); Horizon 2020 Framework Programme, H2020, (641762, ADAPTAMED LIFE14 CCA/ES/000612); Ministry of Science and Technology, MOST, (CGL2010-22314, CGL2014-61610-EXP, JC2015-00316, TIN2014-57251-P); Ministerio de Ciencia y Tecnología, MICYT; European Regional Development Fund, FEDER","Funding text 1: Acknowledgments: We are grateful to the reviewers for their comments that helped to improve the paper. We also thank Ivan Poyatos, Diego Zapata and Anabel Gomez for their technical support. Siham Tabik was supported by the Ramón y Cajal Programme (RYC-2015-18136). The work was partially supported by the Spanish Ministry of Science and Technology under the projects: TIN2014-57251-P, CGL2014-61610-EXP, CGL2010-22314 and grant JC2015-00316, and ERDF and Andalusian Government under the projects: GLOCHARID, RNM-7033, P09-RNM-5048 and P11-TIC-7765. This research was also developed as part of project ECOPOTENTIAL, which received funding from the European Union Horizon 2020 Research and Innovation Programme under grant agreement No. 641762, and by the European LIFE Project ADAPTAMED LIFE14 CCA/ES/000612.; Funding text 2: We are grateful to the reviewers for their comments that helped to improve the paper. We also thank Ivan Poyatos, Diego Zapata and Anabel Gomez for their technical support. Siham Tabik was supported by the Ramón y Cajal Programme (RYC-2015-18136). The work was partially supported by the Spanish Ministry of Science and Technology under the projects: TIN2014-57251-P, CGL2014-61610-EXP, CGL2010-22314 and grant JC2015-00316, and ERDF and Andalusian Government under the projects: GLOCHARID, RNM-7033, P09-RNM-5048 and P11-TIC-7765. This research was also developed as part of project ECOPOTENTIAL, which received funding from the European Union Horizon 2020 Research and Innovation Programme under grant agreement No. 641762, and by the European LIFE Project ADAPTAMED LIFE14 CCA/ES/000612","Congalton R., Gu J., Yadav K., Thenkabail P., Ozdogan M., Global land cover mapping: A review and uncertainty analysis, Remote Sens, 6, pp. 12070-12093, (2014); Rogan J., Chen D., Remote sensing technology for mapping and monitoring land-cover and land-use change, Prog. Plan, 61, pp. 301-325, (2004); Blaschke T., Object based image analysis for remote sensing, ISPRS J. Photogramm. Remote Sens, 65, pp. 2-16, (2010); Li X., Shao G., Object-based land-cover mapping with high resolution aerial photography at a county scale in midwestern USA, Remote Sens, 6, pp. 11372-11390, (2014); Pierce K., Accuracy optimization for high resolution object-based change detection: An example mapping regional urbanization with 1-m aerial imagery, Remote Sens, 7, pp. 12654-12679, (2015); Knoth C., Nust D., Reproducibility and Practical Adoption of GEOBIA with Open-Source Software in Docker Containers, Remote Sens, 9, (2017); Teodoro A., Araujo R., Exploration of the OBIA methods available in SPRING noncommercial software to UAV data processing, Proceedings of the Earth Resources and Environmental Remote Sensing/GIS Applications V, (2014); Pal M., Random forest classifier for remote sensing classification, Int. J. Remote Sens, 26, pp. 217-222, (2005); Heumann B.W., An object-based classification of mangroves using a hybrid decision tree-Support vector machine approach, Remote Sens, 3, pp. 2440-2460, (2011); Feng Q., Liu J., Gong J., UAV remote sensing for urban vegetation mapping using random forest and texture analysis, Remote Sens, 7, pp. 1074-1094, (2015); Ma L., Li M., Ma X., Cheng L., Du P., Liu Y., A review of supervised object-based land-cover image classification, ISPRS J. Photogramm. Remote Sens, 130, pp. 277-293, (2017); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, Proceedings of the 25th International Conference on Neural Information Processing Systems, Lake Tahoe, NV, USA, 3-6 December 2012, 1, pp. 1097-1105, (2012); Le Q.V., Building high-level features using large scale unsupervised learning, Proceedings of the 2013 IEEE International Conference on Acoustics, pp. 8595-8598, (2013); Hinton G., Deng L., Yu D., Dahl G., Mohamed A., Jaitly N., Senior A., Vanhoucke V., Nguyen P., Sainath T., Et al., Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups, IEEE Signal Process. Mag, 29, pp. 82-97, (2012); Sainath T.N., Mohamed A., Kingsbury B., Ramabhadran B., Deep convolutional neural networks for LVCSR, Proceedings of the 2013 IEEE International Conference on Acoustics, pp. 8614-8618, (2013); Zhu X.X., Tuia D., Mou L., Xia G.S., Zhang L., Xu F., Fraundorfer F., Deep learning in remote sensing: A review. arXiv Prepr, (2017); Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G., Davis A., Dean J., Devin M., Et al., Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv Prepr, (2016); Tirado R., 5220 Matorrales arborescentes con Ziziphus (*), VV. AA., Bases ecológicas Preliminares Para la Conservación de Los Tipos de Hábitat de Interés Comunitario en Espana, (2009); Zhao W., Du S., Emery W., Object-Based Convolutional Neural Network for High-Resolution Imagery Classification, IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens, 10, pp. 3386-3396, (2017); Langkvist M., Kiselev A., Alirezaie M., Loutfi A., Classification and segmentation of satellite orthoimagery using convolutional neural networks, Remote Sens, 8, (2016); Hu F., Xia G., Hu J., Zhang L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Remote Sens, 7, pp. 14680-14707, (2015); Santara A., Mani K., Hatwar P., Singh A., Garg A., Padia K., Mitra P., BASS Net: Band-Adaptive Spectral-Spatial Feature Learning Neural Network for Hyperspectral Image Classification. arXiv Prepr, (2016); Ding C., Li Y., Xia Y., Wei W., Zhang L., Zhang Y., Convolutional Neural Networks Based Hyperspectral Image Classification Method with Adaptive Kernels, Remote Sens, 9, (2017); Liang H., Li Q., Hyperspectral imagery classification using sparse representations of convolutional neural network features, Remote Sens, 8, (2016); Castelluccio M., Poggi G., Sansone C., Verdoliva L., Land use classification in remote sensing images by convolutional neural networks. arXiv Prepr, (2015); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens, 9, (2016); Tiede D., Krafft P., Fureder P., Lang S., Stratified Template Matching to Support Refugee Camp Analysis in OBIAWorkflows, Remote Sens, 9, (2017); Laliberte A.S., Rango A., Havstad K.M., Paris J.F., Beck R.F., McNeely R., Gonzalez A.L., Object-oriented image analysis for mapping shrub encroachment from 1937 to 2003 in southern New Mexico, Remote Sens. Environ, 93, pp. 198-210, (2004); Hellesen T., Matikainen L., An object-based approach for mapping shrub and tree cover on grassland habitats by use of LiDAR and CIR orthoimages, Remote Sens, 5, pp. 558-583, (2013); Stow D., Hamada Y., Coulter L., Anguelova Z., Monitoring shrubland habitat changes through object-based change identification with airborne multispectral imagery, Remote Sens. Environ, 112, pp. 1051-1061, (2008); Baatz M., Schape A., Multiresolution segmentation: An optimization approach for high quality multi-scale image segmentation, Angew. Geogr. Inf. XII, 58, pp. 12-23, (2000); Tian J., Chen D.M., Optimization in multi-scale segmentation of high-resolution satellite images for artificial feature recognition, Int. J. Remote Sens, 28, pp. 4625-4644, (2007); Liu Y., Bian L., Meng Y., Wang H., Zhang S., Yang Y., Shao X., Wang B., Discrepancy measures for selecting optimal combination of parameter values in object-based image analysis, ISPRS J. Photogramm. Remote Sens, 68, pp. 144-156, (2012); Belgiu M., Dragut L., Random forest in remote sensing: A review of applications and future directions, ISPRS J. Photogramm. Remote Sens, 114, pp. 24-31, (2016); Ghosh A., Joshi P., A comparison of selected classification algorithms for mapping bamboo patches in lower Gangetic plains using very high resolutionWorldView 2 imagery, Int. J. Appl. Earth Observ. Geoinf, 26, pp. 298-311, (2014); Congalton R., Green K., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices, (2008); Guo Y., Liu Y., Oerlemans A., Lao S., Wu S., Lew M., Deep learning for visual understanding: A review, Neurocomputing, 187, pp. 27-48, (2016); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Tabik S., Peralta D., Herrera-Poyatos A., Herrera F., A snapshot of image pre-processing for convolutional neural networks: Case study of MNIST, Int. J. Comput. Intell. Syst, 10, pp. 555-568, (2017); Shin H., Roth H.R., Gao M., Lu L., Xu Z., Nogues I., Yao J., Mollura D., Summers R.M., Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning, IEEE Trans. Med. Imaging, 35, pp. 1285-1298, (2016); Pan S.J., Yang Q., A survey on transfer learning, IEEE Trans. Knowl. Data Eng, 22, pp. 1345-1359, (2010); Peralta D., Triguero I., Garcia S., Saeys Y., Benitez J., Herrera F., On the use of convolutional neural networks for robust classification of multiple fingerprint captures, Int. J. Intell. Syst, 33, pp. 230-313, (2018); Hosang J., Benenson R., Dollar P., Schiele B., What makes for effective detection proposals, IEEE Trans. Pattern Anal. Mach. Intell, 38, pp. 814-830, (2016); Tirado R., Pugnaire F., Shrub spatial aggregation and consequences for reproductive success, Oecologia, 136, pp. 296-301, (2003); Guirado E., Factores que Afectan a la Distribucion Especial de Vegetacion Freatofita (Zipiphus lotus) en el Acuifero Costero de Torre Garcia (Sureste de España), (2015); Guirado E., Alcaraz-Segura D., Rigol-Sanchez J., Gisbert J., Martinez-Moreno F., Galindo-Zaldivar J., Gonzalez-Castillo L., Cabello J., Remote sensing-derived fractures and shrub patterns to identify groundwater dependence, Ecohydrology, (2017); Rivas Goday S., Bellot F., Las formaciones de Zizyphus lotus (L.) Lamk., en las dunas del Cabo de Gata, Anales del Instituto Español de Edafología, Ecología y Fisiología Vegetal, 3, pp. 109-126, (1944); Lagarde F., Louzizi T., Slimani T., El Mouden H., Kaddour K., Moulherat S., Bonnet X., Bushes protect tortoises from lethal overheating in arid areas of Morocco, Environ. Conserv, 39, pp. 172-182, (2012); Manolaki P., Andreou M., Christodoulou C., Improving the conservation status of priority habitat types 1520 and 5220 at Rizoelia National Forest Park. EC LIFE Project; Nussbaum S., Niemeyer I., Canty M., SEATH-A New Tool for Automated Feature Extraction in the Context of Object-Based Image Analysis, (2006); Garcia Garcia J., Sanchez Caparros A., Castillo E., Marin I., Padilla A., Rosso J., Hidrogeoquímica de las aguas subterráneas en la zona de Cabo de Gata, Tecnología de la Intrusión de Agua de mar en Acuíferos Costeros: Países Mediterráneos, pp. 413-422, (2003); Poli D., Remondino F., Angiuli E., Agugiaro G., Radiometric and geometric evaluation of GeoEye-1, WorldView-2 and Pléiades-1A stereo images for 3D information extraction, ISPRS J. Photogramm. Remote Sens, 100, pp. 35-47, (2015); Basaeed E., Bhaskar H., Al-Mualla M., CNN-based multi-band fused boundary detection for remotely sensed images, Proceedings of the 6th International Conference on Imaging for Crime Prevention and Detection (ICDP-15), (2015); Dragut L., Csillik O., Eisank C., Tiede D., Automated parameterisation for multi-scale image segmentation on multiple layers, ISPRS J. Photogramm. Remote Sens, 88, pp. 119-127, (2014); Bastin J., Berrahmouni N., Grainger A., Maniatis D., Mollicone D., Moore R., Patriarca C., Picard N., Sparrow B., Abraham E., Et al., The extent of forest in dryland biomes, Science, 356, pp. 635-638, (2017); Schepaschenko D., Fritz S., See L., Bayas J.C.L., Lesiv M., Kraxner F., Obersteiner M., Comment on ""The extent of forest in dryland biomes, Science, 358, (2017); Prevedello J.A., Almeida-Gomes M., Lindenmayer D., The importance of scattered trees for biodiversity conservation: A global meta-analysis, J. Appl. Ecol, (2017); Griffith D.M., Lehmann C.E., Stromberg C.A., Parr C.L., Pennington R.T., Sankaran M., Ratnam J., Still C.J., Powell R.L., Hanan N.P., Et al., Comment on ""The extent of forest in dryland biomes, Science, 358, (2017); de la Cruz M., Quintana-Ascencio P.F., Cayuela L., Espinosa C.I., Escudero A., Comment on ""The extent of forest in dryland biomes, Science, 358, (2017); Pettorelli N., Wegmann M., Skidmore A., Mucher S., Dawson T.P., Fernandez M., Lucas R., Schaepman M.E., Wang T., O'Connor B., Et al., Framing the concept of satellite remote sensing essential biodiversity variables: challenges and future directions, Remote Sens. Ecol. Conserv, 2, pp. 122-131, (2016)","S. Tabik; Soft Computing and Intelligent Information System Research Group, University of Granada, Granada, 18071, Spain; email: siham@ugr.es","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85038210007"
"Chen X.; Xiang S.; Liu C.-L.; Pan C.-H.","Chen, Xueyun (56076920300); Xiang, Shiming (8938807200); Liu, Cheng-Lin (36064176500); Pan, Chun-Hong (8558023500)","56076920300; 8938807200; 36064176500; 8558023500","Aircraft detection by deep belief nets","2013","Proceedings - 2nd IAPR Asian Conference on Pattern Recognition, ACPR 2013","","","6778281","54","58","4","54","10.1109/ACPR.2013.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899078900&doi=10.1109%2fACPR.2013.5&partnerID=40&md5=63def32c3f48539a44e50e018a3ef899","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China","Chen X., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Xiang S., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Liu C.-L., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Pan C.-H., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China","Aircraft detection is a difficult task in high-resolution remote sensing images, due to the variable sizes, colors, orientations and complex backgrounds. In this paper, an effective aircraft detection method is proposed which exactly locates the object by outputting its geometric center, orientation, position. To reduce the influence of background, multi-images including gradient image and gray thresholding images of the object were input to a Deep Belief Net (DBN), which was pre-trained first to learn features and later fine-tuned by back-propagation to yield a robust detector. Experimental results show that DBNs can detecte the tiny blurred aircrafts correctly in many difficult airport images, DBNs outperform the traditional Feature Classifier methods in robustness and accuracy, and the multi-images help improve the detection precision of DBN than using only single-image. © 2013 IEEE.","Deep convolutional Neural Networks; Object detection; Remote Sensing","Image reconstruction; Neural networks; Pattern recognition; Remote sensing; Complex background; Convolutional neural network; Detection precision; Feature classifiers; Geometric center; High-resolution remote sensing images; Object Detection; Thresholding images; Aircraft detection","","","","","","","Hsieh J.-W., Chen J.-M., Chuang C.-H., Fan K.-C., Aircraft type recognition in satellite images, IEE Proceedings Vision, Image and Signal Processing, 152, 3, pp. 307-315, (2005); Yildiz C., Polat E., Detection of stationary aircrafts from satelitte images, 2011 IEEE 19th Conference on Signal Processing and Communications Applications, pp. 515-521, (2011); Liu G., Sun X., Fu K., Wang H., Aircraft recognition in high-resolution satellite images using coarse-to-fine shape prior, IEEE Geoscience and Remote Sensing Letters, 10, 3, pp. 573-577, (2013); Cai K., Shao W., Yin X., Liu G., Co-segmentation of aircrafts from high-resolution satellite images, Proc. ICSP 2012, pp. 993-996, (2012); Sun H., Sun X., Wang H., Li Y., Li X., Automatic target detection in high-resolution remote sensing im-ages using spatial sparse coding bag-of-words model, IEEE Geoscience and Remote Sensing Letters, 9, 1, pp. 109-113, (2012); Li W., Xiang S., Wang H., Pan C., Robust airplane detection in satellite images, Proc. ICIP, pp. 2877-2880, (2011); Filippidis A., Jain L.C., Martin N., Fusion of intelligent agents for the detection of aircraft in sar images, IEEE Trans. PAMI, 22, pp. 378-384, (2000); Tien S.C., Chia T.L., Lu Y., Using cross-ratios to model curve data for aircraft recognition, Pattern Recognit. Lett, 24, 12, pp. 2047-2060, (2003); Xu C.F., Duan H.B., Artificial bee colony (ABC) optimized edge potential function (EPF) approach to target recognition for low-altitude aircraft, Pattern Recognit. Lett, 31, 13, pp. 1759-1772, (2010); Scott G.J., Klaric M.N., Davis C.H., Shyu C.-R., Entropybalanced bitmap tree for shape-based object retrieval from largescale satellite imagery databases, IEEE Trans. Geosci. Remote Sens, 49, 5, pp. 1603-1616, (2011); Zunic J., Kopanja L., On the orientability of shapes, IEEE Trans. on Image Processing, 15, 11, pp. 3478-3487, (2006); Kembhavi D.H.A., Davis L.S., Vehicle detection using partial least squares, IEEE Trans. PAMI, 63, 3, pp. 1250-1265, (2011); Grabner H., Nguyen T., Gruber B., Bischof H., On-line boosting-based car detection from aerial images, ISPRS J. Photogrammetry and Remote Sensing, 63, 3, pp. 382-396, (2008); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proc. CVPR, 1, pp. 888-893, (2005); Hinton G.E., Osindero S., A fast learning algorithm for deep belief nets, Neural Computation, 18, pp. 1527-1554, (2006); Hinton G.E., A Practical Guide to Training Restricted Boltzmann Machines, pp. 1-20, (2010); Hinton G.E., Reducing the dimensionality of data with neural networks, Science, 313, pp. 504-507, (2006); Sarikaya R., Hinton G.E., Deep belief nets for natural language callcrouting, Proc. ICASSP, pp. 5680-5683, (2011); Mohamed A., Dahl G.E., Hinton G., Acoustic modeling using deep belief networks, IEEE Transactions on Audio, Speech, and Language Processing, 20, 1, pp. 14-22, (2012); Daugman J.G., Complete discrete 2-D gabor transforms by neural networks for image analysis and compression, IEEE Trans. Acoust.,Speech, Signal Processing, 36, 7, pp. 1169-1179, (1988); Ojala T., Pietikainen M., Maenpaa T., Multiresolution gray scale and rotation invariant texture classification with local binary patterns, IEEE Trans. PAMI, 24, 7, pp. 971-987, (2002); Matas J., Chum O., Urban M., Pajdla T., Robust wide baseline stereo from maximally stable extremal regions, Proc. of British Machine Vision Conference, pp. 384-396, (2002)","","","IEEE Computer Society","International Association of Pattern Recognition (IAPR)","2013 2nd IAPR Asian Conference on Pattern Recognition, ACPR 2013","5 November 2013 through 8 November 2013","Naha, Okinawa","104624","","","","","English","Proc. - IAPR Asian Conf. Pattern Recogn., ACPR","Conference paper","Final","","Scopus","2-s2.0-84899078900"
"Osco L.P.; dos Santos de Arruda M.; Gonçalves D.N.; Dias A.; Batistoti J.; de Souza M.; Gomes F.D.G.; Ramos A.P.M.; de Castro Jorge L.A.; Liesenberg V.; Li J.; Ma L.; Marcato J., Jr.; Gonçalves W.N.","Osco, Lucas Prado (57196329154); dos Santos de Arruda, Mauro (57221930407); Gonçalves, Diogo Nunes (56797665900); Dias, Alexandre (14025423500); Batistoti, Juliana (57211501362); de Souza, Mauricio (57211332159); Gomes, Felipe David Georges (57216938738); Ramos, Ana Paula Marques (56198690100); de Castro Jorge, Lúcio André (6503932315); Liesenberg, Veraldo (15848875300); Li, Jonathan (57235557700); Ma, Lingfei (57190372479); Marcato, José (53863759800); Gonçalves, Wesley Nunes (23396539500)","57196329154; 57221930407; 56797665900; 14025423500; 57211501362; 57211332159; 57216938738; 56198690100; 6503932315; 15848875300; 57235557700; 57190372479; 53863759800; 23396539500","A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery","2021","ISPRS Journal of Photogrammetry and Remote Sensing","174","","","1","17","16","34","10.1016/j.isprsjprs.2021.01.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100623552&doi=10.1016%2fj.isprsjprs.2021.01.024&partnerID=40&md5=407768dbca1b3c444534d94b2b26866f","Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo, R. José Bongiovanni, 700 - Cidade Universitária, Presidente Prudente, 19050-920, SP, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Av. Costa e Silva, s/n, Campo Grande, 79070-900, MS, Brazil; Faculty of Veterinary Medicine and Animal Science, Federal University of Mato Grosso do Sul, Avenida Senador Filinto Muller 2443, Campo Grande, 79074-960, MS, Brazil; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil; Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572 - Limoeiro, Pres. Prudente, 19067-175, SP, Brazil; National Research Center of Development of Agricultural Instrumentation, Brazilian Agricultural Research Agency (EMBRAPA), 13560-970, R. XV de Novembro, São Carlos, 1452, SP, Brazil; Forest Engineering Department, University of Santa Catarina State (UDESC), 88520-000, Av. Luiz de Camões, 2090, Conta Dinheiro, Lages, SC, Brazil; Department of Geography and Environmental Management, University of Waterloo, Waterloo, N2L 3G1, ON, Canada","Osco L.P., Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo, R. José Bongiovanni, 700 - Cidade Universitária, Presidente Prudente, 19050-920, SP, Brazil; dos Santos de Arruda M., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Av. Costa e Silva, s/n, Campo Grande, 79070-900, MS, Brazil; Gonçalves D.N., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Av. Costa e Silva, s/n, Campo Grande, 79070-900, MS, Brazil; Dias A., Faculty of Veterinary Medicine and Animal Science, Federal University of Mato Grosso do Sul, Avenida Senador Filinto Muller 2443, Campo Grande, 79074-960, MS, Brazil; Batistoti J., Faculty of Veterinary Medicine and Animal Science, Federal University of Mato Grosso do Sul, Avenida Senador Filinto Muller 2443, Campo Grande, 79074-960, MS, Brazil; de Souza M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil; Gomes F.D.G., Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572 - Limoeiro, Pres. Prudente, 19067-175, SP, Brazil; Ramos A.P.M., Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572 - Limoeiro, Pres. Prudente, 19067-175, SP, Brazil; de Castro Jorge L.A., National Research Center of Development of Agricultural Instrumentation, Brazilian Agricultural Research Agency (EMBRAPA), 13560-970, R. XV de Novembro, São Carlos, 1452, SP, Brazil; Liesenberg V., Forest Engineering Department, University of Santa Catarina State (UDESC), 88520-000, Av. Luiz de Camões, 2090, Conta Dinheiro, Lages, SC, Brazil; Li J., Department of Geography and Environmental Management, University of Waterloo, Waterloo, N2L 3G1, ON, Canada; Ma L., Department of Geography and Environmental Management, University of Waterloo, Waterloo, N2L 3G1, ON, Canada; Marcato J., Jr., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil; Gonçalves W.N., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil","Accurately mapping croplands is an important prerequisite for precision farming since it assists in field management, yield-prediction, and environmental management. Crops are sensitive to planting patterns and some have a limited capacity to compensate for gaps within a row. Optical imaging with sensors mounted on Unmanned Aerial Vehicles (UAV) is a cost-effective option for capturing images covering croplands nowadays. However, visual inspection of such images can be a challenging and biased task, specifically for detecting plants and rows on a one-step basis. Thus, developing an architecture capable of simultaneously extracting plant individually and plantation-rows from UAV-images is yet an important demand to support the management of agricultural systems. In this paper, we propose a novel deep learning method based on a Convolutional Neural Network (CNN) that simultaneously detects and geolocates plantation-rows while counting its plants considering highly-dense plantation configurations. The experimental setup was evaluated in (a) a cornfield (Zea mays L.) with different growth stages (i.e. recently planted and mature plants) and in a (b) Citrus orchard (Citrus Sinensis Pera). Both datasets characterize different plant density scenarios, in different locations, with different types of crops, and from different sensors and dates. This scheme was used to prove the robustness of the proposed approach, allowing a broader discussion of the method. A two-branch architecture was implemented in our CNN method, where the information obtained within the plantation-row is updated into the plant detection branch and retro-feed to the row branch; which are then refined by a Multi-Stage Refinement method. In the corn plantation datasets (with both growth phases – young and mature), our approach returned a mean absolute error (MAE) of 6.224 plants per image patch, a mean relative error (MRE) of 0.1038, precision and recall values of 0.856, and 0.905, respectively, and an F-measure equal to 0.876. These results were superior to the results from other deep networks (HRNet, Faster R-CNN, and RetinaNet) evaluated with the same task and dataset. For the plantation-row detection, our approach returned precision, recall, and F-measure scores of 0.913, 0.941, and 0.925, respectively. To test the robustness of our model with a different type of agriculture, we performed the same task in the citrus orchard dataset. It returned an MAE equal to 1.409 citrus-trees per patch, MRE of 0.0615, precision of 0.922, recall of 0.911, and F-measure of 0.965. For the citrus plantation-row detection, our approach resulted in precision, recall, and F-measure scores equal to 0.965, 0.970, and 0.964, respectively. The proposed method achieved state-of-the-art performance for counting and geolocating plants and plant-rows in UAV images from different types of crops. The method proposed here may be applied to future decision-making models and could contribute to the sustainable management of agricultural systems. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Object detection; Precision agriculture; Remote sensing; UAV imagery","Citrus; Citrus sinensis; Zea mays; Agricultural robots; Antennas; Convolutional neural networks; Cost effectiveness; Crops; Decision making; Deep learning; Environmental management; Network architecture; Orchards; Statistical tests; Sustainable development; Unmanned aerial vehicles (UAV); Agricultural system; Decision making models; Different growth stages; Mean absolute error; Mean relative error; Precision and recall; State-of-the-art performance; Sustainable management; data set; decision making; detection method; growth; image analysis; orchard; performance assessment; satellite imagery; unmanned vehicle; Learning systems","","","","","CAPES-Print, (59/300.066/2015, 59/300.095/2015, 88881.311850/2018-01); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Empresa Brasileira de Pesquisa Agropecuária, EMBRAPA, (23700.19/0192-9-1); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303559/2019-5, 304173/2016-9, 310517/2020-6, 313887/2018-7, 433783/2018-4); Fundação de Amparo à Pesquisa e Inovação do Estado de Santa Catarina, FAPESC, (2017TR1762); Universidade Federal de Mato Grosso do Sul, UFMS","Funding text 1: The authors acknowledge the support of UFMS (Federal University of Mato Grosso do Sul) and CAPES (Finance code 001), the donation of a Titan X and a Titan V by ©NVIDIA, the EMBRAPA (Brazilian Agricultural Research Corporation) for providing additional imagery datasets, and CAPES ( Coordination for the Improvement of Higher Education Personnel - Finance code 001);.; Funding text 2: The authors are funded by EMBRAPA (p: 23700.19/0192-9-1), National Council for Scientific and Technological Development (CNPq) (grant number 303559/2019-5, 433783/2018-4, 310517/2020-6, 313887/2018-7, and 304173/2016-9), CAPES-Print (p: 88881.311850/2018-01), and Fundect (p: 59/300.066/2015 and 59/300.095/2015). V. Liesenberg is supported by FAPESC (2017TR1762). ","Adhikari S.P., Yang H., Kim H., Learning semantic graphics using convolutional encoder–decoder network for autonomous weeding in paddy, Front. Plant Sci., 10, pp. 1-12, (2019); Aich S., Stavness I., (2018); Alshehhi R., Marpu P.R., Woon W.L., Mura M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks, ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149, (2017); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sens., 11, 4, pp. 410-429, (2019); An J., Li W., Li M., Cui S., Yue H., Identification and classification of maize drought stress using deep convolutional neural network, Symmetry, 11, 2, pp. 1-14, (2019); Bah M.D., Hafiane A., Canals R., CRowNet: Deep Network for Crop Row Detection in UAV Images, IEEE Access, 8, pp. 5189-5200, (2020); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Ball J.E., Anderson D.T., Chan C.S., (2017); Chen S.W., Shivakumar S.S., Dcunha S., Das J., Okon E., Qu C., Taylor C.J., Kumar V., Counting apples and oranges with deep learning: a data-driven approach, IEEE Robot. Autom. Lett., 2, 2, pp. 781-788, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks, Drones, 2, 4, pp. 39-55, (2018); Delloye C., Weiss M., Defourny P., Retrieval of the canopy chlorophyll content from Sentinel-2 spectral bands to estimate nitrogen uptake in intensive winter wheat cropping systems, Remote Sens. Environ., 216, pp. 245-261, (2018); Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens., 146, pp. 124-136, (2018); Dian Bah M., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens., 10, 11, pp. 1-20, (2018); Djerriri K., Ghabi M., Karoui M.S., Adjoudj R., (2018); Fan Z., Lu J., Gong M., Xie H., Goodman E.D., Automatic Tobacco Plant Detection in UAV Images via Deep Neural Networks, IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing, 11, 3, pp. 876-887, (2018); Freudenberg M., Nolke N., Agostini A., Urban K., Worgotter F., Kleinn C., Large scale palm tree detection in high resolution satellite images using U-Net, Remote Sens., 11, 3, pp. 1-18, (2019); Ghamisi P., Plaza J., Chen Y., Li J., Plaza A.J., Advanced Spectral Classifiers for Hyperspectral Images: A review, IEEE Geosci. Remote Sens. Mag., 5, 1, pp. 8-32, (2017); Gnadinger F., Schmidhalter U., (2017); Goldman E., Herzig R., Eisenschtat A., Ratzon O., Levi I., Goldberger J., Hassner T., (2019); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a WorldView-2/3 and LiDAR data fusion approach and deep learning, Sensors, 19, 6, pp. 1-23, (2019); Hassanein M., Khedr M., El-Sheimy N., Crop row detection procedure using low-cost UAV imagery system, ISPRS Archives, 42, 2/W13, pp. 349-356, (2019); Ho Tong Minh D., Ienco D., Gaetano R., Lalande N., Ndikumana E., Osman F., Maurel P., Deep Recurrent Neural Networks for Winter Vegetation Quality Mapping via Multitemporal SAR Sentinel-1, IEEE Geosci. Remote Sensing Lett., 15, 3, pp. 464-468, (2018); Huang X., Liu X., Zhang L., (2014); Hunt E.R., Daughtry C.S.T., What good are unmanned aircraft systems for agricultural remote sensing and precision agriculture?, Int. J. Remote Sens., 39, 15-16, pp. 5345-5376, (2018); Hunt M.L., Blackburn G.A., Carrasco L., Redhead J.W., Rowland C.S., High resolution wheat yield mapping using Sentinel-2, Remote Sens. Environ., 233, (2019); Hussain M., Chen D., Cheng A., Wei H., Stanley D., Change detection from remotely sensed images: From pixel-based to object-based approaches, ISPRS J. Photogramm. Remote Sens., 80, pp. 91-106, (2013); Ioffe S., Szegedy C., 1, pp. 448-456, (2015); Jakubowski M.K., Li W., Guo Q., Kelly M., Delineating individual trees from lidar data: A comparison of vector- and raster-based segmentation approaches, Remote Sens., 5, 9, pp. 4163-4186, (2013); Jensen J.R., Introductory Digital Image Processing: A Remote Sensing Perspective, (2015); Jiang H., Chen S., Li D., Wang C., Yang J., Papaya Tree detection with UAV images using a GPU-accelerated scale-space filtering method, Remote Sens., 9, 7, pp. 721-734, (2017); Jin Z., Azzari G., You C., Di Tommaso S., Aston S., Burke M., Lobell D.B., Smallholder maize area and yield mapping at national scales with Google Earth Engine, Remote Sens. Environ., 228, pp. 115-128, (2019); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric., 147, pp. 70-90, (2018); Khamparia A., Singh K.M., A systematic review on deep learning architectures and applications, Expert Systems, 36, 3, (2019); Kitano B.T., Mendes C.C.T., Geus A.R., Oliveira H.C., Souza J.R., (2019); Krizhevsky A., (2014); Larsen M., Eriksson M., Descombes X., Perrin G., Brandtberg T., Gougeon F.A., (2011); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Leiva J.N., Robbins J., Saraswat D., She Y., Ehsani R., Evaluating remotely sensed plant count accuracy with differing unmanned aircraft system altitudes, physical canopy separations, and ground covers, J. Appl. Remote Sens, 11, 3, (2017); Li D., Guo H., Wang C., Li W., Chen H., Zuo Z., Individual Tree Delineation in Windbreaks Using Airborne-Laser-Scanning Data and Unmanned Aerial Vehicle Stereo Images, IEEE Geosci. Remote Sensing Lett., 13, 9, pp. 1330-1334, (2016); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens., 9, 1, pp. 22-35, (2017); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., (2017); Miyoshi G.T., Arruda M., (2020); Mochida K., Koda S., Inoue K., Hirayama T., Tanaka S., Nishii R., Melgani F., Computer vision-based phenotyping for improvement of plant productivity: A machine learning perspective, GigaScience, 8, 1, pp. 1-12, (2018); Mohanty S.K., Swain M.R., (2019); Ndikumana E., Minh D.H.T., Baghdadi N., Courault D., Hossard L., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France. Remote Sens., 10, 8, pp. 1-16, (2018); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., (2017); Oliveira H.C., Guizilini V.C., Nunes I.P., Souza J.R., Failure Detection in Row Crops From UAV Images Using Morphological Operators, IEEE Geosci. Remote Sensing Lett., 15, 7, pp. 991-995, (2018); Osco L.P., Paula A., Ramos M., Pereira D.R., (2019); Osco L.P., Ramos A.P.M., Pinheiro M.M.F.; Osco L.P., Arruda M.D.S.D., Marcato Junior J., da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Li J., Goncalves W.N., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens., 160, pp. 97-106, (2020); Ozcan A.H., Hisar D., Sayar Y., Unsalan C., Tree crown detection and delineation in satellite images using probabilistic voting, Remote Sensing Letters, 8, 8, pp. 761-770, (2017); Ozdarici-Ok A., Automatic detection and delineation of citrus trees from VHR satellite imagery, Int. J. Remote Sens., 36, 17, pp. 4275-4296, (2015); Paoletti M.E., Haut J.M., Plaza J., Plaza A., A new deep convolutional neural network for fast hyperspectral image classification, ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147, (2018); Primicerio J., Caruso G., Comba L., Crisci A., Gay P., Guidoni S., Genesio L., Ricauda Aimonino D., Vaccari F.P., Individual plant definition and missing plant characterization in vineyards from high-resolution UAV imagery, Eur. J. Remote Sens., 50, 1, pp. 179-186, (2017); Quan L., Feng H., Lv Y., Wang Q.I., Zhang C., Liu J., Yuan Z., Maize seedling detection under different growth stages and complex field environments based on an improved Faster R–CNN, Biosyst. Eng., 184, pp. 1-23, (2019); Redmon J., Farhadi A., (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proc. NIPS, 28, pp. 91-99, (2015); Ribera J., Chen Y., Boomsma C., Delp E.J., Counting plants using deep learning, Prof. Global SIP, 2017, pp. 1344-1348, (2018); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Lecture Notes Comput. Sci. (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9351, pp. 234-241, (2015); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of fir trees (Abies sibirica) damaged by the bark beetle in unmanned aerial vehicle images with deep learning, Remote Sens., 11, 6, (2019); Salami E., Gallardo A., Skorobogatov G., Barrado C., On-the-fly olive tree counting using a UAS and cloud services, Remote Sens., 11, 3, pp. 316-337, (2019); Santos A.A., (2019); Simonyan K., Zisserman A., (2015); Sun J., Di L., Sun Z., Shen Y., Lai Z., County-level soybean yield prediction using deep CNN-LSTM model, Sensors, 19, 20, pp. 1-21, (2019); Sylvain J.-D., Drolet G., Brown N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography, ISPRS J. Photogramm. Remote Sens., 156, pp. 14-26, (2019); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., (2016); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., 2017, pp. 4278-4284, (2017); Tao S., Wu F., Guo Q., Wang Y., Li W., Xue B., Hu X., Li P., Tian D.I., Li C., Yao H., Li Y., Xu G., Fang J., Segmenting tree crowns from terrestrial and mobile LiDAR data by exploring ecological theories, ISPRS J. Photogramm. Remote Sens., 110, pp. 66-76, (2015); Varela S., Dhodda P.R., Hsu W.H., Prasad P.V.V., Assefa Y., Peralta N.R., (2018); Verma N.K., Lamb D.W., Reid N., Wilson B., Comparison of canopy volume measurements of scattered eucalypt farm trees derived from high spatial resolution imagery and LiDAR, Remote Sens., 8, 5, pp. 388-404, (2016); Wang H., Magagi R., Goita K., Trudel M., McNairn H., Powers J., Crop phenology retrieval via polarimetric SAR decomposition and Random Forest algorithm, Remote Sens. Environ., 231, (2019); Wang S., Azzari G., Lobell D.B., Crop type mapping without field-level labels: Random forest transfer and unsupervised clustering techniques, Remote Sens. Environ., 222, pp. 303-317, (2019); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks, Remote Sens., 11, 11, pp. 1309-1322, (2019); Weiss M., Jacob F., Duveiller G., Remote sensing for agricultural applications: A meta-review, Remote Sens. Environ., 236, (2020); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network, Remote Sens., 11, 6, pp. 691-710, (2019); Zhang T.Y., Suen C.Y., A fast parallel algorithm for thinning digital patterns, Commun. ACM, 27, 3, pp. 236-239, (1984); Zhang H., Li Y., Zhang Y., Shen Q., Spectral-spatial classification of hyperspectral imagery using a dual-channel convolutional neural network, Remote Sens. Lett., 8, 5, pp. 438-447, (2017); Zhao H., Shi J., Qi X., Wang X., Jia J., Pyramid scene parsing network, Proc. CVPR, pp. 2881-2890, (2017); Zhong L., Hu L., Zhou H., Deep learning based multi-temporal crop classification, Remote Sens. Environ., 221, pp. 430-443, (2019); Zhou C., Yang G., Liang D., Yang X., Xu B.O., An integrated skeleton extraction and pruning method for spatial recognition of maize seedlings in MGV and UAV remote Images, IEEE Trans. Geosci. Remote Sens., 56, 8, pp. 4618-4632, (2018)","A.P.M. Ramos; Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, Pres. Prudente, km 572 - Limoeiro, 19067-175, Brazil; email: anaramos@unoeste.br","","Elsevier B.V.","","","","","","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85100623552"
"Coelho Eugenio F.; Badin T.L.; Fernandes P.; Mallmann C.L.; Schons C.; Schuh M.S.; Soares Pereira R.; Fantinel R.A.; Pereira da Silva S.D.","Coelho Eugenio, Fernando (57216183600); Badin, Tiago Luis (57202289568); Fernandes, Pablo (57218353033); Mallmann, Caroline Lorenci (57218353857); Schons, Cristine (57210418401); Schuh, Mateus Sabadi (56094380000); Soares Pereira, Rudiney (7201455822); Fantinel, Roberta Aparecida (57217196239); Pereira da Silva, Sally Deborah (57298621500)","57216183600; 57202289568; 57218353033; 57218353857; 57210418401; 56094380000; 7201455822; 57217196239; 57298621500","Remotely Piloted Aircraft Systems (RPAS) and machine learning: A review in the context of forest science","2021","International Journal of Remote Sensing","42","21","","8207","8235","28","5","10.1080/01431161.2021.1975845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117238548&doi=10.1080%2f01431161.2021.1975845&partnerID=40&md5=b1cbebd576c8dc62761fb711cef7f5ba","Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM – Ernesto Barros, Cachoeira Do Sul, RS, Brazil; Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Graduate Program in Geograph– Department of Geosciences, Federal University of Santa Maria/UFSM – University City, Camobi, Santa Maria, RS, Brazil","Coelho Eugenio F., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM – Ernesto Barros, Cachoeira Do Sul, RS, Brazil; Badin T.L., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Fernandes P., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Mallmann C.L., Graduate Program in Geograph– Department of Geosciences, Federal University of Santa Maria/UFSM – University City, Camobi, Santa Maria, RS, Brazil; Schons C., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Schuh M.S., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Soares Pereira R., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Fantinel R.A., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil; Pereira da Silva S.D., Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, RS, Brazil","The combination of machine learning and products obtained by Remote Piloted Aircraft Systems (RPAS) has stood out in the forest sector in the last decade due to the wide range of applications and contributions to the classification and modelling of spatial attributes. The literature presents an overview of these applications, their limitations, and the perspectives related to the theme. Many applications are promising and have different approaches in practices such as identifying rare or invasive trees species, quantifying biomass, early detection of diseases, monitoring deforestation and forest fires, among others. In this context of large data sets generated by RPAS surveys, observed significant influence of deep learning methods, mainly due to their ability to automatically extract spatial characteristics and design flexibility, as in the case of different Neural Networks (NN) architectures. Other algorithms such as Random Forest (RF) and Support Vector Machine (SVM) also occupy a prominent position in this context, having a proven ability to deal with complex problems. However, reduction of computational load, consolidation of analysis protocols, automated selection of predictors, use of proprietary software, and expansion of the research scope are some of the challenges that still need to be overcome to disseminate and improve the use of RPA data sets. Thus, computer vision through deep learning networks emerges as a promising approach in object-based image analysis, considering its ability to recover patterns, even when operating in less complex databases, such as RGB compositions. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","","Aircraft detection; Complex networks; Decision trees; Deep learning; Support vector machines; Unmanned aerial vehicles (UAV); Aircraft systems; Disease monitoring; Forest fires; Forest science; Forest sectors; Large datasets; Remotely piloted aircraft; Spatial attribute; System survey; Tree species; biomass; computer vision; deforestation; disease; forest fire; image analysis; invasive species; machine learning; remote sensing; tree; Deforestation","","","","","","","Adao T., Hruska J., Padua L., Bessa J., Peres E., Morais R., Sousa J.J., Hyperspectral Imaging: A Review on UAV-Based Sensors, Data Processing and Applications for Agriculture and Forestry, Remote Sensing, 9, 11, (2017); Aggarwal C.C., Neural Networks and Deep Learning, (2018); Akar O., Mapping Land Use with Using Rotation Forest Algorithm from UAV Images, European Journal of Remote Sensing, 50, 1, pp. 269-279, (2017); Anuradha, Gupta G., A Self Explanatory Review of Decision Tree Classifiers, International Conference on Recent Advances and Innovations in Engineering (ICRAIE-2014), (2014); Atkinson P.M., Tatnall A.R.L., Introduction Neural Networks in Remote Sensing, International Journal of Remote Sensing, 18, 4, pp. 699-709, (1997); Belgiu M., Dragut L., Random Forest in Remote Sensing: A Review of Applications and Future Directions, ISPR Journal of Photogrammetry and Remote Sensing, 114, pp. 24-31, (2016); Biau G., Analysis of a Random Forests Model, Journal of Machine Learning Research, 13, pp. 1063-1095, (2012); Biau G., Scornet E., A Random Forest Guided Tour, TEST, 25, 2, pp. 197-227, (2016); Bouachir W., Ihou K.E., Gueziri H.E., Bouguila N., Belanger N., Computer Vision System for Automatic Counting of Planting Microsites Using UAV Imagery, IEEE Access, 7, pp. 82491-82500, (2019); Breiman L., Random Forests, Machine Learning, 45, pp. 5-32, (2001); Bzdok D., Krzywinski M., Altman N., Machine Learning: A Primer, Nature Methods, 14, 12, pp. 1119-1120, (2017); Bzdok D., Altman N., Krzywinski M., Points of Significance: Statistics versus Machine Learning, Nature Methods, 15, 4, pp. 233-234, (2018); Cao J., Leng W., Liu K., Liu L., He Z., Zhu Y., Object-Based Mangrove Species Classification Using Unmanned Aerial Vehicle Hyperspectral Images and Digital Surface Models, Remote Sensing, 10, 1, (2018); Carrio A., Sampedro C., Rodriguez-Ramos A., Campoy P., A Review of Deep Learning Methods and Applications for Unmanned Aerial Vehicles, Journal of Sensors, 2017, pp. 1-13, (2017); Carvajal-Ramirez F., Jr M.D.S., Aguera-Vega F., Martinez-Carricondo P., Serrano J., Fj M., Evaluation of Fire Severity Indices Based on Pre- and Post-Fire Multispectral Imagery Sensed from UAV, Remote Sensing, 11, 9, (2019); Chen T., Guestrin C., XGBoost: A Scalable Tree Boosting System.In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16), 785–794. New York, NY: Association for Computing Machinery., (2016); Chirici G., Mura M., McInerney D., Py N., Tomppo E.O., Waser L.T., Lars T., McRoberts, Davide R.E., Mcroberts R., A Meta-analysis and Review of the Literature on the k-Nearest Neighbors Technique for Forestry Applications that Use Remotely Sensed Data, Remote Sensing Environment, 176, pp. 282-294, (2016); Cormen T.H., Leiserson C.E., Rivest R.L., Stein C., Introduction to Algorithms, (2009); Corte, AP D., Souza D.V., Rex F.E., Sanquetta C.R., Mohan M., Silva C.A., Zambrano A.M.A., Et al., Forest Inventory with High-density UAV-Lidar: Machine Learning Approaches for Predicting Individual Tree Attributes, Computers and Electronics in Agriculture, 179, (2020); Costa-Filho S.V.S.D., Arce J.E., Montano R.N.R., Pelissari A.L., Configuração De Algoritmos De Aprendizado De Máquina Na Modelagem Florestal: Um Estudo De Caso Na Modelagem Da Relação Hipsométrica, Ciência Florestal, 29, 4, pp. 1501-1515, (2019); Dandois J.P., Baker M., Olano M., Parker G.G., Ellis E.C., What Is the Point? Evaluating the Structure, Color, and Semantic Traits of Computer Vision Point Clouds of Vegetation, Remote Sensing, 9, 4, (2017); Dash J.P., Watt M.S., Paul T.S.H., Morgenroth J., Pearse G.D., Early Detection of Invasive Exotic Trees Using UAV and Manned Aircraft Multispectral and LiDAR Data, Remote Sensing, 11, 15, (2019); De Luca G., Silva JM N., Cerasoli S., Araujo J., Campos J., Di F.S., Modica G., Object-based Land Cover Classification of Cork Oak Woodlands Using UAV Imagery and Orfeo Toolbox, Remote Sensing, 11, 10, (2019); De Sa N.C., Castro P., Carvalho S., Marchante E., Lopez-Nunez F.A., Marchante H., Mapping the Flowering of an Invasive Plant Using Unmanned Aerial Vehicles: Is There Potential for Biocontrol Monitoring?, Frontiers in Plant Science, 9, pp. 1-13, (2018); Dekker A.G., Peters S., Vos R., Rijkeboer M., Remote Sensing for Inland Water Quality Detection and Monitoring: State-of-the-art Application in Friesland Waters. GIS Remote Sens Tech Land- Water-management, pp. 17-38, (2001); Dhanabal S., Chandramathi S., A Review of Various k-Nearest Neighbor Query Processing Techniques, International Journal of Computer Applications, 31, 7, pp. 14-22, (2011); Dietterich T.G., Ensemble Methods in Machine Learning, Lecture Notes in Computer Science, pp. 1-15, (2000); Dixon B., Candade N., Multispectral Landuse Classification Using Neural Networks and Support Vector Machines: One or the Other, or Both?, International Journal of Remote Sensing, 29, 4, pp. 1185-1206, (2008); Dos Santos A.A., Marcato Junior J., Araujo M.S., Di Martini D.R., Tetila E.C., Siqueira H.L., Aoki C., Et al., Assessment of CNN-based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVS, Sensors, 19, 16, (2019); Eugenio F.C., Schons C.T., Mallmann C.L., Schuh M.S., Fernandes P., Badin T.L., Remotely Piloted Aircraft System and Forests: A Global State of the Art and Future Challenges, Canadian Journal of Forest Research, 50, 8, pp. 705-716, (2020); Fagua J.C., Jantz P., Rodriguez-Buritica S., Duncanson L., Goetz S.J., Integrating LiDAR, Multispectral and SAR Data to Estimate and Map Canopy Height in Tropical Forests, Remote Sensing, 11, 22, (2019); Feng Q., Liu J., Gong J., UAV Remote Sensing for Urban Vegetation Mapping Using Random Forest and Texture Analysis, Remote Sensing, 7, 1, pp. 1074-1094, (2015); Foody G.M., Mathur A., Toward Intelligent Training of Supervised Image Classifications: Directing Training Data Acquisition for SVM Classification, Remote Sensing of Environment, 93, 1-2, pp. 107-117, (2004); Franklin S.E., Pixel- and Object-based Multispectral Classification of Forest Tree Species from Small Unmanned Aerial Vehicles, Journal of Unmanned Vehicle Systems, 6, 4, pp. 195-211, (2018); Franklin S.E., Ahmed O.S., Deciduous Tree Species Classification Using Object-based Analysis and Machine Learning with Unmanned Aerial Vehicle Multispectral Data, International Journal Remote Sensing, 39, 15-16, pp. 5236-5245, (2018); Franklin S.E., Ahmed O.S., Williams G., Northern Conifer Forest Species Classification Using Multispectral Data Acquired from an Unmanned Aerial Vehicle, Photogrammetric Engineering & Remote Sensing, 83, 7, pp. 501-507, (2017); Freund Y., Schapire R., (1996); A Recursive Partitioning Decision Rule for Nonparametric Classification, IEEE Transactions on Computers, 26, pp. 404-408, (1977); Friedman J.H., Stochastic Gradient Boosting, Computational Statistics & Data Analysis, 38, 4, pp. 367-378, (2002); Furuya D.E.G., Et al., A Machine Learning Approach for Mapping Forest Vegetation in Riparian Zones in an Atlantic Biome Environment Using Sentinel-2 Imagery, Remote Sensing, 12, 24, (2020); Gamon J.A., Somers B., Malenovsky Z., Middleton E.M., Rascher U., Schaepman M.E., Assessing Vegetation Function with Imaging Spectroscopy, Surveys in Geophysics, 40, 3, pp. 489-513, (2019); Ghose M.K., Ratika P., Ghose S.S., Decision Tree Classification of Remotely Sensed Satellite Data Using Spectral Separability Matrix, International Journal of Advanced Computer Science and Applications, 1, pp. 93-101, (2010); Hah A.-N., Kalantar B., Pradhan B., Saeidi V., Halin A.A., Ueda N., Mansor S., Land Cover Classification from Fused DSM and UAV Images Using Convolutional Neural Networks, Remote Sensing, 11, 12, (2019); Haykin S., Neural Networks: A Comprehensive Foundation, (1999); Holloway J., Mengersen K., Statistical Machine Learning Methods and Remote Sensing for Sustainable Development Goals: A Review, Remote Sensing, 10, 9, (2018); Hughes G., On the Mean Accuracy of Statistical Pattern Recognizers, IEEE Transactions on Information Theory, 14, 1, pp. 55-63, (1968); Imangholiloo M., Saarinen N., Markelin L., Rosnell T., Nasi R., Hakala T., Honkavaara E., Holopainen M., Hyyppa J., Vastaranta M., Characterizing Seedling Stands Using Leaf-Off and Leaf-On Photogrammetric Point Clouds and Hyperspectral Imagery Acquired from Unmanned Aerial Vehicle, Forests, 10, 5, (2019); James G., Witten D., Hastie T., Tibshirani R., An Introduction to Statistical Learning, (2013); Jensen J.R., Introductory Digital Image Processing: A Remote Sensing Perspective, (2016); Jung M., Reichstein M., Margolis H.A., Cescatti A., Richardson A.D., Arain M.A., Arneth A., Et al., Global Patterns of Land-atmosphere Fluxes of Carbon Dioxide, Latent Heat, and Sensible Heat Derived from Eddy Covariance, Satellite, and Meteorological Observations, Journal of Geophysical Research, 116, (2011); Kamilaris A., Prenafeta-Boldu F.X., A Review of the Use of Convolutional Neural Networks in Agriculture, The Journal of Agricultural Science, 156, 3, pp. 312-322, (2018); Kattenborn T., Lopatin J., Forster M., Braun A.C., Fassnacht F.E., UAV Data as Alternative to Field Sampling to Map Woody Invasive Species Based on Combined Sentinel-1 and Sentinel-2 Data, Remote Sensing of Environment, 227, pp. 61-73, (2019); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks. NIPS’12 Proc 25th Int Conf Neural Inf Process Syst, pp. 1097-1105, (2012); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geoscience and Remote Sensing Letters, 14, 5, pp. 778-782, (2017); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, 7553, pp. 436-444, (2015); Lee S., Application of Logistic Regression Model and Its Validation for Landslide Susceptibility Mapping Using GIS and Remote Sensing Data, International Journal Remote Sensing, 26, 7, pp. 1477-1491, (2005); Li W., Campos-Vargas C., Marzahn P., Sanchez-Azofeifa A., On the Estimation of Tree Mortality and Liana Infestation Using a Deep Self-encoding Network, International Journal of Applied Earth Observation and Geoinformation, 73, pp. 1-13, (2018); Li Y., Li C., Li M., Liu Z., Influence of Variable Selection and Forest Type on Forest Aboveground Biomass Estimation Using Machine Learning Algorithms, Forests, 10, 12, (2019); Liberati A., Altman D.G., Tetzlaff J., Cynthia M., Peter C.G., Ioannidis J.P.A., Mike C., Devereaux P.J., Kleijnen J., Moher D., The PRISMA Statement for Reporting Systematic Reviews and Meta-analyses of Studies that Evaluate Health Care Interventions: Explanation and Elaboration, BMJ, 339, (2009); Lin Y., Jiang M., Yao Y., Zhang L., Lin J., Use of UAV Oblique Imaging for the Detection of Individual Trees in Residential Environments, Urban Forestry & Urban Greening, 14, 2, pp. 404-412, (2015); Liu C., Ai M., Chen Z., Zhou Y., Wu H., Detection of Firmiana Danxiaensis Canopies by a Customized Imaging System Mounted on an UAV Platform, Journal of Sensors, 12, (2018); Liu K., “Estimating Forest Structural Attributes Using UAV-LiDAR Data in Ginkgo Plantations.”, pp. 465-482, (2018); Louppe G., Understanding Random Forests: From Theory to Practice, (2014); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep Learning in Remote Sensing Applications: A Meta-analysis and Review, ISPRS Journal of Photogrammetry and Remote Sensing, 152, pp. 166-177, (2019); Malenovsky Z., Lucieer A., King D.H., Turnbull J.D., Robinson S.A., Unmanned Aircraft System Advances Health Mapping of Fragile Polar Vegetation, Methods Ecology and Evolution, 8, 12, pp. 1842-1857, (2017); Mas J.F., Flores J.J., The Application of Artificial Neural Networks to the Analysis of Remotely Sensed Data, International Journal of Remote Sensing, 29, 3, pp. 617-663, (2008); McCulloch W.S., Pitts W., A Logical Calculus of the Ideas Immanent in Nervous Activity, Bulletin of Mathematical Biophysics, 5, 4, pp. 115-133, (1943); Mehta P., Bukov M., Wang C.-H., Day A.G.R., Richardson C., Fisher C.K., Schwab D.J., Et al., A High-bias, Low-variance Introduction to Machine Learning for Physicists, Phyics Reports, 810, pp. 1-124, (2019); Melgani F., Bruzzone L., Classification of Hyperspectral Remote Sensing Images with Support Vector Machines, IEEE Transactions on Geoscience and Remote Sensing, 42, 8, pp. 1778-1790, (2004); Michez A., Piegay H., Lisein J., Claessens H., Lejeune P., Classification of Riparian Forest Species and Health Condition Using Multi-temporal and Hyperspatial Imagery from Unmanned Aerial System, Environmental Monitoring and Assessment, 188, 3, pp. 1-19, (2016); Mitchell M., An Introduction to Genetic Algorithms, Cambridge: MIT Press; Reprint Edition (February, 6, (1998); Morales G., Kemper G., Sevillano G., Arteaga D., Ortega I., Telles J., Automatic Segmentation of Mauritia Flexuosa in Unmanned Aerial Vehicle (UAV) Imagery Using Deep Learning, Forests, 9, 12, (2018); Mountrakis G., Im J., Ogole C., Support Vector Machines in Remote Sensing: A Review, ISPRS Journal of Photogrammetry and Remote Sensing, 66, 3, pp. 247-259, (2011); Navarro J.A., Algeet N., Fernandez-Landa A., Esteban J., Rodriguez-Noriega P., Guillen-Climent M.L., Integration of UAV, Sentinel-1, and Sentinel-2 Data for Mangrove Plantation Aboveground Biomass Monitoring in Senegal, Remote Sensing, 11, 1, (2019); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., Hyyppa J., Et al., Individual Tree Detection and Classification with UAV-Based Photogrammetric Point Clouds and Hyperspectral Imaging, Remote Sensing, 9, 3, (2017); Pacheco A.D.P., Junior D.S., Ruiz-Armenteros A.M., Henriques R.F.F., Assessment of k-Nearest Neighbor and Random Forest Classifiers for Mapping Forest Fire Areas in Central Portugal Using Landsat-8, Sentinel-2, and Terra Imagery, Remote Sensing, 13, n. 7, (2021); Pal M., Mather P.M., Support Vector Machines for Classification in Remote Sensing, International Journal Remote Sensing, 26, 5, pp. 1007-1011, (2005); Pan S.J., Yang Q., A Survey on Transfer Learning, IEEE Transactions on Knowledge and Data Engineering, 22, 10, pp. 1345-1359, (2010); Paoletti M.E., Haut J.M., Plaza J., Plaza A., Deep Learning Classifiers for Hyperspectral Imaging: A Review, ISPRS Journal of Photogrammetry and Remote Sensing, 158, pp. 279-317, (2019); Park J.Y., Muller-Landau H.C., Lichstein J.W., Rifai S.W., Dandois J.P., Bohlman S.A., Quantifying Leaf Phenology of Individual Trees and Species in a Tropical Forest Using Unmanned Aerial Vehicle (UAV) Images, Remote Sensing, 11, 13, (2019); Puliti S., Talbot B., Astrup R., Tree-stump Detection, Segmentation, Classification, and Measurement Using Unmanned Aerial Vehicle (UAV) Imagery, Forests, 9, 3, (2018); Puliti S., Solberg S., Granhus A., Use of UAV Photogrammetric Data for Estimation of Biophysical Properties in Forest Stands under Regeneration, Remote Sensing, 11, 3, (2019); Quinlan J.R., Induction of Decision Trees, Machine Learning, 1, 1, pp. 81-106, (1986); Quinlan J.R., C4.5: Programs for Machine Learning, (1993); Rasanen A., Virtanen T., Data and Resolution Requirements in Mapping Vegetation in Spatially Heterogeneous Landscapes, Remote Sensing Environment, 230, (2019); Reis B.P., Martins S.V., Fernandes Filho E.I., Sarcinelli T.S., Gleriani J.M., Marcatti G.E., Leite H.G., Halassy M., Management Recommendation Generation for Areas under Forest Restoration Process through Images Obtained by UAV and LiDAR, Remote Sensing, 11, 13, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, pp. 1137-1149, (2017); Rishickesh R., Shahina A., Khan N.A., Predicting Forest Fires Using Supervised and Ensemble Machine Learning Algorithms, International Journal of Recent Technology and Engineering, 8, pp. 3697-3705, (2019); Rodriguez J.J., Kuncheva L.I., Alonso C.J., Rotation Forest: A New Classifier Ensemble Method, IEEE Transactions on Pattern Analysis and Machine Intelligence, 28, 10, pp. 1619-1630, (2006); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of Fir Trees (Abies Sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning, Remote Sensing, 11, 6, (2019); Salami E., Barrado C., Pastor E., UAV Flight Experiments Applied to the Remote Sensing of Vegetated Areas, Remote Sensing, 6, 11, pp. 11051-11081, (2014); Sandino J., Pegg G., Gonzalez F., Smith G., Aerial Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral Sensors, and Artificial Intelligence, Sensors, 18, 4, (2018); Schmidhuber J., Deep Learning in Neural Networks: An Overview, Neural Networks, 61, pp. 85-117, (2015); Scholkopf B., Smola A.J., Williamson R.C., Bartlett P.L., New Support Vector Algorithms, Neural Computation, 12, 5, pp. 1207-1245, (2000); Seber G.A.F., Multivariate Observations, (1984); Shayaa S., Jaafar N.I., Bahri S., Sulaiman A., Seuk Wai P., Wai Chung Y., Piprani A.Z., Et al., Sentiment Analysis of Big Data: Methods, Applications, and Open Challenges, IEEE Access, 6, pp. 37807-37827, (2018); Shoombuatong W., Schaduangrat N., Nantasenamat C., Towards Understanding Aromatase Inhibitory Activity via QSAR Modeling, EXCLI J, 17, pp. 688-708, (2018); Silver M., Sakata T., Su H.C., Herman C., Dolins S.B., O'Shea M.J., Case Study: How to Apply Data Mining Techniques in a Healthcare Data Warehouse, Journal of Healthcare Information Management, 15, pp. 155-164, (2001); Sivanandam S.N., Deepa S.N., Introduction to Genetic Algorithms, (2008); Sothe C., Dalponte M., CMd A., Schimalski M.B., Lima C.L., Liesenberg V., Miyoshi G.T., Tommaselli A.M.G., Tree Species Classification in a Highly Diverse Subtropical Forest Integrating UAV-Based Photogrammetric Point Cloud and Hyperspectral Data, Remote Sensing, 11, 11, (2019); Steinwart I., Christmann A., Support Vector Machines, (2008); Stumpf A., Kerle N., Object-oriented Mapping of Landslides Using Random Forests, Remote Sensing of Environment, 115, 10, pp. 2564-2577, (2011); Tan Y., Xia W., Xu B., Bai L., Multi-Feature Classification Approach for High Spatial Resolution Hyperspectral Images, Journal of the Indian Society of Remote Sensing, 46, 1, pp. 9-17, (2018); Tinchev G., Penate-Sanchez A., Fallon M., Learning to See the Wood for the Trees: Deep Laser Localization in Urban and Natural Environments on a CPU, IEEE Robotics and Automation Letters, 4, 2, pp. 1327-1334, (2019); Torres V.A.M.F., Jaimes B.R.A., Ribeiro E.S., Braga M.T., Shiguemori E.H., Velho H.F.C., Torres L.C.B., Et al., Combined Weightless Neural Network FPGA Architecture for Deforestation Surveillance and Visual Navigation of UAVs, Engineering Applications of Artificial Intelligence, 87, (2020); Tuominen S., Nasi R., Honkavaara E., Balazs A., Hakala T., Viljanen N., Polonen I., Saari H., Ojanen H., Assessment of Classifiers and Remote Sensing Features of Hyperspectral Imagery and Stereo-photogrammetric Point Clouds for Recognition of Tree Species in a Forest Area of High Species Diversity, Remote Sensing, 10, 5, (2018); Earth Observations for Official Statistics: Satellite Imagery and Geospatial Data Task Team Report, (2017); Vapnik V., The Nature of Statistical Learning Theory, (1999); Virnodkar S.S., Pachghare V.K., Patil V.C., Jha S.K., Application of Machine Learning on Remote Sensing Data for Sugarcane Crop Classification: A Review, ICT Analysis and Applications. Lecture Notes in Networks and Systems, (2020); Wang H., Han D., Mu Y., Jiang L., Yao X., Bai Y., Lu Q., Wang F., Landscape-level Vegetation Classification and Fractional Woody and Herbaceous Vegetation Cover Estimation over the Dryland Ecosystems by Unmanned Aerial Vehicle Platform, Agricultural and Forest Meteorology, 278, (2019); Wikle C.K., A Kernel-based Spectral Model for non-Gaussian Spatio-temporal Processes, Statistical Modelling, 2, 4, pp. 299-314, (2002); Windrim L., Bryson M., McLean M., Randle J., Stone C., Automated Mapping of Woody Debris over Harvested Forest Plantations Using UAVS, High-resolution Imagery, and Machine Learning, Remote Sensing, 11, 6, (2019); Xu M., Watanachaturaporn P., Varshney P., Arora M., Decision Tree Regression for Soft Classification of Remote Sensing Data, Remote Sensing of Environment, 97, 3, pp. 322-336, (2005); Zhou X., Sun Z., Liu S., Yu P., Wang X., Wang Y., A Method for Extracting the Leaf Litter Distribution Area in Forest Using Chip Feature, International Journal Remote Sensing, 39, pp. 5310-5329, (2018); Zou X., Liang A., Wu B., Su J., Zheng R., Li J., UAV-based High-throughput Approach for Fast Growing Cunninghamia Lanceolata (Lamb.) Cultivar Screening by Machine Learning, Forests, 10, 9, (2019)","S.D. Pereira da Silva; Postgraduate Program in Forestry Engineering, Department of Forestry Engineering, Federal University of Santa Maria/UFSM - University City, Camobi, Santa Maria, Brazil; email: sallydeborah@outlook.com","","Taylor and Francis Ltd.","","","","","","01431161","","IJSED","","English","Int. J. Remote Sens.","Review","Final","","Scopus","2-s2.0-85117238548"
"Alburshaid E.A.; Mangoud M.A.","Alburshaid, E.A. (57306196600); Mangoud, M.A. (6602536151)","57306196600; 6602536151","Developing Date Palm Tree Inventory from Satellite Remote Sensed Imagery using Deep Learning","2021","2021 3rd IEEE Middle East and North Africa COMMunications Conference, MENACOMM 2021","","","","54","59","5","3","10.1109/MENACOMM50742.2021.9678262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125405134&doi=10.1109%2fMENACOMM50742.2021.9678262&partnerID=40&md5=1c609379fe4374474709d314974e226b","National Space Science Agency; University of Bahrain, Bahrain","Alburshaid E.A., National Space Science Agency; Mangoud M.A., University of Bahrain, Bahrain","A deep learning technique was used to recognize, count, and categorize date palm trees in the Kingdom of Bahrain. The MRCNN model of convolutional neural networks has been presented and assessed. The test area was comprised of the entire land region of Bahrain's Northern Governorate, which encompassed around 175 square kilometers. The raw material for extracting palm tree distributions and specifying the exact inventory areas that can contribute to Bahrain's overall food security has been very high-resolution satellite images. This paper has demonstrated the value of incorporating deep learning techniques into the GIS environment to aid in the successful generation of agricultural data from satellite imagery.  © 2021 IEEE.","CNN; Deep learning; GIS; high-resolution satellite imagery; Mask R-CNN; object detection; Palm trees; Remote Sensing; segmentation","Convolutional neural networks; Deep learning; Food supply; Forestry; Geographic information systems; Learning algorithms; Object detection; Object recognition; Palmprint recognition; Satellite imagery; Bahrain; CNN; Date palm; Deep learning; High resolution satellite imagery; Learning techniques; PaLM-tree; Remote sensed imagery; Remote-sensing; Segmentation; Remote sensing","","","","","College of Engineering, University of Bahrain; National Space Science Agency; University of Balamand, UOB","The authors gratefully acknowledge the National Space Science Agency (NSSA) and the College of Engineering, University of Bahrain (UOB), for their valuable resources that do support this paper.","Ibrahim K.M., The role of date palm tree in improvement of the environment, Acta Hortic., 882, 882, pp. 777-778, (2010); Almansoori T.A., Al-Khalifa M.A., Mohamed A.M.A., Date palm status and perspective in Bahrain, Date Palm Genetic Resources and Utilization: Volume 2: Asia and Europe, 2, pp. 353-386, (2015); Al-Ruzouq R., Shanableh A., Gibril M.B.A., Al-Mansoori S., Image segmentation parameter selection and ant colony optimization for date palm tree detection and mapping from very-high-spatial-resolution aerial imagery, Remote Sens., 10, 9, (2018); Chong K.L., Kanniah K.D., Pohl C., Tan K.P., A review of remote sensing applications for oil palm studies, Geo-Spatial Inf. Sci., 20, 2, pp. 184-200, (2017); Mubin N.A., Nadarajoo E., Shafri H.Z.M., Hamedianfar A., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method, Int. J. Remote Sens., 40, 19, pp. 7500-7515, (2019); Alburshaid E.A., Mangoud M.A., Benefit and Cost Analysis for Utilizing Machine Learning in Geospatial Projects, (2021); Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28, (2016); Alburshaid E.A., Mangoud M.A., Palm trees detection using the integration between GIS and deep learning, 2021 International Symposium on Networks, Computers and Communications (ISNCC), pp. 1-6, (2021); Campbell J.B., Wynne R.H., Introduction to Remote Sensing, (2011); Zhang L., Xia G.-S., Wu T., Lin L., Tai X.C., Deep learning for remote sensing image understanding, J. Sensors, 2016, pp. 1-2, (2016); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning, Sensors (Switzerland), 19, 6, pp. 1-23, (2019); Belgiu M., Dragu L., Random forest in remote sensing: A review of applications and future directions, ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31, (2016); Dhingra S., Kumar D., A review of remotely sensed satellite image classification, Int. J. Electr. Comput. Eng., 9, 3, pp. 1720-1731, (2019); Lu D., Weng Q., A survey of image classification methods and techniques for improving classification performance, Int. J. Remote Sens., 28, 5, pp. 823-870, (2007); Zhong L., Hu L., Zhou H., Deep learning based multi-temporal crop classification, Remote Sens. Environ., 221, pp. 430-443, (2019); Komati P.R., Komati R.D., Review on Deep Learning in Remote Sensing Image Classification, 9, VI, pp. 2228-2232, (2020); Hoeser T., Kuenzer C., Object detection and image segmentation with deep learning on earth observation data: A review-part I: Evolution and recent trends, Remote Sens., 12, 10, (2020); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric., 147, pp. 70-90, (2018); Gu Y., Wang Y., Li Y., A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection, Appl. Sci., 9, 10, (2019); Culman M.M.M.M., Delalieux S., Van-Tricht K., Van Tricht K., Palm tree inventory from aerial images using retinanet, 2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS), pp. 314-317, (2020); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens., 9, 1, (2017); Lu X., Li Q., Li B., Yan J., MimicDet: Bridging the Gap between One-Stage and Two-Stage Object Detection, (2020); Temitope Yekeen S., Balogun A.L., Wan Yusof K.B., A novel deep learning instance segmentation model for automated marine oil spill detection, ISPRS J. Photogramm. Remote Sens., 167, pp. 190-200, (2020); Czum J.M., Dive into deep learning, J. Am. Coll. Radiol., 17, 5, pp. 637-638, (2020); Alburshaid E.A., Mangoud M.A., Spatio-temporal land use analysis and urban growth dynamic prediction in the kingdom of Bahrain, 3rd Smart Cities Symposium (SCS 2020), pp. 633-639, (2021)","","","Institute of Electrical and Electronics Engineers Inc.","","3rd IEEE Middle East and North Africa COMMunications Conference, MENACOMM 2021","3 December 2021 through 5 December 2021","Virtual, Online","176525","","978-166543443-0","","","English","IEEE Middle East North Africa COMMun. Conf., MENACOMM","Conference paper","Final","","Scopus","2-s2.0-85125405134"
"Ghosh S.; Das N.; Das I.; Maulik U.","Ghosh, Swarnendu (57209977299); Das, Nibaran (55431494200); Das, Ishita (57210956577); Maulik, Ujjwal (6603607810)","57209977299; 55431494200; 57210956577; 6603607810","Understanding deep learning techniques for image segmentation","2019","ACM Computing Surveys","52","4","73","","","","197","10.1145/3329784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072022224&doi=10.1145%2f3329784&partnerID=40&md5=4df8566bf9674839cf9cd1ab989c4682","Jadavpur University, Kolkata, 700032, WB, India","Ghosh S., Jadavpur University, Kolkata, 700032, WB, India; Das N., Jadavpur University, Kolkata, 700032, WB, India; Das I., Jadavpur University, Kolkata, 700032, WB, India; Maulik U., Jadavpur University, Kolkata, 700032, WB, India","The machine learning community has been overwhelmed by a plethora of deep learning-based approaches. Many challenging computer vision tasks, such as detection, localization, recognition, and segmentation of objects in an unconstrained environment, are being efficiently addressed by various types of deep neural networks, such as convolutional neural networks, recurrent networks, adversarial networks, and autoencoders. Although there have been plenty of analytical studies regarding the object detection or recognition domain, many new deep learning techniques have surfaced with respect to image segmentation techniques. This article approaches these various deep learning techniques of image segmentation from an analytical perspective. The main goal of this work is to provide an intuitive understanding of the major techniques that have made a significant contribution to the image segmentation domain. Starting from some of the traditional image segmentation approaches, the article progresses by describing the effect that deep learning has had on the image segmentation domain. Thereafter, most of the major segmentation algorithms have been logically categorized with paragraphs dedicated to their unique contribution. With an ample amount of intuitive explanations, the reader is expected to have an improved ability to visualize the internal dynamics of these processes. © 2019 Association for Computing Machinery.","Convolutional neural networks; Deep learning; Semantic image segmentation","Convolution; Deep learning; Deep neural networks; Learning algorithms; Object detection; Recurrent neural networks; Semantics; Convolutional neural network; Intuitive understanding; Learning-based approach; Machine learning communities; Segmentation algorithms; Segmentation techniques; Semantic image segmentations; Unconstrained environments; Image segmentation","","","","","","","Achanta R., Shaji A., Smith K., Lucchi A., Fua P., Susstrunk S., Et al., SLIC superpixels compared to state-of-The-art superpixel methods, IEEE Transactions on Pattern Analysis and Machine Intelligence, 34, 11, pp. 2274-2282, (2012); Agarwala A., Hertzmann A., Salesin D.H., Seitz S.M., Keyframe-based Tracking for Rotoscoping and Animation, 23, pp. 584-591, (2004); Ahmad J., Mehmood I., Baik S.W., Efficient object-based surveillance image search using spatial pooling of convolutional features, Journal of Visual Communication and Image Representation, 45, pp. 62-76, (2017); Alam F.I., Zhou J., Wee-Chung Liew A., Jia X., CRF learning with CNN features for hyperspectral image segmentation, Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS'16), pp. 6890-6893, (2016); Albiol A., Torres L., Delp E.J., An unsupervised color image segmentation algorithm for face detection applications, Proceedings of the 2001 International Conference on Image Processing, 2, pp. 681-684, (2001); Araujo T., Aresta G., Castro E., Rouco J., Aguiar P., Eloy C., Polonia A., Campilho A., Classification of breast cancer histology images using convolutional neural networks, PloS One, 12, 6, (2017); Ather A., A Quality Analysis of Open Street Map Data, (2009); Badrinarayanan V., Kendall A., Cipolla R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 12, pp. 2481-2495, (2017); Barlow J., Franklin S., Martin Y., High spatial resolution satellite imagery, DEM derivatives, and image segmentation for the detection of mass wasting processes, Photogrammetric Engineering and Remote Sensing, 72, 6, pp. 687-692, (2006); Belongie S., Carson C., Greenspan H., Malik J., Color-and texture-based image segmentation using em and its application to content-based image retrieval, Proceedings of the 6th International Conference on Computer Vision, pp. 675-682, (1998); Bengio Y., Lamblin P., Popovici D., Larochelle H., Greedy layer-wise training of deep networks, Advances in Neural Information Processing Systems, pp. 153-160, (2007); Sant'Anna Bins L., Garcia Fonseca L.M., Erthal G.J., Mitsuo F., Satellite imagery segmentation: A region growing approach, Simpósio Brasileiro de Sensoriamento Remoto, 8, 1996, pp. 677-680, (1996); Borji A., What is a salient object? A dataset and a baseline model for salient object detection, IEEE Transactions on Image Processing, 24, 2, pp. 742-756, (2015); Borji A., Cheng M.-M., Hou Q., Jiang H., Li J., Salient Object Detection: A Survey, (2014); Borji A., Cheng M.-M., Jiang H., Li J., Salient object detection: A benchmark, IEEE Transactions on Image Processing, 24, 12, pp. 5706-5722, (2015); Brostow G.J., Fauqueur J., Cipolla R., Semantic object classes in video: A high-definition ground truth database, Pattern Recognition Letters, 30, 2, pp. 88-97, (2009); Cahill N.D., Ray L.A., Method and System for Compositing Images to Produce A Cropped Image, (2007); Carass A., Roy S., Jog A., Cuzzocreo J.L., Magrath E., Gherman A., Et al., Longitudinal multiple sclerosis lesion segmentation data resource, Data in Brief, 12, pp. 346-350, (2017); Castrejon L., Kundu K., Urtasun R., Fidler S., Annotating object instanceswith a polygonrnn, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5230-5238, (2017); Chang P.-L., Teng W.-G., Exploiting the self-organizing map for medical image segmentation, Proceedings of the 20th IEEE International Symposium on Computer-Based Medical Systems (CBMS'07), pp. 281-288, (2007); Chen J., Yang L., Zhang Y., Alber M., Chen D.Z., Combining fully convolutional and recurrent neural networks for 3D biomedical image segmentation, Advances in Neural Information Processing Systems, pp. 3036-3044, (2016); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs, (2014); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, IEEE Transactions on Pattern Analysis and Machine Intelligence, 40, 4, pp. 834-848, (2018); Zheng S., Jayasumana S., Romera-Paredes B., Vineet V., Su Z., Du D., Huang C., Torr P.H.S., Conditional random fields as recurrent neural networks, Proceedings of the IEEE International Conference on Computer Vision, pp. 1529-1537, (2015); Chen L.-C., Papandreou G., Schroff F., Adam H., Rethinking Atrous Convolution for Semantic Image Segmentation, (2017); Chen L.-C., Yang Y., Wang J., Xu W., Yuille A.L., Attention to scale: Scale-aware semantic image segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3640-3649, (2016); Chen L.-C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-decoder with Atrous Separable Convolution for Semantic Image Segmentation, (2018); Cheng K.-S., Lin J.-S., Mao C.-W., The application of competitive hopfield neural network to medical image segmentation, IEEE Transactions on Medical Imaging, 15, 4, pp. 560-567, (1996); Cheng M.-M., Mitra N.J., Huang X., Hu S.-M., Salient shape: Group saliency in image collections, Visual Computer, 30, 4, pp. 443-453, (2014); Cheng M., Mitra N.J., Huang X., Torr P.H.S., Hu S.-M., Global contrast based salient region detection, IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, 3, pp. 569-582, (2015); Chuang K.-S., Tzeng H.-L., Chen S., Wu J., Chen T.-J., Fuzzy c-means clustering with spatial information for image segmentation, Computerized Medical Imaging and Graphics, 30, 1, pp. 9-15, (2006); Comaniciu D., Meer P., Robust analysis of feature spaces: Color image segmentation, Proceedings of the 1997 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 750-755, (1997); Cordts M., Omran M., Ramos S., Rehfeld T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The cityscapes dataset for semantic urban scene understanding, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223, (2016); Dai J., He K., Sun J., BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation, Proceedings of the IEEE International Conference on Computer Vision, pp. 1635-1643, (2015); Dai J., He K., Sun J., Instance-aware semantic segmentation via multi-task network cascades, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3150-3158, (2016); Dai J., Li Y., He K., Sun J., R-FCN: Object detection via region-based fully convolutional networks, Advances in Neural Information Processing Systems, pp. 379-387, (2016); Das A., Ghosh S., Sarkhel R., Choudhuri S., Das N., Nasipuri M., Combining multilevel contexts of superpixel using convolutional neural networks to perform natural scene labeling, Recent Developments in Machine Learning and Data Analytics, pp. 297-306, (2019); Portes De-Albuquerque M., Esquef I.A., Gesualdi Mello A.R., Image thresholding using tsallis entropy, Pattern Recognition Letters, 25, 9, pp. 1059-1065, (2004); De Bruijne M., Van Ginneken B., Viergever M.A., Niessen W.J., Interactive segmentation of abdominal aortic aneurysms in CTA images, Medical Image Analysis, 8, 2, pp. 127-138, (2004); Demir I., Koperski K., Lindenbaum D., Pang G., Huang J., Basu S., Hughes F., Tuia D., Raskar R., Deep Globe 2018:A Challenge to Parse the Earth through Satellite Images, (2018); Du Y., Arslanturk E., Zhou Z., Belcher C., Video-based noncooperative iris image segmentation, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 41, 1, pp. 64-74, (2011); Duan L., Tsang I.W., Xu D., Chua T.-S., Domain adaptation from multiple sources via auxiliary classifiers, Proceedings of the 26th Annual International Conference on Machine Learning, pp. 289-296, (2009); Dumoulin V., Visin F., A Guide to Convolution Arithmetic for Deep Learning, (2016); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The pascal visual object classes (VOC) challenge, International Journal of Computer Vision, 88, 2, pp. 303-338, (2010); Farabet C., Couprie C., Najman L., LeCun Y., Learning hierarchical features for scene labeling, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 8, pp. 1915-1929, (2013); ISPRS 2D Semantic Labeling Contest; Fraz M.M., Remagnino P., Hoppe A., Uyyanonvara B., Rudnicka A.R., Owen C.G., Barman S.A., Blood vessel segmentation methodologies in retinal images-A survey, Computer Methods and Programs in Biomedicine, 108, 1, pp. 407-433, (2012); Freixenet J., Munoz X., Raba D., Marti J., Xavier Cufi C., Yet another survey on image segmentation: Region and boundary information integration, Proceedings of the European Conference on Computer Vision, pp. 408-422, (2002); Friedman N., Russell S., Image segmentation in video sequences: A probabilistic approach, Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence, pp. 175-181, (1997); Fu K.-S., Mui J.K., A survey on image segmentation, Pattern Recognition, 13, 1, pp. 3-16, (1981); Galasso F., Nagaraja N.S., Cardenas T.J., Brox T., Schiele B., A unified video segmentation benchmark: Annotation, metrics and analysis, Proceedings of the IEEE International Conference on Computer Vision, pp. 3527-3534, (2013); Gangwar A., Joshi A., DeepIrisNet: Deep iris representation with applications in iris recognition and cross-sensor iris recognition, Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP'16), pp. 2301-2305, (2016); Garcia-Garcia A., Orts-Escolano S., Oprea S., Villena-Martinez V., Garcia-Rodriguez J., A Review on Deep Learning Techniques Applied to Semantic Segmentation, (2017); Geiger A., Lenz P., Urtasun R., Are we ready for autonomous driving? The kitti vision benchmark suite, Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'12), pp. 3354-3361, (2012); Geng Q., Zhou Z., Cao X., Survey of recent progress in semantic image segmentation with CNNs, Science China Information Sciences, 61, 5, (2018); Girshick R., Fast R-CNN, (2015); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Gould S., Fulton R., Koller D., Decomposing a scene into geometric and semantically consistent regions, Proceedings of the 2009 IEEE 12th International Conference on Computer Vision, pp. 1-8, (2009); Han X., Automatic Liver Lesion Segmentation Using A Deep Convolutional Neural Network Method, (2017); Hariharan B., Arbelaez P., Bourdev L., Maji S., Malik J., Semantic contours from inverse detectors, Proceedings of the International Conference on Computer Vision (ICCV'11), (2011); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV'17), pp. 2980-2988, (2017); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, 9, pp. 1904-1916, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Hong S., You T., Kwak S., Han B., Online tracking by learning discriminative saliency map with convolutional neural network, Proceedings of the International Conference on Machine Learning, pp. 597-606, (2015); Hu Y., Soltoggio A., Lock R., Carter S., A fully convolutional two-stream fusion network for interactive image segmentation, Neural Networks, 109, pp. 31-42, (2019); Ioffe S., Szegedy C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, (2015); Irshad H., Veillard A., Roux L., Racoceanu D., Methods for nuclei detection, segmentation, and classification in digital histopathology: A review-current status and future potential, IEEE Reviews in Biomedical Engineering, 7, pp. 97-114, (2014); Isola P., Zhu J.-Y., Zhou T., Efros A.A., Image-to-image Translation with Conditional Adversarial Networks, (2017); Jassim F.A., Altaani F.H., Hybridization of Otsu Method and Median Filter for Color Image Segmentation, (2013); Jegou S., Drozdzal M., Vazquez D., Romero A., Bengio Y., The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW'17), pp. 1175-1183, (2017); Jin C.-B., Li S., Do T.D., Kim H., Real-time human action recognition using CNN over temporal images for static video surveillance cameras, Proceedings of the Pacific Rim Conference on Multimedia, pp. 330-339, (2015); Kam A.H., Ng T.T., Kingsbury N.G., Fitzgerald W.J., Content based image retrieval through object extraction and querying, Proceedings of the Workshop on Content-Based Access of Image and Visual Libraries (CBAIVL'00), (2000); Kamnitsas K., Ledig C., Newcombe V.F.J., Simpson J.P., Kane A.D., Menon D.K., Rueckert D., Glocker B., Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation, Medical Image Analysis, 36, pp. 61-78, (2017); Kanezaki A., Unsupervised image segmentation by backpropagation, Proceedings of the 2018 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP'18), pp. 1543-1547, (2018); Kang J., Li X., Luan Q., Liu J., Min L., Dental plaque quantification using cellular neural network-based image segmentation, Intelligent Computing in Signal Processing and Pattern Recognition, pp. 797-802, (2006); Kang J., Zhang W., Fingerprint segmentation using cellular neural network, Proceedings of the International Conference on Computational Intelligence and Natural Computing (CINC'09), 2, pp. 11-14, (2009); Kang K., Wang X., Fully Convolutional Neural Networks for Crowd Segmentation, (2014); Kingma D.P., Ba J., Adam: A Method for Stochastic Optimization, (2014); Kong T., Yao A., Chen Y., Sun F., HyperNet: Towards accurate region proposal generation and joint object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 845-853, (2016); Krahenbuhl P., Koltun V., Efficient inference in fully connected CRFs with Gaussian edge potentials, Advances in Neural Information Processing Systems, pp. 109-117, (2011); Zhou B., Zhao H., Puig X., Fidler S., Barriuso A., Torralba A., Semantic Understanding of Scenes through the ADE20K Dataset, (2016); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Labao A.B., Naval P.C., Weakly-labelled semantic segmentation of fish objects in underwater videos using a deep residual network, Proceedings of the Asian Conference on Intelligent Information and Database Systems, pp. 255-265, (2017); Ladys Law-Skarbek W., Koschan A., Colour image segmentation a survey, IEEE Transactions on Circuits and Systems for Video Technology, 14, 7, (1994); LaLonde R., Bagci U., Capsules for Object Segmentation, (2018); Langkvist M., Kiselev A., Alirezaie M., Loutfi A., Classification and segmentation of satellite orthoimagery using convolutional neural networks, Remote Sensing, 8, 4, (2016); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proceedings of the IEEE, 86, 11, pp. 2278-2324, (1998); Lee S.-H., Cho M.S., Jung K., Kim J.H., Scene text extraction with edge constraint and text collinearity, Proceedings of the 2010 International Conference on Pattern Recognition, pp. 3983-3986, (2010); Leibe B., Seemann E., Schiele B., Pedestrian detection in crowded scenes, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), 1, pp. 878-885, (2005); Levi D., Garnett N., Fetaya E., Herzlyia I., StixelNet: A deep convolutional network for obstacle detection and road segmentation, Proceedings of the British Machine Vision Association (BMVC'15), (2015); Levin A., Lischinski D., Weiss Y., A closed-form solution to natural image matting, IEEE Transactions on Pattern Analysis and Machine Intelligence, 30, 2, pp. 228-242, (2008); Li X., Liu Z., Luo P., Loy C.C., Tang X., Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3193-3202, (2017); Li Y., Hou X., Koch C., Rehg J.M., Yuille A.L., The secrets of salient object segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 280-287, (2014); Li Y., Qi H., Dai J., Ji X., Wei Y., Fully Convolutional Instance-aware Semantic Segmentation, (2016); Lie W.-N., Automatic target segmentation by locally adaptive image thresholding, IEEE Transactions on Image Processing, 4, 7, pp. 1036-1041, (1995); Lin G., Milan A., Shen C., Reid I., RefineNet: Multi-path refinement networks for highresolution semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17), (2017); Lin M., Chen Q., Yan S., Network in Network, (2013); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Lawrence Zitnick C., Microsoft COCO: Common objects in context, Proceedings of the European Conference on Computer Vision, pp. 740-755, (2014); Litjens G., Kooi T., Bejnordi B.E., Adiyoso Setio A.A., Ciompi F., Ghafoorian M., Van Der-Laak J.A.W.M., Van Ginneken B., Sanchez C.I., A survey on deep learning in medical image analysis, Medical Image Analysis, 42, pp. 60-88, (2017); Liu N., Li H., Zhang M., Liu J., Sun Z., Tan T., Accurate iris segmentation in noncooperative environments using fully convolutional networks, Proceedings of the 2016 International Conference on Biometrics (ICB'16), pp. 1-8, (2016); Liu S., De Mello S., Gu J., Zhong G., Yang M.-H., Kautz J., Learning affinity via spatial propagation networks, Advances in Neural Information Processing Systems, pp. 1520-1530, (2017); Liu Y., Zhang D., Lu G., Ma W.-Y., A survey of content-based image retrieval with high-level semantics, Pattern Recognition, 40, 1, pp. 262-282, (2007); Liu Z., Li X., Luo P., Loy C.-C., Tang X., Semantic image segmentation via deep parsing network, Proceedings of the IEEE International Conference on Computer Vision, pp. 1377-1385, (2015); Loizou C.P., Murray V., Pattichis M.S., Seimenis I., Pantziaris M., Pattichis C.S., Multiscale amplitude-modulation frequency-modulation (AM-FM) texture analysis ofmultiple sclerosis in brain MRI images, IEEE Transactions on Information Technology in Biomedicine, 15, 1, pp. 119-129, (2011); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, (2015); Lopez-Linares K., Lete N., Kabongo L., Ceresa M., Maclair G., Garcia-Familiar A., Macia I., Ballester M.A.G., Comparison of regularization techniques for DCNN-based abdominal aortic aneurysm segmentation, Proceedings of the 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI'18), pp. 864-867, (2018); Lu P., Barazzetti L., Chandran V., Gavaghan K., Weber S., Gerber N., Reyes M., Highly accurate facial nerve segmentation refinement from CBCT/CT imaging using a super-resolution classification approach, IEEE Transactions on Biomedical Engineering, 65, 1, pp. 178-188, (2018); Luc P., Couprie C., Chintala S., Verbeek J., Semantic Segmentation Using Adversarial Networks, (2016); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can semantic labeling methods generalize to any city? The INRIA aerial image labeling benchmark, Proceedings of the IEEE International Symposium on Geoscience and Remote Sensing (IGARSS'17), (2017); Maier O., Menze B.H., Von Der-Gablentz J., Hani L., Heinrich M.P., Liebrand M., Et al., ISLES 2015-A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI, Medical Image Analysis, 35, pp. 250-269, (2017); Mandal R., Choudhury N., Automatic video surveillance for theft detection in ATM machines: An enhanced approach, Proceedings of the 2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom'16), pp. 2821-2826, (2016); Maninis K.-K., Caelles S., Pont-Tuset J., Van Gool L., Deep extreme cut: From extreme points to object segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 616-625, (2018); Maninis K.-K., Pont-Tuset J., Arbelaez P., Van Gool L., Convolutional oriented boundaries, Proceedings of the European Conference on Computer Vision, pp. 580-596, (2016); Martin D., Fowlkes C., Tal D., Malik J., A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics, Proceedings of the 8th International Conference on Computer Vision, 2, pp. 416-423, (2001); Masci J., Meier U., Ciresan D., Schmidhuber J., Stacked convolutional auto-encoders for hierarchical feature extraction, Proceedings of the International Conference on Artificial Neural Networks, pp. 52-59, (2011); Medsker L.R., Jain L.C., Recurrent Neural Networks: Design and Applications, (2001); Mehtre B.M., Murthy N.N., Kapoor S., Chatterjee B., Segmentation of fingerprint images using the directional image, Pattern Recognition, 20, 4, pp. 429-435, (1987); Menze B.H., Jakab A., Bauer S., Kalpathy-Cramer J., Farahani K., Kirby J., Et al., The multimodal brain tumor image segmentation benchmark (BRATS), IEEE Transactions on Medical Imaging, 34, 10, pp. 1993-2024, (2015); Zhou B., Zhao H., Puig X., Fidler S., Barriuso A., Torralba A., Scene parsing through ADE20K dataset, Proceedings of the Conference on Computer Vision and Pattern Recognition, (2017); Merlino A., Morey D., Maybury M., Broadcast news navigation using story segmentation, Proceedings of the 5th ACM International Conference on Multimedia, pp. 381-391, (1997); Molinari F., Zeng G., Suri J.S., A state of the art reviewon intima-media thickness (IMT)measurement and wall segmentation techniques for carotid ultrasound, Computer Methods and Programs in Biomedicine, 100, 3, pp. 201-221, (2010); Moriya T., Roth H.R., Nakamura S., Oda H., Nagara K., Oda M., Mori K., Unsupervised segmentation of 3D medical images based on clustering and deep representation learning, Medical Imaging 2018: Biomedical Applications in Molecular, Structural, and Functional Imaging, (2018); Nathan Mundhenk T., Ho D., Chen B.Y., Improvements to context based self-supervised learning, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR'18), (2018); Nair V., Hinton G.E., Rectified linear units improve restricted boltzmann machines, Proceedings of the 27th International Conference on Machine Learning (ICML'10), pp. 807-814, (2010); Nassar A., Amer K., Hakim R.E., Helw M.E., A deep CNN-based framework for enhanced aerial imagery registration with applications to UAV geolocalization, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1513-1523, (2018); Neuhold G., Ollmann T., Rota Bulo S., Kontschieder P., The mapillary vistas dataset for semantic understanding of street scenes, Proceedings of the International Conference on Computer Vision (ICCV'17), pp. 22-29, (2017); Noh H., Hong S., Han B., Learning deconvolution network for semantic segmentation, Proceedings of the IEEE International Conference on Computer Vision, pp. 1520-1528, (2015); Noroozi M., Favaro P., Unsupervised learning of visual representations by solving jigsaw puzzles, Proceedings of the European Conference on Computer Vision, pp. 69-84, (2016); Onyango C.M., Marchant J.A., Physics-based colour image segmentation for scenes containing vegetation and soil, Image and Vision Computing, 19, 8, pp. 523-538, (2001); Pal A., Jaiswal S., Ghosh S., Das N., Nasipuri M., SegFast: Afaster squeeze net based semantic image segmentation technique using depth-wise separable convolutions, Proceedings of the 11th Indian Conference on Computer Vision, Graphics, and Image Processing (ICVGIP'18); Pal N.R., Pal S.K., A review on image segmentation techniques, Pattern Recognition, 26, 9, pp. 1277-1294, (1993); Papandreou G., Chen L.-C., Murphy K.P., Yuille A.L., Weakly- And semi-supervised learning of a deep convolutional network for semantic image segmentation, Proceedings of the IEEE International Conference on Computer Vision, pp. 1742-1750, (2015); Paszke A., Chaurasia A., Kim S., Culurciello E., ENet: A Deep Neural Network Architecture for Real-time Semantic Segmentation, (2016); Pathak D., Krahenbuhl P., Donahue J., Darrell T., Efros A.A., Context encoders: Feature learning by inpainting, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2536-2544, (2016); Peng C., Zhang X., Yu G., Luo G., Sun J., Large Kernel Matters-Improve Semantic Segmentation by Global Convolutional Network, (2017); Pinheiro P.O., Collobert R., Dollar P., Learning to segment object candidates, Advances in Neural Information Processing Systems, pp. 1990-1998, (2015); Pinheiro P.O., Lin T.-Y., Collobert R., Dollar P., Learning to refine object segments, Proceedings of the European Conference on Computer Vision, pp. 75-91, (2016); Pont-Tuset J., Perazzi F., Caelles S., Arbelaez P., Sorkine-Hornung A., Van Gool L., The 2017 Davis Challenge on Video Object Segmentation, (2017); Porwal P., Pachade S., Kamble R., Kokare M., Deshmukh G., Sahasrabuddhe V., Et al., Diabetic retinopathy: Segmentation and grading challenge workshop, Proceedings of the IEEE International Symposium on Biomedical Imaging(ISBI'18), (2018); Qin H., El-Yacoubi M.A., Deep representation-based feature extraction and recovering for finger-vein verification, IEEE Transactions on Information Forensics and Security, 12, 8, pp. 1816-1829, (2017); Radau P., Lu Y., Connelly K., Paul G., Dick A., Wright G., Evaluation framework for algorithms segmenting short axis cardiac MRI, MIDAS Journal, 49, (2009); Ranjan A., Jampani V., Kim K., Sun D., Wulff J., Black M.J., Adversarial Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation, (2018); Ravanbakhsh M., Nabi M., Mousavi H., Sangineto E., Sebe N., Plug-and-play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection, (2016); Ren M., Zemel R.S., End-to-end Instance Segmentation with Recurrent Attention, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, pp. 91-99, (2015); Romera-Paredes B., Torr P.H.S., Recurrent instance segmentation, Proceedings of the European Conference on Computer Vision, pp. 312-329, (2016); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Ros G., Sellart L., Materzynska J., Vazquez D., Lopez A.M., The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3234-3243, (2016); Rothrock B., Kennedy R., Cunningham C., Papon J., Heverly M., Ono M., Spoc: Deep learning-based terrain classification for mars rover missions, Proceedings of the AIAA SPACE 2016 Conference, (2016); Rumelhart D.E., Hinton G.E., Williams R.J., Learning representations by back-propagating errors, Nature, 323, 6088, (1986); Sabour S., Frosst N., Hinton G.E., Dynamic routing between capsules, Advances in Neural Information Processing Systems, pp. 3856-3866, (2017); Senthilkumaran N., Rajesh R., Edge detection techniques for image segmentation-A survey of soft computing approaches, International Journal of Recent Trends in Engineering, 1, 2, pp. 250-254, (2009); Sharma N., Aggarwal L.M., Automated medical image segmentation techniques, Journal of Medical Physics/Association of Medical Physicists of India, 35, 1, (2010); Shi J., Malik J., Normalized cuts and image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 22, 8, pp. 888-905, (2000); Shi J., Yan Q., Xu L., Jia J., Hierarchical image saliency detection on extended CSSD, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 4, pp. 717-729, (2016); Shotton J., Winn J., Rother C., Criminisi A., Texton boost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation, Proceedings of the European Conference on Computer Vision, pp. 1-15, (2006); Silveira M., Nascimento J.C., Marques J.S., Marcal A.R.S., Mendonca T., Yamauchi S., Maeda J., Rozeira J., Comparison of segmentation methods for melanoma diagnosis in dermoscopy images, IEEE Journal of Selected Topics in Signal Processing, 3, 1, pp. 35-45, (2009); Song Y., Zhu Y., Li G., Feng C., He B., Yan T., Side scan sonar segmentation using deep convolutional neural network, Proceedings of the 2017 OCEANS-Anchorage Conference, pp. 1-4, (2017); Staal J., Abramoff M.D., Niemeijer M., Viergever M.A., Van Ginneken B., Ridge-based vessel segmentation in color images of the retina, IEEE Transactions on Medical Imaging, 23, 4, pp. 501-509, (2004); Sziranyi T., Laszlo K., Czuni L., Ziliani F., Object oriented motion-segmentation for video-compression in the CNN-UM, Journal of VLSI Signal Processing Systems for Signal, Image and Video Technology, 23, 2-3, pp. 479-496, (1999); Tan K.S., Isa N.A.M., Color image segmentation using histogram thresholding-fuzzy c-means hybrid approach, Pattern Recognition, 44, 1, pp. 1-15, (2011); Tobias O.J., Seara R., Image segmentation by histogram thresholding using fuzzy sets, IEEE Transactions on Image Processing, 11, 12, pp. 1457-1465, (2002); Treml M., Arjona-Medina J., Unterthiner T., Durgesh R., Friedmann F., Schuberth P., Et al., Speeding up semantic segmentation for autonomous driving, Proceedings of the MLITS NIPS Workshop, (2016); Tu W.-C., Liu M.-Y., Jampani V., Sun D., Chien S.-Y., Yang M.-H., Kautz J., Learning superpixels with segmentation-aware affinity loss, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 568-576, (2018); Uijlings J.R.R., Van De-Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, International Journal of Computer Vision, 104, 2, pp. 154-171, (2013); Van De-Sande K.E.A., Uijlings J.R.R., Gevers T., Smeulders A.W.M., Segmentation as selective search for object recognition, Proceedings of the 2011 IEEE International Conference on Computer Vision (ICCV'11), pp. 1879-1886, (2011); Ginneken B.V., Stegmann M.B., Loog M., Segmentation of anatomical structures in chest radiographs using supervised methods: A comparative study on a public database, Medical Image Analysis, 10, 1, pp. 19-40, (2006); Varma G., Subramanian A., Namboodiri A., Chandraker M., Jawahar C.V., IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments, (2018); Veit A., Matera T., Neumann L., Matas J., Belongie S., Coco-text: Dataset and Benchmark for Text Detection and Recognition in Natural Images, (2016); Vilarino D.L., Cabello D., Brea V.M., An analogic CNN-algorithm of pixel level snakes for tracking and surveillance tasks, Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications (CNNA'02), pp. 84-91, (2002); Wang K., Babenko B., Belongie S., End-to-end scene text recognition, Proceedings of the 2011 IEEE International Conference on Computer Vision (ICCV'11), pp. 1457-1464, (2011); Wei G.-Q., Arbter K., Hirzinger G., Real-time visual servoing for laparoscopic surgery. Controlling robot motion with color image segmentation, IEEE Engineering in Medicine and Biology Magazine, 16, 1, pp. 40-45, (1997); Xia X., Kulis B., W-Net: A Deep Model for Fully Unsupervised Image Segmentation, (2017); Xu J., Wang G., Sun F., A novel method for detecting and tracking vehicles in traffic-image sequence, Proceedings of the 5th International Conference on Digital Image Processing (ICDIP'13), 8878, (2013); Xu N., Yang L., Fan Y., Yue D., Liang Y., Yang J., Huang T., You Tube-VOS: A Large-scale Video Object Segmentation Benchmark, (2018)","","","Association for Computing Machinery","","","","","","03600300","","ACSUE","","English","ACM Comput Surv","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85072022224"
"Khan A.; Asim W.; Ulhaq A.; Ghazi B.; Robinson R.W.","Khan, Asim (57212307418); Asim, Warda (57224682667); Ulhaq, Anwaar (57191592708); Ghazi, Bilal (57224683370); Robinson, Randall W. (15023044500)","57212307418; 57224682667; 57191592708; 57224683370; 15023044500","Health assessment of eucalyptus trees using siamese network from google street and ground truth images","2021","Remote Sensing","13","11","2194","","","","6","10.3390/rs13112194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108141951&doi=10.3390%2frs13112194&partnerID=40&md5=891c1a70b19cc7ea3a9f04464554d9e2","The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, 8001, VIC, Australia; School of Computing and Mathematics, Charles Sturt University, Port Macquarie, 2444, NSW, Australia; Shaheed Zulfikar Ali Bhutto Institute of Science and Technology (SZABIST), Karachi Sindh, 75600, Pakistan; Applied Ecology Research Group, Victoria University, Melbourne, 8001, VIC, Australia","Khan A., The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, 8001, VIC, Australia; Asim W., The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, 8001, VIC, Australia; Ulhaq A., The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, 8001, VIC, Australia, School of Computing and Mathematics, Charles Sturt University, Port Macquarie, 2444, NSW, Australia; Ghazi B., Shaheed Zulfikar Ali Bhutto Institute of Science and Technology (SZABIST), Karachi Sindh, 75600, Pakistan; Robinson R.W., The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, 8001, VIC, Australia, Applied Ecology Research Group, Victoria University, Melbourne, 8001, VIC, Australia","Urban greenery is an essential characteristic of the urban ecosystem, which offers various advantages, such as improved air quality, human health facilities, storm-water run-off control, carbon reduction, and an increase in property values. Therefore, identification and continuous monitoring of the vegetation (trees) is of vital importance for our urban lifestyle. This paper proposes a deep learning-based network, Siamese convolutional neural network (SCNN), combined with a modified brute-force-based line-of-bearing (LOB) algorithm that evaluates the health of Eucalyptus trees as healthy or unhealthy and identifies their geolocation in real time from Google Street View (GSV) and ground truth images. Our dataset represents Eucalyptus trees’ various details from multiple viewpoints, scales and different shapes to texture. The experiments were carried out in the Wyndham city council area in the state of Victoria, Australia. Our approach obtained an average accuracy of 93.2% in identifying healthy and unhealthy trees after training on around 4500 images and testing on 500 images. This study helps in identifying the Eucalyptus tree with health issues or dead trees in an automated way that can facilitate urban green management and assist the local council to make decisions about plantation and improvements in looking after trees. Overall, this study shows that even in a complex background, most healthy and unhealthy Eucalyptus trees can be detected by our deep learning algorithm in real time. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","CNN; Deep learning; Eucalyptus trees; Geolocation; Google Street View (GSV); Image processing; Line-of-bearing (LOB); Siamese network; Target object detection; Vegetation health","Air quality; Convolutional neural networks; Deep learning; Forestry; Health; Quality control; Textures; Complex background; Continuous monitoring; Essential characteristic; Eucalyptus trees; Green managements; Health assessments; Multiple viewpoints; Victoria , Australia; Learning algorithms","","","","","","","Branson S., Wegner J.D., Hall D., Lang N., Schindler K., Perona P., From Google Maps to a fine-grained catalog of street trees, ISPRS J. Photogramm. Remote Sens, 135, pp. 13-30, (2018); Salmond J.A., Tadaki M., Vardoulakis S., Arbuthnott K., Coutts A., Demuzere M., Dirks K.N., Heaviside C., Lim S., Macintyre H., Et al., Health and climate related ecosystem services provided by street trees in the urban environment, Environ. Health, 15, pp. 95-111, (2016); Ladiges P., The Story of Our Eucalypts—Curious; Eucalypt Forest—Department of Agriculture; Berrang P., Karnosky D.F., Stanton B.J., Environmental factors affecting tree health in New York City, J. Arboric, 11, pp. 185-189, (1985); Cregg B.M., Dix M.E., Tree moisture stress and insect damage in urban areas in relation to heat island effects, J. Arboric, 27, pp. 8-17, (2001); Winn M.F., Lee S.M., Araman P.A., Urban tree crown health assessment system: A tool for communities and citizen foresters, Proceedings of the Proceedings, Emerging Issues Along Urban-Rural Interfaces II: Linking Land-Use Science and Society, pp. 180-183; Czerniawska-Kusza I., Kusza G., Duzynski M., Effect of deicing salts on urban soils and health status of roadside trees in the Opole region, Environ. Toxicol. Int. J, 19, pp. 296-301, (2004); Day S.D., Bassuk N.L., A review of the effects of soil compaction and amelioration treatments on landscape trees, J. Arboric, 20, pp. 9-17, (1994); Doody T., Overton I., Environmental management of riparian tree health in the Murray-Darling Basin, Australia, River Basin Manag. V, 124, (2009); Butt N., Pollock L.J., McAlpine C.A., Eucalypts face increasing climate stress, Ecol. Evol, 3, pp. 5011-5022, (2013); Chicco D., Siamese neural networks: An overview, Artificial Neural Networks, pp. 73-94, (2021); Google Street View Imagery—Google Search; Anguelov D., Dulong C., Filip D., Frueh C., Lafon S., Lyon R., Ogale A., Vincent L., Weaver J., Google street view: Capturing the world at street level, Computer, 43, pp. 32-38, (2010); Li X., Zhang C., Li W., Ricard R., Meng Q., Zhang W., Assessing street-level urban greenery using Google Street View and a modified green view index, Urban For. Urban Green, 14, pp. 675-685, (2015); Li X., Zhang C., Li W., Kuzovkina Y.A., Weiner D., Who lives in greener neighborhoods? The distribution of street greenery and its association with residents’ socioeconomic conditions in Hartford, Connecticut, USA, Urban For. Urban Green, 14, pp. 751-759, (2015); Li X., Zhang C., Li W., Building block level urban land-use information retrieval based on Google Street View images, GIScience Remote Sens, 54, (2017); Zhang W., Li W., Zhang C., Hanink D.M., Li X., Wang W., Parcel-based urban land use classification in megacity using airborne LiDAR, high resolution orthoimagery, and Google Street View, Comput. Environ. Urban Syst, 64, pp. 215-228, (2017); Li X., Ratti C., Seiferling I., Quantifying the shade provision of street trees in urban landscape: A case study in Boston, USA, using Google Street View, Landsc. Urban Plan, 169, pp. 81-91, (2018); Khan A., Nawaz U., Ulhaq A., Robinson R.W., Real-time plant health assessment via implementing cloud-based scalable transfer learning on AWS DeepLens, PLoS ONE, 15, (2020); Khan A., Ulhaq A., Robinson R., Ur Rehman M., Detection of Vegetation in Environmental Repeat Photography: A New Algorithmic Approach in Data Science; Statistics for Data Science and Policy Analysis, pp. 145-157, (2020); Tumas P., Nowosielski A., Serackis A., Pedestrian detection in severe weather conditions, IEEE Access, 8, pp. 62775-62784, (2020); Guo G., Wang H., Yan Y., Zheng J., Li B., A fast face detection method via convolutional neural network, Neurocomputing, 395, pp. 128-137, (2020); Wu J., Song L., Wang T., Zhang Q., Yuan J., Forest r-cnn: Large-vocabulary long-tailed object detection and instance segmentation, Proceedings of the 28th ACM International Conference on Multimedia, pp. 1570-1578; Islam M.M., Yang H.C., Poly T.N., Jian W.S., Li Y.C.J., Deep learning algorithms for detection of diabetic retinopathy in retinal fundus photographs: A systematic review and meta-analysis, Comput. Methods Programs Biomed, 191, (2020); Zeng D., Yu F., Research on the Application of Big Data Automatic Search and Data Mining Based on Remote Sensing Technology, Proceedings of the 2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD), pp. 122-127; Voulodimos A., Doulamis N., Doulamis A., Protopapadakis E., Deep learning for computer vision: A brief review, Comput. Intell. Neurosci, 2018, (2018); Miltiadou M., Campbell N.D., Aracil S.G., Brown T., Grant M.G., Detection of dead standing Eucalyptus camaldulensis without tree delineation for managing biodiversity in native Australian forest, Int. J. Appl. Earth Obs. Geoinf, 67, pp. 135-147, (2018); Shendryk I., Broich M., Tulbure M.G., Alexandrov S.V., Bottom-up delineation of individual trees from full-waveform airborne laser scans in a structurally complex eucalypt forest, Remote Sens. Environ, 173, pp. 69-83, (2016); Kaminska A., Lisiewicz M., Sterenczak K., Kraszewski B., Sadkowski R., Species-related single dead tree detection using multi-temporal ALS data and CIR imagery, Remote Sens. Environ, 219, pp. 31-43, (2018); Weinmann M., Weinmann M., Mallet C., Bredif M., A classification-segmentation framework for the detection of individual trees in dense MMS point cloud data acquired in urban areas, Remote Sens, 9, (2017); Briechle S., Krzystek P., Vosselman G., Classification of tree species and standing dead trees by fusing UAV-based lidar data and multispectral imagery in the 3D deep neural network PointNet++, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 5, pp. 203-210, (2020); Mollaei Y., Karamshahi A., Erfanifard S., Detection of the Dry Trees Result of Oak Borer Beetle Attack Using Worldview-2 Satellite and UAV Imagery an Object-Oriented Approach, J Remote Sens. GIS, 7, (2018); Yao W., Krzystek P., Heurich M., Identifying standing dead trees in forest areas based on 3D single tree detection from full waveform lidar data, ISPRS Ann. Protogrammetry, Remote Sens. Spat. Inf. Sci, 1, (2012); Deng X., Lan Y., Hong T., Chen J., Citrus greening detection using visible spectrum imaging and C-SVC, Comput. Electron. Agric, 130, pp. 177-183, (2016); Lan Y., Huang Z., Deng X., Zhu Z., Huang H., Zheng Z., Lian B., Zeng G., Tong Z., Comparison of machine learning methods for citrus greening detection on UAV multispectral images, Comput. Electron. Agric, 171, (2020); Shendryk I., Broich M., Tulbure M.G., McGrath A., Keith D., Alexandrov S.V., Mapping individual tree health using full-waveform airborne laser scans and imaging spectroscopy: A case study for a floodplain eucalypt forest, Remote Sens. Environ, 187, pp. 202-217, (2016); Meng R., Dennison P.E., Zhao F., Shendryk I., Rickert A., Hanavan R.P., Cook B.D., Serbin S.P., Mapping canopy defoliation by herbivorous insects at the individual tree level using bi-temporal airborne imaging spectroscopy and LiDAR measurements, Remote Sens. Environ, 215, pp. 170-183, (2018); Lopez-Lopez M., Calderon R., Gonzalez-Dugo V., Zarco-Tejada P.J., Fereres E., Early detection and quantification of almond red leaf blotch using high-resolution hyperspectral and thermal imagery, Remote Sens, 8, (2016); Barnes C., Balzter H., Barrett K., Eddy J., Milner S., Suarez J.C., Airborne laser scanning and tree crown fragmentation metrics for the assessment of Phytophthora ramorum infected larch forest stands, For. Ecol. Manag, 404, pp. 294-305, (2017); Fassnacht F.E., Latifi H., Ghosh A., Joshi P.K., Koch B., Assessing the potential of hyperspectral imagery to map bark beetle-induced tree mortality, Remote Sens. Environ, 140, pp. 533-548, (2014); Chi D., Degerickx J., Yu K., Somers B., Urban Tree Health Classification Across Tree Species by Combining Airborne Laser Scanning and Imaging Spectroscopy, Remote Sens, 12, (2020); Nasi R., Honkavaara E., Lyytikainen-Saarenmaa P., Blomqvist M., Litkey P., Hakala T., Viljanen N., Kantola T., Tanhuanpaa T., Holopainen M., Using UAV-based photogrammetry and hyperspectral imaging for mapping bark beetle damage at tree-level, Remote Sens, 7, pp. 15467-15493, (2015); Nasi R., Honkavaara E., Blomqvist M., Lyytikainen-Saarenmaa P., Hakala T., Viljanen N., Kantola T., Holopainen M., Remote sensing of bark beetle damage in urban forests at individual tree level using a novel hyperspectral camera from UAV and aircraft, Urban For. Urban Green, 30, pp. 72-83, (2018); Degerickx J., Roberts D.A., McFadden J.P., Hermy M., Somers B., Urban tree health assessment using airborne hyperspectral and LiDAR imagery, Int. J. Appl. Earth Obs. Geoinf, 73, pp. 26-38, (2018); Xiao Q., McPherson E.G., Tree health mapping with multispectral remote sensing data at UC Davis, California, Urban Ecosyst, 8, pp. 349-361, (2005); Goldbergs G., Maier S.W., Levick S.R., Edwards A., Efficiency of individual tree detection approaches based on light-weight and low-cost UAS imagery in Australian Savannas, Remote Sens, 10, (2018); Fassnacht F.E., Mangold D., Schafer J., Immitzer M., Kattenborn T., Koch B., Latifi H., Estimating stand density, biomass and tree species from very high resolution stereo-imagery – towards an all-in-one sensor for forestry applications?, For. Int. J. For. Res, 90, pp. 613-631, (2017); Li W., He C., Fu H., Zheng J., Dong R., Xia M., Yu L., Luk W., A Real-Time Tree Crown Detection Approach for Large-Scale Remote Sensing Images on FPGAs, Remote Sens, 11, (2019); Ruiz V., Linares I., Sanchez A., Velez J.F., Off-line handwritten signature verification using compositional synthetic generation of signatures and Siamese Neural Networks, Neurocomputing, 374, pp. 30-41, (2020); Zhao X., Zhou S., Lei L., Deng Z., Siamese network for object tracking in aerial video, Proceedings of the 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC), pp. 519-523; Rao D.J., Mittal S., Ritika S., Siamese Neural Networks for One-Shot Detection of Railway Track Switches; Chandra M., Redkar S., Roy S., Patil P., Classification of Various Plant Diseases Using Deep Siamese Network; Shorfuzzaman M., Hossain M.S., MetaCOVID: A Siamese neural network framework with contrastive loss for n-shot diagnosis of COVID-19 patients, Pattern Recognit, 113, (2020); Bromley J., Bentz J.W., Bottou L., Guyon I., Lecun Y., Moore C., Sackinger E., Shah R., Signature Verification Using a “Siamese” Time Delay Neural Network, Int. J. Pattern Recognit. Artif. Intell, pp. 669-688, (1993); Wang B., Wang D., Plant Leaves Classification: A Few-Shot Learning Method Based on Siamese Network, IEEE Access, 7, pp. 27008-27016, (2019); What Is an Application Programming Interface (API)? | IBM; Berners-Lee T., Masinter L., McCahill M., Uniform Resource Locators, (1994); GitHub-Robolyst/Streetview: Python Module for Retrieving Current and Historical Photos from Google Street View; GitHub-Tzutalin/labelImg: LabelImg Is a Graphical Image Annotation Tool and Label Object Bounding Boxes in Images; A Friendly Introduction to Siamese Networks | by Sean Benhur J | Towards Data Science; Zhang K., Zuo W., Gu S., Zhang L., Learning deep CNN denoiser prior for image restoration, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3929-3938; Santurkar S., Tsipras D., Ilyas A., Madry A., How does batch normalization help optimization?, Proceedings of the Advances in Neural Information Processing Systems, pp. 2483-2493; Li Y., Yuan Y., Convergence analysis of two-layer neural networks with relu activation, Proceedings of the Advances in Neural Information Processing Systems, pp. 597-607; Dunne R.A., Campbell N.A., On the Pairing of the Softmax Activation and Cross-Entropy Penalty Functions and the Derivation of the Softmax Activation Function; Nassar A.S., Lefevre S., Wegner J.D., Multi-View Instance Matching with Learned Geometric Soft-Constraints, ISPRS Int. J. Geo-Inf, 9, (2020); Williams Brian, Contrastive Loss Has Been Used Recently; Chopra S., Hadsell R., LeCun Y., Learning a similarity metric discriminatively, with application to face verification, Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 1, pp. 539-546, (2005); Hadsell R., Chopra S., LeCun Y., Dimensionality reduction by learning an invariant mapping, Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), 2, pp. 1735-1742, (2006); Rutstrum C., The Wilderness Route Finder, (1967); Gavish M., Weiss A., Performance analysis of bearing-only target location algorithms, IEEE Trans. Aerosp. Electron. Syst, 28, pp. 817-828, (1992); Zhang H., Jing Z., Hu S., Localization of Multiple Emitters Based on the Sequential PHD Filter, Signal Process, 90, pp. 34-43, (2010); Reed J.D., da Silva C.R., Buehrer R.M., Multiple-source localization using line-of-bearing measurements: Approaches to the data association problem, Proceedings of the MILCOM 2008-2008 IEEE Military Communications Conference, pp. 1-7; Grabbe M.T., Hamschin B.M., Douglas A.P., A measurement correlation algorithm for line-of-bearing geo-location, Proceedings of the 2013 IEEE Aerospace Conference, pp. 1-8; Tan K., Chen H., Cai X., Research into the algorithm of false points elimination in three-station cross location, Shipboard Electron. Countermeas, 32, pp. 79-81, (2009); Reed J., Approaches to Multiple Source Localization and Signal Classification, (2009); Spatial Aggregation—ArcGIS Insights | Documentation; Rao A.S., What Do You Mean by GIS Aggregation, Geography Knowledge Hub, (2016); Ketkar N., Introduction to keras, Deep Learning with Python, pp. 97-111, (2017); Ketkar N., Introduction to Tensorflow, Deep Learning with Python, pp. 159-194, (2017); Alippi C., Disabato S., Roveri M., Moving convolutional neural networks to embedded systems: The alexnet and VGG-16 case, Proceedings of the 2018 17th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), pp. 212-223; Tran D., Wang H., Torresani L., Ray J., LeCun Y., Paluri M., A closer look at spatiotemporal convolutions for action recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6450-6459; Szegedy C., Ioffe S., Vanhoucke V., Alemi A., Inception-v4, inception-resnet and the impact of residual connections on learning, (2016); Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Et al., Scikit-learn: Machine Learning in Python, J. Mach. Learn. Res, 12, pp. 2825-2830, (2011); Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming | by James Dellinger | Towards Data Science; Signs That Your Tree Is Dying | Perth Arbor Services; Common Eucalyptus Tree Problems: Eucalyptus Tree Diseases; How Often Does Google Maps Update Satellite Images?; Things to Know about Google’s Maps Data: Beyond the Map | Google Cloud Blog","A. Khan; The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, 8001, Australia; email: asim.khan@vu.edu.au","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85108141951"
"Park H.G.; Yun J.P.; Kim M.Y.; Jeong S.H.","Park, Hae Gwang (57226778835); Yun, Jong Pil (16644164300); Kim, Min Young (56739349100); Jeong, Seung Hyun (57219224526)","57226778835; 16644164300; 56739349100; 57219224526","Multichannel Object Detection for Detecting Suspected Trees with Pine Wilt Disease Using Multispectral Drone Imagery","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9507079","8350","8358","8","18","10.1109/JSTARS.2021.3102218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112653782&doi=10.1109%2fJSTARS.2021.3102218&partnerID=40&md5=448e67beebd50d2731633e3f79b07d3a","Korea Institute of Industrial Technology, Cheonan-si, 31056, South Korea; School of Electronics Engineering, Kyungpook National University, Deagu, 41566, South Korea","Park H.G., Korea Institute of Industrial Technology, Cheonan-si, 31056, South Korea; Yun J.P., Korea Institute of Industrial Technology, Cheonan-si, 31056, South Korea; Kim M.Y., School of Electronics Engineering, Kyungpook National University, Deagu, 41566, South Korea; Jeong S.H., Korea Institute of Industrial Technology, Cheonan-si, 31056, South Korea","In this article, a multichannel convolutional neural network (CNN) based object detection was used to detect suspected trees of pine wilt disease after acquiring aerial photographs through a rotorcraft drone equipped with a multispectral camera. The acquired multispectral aerial photographs consist of RGB, green, red, NIR, and red edge spectral bands per shooting point. The aerial photographs for each band performed image calibration to correct radiation distortion, image alignment to correct the distance error of the lenses of a multispectral camera, and image enhancement to edge enhancement to highlight the features of objects in the image. After that, a large amount of data obtained through data augmentation were put into multichannel CNN-based object detection for training and test. As a result of verifying the detection performance of the trained model, excellent detection results were obtained with mAP 86.63% and average intersection over union 71.47%.  © 2008-2012 IEEE.","Convolutional neural network (CNN); deep learning; drone; multispectral; pine wilt disease (PWD); remote sensing","Aerial photography; Aircraft detection; Antennas; Cameras; Convolutional neural networks; Drones; Forestry; Image enhancement; Object recognition; Aerial Photographs; Data augmentation; Detection performance; Edge enhancements; Image alignment; Image calibration; Multi-spectral cameras; Pine wilt disease; artificial neural network; detection method; imagery; machine learning; multispectral image; remote sensing; tree; unmanned vehicle; wilt; Object detection","","","","","Korea Institute of Industrial Technology, KITECH, (JA-21-0002); National Research Foundation of Korea, NRF, (NBF-2018R1D1A1B07043406)","Manuscript received December 29, 2020; revised April 11, 2021 and May 16, 2021; accepted July 28, 2021. Date of publication August 4, 2021; date of current version September 2, 2021. This work was supported in part by the National Research Foundation of Korea under Grant NBF-2018R1D1A1B07043406; and in part by the Korea Institute of Industrial Technology (KITECH, JA-21-0002). (Corresponding authors: Seung Hyun Jeong; Min Young Kim.) Hae Gwang Park, Jong Pil Yun, and Seung Hyun Jeong are with the Korea Institute of Industrial Technology, Cheonan-si 31056, South Korea (e-mail: dd4680@kitech.re.kr; rebirth@kitech.re.kr; shjeong@kitech.re.kr).","Rutherford T.A., Mamiya Y., Webster J.W., Nematode-induced pine wilt disease: Factors influencing its occurrence and distribution, Forest Sci., 36, 1, pp. 145-155, (1990); Futai K., Pine wood nematode, Bursaphelenchus xylophilus, Annu. Rev. Phytopathol., 51, pp. 61-83, (2013); Kim D.-S., Et al., Escape of pine wood nematode, Bursaphelenchus xylophilus, through feeding and oviposition behavior of monochamus alternatus and M. saltuarius (Coleoptera: Cerambycidae) adults, Korean J. Appl. Entomol., 48, 4, pp. 527-533, (2009); Bergdahl D.R., Impact of pinewood nematode in NorthAmerica: Present and future, J. Nematol., 20, 2, pp. 260-265, (1988); Mamiya Y., History of pine wilt disease in Japan, J. Nematol., 20, 2, pp. 219-226, (1988); Mota M.M., Vieira P.R., Pine Wilt Disease: A Worldwide Threat to Forest Ecosystems., (2008); Shin S.-C., Pine wilt disease in Korea, Pine Wilt Disease., pp. 26-32, (2008); Sin S.-C., Current status of damage, control and research of pine barley nematode disease, Proc. Korean Soc. Appl. Entomol. Conf., pp. 33-39, (2005); Kim D.-S., Et al., The allegorical ecology of the adult imago of the sea salt of pine tree nematode, Korean J. Appl. Entomol., 42, 4, pp. 307-313, (2003); Lee J.-B., Et al., Spectroscopic characterization of pine tree nematode disease infected trees using a ground hyperspectral camera, Korean J. Remote Sens., 30, 5, pp. 665-675, (2014); Kim S.-R., Et al., Hyperspectral analysis of pine wilt disease to determine an optimal detection index, Forests, 9, 3, (2018); Kim J.-B., Kim D.-Y., Park N.-C., Development of an aerial precision forecasting techniques for the pine wilt disease damaged area based on GIS and GPS, J. Korean Assoc. Geographic Inf. Stud., 13, 1, pp. 28-34, (2010); Park J.-M., Sim W., Lee J., Detection of trees with pine wilt disease using object-based classification method, J. Forest Environ. Sci., 32, 4, pp. 384-391, (2016); Everaerts J., The use of unmanned aerial vehicles (UAVs) for remote sensing and mapping, Int. Arch. Photogrammetry Remote Sens. Spatial Inf. Sci., 37, pp. 1187-1192, (2008); Tang L., Shao G., Drone remote sensing for forestry research and practices, J. Forestry Res., 26, 4, pp. 791-797, (2015); Franke J., Menz G., Multi-temporal wheat disease detection bymultispectral remote sensing, Precis. Agriculture, 8, 3, pp. 161-172, (2007); Laliberte A.S., Goforth M.A., Steele C.M., Rango A., Multispectral remote sensing from unmanned aircraft: Image processing workflows and applications for rangeland environments, Remote Sens., 3, 11, pp. 2529-2551, (2011); Puliti S., Talbot B., Astrup R., Tree-stump detection, segmentation, classification, and measurement using unmanned aerial vehicle (UAV) imagery, Forests, 9, 3, (2018); Feduck C., McDermid G.J., Castilla G., Detection of coniferous seedlings in UAV imagery, Forests, 9, 7, (2018); Trichon V., Crown typology and the identification of rain forest trees on large-scale aerial photographs, Tropical Forest Canopies: Ecology and Management., pp. 301-312, (2001); Al Mansoori S., Kunhu A., Al Ahmad H., Automatic palm trees detection from multispectral UAV data using normalized difference vegetation index and circular Hough transform, High-PerformanceComput. Geoscience Remote Sensing VIII. Int. Society Optics Photonics, (2018); Hoshikawa T., Yamamoto K., Individual tree detection and classification for mapping pine wilt disease using multispectral and visible color imagery acquired from unmanned aerial vehicle, J. Remote Sens. Soc. Jpn., 40, 1, pp. 13-19, (2020); Mutiara S., Park S.-J., Lee C.-W., Detection of the pine wilt disease tree candidates for drone remote sensing using artificial intelligence techniques, Engineering, 6, 8, pp. 919-926, (2020); Morales G., Kemper G., Sevillano G., Arteaga D., Ortega I., Telles J., Automatic segmentation ofMauritia Flexuosa in unmanned aerial vehicle (UAV) imagery using deep learning, Forests, 9, 12, (2018); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of fir trees (Abies sibirica) damaged by the bark beetle in unmanned aerial vehicle images with deep learning, Remote Sens., 11, 6, (2019); Deng X., Tong Z., Lan Y., Huang Z., Detection and location of dead trees with pine wilt disease based on deep learning and UAV remote sensing, AgriEngineering, 2, 2, pp. 294-307, (2020); Jiao L., Et al., A survey of deep learning-based object detection, Ieee Access, 7, pp. 128837-128868, (2019); Han J., Zhang D., Cheng G., Liu N., Xu D., Advanced deep-learning techniques for salient and category-specific object detection: A survey, Ieee Signal Process. Mag., 35, 1, pp. 84-100, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); (2020); (2020); Yahyanejad S., Misiorny J., Rinner B., Lens distortion correction for thermal cameras to improve aerial imaging with small-scale UAVs, Proc. Ieee Int. Symp. Robot. Sensors Environ., pp. 231-236, (2011); Karami E., Prasad S., Shehata M., Image matching using SIFT, SURF, BRIEF and ORB: Performance comparison for distorted images, Newfoundland Electrical Comput. Eng. Conf., (2015); Tai H.T., Pixel Interpolatorwith Edge Sharpening, (1991); Mikolajczyk A., Grochowski M., Data augmentation for improving deep learning in image classification problem, Proc. Int. Interdisciplinary Ph.D. Workshop, pp. 117-122, (2018); Carlson T.N., Ripley D.A., On the relation between NDVI, fractional vegetation cover, and leaf area index, Remote Sens.Environ., 62, 3, pp. 241-252, (1997); Handique B.K., Khan A.Q., Goswami C., Prashnani M., Gupta C., Raju P.L.N., Crop discrimination using multispectral sensor onboard unmanned aerial vehicle, Proc. Nat. Acad. Sci., India Sect. A, Phys. Sci., 87, pp. 713-719, (2017); Kingma D.P., Ba J., Adam: A method for stochastic optimization, Proc. Int. Conf. Learn. Representations, (2015); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Proc. Int. Conf. Learn. Representations, (2015); Srivastava N., Hinton G., Krizhevsky A., Sutskever I., Salakhutdinov R., Dropout: A simple way to prevent neural networks from overfitting, J. Mach. Learn. Res., 15, 56, pp. 1929-1958, (2014); Zeiler M.D., Rob F., Visualizing and understanding convolutional networks, Proc. Eur. Conf. Comput. Vision, pp. 818-833, (2014); Phillips M.C., Stein R., Park T., Analyzing pre-trained neural network behavior with layer activation optimization, Proc. Syst. Inf. Eng. Des. Symp., pp. 1-6, (2020)","M.Y. Kim; School of Electronics Engineering, Kyungpook National University, Deagu, 41566, South Korea; email: minykim@knu.ac.kr","","Institute of Electrical and Electronics Engineers Inc.","","","","","","19391404","","","","English","IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112653782"
"Kuo C.-L.; Tsai M.-H.","Kuo, Chiao-Ling (55349097400); Tsai, Ming-Hua (57224892986)","55349097400; 57224892986","Road characteristics detection based on joint convolutional neural networks with adaptive squares","2021","ISPRS International Journal of Geo-Information","10","6","377","","","","5","10.3390/ijgi10060377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108555347&doi=10.3390%2fijgi10060377&partnerID=40&md5=4c60d7fd38e001e261c6508e55299990","Research Center for Humanities and Social Sciences, Academia Sinica, Taipei, 11529, Taiwan; Department of Geography, National Taiwan University, Taipei, 10617, Taiwan","Kuo C.-L., Research Center for Humanities and Social Sciences, Academia Sinica, Taipei, 11529, Taiwan, Department of Geography, National Taiwan University, Taipei, 10617, Taiwan; Tsai M.-H., Research Center for Humanities and Social Sciences, Academia Sinica, Taipei, 11529, Taiwan","The importance of road characteristics has been highlighted, as road characteristics are fundamental structures established to support many transportation-relevant services. However, there is still huge room for improvement in terms of types and performance of road characteristics detection. With the advantage of geographically tiled maps with high update rates, remarkable accessibility, and increasing availability, this paper proposes a novel simple deep-learning-based approach, namely joint convolutional neural networks (CNNs) adopting adaptive squares with combination rules to detect road characteristics from roadmap tiles. The proposed joint CNNs are responsible for the foreground and background image classification and various types of road characteristics classification from previous foreground images, raising detection accuracy. The adaptive squares with combination rules help efficiently focus road characteristics, augmenting the ability to detect them and provide optimal detection results. Five types of road characteristics— crossroads, T-junctions, Y-junctions, corners, and curves—are exploited, and experimental results demonstrate successful outcomes with outstanding performance in reality. The information of exploited road characteristics with location and type is, thus, converted from human-readable to machine-readable, the results will benefit many applications like feature point reminders, road condition reports, or alert detection for users, drivers, and even autonomous vehicles. We believe this approach will also enable a new path for object detection and geospatial information extraction from valuable map tiles. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Adaptive squares; CNN; Combination rules; Deep learning; Road characteristics detection; Roadmap tiles","","","","","","Academia Sinica; Ministry of Science and Technology, Taiwan, MOST, (108-2621-M-001-001-)","Funding text 1: Acknowledgments: This research was also supported by the Spatial Economics project from the Center for Institution and Behavior Studies, RCHSS, Academia Sinica.; Funding text 2: This research was funded by the Ministry of Science and Technology, Taiwan (R.O.C.), grant number MOST 108-2621-M-001-001-.","Xie K., Wang X., Ozbay K., Yang H., Crash frequency modeling for signalized intersections in a high-density urban road network, Anal. Methods Accid. Res, 2, pp. 39-51, (2014); Farahani R.Z., Miandoabchi E., Szeto W.Y., Rashidi H., A review of urban transportation network design problems, European J. Oper. Res, 229, pp. 281-302, (2013); Marshall W.E., Garrick N.W., Does street network design affect traffic safety?, Accid. Anal. Prev, 43, pp. 769-781, (2011); Ewing R., Hamidi S., Grace J.B., Urban sprawl as a risk factor in motor vehicle crashes, Urban Stud, 53, pp. 247-266, (2016); Wang X., Wu X., Abdel-Aty M., Tremont P.J., Investigation of road network features and safety performance, Accid. Anal. Prev, 56, pp. 22-31, (2013); Montella A., Guida C., Mosca J., Lee J., Abdel-Aty M., Systemic approach to improve safety of urban unsignalized intersections: Development and validation of a Safety Index, Accid. Anal. Prev, 141, (2020); Yang C., Gidofalvi G., Detecting regional dominant movement patterns in trajectory data with a convolutional neural network, Int. J. Geogr. Inf. Sci, 34, pp. 996-1021, (2020); Iagnemma K., Route Planning for an Autonomous Vehicle, (2018); Chen C., Rickert M., Knoll A., Combining task and motion planning for intersection assistance systems, Proceedings of the 2016 IEEE Intelligent Vehicles Symposium (IV), (2016); Qiu J., Wang R., Automatic extraction of road networks from GPS traces, Photogramm. Eng. Remote. Sens, 82, pp. 593-604, (2016); Li L., Li D., Xing X., Yang F., Rong W., Zhu H., Extraction of road intersections from GPS traces based on the dominant orientations of roads, ISPRS Int. J. Geo-Inf, 6, (2017); Wang J., Wang C., Song X., Raghavan V., Automatic intersection and traffic rule detection by mining motor-vehicle GPS trajectories, Comput. Environ. Urban Syst, 64, pp. 19-29, (2017); Yang X., Tang L., Niu L., Zhang X., Li Q., Generating lane-based intersection maps from crowdsourcing big trace data, Transp. Res. Part C Emerg. Technol, 89, pp. 168-187, (2018); Xie X., Philips W., Road intersection detection through finding common sub-tracks between pairwise GNSS traces, ISPRS Int. J. Geo-Inf, 6, (2017); Xie X., Wong B.-Y.K., Aghajan H., Veelaert P., Philips W., Inferring directed road networks from GPS traces by track alignment, ISPRS Int. J. Geo-Inf, 4, pp. 2446-2471, (2015); Chen B., Ding C., Ren W., Xu G., Extended Classification Course Improves Road Intersection Detection from Low-Frequency GPS Trajectory Data, ISPRS Int. J. Geo-Inf, 9, (2020); Munoz-Organero M., Ruiz-Blaquez R., Sanchez-Fernandez L., Automatic detection of traffic lights, street crossings and urban roundabouts combining outlier detection and deep learning classification techniques based on GPS traces while driving, Comput. Environ. Urban Syst, 68, pp. 1-8, (2018); Zourlidou S., Fischer C., Sester M., Classification of street junctions according to traffic regulators, Geospatial Technologies for Local and Regional Development: Short Papers, Posters and Poster Abstracts, Proceedings of the 22nd AGILE Conference on Geographic Information Science, (2019); Soilan M., Truong-Hong L., Riveiro B., Laefer D., Automatic extraction of road features in urban environments using dense ALS data, Int. J. Appl. Earth Obs. Geoinf, 64, pp. 226-236, (2018); Jung J., Bae S.-H., Real-time road lane detection in urban areas using LiDAR data, Electronics, 7, (2018); Hu J., Razdan A., Femiani J.C., Cui M., Wonka P., Road network extraction and intersection detection from aerial images by tracking road footprints, IEEE Trans. Geosci. Remote. Sens, 45, pp. 4144-4157, (2007); Chiang Y.-Y., Knoblock C.A., Shahabi C., Chen C.-C., Automatic and accurate extraction of road intersections from raster maps, GeoInformatica, 13, pp. 121-157, (2009); Chiang Y.-Y., Knoblock C.A., Chen C.-C., Automatic extraction of road intersections from raster maps, Proceedings of the 13th annual ACM international workshop on Geographic information systems, (2005); Liu J., Qin Q., Li J., Li Y., Rural road extraction from high-resolution remote sensing images based on geometric feature inference, ISPRS Int. J. Geo-Inf, 6, (2017); Bakhtiari H.R.R., Abdollahi A., Rezaeian H., Semi automatic road extraction from digital images, Egypt. J. Remote. Sens. Space Sci, 20, pp. 117-123, (2017); Mokhtarzade M., Zoej M.V., Road detection from high-resolution satellite images using artificial neural networks, Int. J. Appl. Earth Obs. Geoinf, 9, pp. 32-40, (2007); Bhatt D., Sodhi D., Pal A., Balasubramanian V., Krishna M., Have i reached the intersection: A deep learning-based approach for intersection detection from monocular cameras, Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), (2017); Bastani F., He S., Abbar S., Alizadeh M., Balakrishnan H., Chawla S., Madden S., DeWitt D., Roadtracer: Automatic extraction of road networks from aerial images, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2018); Saeedimoghaddam M., Stepinski T., Automatic extraction of road intersection points from USGS historical map series using deep convolutional neural networks, Int. J. Geogr. Inf. Sci, 34, pp. 947-968, (2020); Ruiz J., Rubio T., Urena M., Automatic extraction of road intersections from images based on texture characterisation, Surv. Rev, 43, pp. 212-225, (2011); Maso J., Pomakis K., Julia N., OGC Web Map Tile Service (WMTS), Implement. Standard. Ver, 1, (2010); Tile Map Service Specification; Kastanakis B., Mapbox Cookbook, (2016); Martinelli L., Roth M., Vector Tiles from OpenStreetMap, (2015); Shi N.X., Wu X., Method of Client Side Map Rendering with Tiled Vector Data, (2010); Zhao Z.-Q., Zheng P., Xu S.-T., Wu X., Object detection with deep learning: A review, IEEE Trans. Neural Netw. Learn. Syst, 30, pp. 3212-3232, (2019); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietikainen M., Deep learning for generic object detection: A survey, Int. J. Comput. Vis, 128, pp. 261-318, (2020); Pathak A.R., Pandey M., Rautaray S., Application of deep learning for object detection, Procedia Comput. Sci, 132, pp. 1706-1717, (2018); Behrendt K., Novak L., Botros R., A deep learning approach to traffic lights: Detection, tracking, and classification, Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), (2017); Mundhenk T.N., Konjevod G., Sakla W.A., Boakye K., A large contextual dataset for classification, detection and counting of cars with deep learning, European Conference on Computer Vision, (2016); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci. Remote. Sens. Lett, 14, pp. 778-782, (2017); Zhang C., Sargent I., Pan X., Li H., Gardiner A., Hare J., Atkinson P.M., Joint Deep Learning for land cover and land use classification, Remote. Sens. Environ, 221, pp. 173-187, (2019); Pourabdollah A., Morley J., Feldman S., Jackson M., Towards an authoritative OpenStreetMap: Conflating OSM and OS OpenData national maps’ road network, ISPRS Int. J. Geo-Inf, 2, pp. 704-728, (2013); Wu S., Du C., Chen H., Xu Y., Guo N., Jing N., Road extraction from very high resolution images using weakly labeled OpenStreetMap centerline, ISPRS Int. J. Geo-Inf, 8, (2019); Nasiri A., Abbaspour R.A., Chehreghan A., Arsanjani J.J., Improving the quality of citizen contributed geodata through their historical contributions: The case of the road network in OpenStreetMap, ISPRS Int. J. Geo-Inf, 7, (2018); Li J., Qin H., Wang J., Li J., OpenStreetMap-based autonomous navigation for the four wheel-legged robot via 3D-Lidar and CCD camera, IEEE Trans. Ind. Electron, pp. 1-1, (2021); Keller S., Gabriel R., Guth J., Machine learning framework for the estimation of average speed in rural road networks with openstreetmap data, ISPRS Int. J. Geo-Inf, 9, (2020); Novack T., Wang Z., Zipf A., A system for generating customized pleasant pedestrian routes based on OpenStreetMap data, Sensors, 18, (2018); Wang Z., Niu L., A data model for using OpenStreetMap to integrate indoor and outdoor route planning, Sensors, 18, (2018); Sehra S.S., Singh J., Rai H.S., Assessing OpenStreetMap Data Using Intrinsic Quality Indicators: An Extension to the QGIS Processing Toolbox, Future Internet, 9, (2017); Jacobs K.T., Mitchell S.W., OpenStreetMap quality assessment using unsupervised machine learning methods, Trans. GIS, 24, pp. 1280-1298, (2020); Mooney P., Minghini M., A review of OpenStreetMap data, Mapping and the Citizen Sensor, pp. 37-59, (2017); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, (2015); Google Maps Platform Documentation-Maps Static API; Google Maps Platform Documentation-Styled Maps; Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., Inception-v4, inception-resnet and the impact of residual connections on learning, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, (2017); Hosang J., Benenson R., Schiele B., Learning non-maximum suppression, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4507-4515; Google Maps Platform Styling Wizard; Git Code","C.-L. Kuo; Research Center for Humanities and Social Sciences, Academia Sinica, Taipei, 11529, Taiwan; email: kuo@chiaoling.com","","MDPI AG","","","","","","22209964","","","","English","ISPRS Int. J. Geo-Inf.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85108555347"
"Reksten J.H.; Salberg A.-B.","Reksten, Jarle Hamar (57207874531); Salberg, Arnt-Børre (9637547100)","57207874531; 9637547100","Estimating Traffic in Urban Areas from Very-High Resolution Aerial Images","2021","International Journal of Remote Sensing","42","3","","865","883","18","2","10.1080/01431161.2020.1815891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097083608&doi=10.1080%2f01431161.2020.1815891&partnerID=40&md5=88f81cfe0440367779569a32b97785f9","Section for Earth Observation, Norwegian Computing Center, Oslo, Norway","Reksten J.H., Section for Earth Observation, Norwegian Computing Center, Oslo, Norway; Salberg A.-B., Section for Earth Observation, Norwegian Computing Center, Oslo, Norway","Traffic estimation from very-high-resolution remote-sensing imagery has received increasing interest during the last few years. In this article, we propose an automatic system for estimation of the annual average daily traffic (AADT) using very-high-resolution optical remote-sensing imagery of urban areas in combination with high-quality, but very spatially limited, ground-based measurements. The main part of the system is the vehicle detection, which is based on the deep learning object detection architecture mask region-based convolutional neural network (Mask R-CNN), modified with an image normalization strategy to make it more robust for test images of various conditions and the use of a precise road mask to assist the filtering of driving vehicles from parked ones. Furthermore, to include the high-quality ground-based measurements and to make the traffic estimates more consistent across neighbouring road links, we propose a graph smoothing strategy that utilizes the road network. The fully automatic processing chain has been validated on a set of aerial images covering the city of Narvik, Norway. The precision and recall rate of detecting driving vehicles was 0.74 and 0.66, respectively, and the AADT was estimated with a root mean squared error (RMSE) of 2279 and bias of −383. We conclude that separating driving vehicles from parked ones may be challenging if vehicles are parked along the roads and that for urban environment with short road links several remote-sensing images covering the road links at different time instances are necessary in order to benefit from the remote-sensing images. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","","Narvik; Norway; Antennas; Convolutional neural networks; Deep learning; Mean square error; Object detection; Road vehicles; Roads and streets; Annual average daily traffics; Automatic processing; Ground based measurement; Optical remote-sensing imagery; Remote sensing images; Root mean squared errors; Very high resolution; Very high resolution aerial images; aerial survey; artificial neural network; estimation method; ground-based measurement; image analysis; image resolution; remote sensing; spatial analysis; traffic management; urban area; urban transport; Remote sensing","","","","","Norwegian Meteorological Institute; Norges Forskningsråd, (267734)","This research was funded by the Research Council of Norway, [grant number 267734] (AirQuip project). Thanks to Michael Gauss and Bruce Denby at the The Norwegian Meteorological Institute for fruitful and interesting discussions during the AirQuip project period.","Audebert N., Le Saux B., Lefevre S., Segment-before-detect: Vehicle Detection and Classification through Semantic Segmentation of Aerial Images, Remote Sensing, 9, 368, (2017); Eikvil L., Aurdal L., Koren H., Classification-based Vehicle Detection in High-resolution Satellite Images, ISPRS Journal of Photogrammetry and Remote Sensing, 64, 1, pp. 65-72, (2009); He K., Gkioxari G., Dollar P., Girshick R.B., Mask R-CNN, arXiv Preprint arXiv, 1703.06870v3, (2018); Hinz S., Stilla U., Detection of Vehicles and Vehicle Queues for Road Monitoring Using High Resolution Aerial Images, 9th World Multiconference on Systemics, Cybernetics, and Informatics, (2005); 2D Semantic Labeling Contest, online, (2018); Kampffmeyer M., Salberg A.B., Jenssen R., Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks, Proc. IEEE Conf. Computer Vision Pattern Recognition Workshops, pp. 1-9, (2016); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, (2012); Larsen S.O., Haug O., Aldrin M.T., Estimating Annual Average Daily Traffic (AADT) Based on Extremely Sparse Traffic Counts - A Study of the Feasibility of Using Satellite Data for AADT Estimation, (2008); Larsen S.O., Salberg A.B., Eikvil L., Automatic System for Operational Traffic Monitoring Using Very-High-Resolution Satellite Imagery, International Journal of Remote Sensing, 34, 13, pp. 4850-4870, (2013); Lato M., Frauenfelder R., Buhler Y., Automated Detection of Snow Avalanche Deposits: Segmentation and Classification of Optical Remote Sensing Imagery, Natural Hazards and Earth System Sciences, 12, 9, pp. 1-14, (2012); Leitloff J., Hinz S., Stilla U., Vehicle Detection in Very High Resolution Satellite Images of City Areas, IEEE Transactions on Geoscience and Remote Sensing, 48, 7, pp. 2795-2806, (2010); Leitloff J., Rosenbaum D., Kurz F., Meynberg O., Reinartz P., An Operational System for Estimating Road Traffic Information from Aerial Images, Remote Sensing, 11, pp. 11315-11341, (2014); Liu K., Mattyus G., Fast Multiclass Vehicle Detection on Aerial Images, IEEE Geoscience and Remote Sensing Letters, 12, 9, pp. 1938-1942, (2015); Long J., Shelhamer E., Darrell T., Fully Convolutional Networks for Semantic Segmentation, Proc. Boston, USA: IEEE Conf. Computer Vision Pattern Recognition, pp. 3431-3440, (2015); Lv Z.Y., Liu T.F., Zhang P., Benediktsson J.A., Lei T., Zhang X., Novel Adaptive Histogram Trend Similarity Approach for Land Cover Change Detection by Using Bitemporal Very-High-Resolution Remote Sensing Images, IEEE Transactions on Geoscience and Remote Sensing, 57, 12, pp. 9554-9574, (2019); Massa F., Girshick R., Maskrcnn-benchmark: Fast, Modular Reference Implementation of Instance Segmentation and Object Detection Algorithms in PyTorch, (2018); Paisitkriangkrai S., Sherrah J., Janney P., Hengel A., Effective Semantic Pixel Labelling with Convolutional Networks and Conditional Random Fields, Proc. Boston, USA: IEEE Conf. Computer Vision and Pattern Recognition Workshops, pp. 36-43, (2015); Sabour S., Frosst N., Hinton G.E., Dynamic Routing between Capsules, Adv. Neural Inform. Process. Syst., (2017); Scharf L.L., Statistical Signal Processing: Detection, Estimation, and Time Series Analysis, (1991); Schaub M.T., Segarra S., Flow Smoothing and Denoising: Graph Signal Processing in the Edge-space, arXiv Preprint arXiv, 1808, 211, (2018); Wind P., Denby B.R., Transboundary Particulate Matter, Photo-oxidants, Acidifying and Eutrophying Components. EMEP Status Report 1/2017, (2017); Yu Y., Ai H., He X., Yu S., Zhong X., Lu M., Ship Detection in Optical Satellite Images Using Haar-like Features and Periphery-Cropped Neural Networks, IEEE Access, 6, pp. 71122-71131, (2018); Yu Y., Gu T., Guan H., Li D., Jin S., Vehicle Detection from High-resolution Remote Sensing Imagery Using Convolutional Capsule Networks, IEEE Geoscience and Remote Sensing Letters, 16, 12, pp. 1894-1898, (2019); Zhao T., Nevatia R., Car Detection in Low Resolution Aerial Images, Image and Vision Computing, 21, pp. 693-703, (2003); Zhao Z., Zheng P., Xu S., Wu X., Object Detection with Deep Learning: A Review, IEEE Transactions on Neural Networks and Learning Systems, 30, 11, pp. 3212-3232, (2019)","J.H. Reksten; Norwegian Computing Center, Oslo, N-0314, Norway; email: jarle.reksten@nr.no","","Taylor and Francis Ltd.","","","","","","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Final","","Scopus","2-s2.0-85097083608"
"Hoeser T.; Bachofer F.; Kuenzer C.","Hoeser, Thorsten (57216967331); Bachofer, Felix (36960527400); Kuenzer, Claudia (55927784300)","57216967331; 36960527400; 55927784300","Object detection and image segmentation with deep learning on earth observation data: A review-part II: Applications","2020","Remote Sensing","12","18","3053","","","","89","10.3390/RS12183053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092269929&doi=10.3390%2fRS12183053&partnerID=40&md5=d10875362df915bd0ef97635a5bf923e","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany; Department of Remote Sensing, Institute of Geography and Geology, University Würzburg, Am Huband, Wuerzburg, D-97074, Germany","Hoeser T., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany; Bachofer F., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany; Kuenzer C., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany, Department of Remote Sensing, Institute of Geography and Geology, University Würzburg, Am Huband, Wuerzburg, D-97074, Germany","In Earth observation (EO), large-scale land-surface dynamics are traditionally analyzed by investigating aggregated classes. The increase in data with a very high spatial resolution enables investigations on a fine-grained feature level which can help us to better understand the dynamics of land surfaces by taking object dynamics into account. To extract fine-grained features and objects, the most popular deep-learning model for image analysis is commonly used: the convolutional neural network (CNN). In this review, we provide a comprehensive overview of the impact of deep learning on EO applications by reviewing 429 studies on image segmentation and object detection with CNNs. We extensively examine the spatial distribution of study sites, employed sensors, used datasets and CNN architectures, and give a thorough overview of applications in EO which used CNNs. Our main finding is that CNNs are in an advanced transition phase from computer vision to EO. Upon this, we argue that in the near future, investigations which analyze object dynamics with CNNs will have a significant impact on EO research. With a focus on EO applications in this Part II, we complete the methodological review provided in Part I. © 2020 by the authors.","AI; Artificial intelligence; CNN; Convolutional neural networks; Deep learning; Earth observation; Image segmentation; Machine learning; Neural networks; Object detection","Convolutional neural networks; Dynamics; Image segmentation; Object detection; Object recognition; Observatories; Surface measurement; Earth observation data; Earth observations; Feature level; Land surface; Learning models; Object dynamics; Transition phase; Very high spatial resolutions; Deep learning","","","","","","","Marconcini M., Metz-Marconcini A., Ureyen S., Palacios-Lopez D., Hanke W., Bachofer F., Zeidler J., Esch T., Gorelick N., Kakarla A., Et al., Outlining where humans live, the World Settlement Footprint 2015, Sci. Data., 7, pp. 1-14, (2020); Zhu Z., Bi J., Pan Y., Ganguly S., Anav A., Xu L., Samanta A., Piao S., Nemani R.R., Myneni R.B., Global Data Sets of Vegetation Leaf Area Index (LAI)3g and Fraction of Photosynthetically Active Radiation (FPAR)3g Derived from Global Inventory Modeling and Mapping Studies (GIMMS) Normalized Difference Vegetation Index (NDVI3g) for the Period 1981 to 2011, Remote Sens., 5, pp. 927-948, (2013); Klein I., Gessner U., Dietz A.J., Kuenzer C., Global WaterPack-A 250 m resolution dataset revealing the daily dynamics of global inland water bodies, Remote Sens. Environ., 198, pp. 345-362, (2017); Reichstein M., Camps-Valls G., Stevens B., Jung M., Denzler J., Carvalhais N., Prabhat. Deep learning and process understanding for data-driven Earth system science, Nature, 566, pp. 195-204, (2019); Long Y., Xia G.S., Li S., Yang W., Yang M.Y., Zhu X.X., Zhang L., Li D., DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation, arXiv., (2020); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, In Advances in Neural Information Processing Systems 25, pp. 1097-1105, (2012); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Et al., ImageNet Large Scale Visual Recognition Challenge, Int. J. Comput. Vis. (Ijcv), 115, pp. 211-252, (2015); Vinuesa R., Azizpour H., Leite I., Balaam M., Dignum V., Domisch S., Fellander A., Langhans S.D., Tegmark M., Nerini F.F., The role of artificial intelligence in achieving the Sustainable Development Goals, Nat. Commun., 11, pp. 1-10, (2020); Copernicus Masters. ESA Digital Twin Earth Challenge.; Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community, J. Appl. Remote Sens., 11, pp. 1-54, (2017); Zhu X.X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36, (2017); Zhang L., Zhang L., Du B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art, IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, Isprs J. Photogramm. Remote Sens., 152, pp. 166-177, (2019); Tsagkatakis G., Aidini A., Fotiadou K., Giannopoulos M., Pentari A., Tsakalides P., Survey of Deep-Learning Approaches for Remote Sensing Observation Enhancement, Sensors, 19, (2019); Petersson H., Gustafsson D., Bergstrom D., Hyperspectral image analysis using deep learning-A review, In Proceedings of the 2016 Sixth International Conference on Image Processing Theory, pp. 1-6, (2016); Audebert N., Le Saux B., Lefevre S., Deep Learning for Classification of Hyperspectral Data: A Comparative Review, IEEE Geosci. Remote Sens. Mag., 7, pp. 159-173, (2019); Paoletti M., Haut J., Plaza J., Plaza A., Deep learning classifiers for hyperspectral imaging: A review, Isprs J. Photogramm. Remote Sens., 158, pp. 279-317, (2019); Zhu X.X., Montazeri S., Ali M., Hua Y., Wang Y., Mou L., Shi Y., Xu F., Bamler R., Deep Learning Meets SAR, arXiv, (2020); Khelifi L., Mignotte M., Deep Learning for Change Detection in Remote Sensing Images: Comprehensive Review and Meta-Analysis, arXiv, (2020); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Hoeser T., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part I: Evolution and Recent Trends, Remote Sens., 12, (2020); 2D Semantic Labeling Challenge.; Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, Int. J. Comput. Vis., 88, pp. 303-338, (2010); Everingham M., Eslami S.M., Gool L., Williams C.K., Winn J., Zisserman A., The Pascal Visual Object Classes Challenge: A Retrospective, Int. J. Comput. Vis., 111, pp. 98-136, (2015); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common Objects in Context, In Computer Vision - ECCV 2014, pp. 740-755, (2014); Cordts M., Omran M., Ramos S., Scharwachter T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The Cityscapes Dataset, In Proceedings of the CVPR Workshop on the Future of Datasets in Vision, 2, (2015); Cordts M., Omran M., Ramos S., Rehfeld T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The Cityscapes Dataset for Semantic Urban Scene Understanding, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223, (2016); GRSS Data Fusion Contest; Mnih V., Machine Learning for Aerial Image Labeling, (2013); SpaceNet 1: Building Detection v1; SpaceNet 2: Building Detection v2; SpaceNet 4: Off-Nadir Buildings; Shermeyer J., Hogan D., Brown J., Etten A.V., Weir N., Pacifici F., Haensch R., Bastidas A., Soenen S., Bacastow T., Lewis R., SpaceNet 6: Multi-Sensor All Weather Mapping Dataset, arXiv, (2020); Ji S., Wei S., Lu M., Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set, IEEE Trans. Geosci. Remote Sens., 57, pp. 574-586, (2019); Demir I., Koperski K., Lindenbaum D., Pang G., Huang J., Basu S., Hughes F., Tuia D., Raskar R., DeepGlobe 2018: A Challenge to Parse the Earth Through Satellite Images, In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 172-181, (2018); SpaceNet 3: Road Network Detection; Etten A.V., City-Scale Road Extraction from Satellite Imagery v2: Road Speeds and Travel Times., In Proceedings of the The IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1786-1795, (2020); Cheng G., Wang Y., Xu S., Wang H., Xiang S., Pan C., Automatic Road Detection and Centerline Extraction via Cascaded End-to-End Convolutional Neural Network, IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337, (2017); Cheng G., Han J., Zhou P., Guo L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors, Isprs J. Photogramm. Remote Sens., 98, pp. 119-132, (2014); Xia G.S., Bai X., Ding J., Zhu Z., Belongie S., Luo J., Datcu M., Pelillo M., Zhang L., DOTA: A Large-Scale Dataset for Object Detection in Aerial Images, In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3974-3983, (2018); Long Y., Gong Y., Xiao Z., Liu Q., Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks, IEEE Trans. Geosci. Remote Sens., 55, pp. 2486-2498, (2017); Li J., Qu C., Shao J., Ship detection in SAR images based on an improved faster R-CNN, In Proceedings of the 2017 SAR in Big Data Era: Models, pp. 1-6, (2017); Huang L., Liu B., Li B., Guo W., Yu W., Zhang Z., Yu W., OpenSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 195-208, (2018); Li B., Liu B., Huang L., Guo W., Zhang Z., Yu W., OpenSARShip 2.0: A large-volume dataset for deeper interpretation of ship targets in Sentinel-1 imagery, In Proceedings of the 2017 SAR in Big Data Era: Models, pp. 1-5, (2017); Liu K., Mattyus G., Fast Multiclass Vehicle Detection on Aerial Images, IEEE Geosci. Remote Sens. Lett., 12, pp. 1938-1942, (2015); Razakarivony S., Jurie F., Vehicle detection in aerial imagery: A small target detection benchmark, J. Vis. Commun. Image Represent., 34, pp. 187-203, (2016); Mou L., Zhu X.X., Vehicle Instance Segmentation From Aerial Image and Video Using a Multitask Learning Residual Fully Convolutional Network, IEEE Trans. Geosci. Remote Sens., 56, pp. 6699-6711, (2018); Isikdogan F., Bovik A., Passalacqua P., Learning a River Network Extractor Using an Adaptive Loss Function, IEEE Geosci. Remote Sens. Lett., 15, pp. 813-817, (2018); Kong F., Huang B., Bradbury K., Malof J.M., The Synthinel-1 dataset: A collection of high resolution synthetic overhead imagery for building segmentation, In Proceedings of the 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1803-1812, (2020); Zhang F., Du B., Zhang L., Xu M., Weakly Supervised Learning Based on Coupled Convolutional Neural Networks for Aircraft Detection, IEEE Trans. Geosci. Remote Sens., 54, pp. 5553-5563, (2016); Ji J., Zhang T., Yang Z., Jiang L., Zhong W., Xiong H., Aircraft Detection from Remote Sensing Image Based on A Weakly Supervised Attention Model, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 322-325, (2019); Wu X., Hong D., Tian J., Kiefl R., Tao R., A Weakly-Supervised Deep Network for DSM-Aided Vehicle Detection, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1318-1321, (2019); Kaiser P., Wegner J.D., Lucchi A., Jaggi M., Hofmann T., Schindler K., Learning Aerial Image Segmentation From Online Maps, IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068, (2017); Krylov V.A., de Martino M., Moser G., Serpico S.B., Large urban zone classification on SPOT-5 imagery with convolutional neural networks, In Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1796-1799, (2016); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification, IEEE Trans. Geosci. Remote Sens., 55, pp. 645-657, (2017); Bittner K., Adam F., Cui S., Korner M., Reinartz P., Building Footprint Extraction From VHR Remote Sensing Images Combined With Normalized DSMs Using Fused Fully Convolutional Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 2615-2629, (2018); Voinov S., Krause D., Schwarz E., Towards Automated Vessel Detection and Type Recognition from VHR Optical Satellite Images, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 4823-4826, (2018); Piramanayagam S., Saber E., Schwartzkopf W., Koehler F.W., Supervised Classification of Multisensor Remotely Sensed Images Using a Deep Learning Framework, Remote Sens., 10, (2018); Kim J.H., Lee H., Hong S.J., Kim S., Park J., Hwang J.Y., Choi J.P., Objects Segmentation From High-Resolution Aerial Images Using U-Net With Pyramid Pooling Layers, IEEE Geosci. Remote Sens. Lett., 16, pp. 115-119, (2019); Shahzad M., Maurer M., Fraundorfer F., Wang Y., Zhu X.X., Buildings Detection in VHR SAR Images Using Fully Convolution Neural Networks, IEEE Trans. Geosci. Remote Sens., 57, pp. 1100-1116, (2019); Shi Y., Li Q., Zhu X., Building Footprint Extraction with Graph Convolutional Network, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5136-5139, (2019); Wu S., Du C., Chen H., Xu Y., Guo N., Jing N., Road Extraction from Very High Resolution Images Using Weakly labeled OpenStreetMap Centerline, Isprs Int. J. -Geo, 8, (2019); Vargas-Munoz J.E., Lobry S., Falcao A.X., Tuia D., Correcting rural building annotations in OpenStreetMap using convolutional neural networks, Isprs J. Photogramm. Remote Sens., 147, pp. 283-293, (2019); Huang J., Zhang X., Xin Q., Sun Y., Zhang P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network, Isprs J. Photogramm. Remote Sens., 151, pp. 91-105, (2019); Griffiths D., Boehm J., Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours, Isprs J. Photogramm. Remote Sens., 154, pp. 70-83, (2019); Li W., He C., Fang J., Zheng J., Fu H., Yu L., Semantic Segmentation-Based Building Footprint Extraction Using Very High-Resolution Satellite Images and Multi-Source GIS Data, Remote Sens., 11, (2019); Manandhar P., Marpu P.R., Aung Z., Melgani F., Towards Automatic Extraction and Updating of VGI-Based Road Networks Using Deep Learning, Remote Sens., 11, (2019); Zeng F., Cheng L., Li N., Xia N., Ma L., Zhou X., Li M., A Hierarchical Airport Detection Method Using Spatial Analysis and Deep Learning, Remote Sens., 11, (2019); Schmitt M., Prexl J., Ebel P., Liebel L., Zhu X.X., Weakly Supervised Semantic Segmentation of Satellite Images for Land Cover Mapping - Challenges and Opportunities, arXiv, (2020); Airborne Synthetic Aperture Radar (AIRSAR); Xiao Z., Liu Q., Tang G., Zhai X., Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images, Int. J. Remote Sens., 36, pp. 618-644, (2015); Liu Z., Yuan L., Weng L., Yang Y., A High Resolution Optical Satellite Image Dataset for Ship Recognition and Some New Baselines, In Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods - Volume 1: ICPRAM, pp. 324-331, (2017); Tong X., Lu Q., Xia G., Zhang L., Large-Scale Land Cover Classification in Gaofen-2 Satellite Imagery, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 3599-3602, (2018); Zhu H., Chen X., Dai W., Fu K., Ye Q., Jiao J., Orientation robust object detection in aerial images using deep convolutional neural network, In Proceedings of the 2015 IEEE International Conference on Image Processing (ICIP), pp. 3735-3739, (2015); Zhu X., Hu J., Qiu C., Shi Y., Kang J., Mou L., Bagheri H., Haberle M., Hua Y., Huang R., Et al., So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones Classification, IEEE Geosci. Remote Sens. Mag., (2020); Cheng G., Han J., Lu X., Remote Sensing Image Scene Classification: Benchmark and State of the Art, Proc. IEEE, 105, pp. 1865-1883, (2017); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can Semantic Labeling Methods Generalize to Any City? The Inria Aerial Image Labeling Benchmark, In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, (2017); Kang M., Ji K., Leng X., Lin Z., Contextual Region-Based Convolutional Neural Network with Multilayer Fusion for SAR Ship Detection, Remote Sens., 9, (2017); Liu W., Ma L., Chen H., Arbitrary-Oriented Ship Detection Framework in Optical Remote-Sensing Images, IEEE Geosci. Remote Sens. Lett., 15, pp. 937-941, (2018); Li Q., Mou L., Liu Q., Wang Y., Zhu X.X., HSF-Net: Multiscale Deep Feature Embedding for Ship Detection in Optical Remote Sensing Imagery, IEEE Trans. Geosci. Remote Sens., 56, pp. 7147-7161, (2018); Zhang X., Wang H., Xu C., Lv Y., Fu C., Xiao H., He Y., A Lightweight Feature Optimizing Network for Ship Detection in SAR Image, IEEE Access, 7, pp. 141662-141678, (2019); Zhang T., Zhang X., Shi J., Wei S., Depthwise Separable Convolution Neural Network for High-Speed SAR Ship Detection, Remote Sens., 11, (2019); Jiao J., Zhang Y., Sun H., Yang X., Gao X., Hong W., Fu K., Sun X., A Densely Connected End-to-End Neural Network for Multiscale and Multiscene SAR Ship Detection, IEEE Access, 6, pp. 20881-20892, (2018); Wu F., Zhou Z., Wang B., Ma J., Inshore Ship Detection Based on Convolutional Neural Network in Optical Satellite Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 4005-4015, (2018); Deng Z., Sun H., Zhou S., Zhao J., Learning Deep Ship Detector in SAR Images From Scratch, IEEE Trans. Geosci. Remote Sens., 57, pp. 4021-4039, (2019); You Y., Cao J., Zhang Y., Liu F., Zhou W., Nearshore Ship Detection on High-Resolution Remote Sensing Image via Scene-Mask R-CNN, IEEE Access, 7, pp. 128431-128444, (2019); You Y., Li Z., Ran B., Cao J., Lv S., Liu F., Broad Area Target Search System for Ship Detection via Deep Convolutional Neural Network, Remote Sens., 11, (2019); Zhang S., Wu R., Xu K., Wang J., Sun W., R-CNN-Based Ship Detection from High Resolution Remote Sensing Imagery, Remote Sens., 11, (2019); Fan W., Zhou F., Bai X., Tao M., Tian T., Ship Detection Using Deep Convolutional Neural Networks for PolSAR Images, Remote Sens., 11, (2019); Chen C., He C., Hu C., Pei H., Jiao L., A Deep Neural Network Based on an Attention Mechanism for SAR Ship Detection in Multiscale and Complex Scenarios, IEEE Access, 7, pp. 104848-104863, (2019); He Y., Sun X., Gao L., Zhang B., Ship Detection Without Sea-Land Segmentation for Large-Scale High-Resolution Optical Satellite Images, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 717-720, (2018); Gao L., He Y., Sun X., Jia X., Zhang B., Incorporating Negative Sample Training for Ship Detection Based on Deep Learning, Sensors, 19, (2019); Zhang Z., Guo W., Zhu S., Yu W., Toward Arbitrary-Oriented Ship Detection With Rotated Region Proposal and Discrimination Networks, IEEE Geosci. Remote Sens. Lett., 15, pp. 1745-1749, (2018); Wang J., Lu C., Jiang W., Simultaneous Ship Detection and Orientation Estimation in SAR Images Based on Attention Module and Angle Regression, Sensors, 18, (2018); Zhang Y., Zhang Y., Shi Z., Zhang J., Wei M., Rotationally Unconstrained Region Proposals for Ship Target Segmentation in Optical Remote Sensing, IEEE Access, 7, pp. 87049-87058, (2019); Chen J., Xie F., Lu Y., Jiang Z., Finding Arbitrary-Oriented Ships From Remote Sensing Images Using Corner Detection, IEEE Geosci. Remote Sens. Lett., (2019); Xiao X., Zhou Z., Wang B., Li L., Miao L., Ship Detection under Complex Backgrounds Based on Accurate Rotated Anchor Boxes from Paired Semantic Segmentation, Remote Sens., 11, (2019); Li M., Guo W., Zhang Z., Yu W., Zhang T., Rotated Region Based Fully Convolutional Network for Ship Detection, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 673-676, (2018); Wang T., Gu Y., Cnn Based Renormalization Method for Ship Detection in Vhr Remote Sensing Images, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 1252-1255, (2018); Fu K., Li Y., Sun H., Yang X., Xu G., Li Y., Sun X., A Ship Rotation Detection Model in Remote Sensing Images Based on Feature Fusion Pyramid Network and Deep Reinforcement Learning, Remote Sens., 10, (2018); Li S., Zhang Z., Li B., Li C., Multiscale Rotated Bounding Box-Based Deep Learning Method for Detecting Ship Targets in Remote Sensing Images, Sensors, 18, (2018); Sun J., Zou H., Deng Z., Cao X., Li M., Ma Q., Multiclass Oriented Ship Localization and Recognition In High Resolution Remote Sensing Images, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1288-1291, (2019); Voinov S., Heymann F., Bill R., Schwarz E., Multiclass Vessel Detection From High Resolution Optical Satellite Images Based On Deep Neural Networks, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 166-169, (2019); Ma J., Zhou Z., Wang B., Zong H., Wu F., Ship Detection in Optical Satellite Images via Directional Bounding Boxes Based on Ship Center and Orientation Prediction, Remote Sens., 11, (2019); Bi F., Hou J., Chen L., Yang Z., Wang Y., Ship Detection for Optical Remote Sensing Images Based on Visual Attention Enhanced Network, Sensors, 19, (2019); Feng Y., Diao W., Zhang Y., Li H., Chang Z., Yan M., Sun X., Gao X., Ship Instance Segmentation from Remote Sensing Images Using Sequence Local Context Module, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1025-1028, (2019); Dechesne C., Lefevre S., Vadaine R., Hajduch G., Fablet R., Ship Identification and Characterization in Sentinel-1 SAR Images with Multi-Task Deep Learning, Remote Sens., 11, (2019); Ma M., Chen J., Liu W., Yang W., Ship Classification and Detection Based on CNN Using GF-3 SAR Images, Remote Sens., 10, (2018); Lin H., Shi Z., Zou Z., Fully Convolutional Network With Task Partitioning for Inshore Ship Detection in Optical Remote Sensing Images, IEEE Geosci. Remote Sens. Lett., 14, pp. 1665-1669, (2017); Sun S., Lu Z., Liu W., Hu W., Li R., Shipnet for Semantic Segmentation on VHR Maritime Imagery, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6911-6914, (2018); Tang T., Zhou S., Deng Z., Lei L., Zou H., Arbitrary-Oriented Vehicle Detection in Aerial Imagery with Single Convolutional Neural Networks, Remote Sens., 9, (2017); Li Q., Mou L., Xu Q., Zhang Y., Zhu X.X., R3-Net: A Deep Network for Multioriented Vehicle Detection in Aerial Images and Videos, IEEE Trans. Geosci. Remote Sens., 57, pp. 5028-5042, (2019); Deng Z., Sun H., Zhou S., Zhao J., Zou H., Toward Fast and Accurate Vehicle Detection in Aerial Images Using Coupled Region-Based Convolutional Neural Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10, pp. 3652-3664, (2017); Schilling H., Bulatov D., Niessner R., Middelmann W., Soergel U., Detection of Vehicles in Multisensor Data via Multibranch Convolutional Neural Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 4299-4316, (2018); Audebert N., Le Saux B., Lefevre S., Segment-before-Detect: Vehicle Detection and Classification through Semantic Segmentation of Aerial Images, Remote Sens., 9, (2017); Merkle N., Azimi S.M., Pless S., Kurz F., Semantic Vehicle Segmentation in Very High Resolution Multispectral Aerial Images Using Deep Neural Networks, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5045-5048, (2019); Koga Y., Miyazaki H., Shibasaki R., A CNN-Based Method of Vehicle Detection from Aerial Images Using Hard Example Mining, Remote Sens., 10, (2018); Gao Z., Ji H., Mei T., Ramesh B., Liu X., EOVNet: Earth-Observation Image-Based Vehicle Detection Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 3552-3561, (2019); Li S., Xu Y., Zhu M., Ma S., Tang H., Remote Sensing Airport Detection Based on End-to-End Deep Transferable Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett., 16, pp. 1640-1644, (2019); Zhang P., Niu X., Dou Y., Xia F., Airport Detection on Optical Satellite Images Using Deep Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett., 14, pp. 1183-1187, (2017); Chen F., Ren R., Van de Voorde T., Xu W., Zhou G., Zhou Y., Fast Automatic Airport Detection in Remote Sensing Images Using Convolutional Neural Networks, Remote Sens., 10, (2018); Cai B., Jiang Z., Zhang H., Yao Y., Nie S., Online Exemplar-Based Fully Convolutional Network for Aircraft Detection in Remote Sensing Images, IEEE Geosci. Remote Sens. Lett., 15, pp. 1095-1099, (2018); Chen Z., Zhang T., Ouyang C., End-to-End Airplane Detection Using Transfer Learning in Remote Sensing Images, Remote Sens., 10, (2018); Wang Y., Li H., Jia P., Zhang G., Wang T., Hao X., Multi-Scale DenseNets-Based Aircraft Detection from Remote Sensing Images, Sensors, 19, (2019); Zhao P., Gao H., Zhang Y., Li H., Yang R., An Aircraft Detection Method Based on Improved Mask R-CNN in Remotely Sensed Imagery, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1370-1373, (2019); Wang H., Gong Y., Wang Y., Wang L., Pan C., DeepPlane: A unified deep model for aircraft detection and recognition in remote sensing images, J. Appl. Remote Sens., 11, pp. 1-10, (2017); Hou B., Li J., Zhang X., Wang S., Jiao L., Object Detection and Trcacking Based on Convolutional Neural Networks for High-Resolution Optical Remote Sensing Video, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5433-5436, (2019); Xu Y., Wu L., Xie Z., Chen Z., Building Extraction in Very High Resolution Remote Sensing Imagery Using Deep Learning and Guided Filters, Remote Sens., 10, (2018); Lu X., Zhong Y., Zheng Z., Liu Y., Zhao J., Ma A., Yang J., Multi-Scale and Multi-Task Deep Learning Framework for Automatic Road Extraction, IEEE Trans. Geosci. Remote Sens., 57, pp. 9362-9377, (2019); Wei Y., Wang Z., Xu M., Road Structure Refined CNN for Road Extraction in Aerial Image, IEEE Geosci. Remote Sens. Lett., 14, pp. 709-713, (2017); Li Y., Guo L., Rao J., Xu L., Jin S., Road Segmentation Based on Hybrid Convolutional Network for High-Resolution Visible Remote Sensing Image, IEEE Geosci. Remote Sens. Lett., 16, pp. 613-617, (2019); Zhang X., Ma W., Li C., Wu J., Tang X., Jiao L., Fully Convolutional Network-Based Ensemble Method for Road Extraction From Aerial Images, IEEE Geosci. Remote Sens. Lett., (2019); He H., Yang D., Wang S., Wang S., Li Y., Road Extraction by Using Atrous Spatial Pyramid Pooling Integrated Encoder-Decoder Network and Structural Similarity Loss, Remote Sens., 11, (2019); Liu Y., Yao J., Lu X., Xia M., Wang X., Liu Y., RoadNet: Learning to Comprehensively Analyze Road Networks in Complex Urban Scenes From High-Resolution Remotely Sensed Images, IEEE Trans. Geosci. Remote Sens., 57, pp. 2043-2056, (2019); Hong Z., Ming D., Zhou K., Guo Y., Lu T., Road Extraction From a High Spatial Resolution Remote Sensing Image Based on Richer Convolutional Features, IEEE Access, 6, pp. 46988-47000, (2018); Yang X., Li X., Ye Y., Lau R.Y.K., Zhang X., Huang X., Road Detection and Centerline Extraction Via Deep Recurrent Convolutional Neural Network U-Net, IEEE Trans. Geosci. Remote Sens., 57, pp. 7209-7220, (2019); Azimi S.M., Fischer P., Korner M., Reinartz P., Aerial LaneNet: Lane-Marking Semantic Segmentation in Aerial Imagery Using Wavelet-Enhanced Cost-Sensitive Symmetric Fully Convolutional Neural Networks, IEEE Trans. Geosci. Remote Sens., 57, pp. 2920-2938, (2019); Zhao W., Du S., Emery W.J., Object-Based Convolutional Neural Network for High-Resolution Imagery Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10, pp. 3386-3396, (2017); Volpi M., Tuia D., Dense Semantic Labeling of Subdecimeter Resolution Images With Convolutional Neural Networks, IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893, (2017); Audebert N., Le Saux B., Lefevre S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks, ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32, (2018); Wang Y., Liang B., Ding M., Li J., Dense Semantic Labeling with Atrous Spatial Pyramid Pooling and Decoder for High-Resolution Remote Sensing Imagery, Remote Sens., 11, (2019); Liu S., Ding W., Liu C., Liu Y., Wang Y., Li H., ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images, Remote Sens., 10, (2018); Luo H., Chen C., Fang L., Zhu X., Lu L., High-Resolution Aerial Images Semantic Segmentation Using Deep Fully Convolutional Network With Channel Attention Mechanism, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 3492-3507, (2019); Chen K., Fu K., Gao X., Yan M., Zhang W., Zhang Y., Sun X., Effective Fusion of Multi-Modal Data with Group Convolutions for Semantic Segmentation of Aerial Imagery, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 3911-3914, (2019); Zhang G., Lei T., Cui Y., Jiang P., A Dual-Path and Lightweight Convolutional Neural Network for High-Resolution Aerial Image Segmentation, ISPRS Int. J. Geo-Inf., 8, (2019); Cao Z., Diao W., Zhang Y., Yan M., Yu H., Sun X., Fu K., Semantic Labeling for High-Resolution Aerial Images Based on the DMFFNet, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1021-1024, (2019); Jia Y., Ge Y., Chen Y., Li S., Heuvelink G.B., Ling F., Super-Resolution Land Cover Mapping Based on the Convolutional Neural Network, Remote Sens., 11, (2019); Guo S., Jin Q., Wang H., Wang X., Wang Y., Xiang S., Learnable Gated Convolutional Neural Network for Semantic Segmentation in Remote-Sensing Images, Remote Sens., 11, (2019); Basaeed E., Bhaskar H., Hill P., Al-Mualla M., Bull D., A supervised hierarchical segmentation of remote-sensing images using a committee of multi-scale convolutional neural networks, Int. J. Remote Sens., 37, pp. 1671-1691, (2016); Mou L., Hua Y., Zhu X.X., Spatial Relational Reasoning in Networks for Improving Semantic Segmentation of Aerial Images, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5232-5235, (2019); Nogueira K., Dalla Mura M., Chanussot J., Schwartz W.R., dos Santos J.A., Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks, IEEE Trans. Geosci. Remote Sens., 57, pp. 7503-7520, (2019); Sun Y., Zhang X., Xin Q., Huang J., Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data, ISPRS J. Photogramm. Remote Sens., 143, pp. 3-14, (2018); Papadomanolaki M., Vakalopoulou M., Karantzalos K., A Novel Object-Based Deep Learning Framework for Semantic Segmentation of Very High-Resolution Remote Sensing Data: Comparison with Convolutional and Fully Convolutional Networks, Remote Sens., 11, (2019); Yue K., Yang L., Li R., Hu W., Zhang F., Li W., TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation, ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13, (2019); Liu Y., Gross L., Li Z., Li X., Fan X., Qi W., Automatic Building Extraction on High-Resolution Remote Sensing Imagery Using Deep Convolutional Encoder-Decoder With Spatial Pyramid Pooling, IEEE Access, 7, pp. 128774-128786, (2019); Zhang Y., Gong W., Sun J., Li W., Web-Net: A Novel Nest Networks with Ultra-Hierarchical Sampling for Building Extraction from Aerial Imageries, Remote Sens., 11, (2019); Yang H.L., Yuan J., Lunga D., Laverdiere M., Rose A., Bhaduri B., Building Extraction at Scale Using Convolutional Neural Network: Mapping of the United States, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 2600-2614, (2018); Hui J., Du M., Ye X., Qin Q., Sui J., Effective Building Extraction From High-Resolution Remote Sensing Images With Multitask Driven Deep Neural Network, IEEE Geosci. Remote Sens. Lett., 16, pp. 786-790, (2019); Wang S., Zhou L., He P., Quan D., Zhao Q., Liang X., Hou B., An Improved Fully Convolutional Network for Learning Rich Building Features, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 6444-6447, (2019); Guo Z., Wu G., Song X., Yuan W., Chen Q., Zhang H., Shi X., Xu M., Xu Y., Shibasaki R., Et al., Super-Resolution Integrated Building Semantic Segmentation for Multi-Source Remote Sensing Imagery, IEEE Access, 7, pp. 99381-99397, (2019); Lin J., Jing W., Song H., Chen G., ESFNet: Efficient Network for Building Extraction From High-Resolution Aerial Images, IEEE Access, 7, pp. 54285-54294, (2019); Schuegraf P., Bittner K., Automatic Building Footprint Extraction from Multi-Resolution Remote Sensing Images Using a Hybrid FCN, ISPRS Int. J. Geo-Inf., 8, (2019); Liu P., Liu X., Liu M., Shi Q., Yang J., Xu X., Zhang Y., Building Footprint Extraction from High-Resolution Images via Spatial Residual Inception Convolutional Neural Network, Remote Sens., 11, (2019); Ye Z., Fu Y., Gan M., Deng J., Comber A., Wang K., Building Extraction from Very High Resolution Aerial Imagery Using Joint Attention Deep Neural Network, Remote Sens., 11, (2019); Shrestha S., Vanneschi L., Improved Fully Convolutional Network with Conditional Random Fields for Building Extraction, Remote Sens., 10, (2018); Ji S., Wei S., Lu M., A scale robust convolutional neural network for automatic building extraction from aerial and satellite imagery, Int. J. Remote Sens., 40, pp. 3308-3322, (2019); Wen Q., Jiang K., Wang W., Liu Q., Guo Q., Li L., Wang P., Automatic Building Extraction from Google Earth Images under Complex Backgrounds Based on Deep Instance Segmentation Network, Sensors, 19, (2019); Shi Q., Liu M., Liu X., Liu P., Zhang P., Yang J., Li X., Domain Adaption for Fine-Grained Urban Village Extraction From Satellite Images, IEEE Geosci. Remote Sens. Lett., 17, pp. 1430-1434, (2020); Wang J., Kuffer M., Roy D., Pfeffer K., Deprivation pockets through the lens of convolutional neural networks, Remote Sens. Environ., 234, (2019); Persello C., Stein A., Deep Fully Convolutional Networks for the Detection of Informal Settlements in VHR Images, IEEE Geosci. Remote Sens. Lett., 14, pp. 2325-2329, (2017); Mboga N., Persello C., Bergado J.R., Stein A., Detection of Informal Settlements from VHR Images Using Convolutional Neural Networks, Remote Sens., 9, (2017)","T. Hoeser; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, Münchner Straße 20, D-82234, Germany; email: Thorsten.Hoeser@dlr.de","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092269929"
"Parico A.I.B.; Ahamed T.","Parico, Addie Ira Borja (57225984686); Ahamed, Tofael (9269729600)","57225984686; 9269729600","An Aerial Weed Detection System for Green Onion Crops Using the You Only Look Once (YOLOv3) Deep Learning Algorithm","2020","Engineering in Agriculture, Environment and Food","13","2","","42","48","6","7","10.37221/eaef.13.2_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125910880&doi=10.37221%2feaef.13.2_42&partnerID=40&md5=5bf03204be592391939339fec75ed4f0","Graduate School of Life and Environmental Sciences, University of Tsukuba, Japan; Faculty of Life and Environmental Sciences, University of Tsukuba, Japan","Parico A.I.B., Graduate School of Life and Environmental Sciences, University of Tsukuba, Japan; Ahamed T., Faculty of Life and Environmental Sciences, University of Tsukuba, Japan","The real-time object detection system You Only Look Once (specifically YOLOv3) has recently shown remarkable speed, making it potentially suitable for Unmanned Aerial Vehicle (UAV) precision spraying. In this study, YOLO-WEED, a weed detection system based on YOLOv3, was developed. The dataset, derived from a five-minute UAV video, was split into a 69: 17: 13 ratio for training, validation, and testing, respectively. YOLO-WEED demonstrated a real-time detection speed (up to 24.4 FPS) and high performance using NVIDIA GeForce GTX 1060, with a mean average precision of 93.81 % and an F1 score of 0.94. These results successfully show the effectiveness of the YOLO-WEED system for real-time UAV weed detection, given its high speed and high accuracy in detection. © 2020 Elsevier B.V.. All rights reserved.","convolutional neural network (CNN); deep learning; real time weed detection; unmanned aerial vehicle; you only look once (YOLO)","Aircraft detection; Antennas; Convolutional neural networks; Learning algorithms; Object detection; Statistical tests; Unmanned aerial vehicles (UAV); Aerial vehicle; Convolutional neural network; Deep learning; Detection system; Real time weed detection; Real- time; Unmanned aerial vehicle; Weed detection; You only look once; Deep learning","","","","","TAG-MEXT; University of Tsukuba","These individuals, who helped me attain the entirety of the study, are worthy of true recognition and appreciation: to my research supervisor, Associate Professor Tofael Ahamed, for sharing his insights and constructive criticisms, and for his constant support and guidance; to my colleagues who has assisted me in commencing my UAV flights and has given suggestions for the improvement of my research; To the TAG-MEXT program for providing me the financial support for my studies here in University of Tsukuba; to Associate Professor Nakao Nomura and Dr. Renato Reyes for giving me the opportunity to study in University of Tsukuba; and finally to my friends, family and partner who have given me moral support throughout my studies.","Afifi M., Ali Y., Amer K., Shaker M., ElHelw M., Robust real-time pedestrian detection on embedded devices, Proceeding of the 13th International Conference on Machine Vision, 45, (2021); Bargoti S., Underwood J., Deep fruit detection in orchards, Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 3626-3633, (2017); Czymmek V., Harders L. O., Knoll F. J., Hussmann S., Vision-based deep learning approach for real-time detection of weeds in organic farming, Proceedings of the 2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC), (2019); Du J., Understanding of object detection based on CNN family and YOLO, Journal of Physics: Conference Series, 1004, (2018); Everingham M., Van Gool L., Williams C. K. I., Winn J., Zisserman A., The pascal visual object classes (VOC) challenge, International Journal of Computer Vision, 88, pp. 303-338, (2010); Green Onions, (2016); Gao J., French A. P., Pound M. P., He Y., Pridmore T. P., Pieters J. G., Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, (2020); Gilreath J. P., Santos B. M., Gilreath P. R., Maynard D. N., Efficacy of early post-transplant herbicides in leeks (Allium porrum L.), Crop Protection, 27, pp. 847-850, (2008); He X. K., Bonds J., Herbst A., Langenakens J., Recent development of unmanned aerial vehicle for plant protection in East Asia, International Journal of Agricultural and Biological Engineering, 10, 3, pp. 18-30, (2017); Hewson R. T., Roberts H. A., Some effects of weed competition on the growth of onions, Journal of Horticultural Science, 48, 1, pp. 51-57, (1973); Krizhevsky A., Sutskever I., Hinton G. E., ImageNet classification with deep convolutional neural networks, Communications of the ACM, 60, 6, pp. 84-90, (2017); Lan Y., Thomson S. J., Huang Y., Hoffmann W. C., Zhang H., Current status and future directions of precision aerial application for site-specific crop management in the USA, Computers and Electronics in Agriculture, 74, 1, pp. 34-38, (2010); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C. L., Microsoft COCO: Common Objects in Context, Computer Vision – ECCV 2014, pp. 740-755, (2014); Montanari A., Kringberg F., Valentini A., Mascolo C., Prorok A., Surveying areas in developing regions through context aware drone mobility, Proceedings of the 4th ACM Workshop on Micro Aerial Vehicle Networks, Systems, and Applications (DroNet’18), pp. 27-32, (2018); Norsworthy J. K., Smith J. P., Meister C., Tolerance of direct-seeded green onions to herbicides applied before or after crop emergence, Weed Technology, 21, 1, pp. 119-123, (2007); Partel V., Charan Kakarla S., Ampatzidis Y., Development and evaluation of a low-cost and smart technology for precision weed management utilizing artificial intelligence, Computers and Electronics in Agriculture, 157, pp. 339-350, (2019); Qiu S., Shrivastava N., BBox-Label-Tool, (2017); Redmon J., Darknet: Open Source Neural Networks in C, (2016); Redmon J., Farhadi A., YOLOv3: An incremental improvement, (2018); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceeding of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6517-6525, (2017); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A. C., Fei-Fei L., ImageNet large scale visual recognition challenge, International Journal of Computer Vision, 115, pp. 211-252, (2015); Smolyanskiy N., Kamenev A., Smith J., Birchfield S., Toward low-flying autonomous MAV trail navigation using deep neural networks for environmental awareness, Proceeding of the 2017 IEEE / RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4241-4247, (2017); Timmermann C., Gerhards R., Kuhbauch W., The economic impact of site-specific weed control, Precision Agriculture, 4, pp. 249-260, (2003); Tsouros D. C., Bibi S., Sarigiannidis P. G., A review on UAV-based applications for precision agriculture, Information, 10, 11, (2019); Yang T., Ren Q., Zhang F., Xie B., Ren H., Li J., Zhang Y., Hybrid camera array-based UAV auto-landing on moving UGV in GPS-denied environment, Remote Sensing, 10, 11, (2018); Zhang J.M., Harman M., Ma L., Liu Y., Machine learning testing: survey, landscapes and horizons, IEEE Transactions on Software Engineering, (2020); Zhang R., Wang C., Hu X., Liu Y., Chen S., Su B., Weed location and recognition based on UAV imaging and deep learning, International Journal of Precision Agricultural Aviation, 3, 1, pp. 23-29, (2018)","T. Ahamed; Faculty of Life and Environmental Sciences, University of Tsukuba, Japan; email: tofael.ahamed.gp@u.tsukuba.ac.jp","","Asian Agricultural and Biological Engineering Association","","","","","","18818366","","","","English","Eng. Agric. Environ. Food","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85125910880"
"Gong J.; Ji S.","Gong, Jianya (23008015000); Ji, Shunping (9633134900)","23008015000; 9633134900","Photogrammetry and Deep Learning","2018","Cehui Xuebao/Acta Geodaetica et Cartographica Sinica","47","6","","693","704","11","47","10.11947/j.AGCS.2018.20170640","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050493742&doi=10.11947%2fj.AGCS.2018.20170640&partnerID=40&md5=c59afb26e3847d0579a1329fd07ee71e","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China","Gong J., School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; Ji S., School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China","Deep learning has become popular and the mainstream in types of researches related to learning, and has shown its impact on photogrammetry. According to the definition of photogrammetry, a subject that researches shapes, locations, sizes, characteristics and inter-relationships of real objects from optical images, photogrammetry considers two aspects, geometry and semantics. From the two aspects, we review the history of deep learning and discuss its current applications on photogrammetry, and forecast the future development of photogrammetry. In geometry, the deep convolutional neural network (CNN) has been widely applied in stereo matching, SLAM and 3D reconstruction, and has made some effect but needs more improvement. In semantics, conventional empirical and handcrafted methods have failed to extract the semantic information accurately and failed to produce types of “semantic thematic map” as 4D productions (DEM, DOM, DLG, DRG) of photogrammetry, which causes the semantic part of photogrammetry be ignored for a long time. The powerful generalization capacity, ability to fit any functions and stability under types of situations of deep leaning is making the automated production of thematic maps possible. We review the achievements that have been obtained in road network extraction, building detection and crop classification, etc., and forecast that producing high-accuracy semantic thematic maps directly from optical images will become reality and these maps will become a type of standard products of photogrammetry. At last, we introduce two current researches related to geometry and semantics respectively. One is stereo matching of aerial images based on deep learning and transfer learning; the other is fine crop classification from satellite special-temporal images based on 3D CNN. © 2018, Surveying and Mapping Press. All right reserved.","Convolutional neural network; Deep learning; Photogrammetry; Stereo matching; Thematic map","Antennas; Convolution; Crops; Deep learning; Deep neural networks; Geometrical optics; Geometry; Maps; Neural networks; Semantics; Stereo image processing; Automated productions; Convolutional neural network; Deep convolutional neural networks; Generalization capacity; Road network extraction; Semantic information; Stereo matching; Thematic maps; artificial neural network; image classification; machine learning; photogrammetry; stereo image; thematic mapping; Photogrammetry","","","","","National Natural Science Foundation of China, NSFC, (41471288)","The National Natural Science Foundation of China(No. 41471288).","Gong J., Ji S., From Photogrammetry to Computer Vision, Geomatics and Information Science of Wuhan University, 42, 11, pp. 1518-1522, (2017); Boyle W.S., Smith G.E., Charge Coupled Semiconductor Devices, The Bell System Technical Journal, 49, 4, pp. 587-593, (1970); Ashby W.R., An Introduction to Cybernetics, (1961); Fodor J.A., Pylyshyn Z.W., Connectionism and Cognitive Architecture: A Critical Analysis, Cognition, 28, 1-2, pp. 3-71, (1988); Hinton G.E., Osindero S., Teh Y.W., A Fast Learning Algorithm for Deep Belief Nets, Neural Computation, 18, 7, pp. 1527-1554, (2006); Suykens J.A.K., Vanderwalle J., Least Squares Support Vector Machine Classifiers, Neural Processing Letters, 9, 3, pp. 293-300, (1999); Koller D., Friedman N., Probabilistic Graphical Models: Principles and Techniques, (2009); Bengio Y., Lamblin P., Popovici D., Et al., Greedy Layer-Wise Training of Deep Networks, Proceedings of the 19th International Conference on Neural Information Processing Systems, pp. 153-160, (2006); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet Classification with Deep Convolutional Neural Networks, Proceedings of the 25th International Conference on Neural Information Processing Systems, pp. 1097-1105, (2012); Mehta P., Schwab D.J., An Exact Mapping between the Variational Renormalization Group and Deep Learning, (2014); Tishby N., Pereira F.C., Bialek W., The Information Bottleneck Method, (2000); Hinton G., Deng L., Yu D., Et al., Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups, IEEE Signal Processing Magazine, 29, 6, pp. 82-97, (2012); Lecun Y., Boser B., Denker J.S., Et al., Backpropagation Applied to Handwritten Zip Code Recognition, Neural Computation, 1, 4, pp. 541-551, (1989); Kendall A., Grimes M., Cipolla R., Posenet: A Convolutional Network for Real-time 6-dof Camera Relocalization, Proceedings of 2015 IEEE International Conference on Computer Vision, pp. 2938-2946, (2015); Kitti, The KITTI Vision Benchmark Suite; Bengio Y., Courville A., Vincent P., Representation Learning: A Review and New Perspectives, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 8, pp. 1798-1828, (2013); Ng A., Sparse Autoencoder, CS294A Lecture Notes, 72, 2011, pp. 1-19, (2011); Sanger T.D., Optimal Unsupervised Learning in A Single-layer Linear Feedforward Neural Network, Neural Networks, 2, 6, pp. 459-473, (1989); Ruck D.W., Rogers S.K., Kabrisky M., Et al., The Multilayer Perceptron as an Approximation to ABayes Optimal Discriminant Function, IEEE Transactions on Neural Networks, 1, 4, pp. 296-298, (1990); Mikolov T., Karafiat M., Burget L., Et al., Recurrent Neural Network Based Language Model, Proceedings of the 11th Annual Conference of the International Speech Communication Association, 2, (2010); Minsky M.L., Papert S.A., Perceptrons, (1969); Nair V., Hinton G.E., Rectified Linear Units Improve Restricted Boltzmann Machines, Proceedings of the 27th International Conference on Machine Learning, pp. 807-814, (2010); Shore J., Johnson R., Axiomatic Derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-entropy, IEEE Transactions on Information Theory, 26, 1, pp. 26-37, (1980); More J.J., The Levenberg-Marquardt Algorithm: Implementation and Theory, Numerical Analysis, pp. 105-116, (1978); Le Cun Y., Boser B.E., Denker J.S., Et al., Handwritten Digit Recognition with a Back-propagation Network, Advances in Neural Information Processing Systems, pp. 396-404, (1990); Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); Horn B., Robot Vision, (1986); Graham B., Fractional Max-pooling, (2014); Zeiler M.D., Fergus R., Visualizing and Understanding Convolutional Networks, European Conference on Computer Vision, pp. 818-833, (2014); Szegedy C., Liu W., Jia Y., Et al., Going Deeper with Convolutions, (2014); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); He K., Zhang X., Ren S., Et al., Deep Residual Learning for Image Recognition, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Kendall A., Cipolla R., Modelling Uncertainty in Deep Learning for Camera Relocalization, Proceedings of 2016 IEEE International Conference on Robotics and Automation, pp. 4762-4769, (2016); Zbontar J., Lecun Y., Computing the Stereo Matching Cost with a Convolutional Neural Network, Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1592-1599, (2015); Kendall A., Martirosyan H., Dasgupta S., Et al., End-to-end Learning of Geometry and Context for Deep Stereo Regression, Proceedings of the IEEE Conference on Computer Vision, pp. 66-75, (2017); Seki A., Pollefeys M., SGM-Nets: Semi-global Matching with Neural Networks, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 21-26, (2017); Mayer N., Ilg E., Hausser P., Et al., A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 4040-4048, (2016); Luo W., Schwing A.G., Urtasun R., Efficient Deep Learning for Stereo Matching, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5695-5703, (2016); Marr D., Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, (1982); Cheng G., Wang Y., Xu S., Et al., Automatic Road Detection and Centerline Extraction via Cascaded End-to-end Convolutional Neural Network, IEEE Transactions on Geoscience and Remote Sensing, 55, 6, pp. 3322-3337, (2017); Li P., Zang Y., Wang C., Et al., Road Network Extraction via Deep Learning and Line Integral Convolution, Proceedings of the IEEE Conference on Geoscience and Remote Sensing Symposium (IGARSS), pp. 1599-1602, (2016); Mnih V., Hinton G.E., Learning to Detect Roads in High-resolution Aerial Images, Proceedings of the 11th European Conference on Computer Vision, pp. 210-223, (2010); Wang J., Song J., Chen M., Et al., Road Network Extraction: A Neural-dynamic Framework Based on Deep Learning and a Finite State Machine, International Journal of Remote Sensing, 36, 12, pp. 3144-3169, (2015); Panboonyuen T., Jitkajornwanich K., Lawawirojwong S., Et al., Road Segmentation of Remotely-sensed Images Using Deep Convolutional Neural Networks with Landscape Metrics and Conditional Random Fields, Remote Sensing, 9, 7, (2017); Vakalopoulou M., Karantzalos K., Komodakis N., Et al., Building Detection in Very High Resolution Multispectral Data with Deep Learning Features, Proceedings of the IEEE Conference on Geoscience and Remote Sensing Symposium (IGARSS), pp. 1873-1876, (2015); Kussul N., Lavreniuk M., Skakun S., Et al., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geoscience and Remote Sensing Letters, 14, 5, pp. 778-782, (2017); Castelluccio M., Poggi G., Sansone C., Et al., Land Use Classification in Remote Sensing Images by Convolutional Neural Networks, (2015); Zhang L., Zhang L., Du B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art, IEEE Geoscience and Remote Sensing Magazine, 4, 2, pp. 22-40, (2016)","S. Ji; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; email: jishunping@whu.edu.cn","","SinoMaps Press","","","","","","10011595","","CEXUE","","Chinese","Cehui Xuebao","Article","Final","","Scopus","2-s2.0-85050493742"
"Larionov R.; Khryashchev V.; Pavlov V.","Larionov, Roman (57211028940); Khryashchev, Vladimir (13006007400); Pavlov, Vladimir (57197879833)","57211028940; 13006007400; 57197879833","Separation of Closely Located Buildings on Aerial Images Using U-Net Neural Network","2020","Conference of Open Innovation Association, FRUCT","2020-April","","9087365","256","261","5","3","10.23919/FRUCT48808.2020.9087365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084983038&doi=10.23919%2fFRUCT48808.2020.9087365&partnerID=40&md5=5c405ca69e5ba092e2f0e179e7cb3223","P.G. Demidov Yaroslavl State University, Yaroslavl, Russian Federation","Larionov R., P.G. Demidov Yaroslavl State University, Yaroslavl, Russian Federation; Khryashchev V., P.G. Demidov Yaroslavl State University, Yaroslavl, Russian Federation; Pavlov V., P.G. Demidov Yaroslavl State University, Yaroslavl, Russian Federation","Deep learning and modern type of neural network technologies are increasingly used for the detection, segmentation and classification of different objects in aerial multichannel images. The goal of given research was to develop a deep learning algorithm for automated building detection on four-channel satellite images. It is proposed to use U-Net neural network with two decoders to separate objects, one decoder is trained to segment buildings and structures, and the other detects narrow distances between buildings. It is shown that optimized U-Net can be used to detect such kind of objects efficiently. The model was implemented by means of open Keras library and launched on modern GPUs of high-performance supercomputer NVIDIA DGX-1. Before testing of developed algorithm on Planet aerial image dataset, modified U-Net had been pre-trained on SpaceNet database. The process of image data augmentation is described. The problem of effective building detection on high-resolution aerial photos can be used for urban planning, building control, search of the best locations for future outlets etc. © 2020 FRUCT.","","Buildings; Decoding; Deep learning; Image segmentation; Learning algorithms; Object detection; Program processors; Statistical tests; Supercomputers; Aerial photos; Automated buildings; Building controls; Building detection; High resolution; Multichannel images; Network technologies; Satellite images; Antennas","","","","","P.G. Demidov Yaroslavl State University","This work was supported by a grant from the P.G. Demidov Yaroslavl State University ʋ Ɉɉ-2Ƚ-09-2019","Zhu X.X., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geoscience and Remote Sensing Magazine, 5, 4, pp. 8-36, (2017); Chen X., Xiang S., Liu C., Pan C., Vehicle detection in satellite images by hybrid deep convolutional neural networks, IEEE Geoscience and Remote Sensing Letters, 11, 10, pp. 1797-1801, (2015); Venkatesan R., Li B., Convolutional Neural Networks in Visual Computing: A Concise Guide, (2017); Shelhamer E., Long J., Darrell T., Fully Convolutional Networks for Semantic Segmentation; Geron A., Hands-On Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools and Techniques to Build Intelligent Systems, (2019); Seferbekov S., Iglovikov V., Buslaev A., Shvets A., Feature Pyramid Network for Multi-Class Land Segmentation; Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Medical Image Computing and Computer-Assisted Intervention (Miccai), 9351, pp. 234-241, (2015); Khryashchev V., Ivanovsky L., Pavlov V., Ostrovskaya A., Rubtsov A., Comparison of different convolutional neural network architectures for satellite image segmentation, Proceedings of the 23rd Conference of Open Innovations Association FRUCT'23, pp. 172-179, (2018); Zhao T., Yang Y., Niu H., Wang D., Comparing u-net convolutional network with mask R-CNN in the performances of pomegranate tree canopy segmentation, Proceedings of SPIE, 1078, pp. 1-9, (2018); Bai M., Urtasun R., Deep Watershed Transform for Instance Segmentation; GeoEye-1 Satellite Images; Pleiades-1B Satellite Sensor; SpaceNet Database; Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geoscience and Remote Sensing Magazine, 4, 2, pp. 22-40, (2016); Weijia L., Conghui H., Jiarui F., Juepeng Z., Semantic segmentation-based building footprint extraction using very high-resolution satellite images and multi-source gis data, Remote Sensing, 11, 4, pp. 1-19; Ohleyer S., Building Segmentation on Satellite Images; Ivanovsky L., Khryashchev V., Pavlov V., Ostrovskaya A., Building detection on aerial images using u-net neural networks, Proceedings of the 24rd Conference of Open Innovations Association FRUCT'24, pp. 116-122, (2019); Khryashchev V., Ivanovsky L., Ostrovskaya A Semenov A., Application of satellite image segmentation for urban planning optimization, Proceedings of 2019 the 9th International Workshop on Computer Science and Engineering, pp. 171-175, (2019); Marmanis D., Schindler K., Wegner J.D., Galliani S., Classification with An Edge: Improving Semantic Image Segmentation with Boundary Detection; Wilt N., The CUDA Handbook. A Comprehensive Guide to GPU Programming, (2013); Atienza R., Advanced Deep Learning with Keras; Kingma D., Ba J., Adam: A Method for Stochastic Optimization; Losses for Image Segmentation","","Balandin S.; Paramonov I.; Tyutina T.","IEEE Computer Society","","26th Conference of Open Innovations Association FRUCT, FRUCT 2020","23 April 2020 through 24 April 2020","Yaroslavl","159716","23057254","978-952692442-7","","","English","Conf. Open Innov. Assoc., FRUCT","Conference paper","Final","","Scopus","2-s2.0-85084983038"
"Shill A.; Rahman M.A.","Shill, Apu (57272064000); Rahman, Md. Asifur (57207376228)","57272064000; 57207376228","Plant disease detection based on YOLOv3 and YOLOv4","2021","2021 International Conference on Automation, Control and Mechatronics for Industry 4.0, ACMI 2021","","","","","","","10","10.1109/ACMI53878.2021.9528179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115699851&doi=10.1109%2fACMI53878.2021.9528179&partnerID=40&md5=f6eb0cf6306b5c3f7b0410a941cff93b","Rajshahi University of Engineering and Technology, Computer Science and Engineering, Rajshahi, Bangladesh","Shill A., Rajshahi University of Engineering and Technology, Computer Science and Engineering, Rajshahi, Bangladesh; Rahman M.A., Rajshahi University of Engineering and Technology, Computer Science and Engineering, Rajshahi, Bangladesh","Although the loss of yield from plant disease can be precluded with early detection and treatment, they cause significant economic losses every year due to the dearth of cost-effective expert knowledge. Not only they are a great threat to the agricultural economy but also they are pernicious to the balance of the natural ecosystem. Many studies have been conducted to develop expert systems to alleviate losses by detecting the diseases at their earlier stage. However, most of these studies are limited to one or two plant species and a few varieties of diseases. This paper introduces a computer vision approach using the latest state of art You Only Look Once (YOLO) algorithms in developing an expert system, which can accurately identify numerous plant diseases from a diverse set of plant species. The latest algorithms from the YOLO family namely YOLO V3 and YOLO V4 have been incorporated in this research and the experimental result presented in this paper clearly reflects on the accuracy and suitability of the plant disease detection system.  © 2021 IEEE.","Computer Vision; Convolutional Neural Networks; Deep Learning; Object detection and recognition; Plant disease detection; PlantDoc dataset; YOLOv3; YOLOv4; You Only Look Once","Agricultural robots; Agriculture; Cost effectiveness; Expert systems; Industry 4.0; Losses; Agricultural economy; Cost effective; Economic loss; Expert knowledge; Natural ecosystem; Plant disease; Plant species; Arts computing","","","","","","","Chandra A.L., Desai S.V., Guo W., Balasubramanian V.N., Computer Vision with Deep Learning for Plant Phenotyping in Agriculture: A Survey, (2020); Oerke E.-C., Dehne H.-W., Schonbeck F., Weber A., Crop Production and Crop Protection: Estimated Losses in Major Food and Cash Crops, (2012); Agrios G., Plant Pathology 5th Edition: Elsevier Academic Press, pp. 79-103, (2005); Mohapatra T., Alagusundaram K., Jena J., Rathore N., Singh A., Singh S., Icar News, (2018); Dusseldorf M., Save Food; Sankaran S., Mishra A., Ehsani R., Davis C., A review of advanced techniques for detecting plant diseases, Computers and Electronics in Agriculture, 72, 1, pp. 1-13, (2010); Patil S.B., Bodhe S.K., Leaf disease severity measurement using image processing, International Journal of Engineering and Technology, 3, 5, pp. 297-301, (2011); Patil S.B., Bodhe S.K., Betel leaf area measurement using image processing, International Journal on Computer Science and Engineering, 3, 7, pp. 2656-2660, (2011); Grinblat G.L., Uzal L.C., Larese M.G., Granitto P.M., Deep learning for plant identification using vein morphological patterns, Computers and Electronics in Agriculture, 127, pp. 418-424, (2016); Fuentes A., Yoon S., Kim S.C., Park D.S., A robust deep-learningbased detector for real-time tomato plant diseases and pests recognition, Sensors, 17, 9, (2017); Barbedo J.G., Factors influencing the use of deep learning for plant disease recognition, Biosystems Engineering, 172, pp. 84-91, (2018); Chaari R., Ellouze F., Koubaa A., Qureshi B., Pereira N., Youssef H., Tovar E., Cyber-physical systems clouds: A survey, Computer Networks, 108, pp. 260-278, (2016); Koubaa A., Qureshi B., Sriti M.-F., Javed Y., Tovar E., A serviceoriented cloud-based management system for the internet-of-drones, 2017 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), pp. 329-335, (2017); Benjdira B., Khursheed T., Koubaa A., Ammar A., Ouni K., Car detection using unmanned aerial vehicles: Comparison between faster r-cnn and yolov3, 2019 1st International Conference on Unmanned Vehicle Systems-Oman (UVS), pp. 1-6, (2019); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A., Yolo9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271, (2017); Redmon J., Farhadi A., Yolov3: An Incremental Improvement, (2018); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., Yolov4: Optimal Speed and Accuracy of Object Detection, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Girshick R., Fast r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-cnn: Towards Realtime Object Detection with Region Proposal Networks, (2015); Wang C.-Y., Bochkovskiy A., Liao H.-Y.M., Scaled-yolov4: Scaling Cross Stage Partial Network, (2020); Singh D., Jain N., Jain P., Kayal P., Kumawat S., Batra N., Plantdoc: A dataset for visual plant disease detection, Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, pp. 249-253, (2020); Adigun J.G., Ground Truth Data for Object Detection in Autonomous Vehicle from A Driving Simulator; Mohanty S.P., Hughes D.P., Salathe M., Using deep learning for image-based plant disease detection, Frontiers in Plant Science, 7, (2016)","","","Institute of Electrical and Electronics Engineers Inc.","","2021 International Conference on Automation, Control and Mechatronics for Industry 4.0, ACMI 2021","8 July 2021 through 9 July 2021","Rajshahi","171642","","978-166543843-8","","","English","Int. Conf. Autom., Control Mechatronics Ind. 4.0, ACMI","Conference paper","Final","","Scopus","2-s2.0-85115699851"
"Albinali H.; Alzahrani F.A.","Albinali, Hussah (56180002000); Alzahrani, Fatimah Abdulraheem (57219483652)","56180002000; 57219483652","Faster R-CNN for detecting regions in human-annotated micrograph images","2021","2021 International Conference of Women in Data Science at Taif University, WiDSTaif 2021","","","9430211","","","","1","10.1109/WIDSTAIF52235.2021.9430211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107464963&doi=10.1109%2fWIDSTAIF52235.2021.9430211&partnerID=40&md5=274ca103c38fa6eacc3b7a906de2750e","Information and Computer Science (ICS) Department, King Fahd University of Petroleum Minerals, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, Dhahran, Saudi Arabia; Information and Computer Science (ICS) Department, King Fahd University of Petroleum Minerals, Deanship of Preparatory Year and Supporting Studies, Imam Abdulrahman Bin Faisal University, Dhahran, Saudi Arabia","Albinali H., Information and Computer Science (ICS) Department, King Fahd University of Petroleum Minerals, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, Dhahran, Saudi Arabia; Alzahrani F.A., Information and Computer Science (ICS) Department, King Fahd University of Petroleum Minerals, Deanship of Preparatory Year and Supporting Studies, Imam Abdulrahman Bin Faisal University, Dhahran, Saudi Arabia","Object detection is a computer vision problem that involves classifying objects and identifying their location in a picture, video, or webcam feed. Applying deep learning techniques to this problem has led to huge contributions and attracted much research in this field. In this work, we use Faster RCNN to detect objects in micrograph images that have been manually annotated. We considered a Faster RCNN model because of its speed and its high accuracy of detection. The objective of this work is to detect objects in medical images and to evaluate the performance of Faster RCNN for medical images. We also compared the detecting highly overlapped objects and non-overlapping objects. Our experiment included 65 micrograph images with approximately 1000 objects, with findings that demonstrate that Faster RCNN obtained highly accurate results with mPA 76% for non overlapping objects. © 2021 IEEE.","Blood cells; CNN; Deep learning; Fast R-CNN; Faster R-CNN; Medical images; R-CNN","Convolutional neural networks; Data Science; Deep learning; Image annotation; Medical imaging; Computer vision problems; High-accuracy; Highly accurate; Learning techniques; Object detection","","","","","King Fahd University of Petroleum and Minerals, KFUPM","This work was conducted by the authors in completion of a graduate course project proposed and supervised by Dr. Rabeah Al-Zaidy, ICS department, KFUPM.","Guide to Build Faster RCNN in PyTorcRunning on the GPU - Deep Learning and Neural Networks with Python and Pytorchh, (2019); Bodla N., Singh B., Chellappa R., Davis L.S., Soft-nms-improving object detection with one line of code, Proceedings of the IEEE International Conference on Computer Vision, pp. 5561-5569, (2017); Davis J., Goadrich M., The relationship between precision-recall and roc curves, Proceedings of the 23rd International Conference on Machine Learning, pp. 233-240, (2006); Felzenszwalb P.F., Huttenlocher D.P., Efficient graph-based image segmentation, International Journal of Computer Vision, 59, 2, pp. 167-181, (2004); Frome A., Cheung G., Abdulkader A., Zennaro M., Wu B., Bissacco A., Adam H., Neven H., Vincent L., Large-scale privacy protection in google street view, 2009 IEEE 12th International Conference on Computer Vision, pp. 2373-2380, (2009); Girshick R., Fast R-CNn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Hung J., Carpenter A., Applying faster r-cnn for object detection on malaria images, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 56-61, (2017); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Lee C., Kim H.J., Oh K.W., Comparison of faster r-cnn models for object detection, 2016 16th International Conference on Control, Automation and Systems (ICCAS), pp. 107-110, (2016); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks, Remote Sensing, 11, 1, (2019); BCCD Dataset, (2017); Padilla S.L.N.R., da Silva E.A.B., Survey on Performance Metrics for Object-Detection Algorithms, (2020); Padilla S.L.N.R., da Silva E.A.B., Survey on Performance Metrics for Object-Detection Algorithms, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, pp. 91-99, (2015); Roblot V., Giret Y., Antoun M.B., Morillot C., Chassin X., Cotten A., Zerbib J., Fournier L., Artificial intelligence to diagnose meniscus tears on mri, Diagnostic and Interventional Imaging, 100, 4, pp. 243-249, (2019); Roh M.-C., Lee J.-Y., Refining faster-rcnn for accurate object detection, 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA), pp. 514-517, (2017); Sa R., Owens W., Wiegand R., Studin M., Capoferri D., Barooha K., Greaux A., Rattray R., Hutton A., Cintineo J., Et al., Intervertebral Disc Detection in X-Ray Images Using Faster R-Cnn: A Deep Learning Approach; Uijlings J.R.R., van de Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, International Journal of Computer Vision, 104, 2, pp. 154-171, (2013); Vaidya B., Paunwala C., Deep learning architectures for object detection and classification, Smart Techniques for a Smarter Planet, pp. 53-79, (2019); Wiegand R., Kettner N.W., Brahee D., Marquina N., Cervical spine geometry correlated to cervical degenerative disease in a symptomatic group, Journal of Manipulative and Physiological Therapeutics, 26, 6, pp. 341-346, (2003); Xue Y., Ray N., Cell Detection in Microscopy Images with Deep Convolutional Neural Network and Compressed Sensing, (2017); Zhang J., Hu H., Chen S., Huang Y., Guan Q., Cancer cells detection in phase-contrast microscopy images based on faster r-cnn, 2016 9th International Symposium on Computational Intelligence and Design (ISCID), 1, pp. 363-367, (2016)","","","Institute of Electrical and Electronics Engineers Inc.","","2021 International Conference of Women in Data Science at Taif University, WiDSTaif 2021","30 March 2021 through 31 March 2021","Taif","169137","","978-166544948-9","","","English","Int. Conf. Women Data Sci. Taif Univ., WiDSTaif","Conference paper","Final","","Scopus","2-s2.0-85107464963"
"Garcia H.; Ochoa-Zezzatti A.; Martínez-Retamoza A.; Ochoa G.; Aguilar L.; Oliva D.; Mejía J.","Garcia, Humberto (57216270551); Ochoa-Zezzatti, Alberto (57215840641); Martínez-Retamoza, Abraham (57216272828); Ochoa, Gilberto (55388097000); Aguilar, Lina (35331718100); Oliva, Diego (55391699600); Mejía, José (36669001900)","57216270551; 57215840641; 57216272828; 55388097000; 35331718100; 55391699600; 36669001900","Use of Deep Learning for Bird Detection to Reduction of Collateral Damage in Fruit Fields","2020","Advances in Intelligent Systems and Computing","1153 AISC","","","381","392","11","0","10.1007/978-3-030-44289-7_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082985736&doi=10.1007%2f978-3-030-44289-7_36&partnerID=40&md5=a6bd36a69a830264cc7bed9769712105","Instituto Tecnológico de Ciudad Juárez, Ciudad Juárez, Mexico; Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, Mexico; ITESM Campus Guadalajara, Zapopan, Mexico; Universidad Autónoma de Guadalajara, Zapopan, Mexico; CUCEI, UdG, Guadalajara, Mexico","Garcia H., Instituto Tecnológico de Ciudad Juárez, Ciudad Juárez, Mexico; Ochoa-Zezzatti A., Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, Mexico; Martínez-Retamoza A., Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, Mexico; Ochoa G., ITESM Campus Guadalajara, Zapopan, Mexico; Aguilar L., Universidad Autónoma de Guadalajara, Zapopan, Mexico; Oliva D., CUCEI, UdG, Guadalajara, Mexico; Mejía J., Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, Mexico","Since the beginning of agriculture there have been all kinds of pests that have reduced crop production. Throughout this time, different methods of scarring have been used to control pests and increase crop yields. In the same way today, pest control has other goals compared to before, one of the main objectives is the integrity of the animal to be treated, this is intended to preserve the ecosystem balance and the protection of the environment in which it develops. Pest detection today has become one of the most important factors to consider in this maintenance area in agricultural and fruit fields. This document is mainly focused on the detection of flocks of birds that are considered as pests in apple orchards, in Cuauhtémoc Chihuahua, although, the same method can be applied for different production areas. © 2020, Springer Nature Switzerland AG.","Bird detection; CNN; Convolutional Neural Network; Flocks; Object detection","Agricultural robots; Birds; Convolutional neural networks; Crops; Cultivation; Damage detection; Fruits; Object detection; Apple orchards; Bird detection; Collateral damage; Crop production; Ecosystem balance; Flocks; Production area; Protection of the environments; Deep learning","","","","","","","Bishop J., McKay H., Parrott D., Allan J., Review of international research literature regarding the effectiveness of auditory bird scaring techniques and potential alternatives, Central Science Laboratories, (2003); Hussain M., Bird J.J., Faria D.R., A Study on CNN Transfer Learning for Image Classification, (2018); Duan G., Wang H., Liu Z., Chen Y., A machine learning-based framework for automatic visual inspection of microdrill bits in PCB production, IEEE Trans. Syst. Man Cybern., 42, 6, pp. 1679-1689, (2012); Saqib M., Khan S.D., Sharma N., Blumenstein M., A Study on Detecting Drones Using Deep Convolutional Neural Networks, (2007); Zhang N., Donahue J., Girshick R., Darrell T., Part-Based R-Cnns for Fine-Grained Category Detection, (2014); Huber A., Weiss A., Developing Human-Robot Interaction for an Industry 4.0 Robot. Institute of Automation and Control Vienna University of Technology, (2017); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet Classification with Deep Convolutional Neural Networks, (2012); Alom Z., Bontupalli V.R., Taha T.M., Intrusion Detection Using Deep Belief Network and Extreme Learning Machine, (2016); Jiang P., Chen Y., Liu B., Real-Time Detection of Apple Leaf Diseases Using Deep Learning Approach Based on Improved Convolutional Neural Networks, (2019); Xiao Y., Xing C., Zhang T., Zhao Z., An intrusion detection model based on feature reduction and convolutional neural networks, College of Information and Communication Engineering, Harbin Engineering University, Harbin, P.R. China, (2019); Girshick R., Donahue J., Darrell T., Malik J., Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, (2014); Adams S.M., Friedland C.J., A Survey of Unmanned Aerial Vehicle (UAV) Usage for Imagery Collection in Disaster Research and Management, (2011); Seo J., Park H., Object recognition in very low resolution images using deep collaborative learning, School of Computer Science and Engineering, Kyungpook National University, Buk-Gu, Korea, (2019); Ma C., An W., Lei Y., Guo Y., BV-CNNs: Binary volumetric convolutional networks for 3D object recognition, College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China, (2017); Si M., Tarnoczi T.J., Wiens B., Du K., Development of Predictive Emissions Monitoring System Using Open Source Machine Learning Library – Keras: A Case Study on A Cogeneration, (2019); Hossain S., Lee D.J., Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices, Sensors, 19, 15, (2019)","A. Ochoa-Zezzatti; Universidad Autónoma de Ciudad Juárez, Ciudad Juárez, Mexico; email: alberto.ochoa@uacj.mx","Hassanien A.-E.; Azar A.T.; Gaber T.; Oliva D.; Tolba F.M.","Springer","","1st International Conference on Artificial Intelligence and Computer Visions, AICV 2020","8 April 2020 through 10 April 2020","Cairo","238759","21945357","978-303044288-0","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85082985736"
"Zamboni P.; Junior J.M.; Silva J.A.; Miyoshi G.T.; Matsubara E.T.; Nogueira K.; Gonçalves W.N.","Zamboni, Pedro (57204799515); Junior, José Marcato (55640064500); Silva, Jonathan de Andrade (23396702400); Miyoshi, Gabriela Takahashi (57192677861); Matsubara, Edson Takashi (23393072700); Nogueira, Keiller (56385805000); Gonçalves, Wesley Nunes (23396539500)","57204799515; 55640064500; 23396702400; 57192677861; 23393072700; 56385805000; 23396539500","Benchmarking anchor-based and anchor-free state-of-the-art deep learning methods for individual tree detection in rgb high-resolution images","2021","Remote Sensing","13","13","2482","","","","16","10.3390/rs13132482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109397264&doi=10.3390%2frs13132482&partnerID=40&md5=73bc26bba41674be78d58353790c0236","Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, 19060-900, Brazil; Computing Science and Mathematics Division, Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, United Kingdom","Zamboni P., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Junior J.M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Silva J.A., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Miyoshi G.T., Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, 19060-900, Brazil; Matsubara E.T., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Nogueira K., Computing Science and Mathematics Division, Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, United Kingdom; Gonçalves W.N., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil, Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil","Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 × 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Object detection; Remote sensing","Antennas; Climate change; Forestry; Geographical distribution; Information management; Learning systems; Population statistics; Remote sensing; Urban growth; Ground sample distances; High resolution image; High-resolution aerial images; Individual tree detections; Population growth; Remote sensing applications; Statistically significant difference; Urban environments; Deep learning","","","","","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, (59/300.066/2015, 88881.311850/2018-01); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303559/2019-5, 304052/2019-1, 433783/2018-4); Universidade Federal de Mato Grosso do Sul, UFMS","Funding text 1: This research was partially funded by CNPq (p: 433783/2018-4, 303559/2019-5 and 304052/2019-1), CAPES Print (p: 88881.311850/2018-01) and Fundect (p: 59/300.066/2015).The authors acknowledge the support of the UFMS (Federal University of Mato Grosso do Sul) and CAPES (Finance Code 001).; Funding text 2: Acknowledgments: The authors acknowledge the support of the UFMS (Federal University of Mato Grosso do Sul) and CAPES (Finance Code 001).","McDonald R.I., Mansur A.V., Ascensao F., Colbert M., Crossman K., Elmqvist T., Gonzalez A., Guneralp B., Haase D., Hamann M., Et al., Research gaps in knowledge of the impact of urban growth on biodiversity, Nat. Sustain, 3, pp. 16-24, (2020); Ke J., Zhang J., Tang M., Does city air pollution affect the attitudes of working residents on work, government, and the city? An examination of a multi-level model with subjective well-being as a mediator, J. Clean. Prod, 265, (2021); Khomenko S., Cirach M., Pereira-Barboza E., Mueller N., Barrera-Gomez J., Rojas-Rueda D., de Hoogh K., Hoek G., Nieuwenhuijsen M., Premature mortality due to air pollution in European cities: A health impact assessment, Lancet Planet. Health, (2021); Abass K., Buor D., Afriyie K., Dumedah G., Segbefi A.Y., Guodaar L., Garsonu E.K., Adu-Gyamfi S., Forkuor D., Ofosu A., Et al., Urban sprawl and green space depletion: Implications for flood incidence in Kumasi, Ghana, Int. J. Disaster Risk Reduct, 51, (2020); (2015); Li H., Zhang S., Qian Z., Xie X.H., Luo Y., Han R., Hou J., Wang C., McMillin S.E., Wu S., Et al., Short-term effects of air pollution on cause-specific mental disorders in three subtropical Chinese cities, Environ. Res, 191, (2020); Heinz A., Deserno L., Reininghaus U., Urbanicity, social adversity and psychosis, World Psychiatry, 12, pp. 187-197, (2013); Summary for Policymakers. Climate Change 2013: The Physical Science Basis Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change, (2013); Managing the Risks of Extreme Events and Disasters to Advance Climate Change Adaptation, (2012); Fasihi H., Parizadi T., Analysis of spatial equity and access to urban parks in Ilam, Iran, J. Environ. Manag, 15, (2020); U.N. Transforming Our World: The 2030 Agenda for Sustainable Development, (2015); Roy S., Byrne J., Pickering C., A systematic quantitative review of urban tree benefits, costs, and assessment methods across cities in different climatic zones, Urban For. Urban Green, 11, pp. 351-363, (2012); Endreny T.A., Strategically growing the urban forest will improve our world, Nat. Commun, 9, (2018); Fassnacht F.E., Latifi H., Sterenczak K., Modzelewska A., Lefsky M., Waser L.T., Straub C., Ghosh A., Review of studies on tree species classification from remotely sensed data, Remote Sens. Environ, 186, pp. 64-77, (2016); Padayaahce A., Irlich U., Faulklner K., Gaertner M., Proches S., Wilson J., Rouget M., How do invasive species travel to and through urban environments?, Biol. Invasions, 19, pp. 3557-3570, (2017); Nielsen A., Ostberg J., Delshammar T., Review of Urban Tree Inventory Methods Used to Collect Data at Single-Tree Level, Arboric. E Urban For, 40, pp. 96-111, (2014); Wagner F., Ferreira M., Sanchez A., Hirye M., Zortea M., Glorr E., ans Carlos Souza Filho O.P., Shimabukuro Y., Aragao L., Individual tree crown delineation in a highly diverse tropical forest using very high resolution satellite images, ISPRS J. Photogramm. Remote Sens, 145, pp. 362-377, (2018); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks, Remote Sens, 11, (2019); dos Santos A.A., Junior J.M., Araujo M.S., Martini D.R.D., Tetila E.C., Siqueira H.L., Aoki C., Eltner A., Matsubara E.T., Pistori H., Et al., Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs, Sensors, 19, (2019); Torres D.L., Feitosa R.Q., Happ P.N., Rosa L.E.C.L., Junior J.M., Martins J., Bressan P.O., Goncalves W.N., Liesenberg V., Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery, Sensors, 20, (2020); Osco L.P., dos Santos de Arruda M., Goncalves D.N., Dias A., Batistoti J., de Souza M., Gomes F.D.G., Ramos A.P.M., Jorge L.A.C., Liesenberg V., Et al., A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery, ISPRS J. Photogramm. Remote Sens, 174, pp. 1-17, (2021); Biffi L.J., Mitishita E., Liesenberg V., dos Santos A.A., Goncalves D.N., Estrabis N.V., de Andrade Silva J., Osco L.P., Ramos A.P.M., Centeno J.A.S., Et al., ATSS Deep Learning-Based Approach to Detect Apple Fruits, Remote Sens, 13, (2021); Gomes M., Silva J., Goncalves D., Zamboni P., Perez J., Batista E., Ramos A., Osco L., Matsubara E., Li J., Et al., Mapping Utility Poles in Aerial Orthoimages Using ATSS Deep Learning Method, Sensors, 20, (2020); Santos A., Junior J.M., de Andrade Silva J., Pereira R., Matos D., Menezes G., Higa L., Eltner A., Ramos A.P., Osco L., Et al., Storm-Drain and Manhole Detection Using the RetinaNet Method, Sensors, 20, (2020); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS J. Photogramm. Remote Sens, 159, pp. 296-307, (2020); Courtrai L., Pham M.T., Lefevre S., Small Object Detection in Remote Sensing Images Based on Super-Resolution with Auxiliary Generative Adversarial Networks, Remote Sens, 12, (2020); Lu X., Li Q., Li B., Yan J., MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection, (2020); Licheng J., Fan Z., Fang L., Shuyuan Y., Lingling L., Zhixi F., Rong Q., A Survey of Deep Learning-Based Object Detection, IEEE Access, 7, pp. 128837-128868, (2019); Zhang S., Chi C., Yao Y., Lei Z., Li S.Z., Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection, (2019); Chen X., Jiang K., Zhu Y., Wang X., Yun T., Individual Tree Crown Segmentation Directly from UAV-Borne LiDAR Data Using the PointNet of Deep Learning, Forests, 12, (2021); Miyoshi G.T., dos Santos Arruda M., Osco L.P., Junior J.M., Goncalves D.N., Imai N.N., Tommaselli A.M.G., Honkavaara E., Goncalves W.N., A Novel Deep Learning Method to Identify Single Tree Species in UAV-Based Hyperspectral Images, Remote Sens, 12, (2020); Ampatzidis Y., Partel V., Meyering B., Albrecht U., Citrus rootstock evaluation utilizing UAV-based remote sensing and artificial intelligence, Comput. Electron. Agric, 164, (2019); Ampatzidis Y., Partel V., UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence, Remote Sens, 11, (2019); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban Tree Species Classification Using a WorldView-2/3 and LiDAR Data Fusion Approach and Deep Learning, Sensors, 19, (2019); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens, 9, (2017); Nezami S., Khoramshahi E., Nevalainen O., Polonen I., Honkavaara E., Tree Species Classification of Drone Hyperspectral and RGB Imagery with Deep Learning Convolutional Neural Networks, Remote Sens, 12, (2020); Plesoianu A.I., Stupariu M.S., Sandric I., Patru-Stupariu I., Dragut L., Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model, Remote Sens, 12, (2020); Culman M., Delalieux S., Tricht K.V., Individual Palm Tree Detection Using Deep Learning on RGB Imagery to Support Tree Inventory, Remote Sens, 12, (2020); Oh S., Chang A., Ashapure A., Jung J., Dube N., Maeda M., Gonzalez D., Landivar J., Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework, Remote Sens, 12, (2020); Roslan Z., Long Z.A., Ismail R., Individual Tree Crown Detection using GAN and RetinaNet on Tropical Forest, Proceedings of the 2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM), pp. 1-7; Roslan Z., Awang Z., Husen M.N., Ismail R., Hamzah R., Deep Learning for Tree Crown Detection In Tropical Forest, Proceedings of the 2020 14th International Conference on Ubiquitous Information Management and Communication (IMCOM), pp. 1-7; Population Census, (2010); (2010); Chen K., Wang J., Pang J., Cao Y., Xiong Y., Li X., Sun S., Feng W., Liu Z., Xu J., Et al., MMDetection: Open MMLab Detection Toolbox and Benchmark, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, (2017); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision, (2017); Micikevicius P., Narang S., Alben J., Diamos G., Elsen E., Garcia D., Ginsburg B., Houston M., Kuchaiev O., Venkatesh G., Mixed precision training, (2017); Zhu X., Hu H., Lin S., Dai J., Deformable ConvNets v2: More Deformable, Better Results, (2018); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Qiao S., Wang H., Liu C., Shen W., Yuille A., Weight Standardization, (2019); Wang J., Chen K., Xu R., Liu Z., Loy C.C., Lin D., CARAFE: Content-Aware ReAssembly of FEatures, Proceedings of the The IEEE International Conference on Computer Vision (ICCV), (2019); Zhu C., He Y., Savvides M., Feature Selective Anchor-Free Module for Single-Shot Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 840-849; Ghiasi G., Lin T.Y., Le Q.V., Nas-fpn: Learning scalable feature pyramid architecture for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7036-7045; Kong T., Sun F., Liu H., Jiang Y., Shi J., FoveaBox: Beyond Anchor-based Object Detector, (2019); Wu Y., Chen Y., Yuan L., Liu Z., Wang L., Li H., Fu Y., Rethinking Classification and Localization for Object Detection, (2019); Li B., Liu Y., Wang X., Gradient Harmonized Single-stage Detector, Proceedings of the AAAI Conference on Artificial Intelligence, (2019); Zhu X., Cheng D., Zhang Z., Lin S., Dai J., An Empirical Study of Spatial Attention Mechanisms in Deep Networks, (2019); Qiao S., Chen L.C., Yuille A., DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution, (2020); Zhang H., Wang Y., Dayoub F., Sunderhauf N., VarifocalNet: An IoU-aware Dense Object Detector, (2020); Wang J., Zhang W., Cao Y., Chen K., Pang J., Gong T., Shi J., Loy C.C., Lin D., Side-Aware Boundary Localization for More Precise Object Detection; ECCV 2020, 12349, (2020); Li X., Wang W., Wu L., Chen S., Hu X., Li J., Tang J., Yang J., Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection; Kim K., Lee H.S., Probabilistic Anchor Assignment with IoU Prediction for Object Detection; ECCV 2020, 12370, (2020); Zhang H., Chang H., Ma B., Wang N., Chen X., Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training, (2020); Wu J., Yang G., Yang H., Zhu Y., Li Z., Lei L., Zhao C., Extracting apple tree crown information from remote imagery using deep learning, Comput. Electron. Agric, 174, (2020); Lumnitz S., Devisscher T., Mayaud J., Radic V., Coops N., Griess V., Mapping trees along urban street networks with deep learning and street-level imagery, ISPRS J. Photogramm. Remote Sens, 175, pp. 144-157, (2021); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), (2017)","P. Zamboni; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; email: pedro.zamboni@ufms.br","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85109397264"
"Zhang J.; Liang X.; Wang M.; Yang L.; Zhuo L.","Zhang, Jing (55720152700); Liang, Xi (57194468225); Wang, Meng (55533321600); Yang, Liheng (57217029414); Zhuo, Li (55568078600)","55720152700; 57194468225; 55533321600; 57217029414; 55568078600","Coarse-to-fine object detection in unmanned aerial vehicle imagery using lightweight convolutional neural network and deep motion saliency","2020","Neurocomputing","398","","","555","565","10","18","10.1016/j.neucom.2019.03.102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085863614&doi=10.1016%2fj.neucom.2019.03.102&partnerID=40&md5=63e9a267ea9b42059533908175d6eb20","Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China; School of Computer Science and Information Engineering, Hefei University of Technology, China; Beijing University of Technology Innovation Center of Electric Vehicles in Beijing, China","Zhang J., Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China; Liang X., Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China; Wang M., Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China, School of Computer Science and Information Engineering, Hefei University of Technology, China; Yang L., Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China; Zhuo L., Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China, Beijing University of Technology Innovation Center of Electric Vehicles in Beijing, China","Unmanned aerial vehicles (UAVs) have been widely applied to various fields, facing mass imagery data, object detection in UAV imagery is under extensive research for its significant status in both theoretical study and practical applications. In order to achieve the accurate object detection in UAV imagery on the premise of real-time processing, a coarse-to-fine object detection method for UAV imagery using lightweight convolutional neural network (CNN) and deep motion saliency is proposed in this paper. The proposed method includes three steps: (1) Key frame extraction using image similarity measurement is performed on the UAV imagery to accelerate the successive object detection procedure; (2) Deep features are extracted by PeleeNet, a lightweight CNN, to achieve the coarse object detection on the key frames; (3) LiteFlowNet and objects prior knowledge is utilized to analyze the deep motion saliency map, which further helps to refine the detection results. The detection results on key frames propagate to the temporally nearest non-key frames to achieve the fine detection. Five experiments are conducted to verify the effectiveness of the proposed method on Stanford drone dataset (SDD). The experimental results demonstrate that the proposed method can achieve comparable detection speed but superior accuracy to six state-of-the-art methods. © 2019 Elsevier B.V.","Coarse-to-fine; Deep motion saliency; Lightweight convolutional neural network (CNN); Object detection; Unmanned aerial vehicle (UAV) imagery","Aircraft detection; Antennas; Convolution; Deep neural networks; Image segmentation; Neural networks; Object recognition; Unmanned aerial vehicles (UAV); Coarse to fine; Convolutional neural network; Key-frame extraction; Motion saliencies; Motion saliency map; Object detection method; Realtime processing; State-of-the-art methods; article; drone; extraction; human; human experiment; imagery; male; motion; velocity; Object detection","","","","","National Natural Science Foundation of China, NSFC, (61531006, 61602018, 61701011); Beijing Education Committee Cooperation Building Foundation Project, (KZ 201810005002)","The work in this paper is supported by Beijing Municipal Natural Science Foundation Cooperation Beijing Education Committee (No. KZ 201810005002) and the National Natural Science Foundation of China (No. 61531006 , No. 61602018 , and No. 61701011 ).  ","Kamate S., Yilmazer N., Application of object detection and tracking techniques for unmanned aerial vehicles, Proc. Comput. Sci., 61, pp. 436-441, (2015); Wu Y., Sui Y., Wang G., Vision-based real-time aerial object localization and tracking for UAV sensing system, IEEE Access, 5, pp. 23969-23978, (2017); Kyrkou C., Plastiras G., Theocharides T., Venieris S.I., Bouganis C.S., DroNet: efficient convolutional neural network detector for real-time UAV applications, Design, Automation Test in Europe Conference & Exhibition (DATE), pp. 967-972, (2018); Moranduzzo T., Melgani F., Detecting cars in UAV images with a catalog-based approach, IEEE Trans. Geosci. Remote Sens., 52, 10, pp. 6356-6367, (2014); Ma'sum M.A., Arrofi M.K., Jati G., Arifin F., Kurniawan M.N., Mursanto P., Jatmiko W., Simulation of intelligent unmanned aerial vehicle (UAV) for military surveillance, International Conference on Advanced Computer Science and Information Systems (ICACSIS), pp. 161-166, (2013); Li Z., Tang J., Mei T., Deep collaborative embedding for social image understanding, IEEE Trans. Pattern Anal. Mach. Intell., pp. 1-14, (2018); Li Z., Tang J., Weakly supervised deep matrix factorization for social image understanding, IEEE Trans. Image Process., 26, 1, pp. 276-288, (2017); Jiang Y.G., Wu Z., Tang J., Li Z., Xue X., Chang S.F., Modeling multimodal clues in a hybrid deep learning framework for video classification, IEEE Trans. Multim., 20, 11, pp. 3137-3147, (2018); Brunetti A., Buongiorno D., Trotta G.F., Bevilacqua V., Computer vision and deep learning techniques for pedestrian detection and tracking: a survey, Neurocomputing, 300, pp. 17-33, (2018); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: single shot multibox detector, European Conference on Computer Vision (ECCV), pp. 21-37, (2016); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 580-587, (2014); Girshick R., Fast r-cnn, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster r-cnn: towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems (NIPS), pp. 91-99, (2015); Dai J., Li Y., He K., Sun J., R-fcn: object detection via region-based fully convolutional networks, Advances in Neural Information Processing Systems (NIPS), pp. 379-387, (2016); Horn B.K., Schunck B.G., Determining optical flow, Artif. Intell., 17, 1-3, pp. 185-203, (1981); Lucas B.D., Kanade T., An iterative image registration technique with an application to stereo vision, International Joint Conference on Artificial Intelligence (IJCAI), (1981); Farneback G., Two-frame motion estimation based on polynomial expansion, Scandinavian Conference on Image Analysis (SCIA), pp. 363-370, (2003); Dosovitskiy A., Fischer P., Ilg E., Hausser P., Hazirbas C., Golkov V., Smagt P.V.D., Cremers D., Brox T., Flownet: learning optical flow with convolutional networks, Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 2758-2766, (2015); Ilg E., Mayer N., Saikia T., Keuper M., Dosovitskiy A., Brox T., Flownet 2.0: evolution of optical flow estimation with deep networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2462-2470, (2017); Hui T.W., Tang X., Loy C.C., LiteFlowNet: a lightweight convolutional neural network for optical flow estimation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8981-8989, (2018); Ioannidis A., Chasanis V., Likas A., Weighted multi-view key-frame extraction, Pattern Recognit. Lett., 72, pp. 52-61, (2016); Asim M., Almaadeed N., Al-maadeed S., Bouridane A., Beghdadi A., A key frame based video summarization using color features, Colour and Visual Computing Symposium (CVCS), pp. 1-6, (2018); Yu L., Cao J., Chen M., Cui X., Key frame extraction scheme based on sliding window and features, Peer-to-Peer Netw. Appl., 11, 5, pp. 1141-1152, (2018); Wang H., Yuan C., Shen J., Yang W., Ling H., Action unit detection and key frame selection for human activity prediction, Neurocomputing, 318, pp. 109-119, (2018); Gao Z., Lu G., Lyu C., Yan P., Key-frame selection for automatic summarization of surveillance videos: a method of multiple change-point detection, Mach. Vis. Appl., 29, 7, pp. 1101-1117, (2018); Ejaz N., BTariq T., Baik S.W., Adaptive key frame extraction for video summarization using an aggregation mechanism, J. Vis. Commun. Image Represent., 23, 7, pp. 1031-1040, (2012); Sheena C.V., Narayanan N.K., Key-frame extraction by analysis of histograms of video frames using statistical methods, Proc. Comput. Sci., 70, pp. 36-40, (2015); Kang D., Emmons J., Abuzaid F., Bailis P., Zaharia M., NoScope: optimizing neural network queries over video at scale, Proc. VLDB Endow., 10, 11, pp. 1586-1597, (2017); Shi Y., Yang H., Gong M., Liu X., Xia Y., A fast and robust key frame extraction method for video copyright protection, J. Electr. Comput. Eng., 2017, (2017); Rubner Y., Tomasi C., Guibas L.J., The earth mover's distance as a metric for image retrieval, Int. J. Comput. Vis., 40, 2, pp. 99-121, (2000); Hore A., Ziou D., Image quality metrics: PSNR vs. SSIM, International Conference on Pattern Recognition (ICPR), pp. 2366-2369, (2010); Wang Z., Bovik A.C., Sheikh H.R., Simoncelli E.P., Image quality assessment: from error visibility to structural similarity, IEEE Trans. Image Process., 13, 4, pp. 600-612, (2004); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., Mobilenets: Efficient convolutional neural networks for mobile vision applications, pp. 1-9, (2017); Zhang X., Zhou X., Lin M., Sun J., Shufflenet: an extremely efficient convolutional neural network for mobile devices, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6848-6856, (2018); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietikainen M., Deep learning for generic object detection: A survey, pp. 1-30, (2018); Wang R.J., Li X., Ao S., Ling C.X.; Huang G., Liu Z., Maaten L.V.D., Weinberger K.Q., Densely connected convolutional networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261-2269, (2017); Vala M.H.J., Baxi A., A review on otsu image segmentation algorithm, Int. J. Adv. Res. Comput. Eng. Technol., 2, 2, pp. 387-389, (2013); Grana C., Borghesani D., Cucchiara R., Optimized block-based connected components labeling with decision trees, IEEE Trans. Image Process., 19, 6, pp. 1596-1609, (2010); Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., Darrell T., Caffe: convolutional architecture for fast feature embedding, Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675-678, (2014); Robicquet A., Sadeghian A., Alahi A., Savarese S., Learning social etiquette: human trajectory understanding in crowded scenes, European Conference on Computer Vision (ECCV), pp. 549-565, (2016); Taha A.A., Hanbury A., An efficient algorithm for calculating the exact hausdorff distance, IEEE Trans. Pattern Anal. Mach. Intell., 37, 11, pp. 2153-2163, (2015); Fu C.Y., Liu W., Ranga A., Tyagi A., CBerg A., DSSD: Deconvolutional single shot detector, pp. 1-11, (2017); Li Z., Zhou F., FSSD: Feature fusion single shot multibox detector, pp. 1-10, (2017); Redmon J., Farhadi A., Yolov3: An incremental improvement, pp. 1-5, (2018); Everingham M., Eslami S.A., Gool L.V., Williams C.K., Winn J., Zisserman A., The pascal visual object classes challenge: a retrospective, Int. J. Comput. Vis., 111, 1, pp. 98-136, (2015); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: common objects in context, European Conference on Computer Vision (ECCV), pp. 740-755, (2014)","J. Zhang; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, China; email: zhj@bjut.edu.cn","","Elsevier B.V.","","","","","","09252312","","NRCGE","","English","Neurocomputing","Article","Final","","Scopus","2-s2.0-85085863614"
"Zhao W.; Bo Y.; Chen J.; Tiede D.; Thomas B.; Emery W.J.","Zhao, Wenzhi (58424818200); Bo, Yanchen (24079880900); Chen, Jiage (56964709100); Tiede, Dirk (21935206500); Thomas, Blaschke (7005427503); Emery, William J. (35598959400)","58424818200; 24079880900; 56964709100; 21935206500; 7005427503; 35598959400","Exploring semantic elements for urban scene recognition: Deep integration of high-resolution imagery and OpenStreetMap (OSM)","2019","ISPRS Journal of Photogrammetry and Remote Sensing","151","","","237","250","13","41","10.1016/j.isprsjprs.2019.03.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063528424&doi=10.1016%2fj.isprsjprs.2019.03.019&partnerID=40&md5=effa59fd1cb8aa1075f8973900a5b0ae","State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Beijing Engineering Research Center for Global Land Remote Sensing Products, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; National Geomatics Center of China, Beijing, 100830, China; Z_GIS, University of Salzburg, Austria; Colorado Center for Astrodynamics Research, University of Colorado Boulder, CO, United States","Zhao W., State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China, Beijing Engineering Research Center for Global Land Remote Sensing Products, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Bo Y., State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China, Beijing Engineering Research Center for Global Land Remote Sensing Products, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; Chen J., State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China, National Geomatics Center of China, Beijing, 100830, China; Tiede D., Z_GIS, University of Salzburg, Austria; Thomas B., Z_GIS, University of Salzburg, Austria; Emery W.J., Colorado Center for Astrodynamics Research, University of Colorado Boulder, CO, United States","Urban scenes refer to city blocks which are basic units of megacities, they play an important role in citizens’ welfare and city management. Remote sensing imagery with largescale coverage and accurate target descriptions, has been regarded as an ideal solution for monitoring the urban environment. However, due to the heterogeneity of remote sensing images, it is difficult to access their geographical content at the object level, let alone understanding urban scenes at the block level. Recently, deep learning-based strategies have been applied to interpret urban scenes with remarkable accuracies. However, the deep neural networks require a substantial number of training samples which are hard to satisfy, especially for high-resolution images. Meanwhile, the crowed-sourced Open Street Map (OSM) data provides rich annotation information about the urban targets but may encounter the problem of insufficient sampling (limited by the places where people can go). As a result, the combination of OSM and remote sensing images for efficient urban scene recognition is urgently needed. In this paper, we present a novel strategy to transfer existing OSM data to high-resolution images for semantic element determination and urban scene understanding. To be specific, the object-based convolutional neural network (OCNN) can be utilized for geographical object detection by feeding it rich semantic elements derived from OSM data. Then, geographical objects are further delineated into their functional labels by integrating points of interest (POIs), which contain rich semantic terms, such as commercial or educational labels. Lastly, the categories of urban scenes are easily acquired from the semantic objects inside. Experimental results indicate that the proposed method has an ability to classify complex urban scenes. The classification accuracies of the Beijing dataset are as high as 91% at the object-level and 88% at the scene level. Additionally, we are probably the first to investigate the object level semantic mapping by incorporating high-resolution images and OSM data of urban areas. Consequently, the method presented is effective in delineating urban scenes that could further boost urban environment monitoring and planning with high-resolution images. © 2019","Data fusion; Deep learning; High-resolution imagery; OpenStreetMap (OSM); Semantic classification; Urban scene recognition","Beijing [China]; China; Classification (of information); Computer software maintenance; Data fusion; Deep learning; Deep neural networks; Neural networks; Object detection; Semantics; Urban planning; Classification accuracy; Convolutional neural network; High resolution imagery; OpenStreetMap (OSM); Remote sensing imagery; Remote sensing images; Semantic classification; Urban scenes; accuracy assessment; artificial neural network; crowdsourcing; data set; environmental monitoring; exploration; heterogeneity; image analysis; image classification; megacity; recognition; remote sensing; sampling; satellite imagery; spectral resolution; urban area; Remote sensing","","","","","Austrian Science Fund FWF, (W-1237); National Key R&D Program of China, (2018YFC1508903); China Postdoctoral Science Foundation, (2018M640087); Fundamental Research Funds for the Central Universities, (2018NTST01)","This research is supported by the National Key R&D Program of China ( 2018YFC1508903 ), the China Postdoctoral Science Foundation ( 2018M640087 ), the Fundamental Research Funds for the Central Universities ( 2018NTST01 ), and the Austrian Science Fund FWF through the Doctoral College GIScience (W-1237).  ","Anwer R.M., Khan F.S., van de Weijer J., Molinier M., Laaksonen J., Binary patterns encoded convolutional neural networks for texture recognition and remote sensing scene classification, ISPRS J. Photogramm. Rem. Sens., 138, pp. 74-85, (2018); Aubrecht C., Steinnocher K., Hollaus M., Wagner W., Integrating earth observation and GIScience for high resolution spatial and functional modeling of urban land use, Comput. Environ. Urban Syst., 33, pp. 15-25, (2009); Blaschke T., Object based image analysis for remote sensing, ISPRS J. Photogramm. Rem. Sens., 65, pp. 2-16, (2010); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., van der Meer F., van der Werff H., van Coillie F., Geographic object-based image analysis–towards a new paradigm, ISPRS J. Photogramm. Rem. Sens., 87, pp. 180-191, (2014); Castelluccio M., Poggi G., Sansone C., Verdoliva L., (2015); Cheng G., Han J., Lu X., Remote sensing image scene classification: benchmark and state of the art, Proc. IEEE, 105, pp. 1865-1883, (2017); Cheng G., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Rem. Sens., 54, pp. 7405-7415, (2016); Fu G., Liu C., Zhou R., Sun T., Zhang Q., Classification for high resolution remote sensing imagery using a fully convolutional network, Rem. Sens., 9, (2017); Gao S., Janowicz K., Couclelis H., Extracting urban functional regions from points of interest and human activities on location-based social networks, Trans. GIS, 21, pp. 446-467, (2017); Haklay M., How good is volunteered geographical information? A comparative study of OpenStreetMap and Ordnance Survey datasets, Environ. Plann. B: Plann. Des., 37, pp. 682-703, (2010); Haklay M., Weber P., Openstreetmap: User-generated street maps, Ieee Pervas Comput, 7, pp. 12-18, (2008); Hinton G.E., Osindero S., Teh Y.-W., A fast learning algorithm for deep belief nets, Neural Comput., 18, pp. 1527-1554, (2006); Hu F., Xia G.-S., Hu J., Zhang L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Rem. Sens., 7, pp. 14680-14707, (2015); Hu F., Xia G.S., Wang Z., Huang X., Zhang L., Sun H., Unsupervised feature learning via spectral clustering of multidimensional patches for remotely sensed scene classification, IEEE J. Sel. Top. Appl. Earth Obs. Rem. Sens., 8, pp. 2015-2030, (2015); Hu T., Yang J., Li X., Gong P., Mapping urban land use by using landsat images and open social data, Rem. Sens., 8, (2016); Johnson B.A., Iizuka K., Integrating OpenStreetMap crowdsourced data and Landsat time-series imagery for rapid land use/land cover (LULC) mapping: Case study of the Laguna de Bay area of the Philippines, Appl. Geogr., 67, pp. 140-149, (2016); Kaiser P., Wegner J.D., Lucchi A., Jaggi M., Hofmann T., Schindler K., Learning aerial image segmentation from online maps, IEEE Trans. Geosci. Rem. Sens., 55, pp. 6054-6068, (2017); Kang J., Korner M., Wang Y., Taubenbock H., Zhu X.X., Building instance classification using street view images, ISPRS J. Photogramm. Rem. Sens., 145, pp. 44-59, (2018); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci. Rem. Sens. Lett., 14, pp. 778-782, (2017); Leichtle T., Geiss C., Wurm M., Lakes T., Taubenbock H., Unsupervised change detection in VHR remote sensing imagery–an object-based clustering approach in a dynamic urban environment, Int. J. Appl. Earth Observ. Geoinform., 54, pp. 15-27, (2017); Li M., Zang S., Zhang B., Li S., Wu C., A review of remote sensing image classification techniques: the role of spatio-contextual information, Eur. J. Rem. Sens., 47, pp. 389-411, (2014); Liu Q., Zhou F., Hang R., Yuan X., Bidirectional-convolutional LSTM based spectral-spatial feature learning for hyperspectral image classification, Rem. Sens., 9, (2017); Liu X., He J., Yao Y., Zhang J., Liang H., Wang H., Hong Y., Classifying urban land use by integrating remote sensing and social media data, Int. J. Geograph. Inform. Sci., 31, pp. 1675-1696, (2017); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional neural networks for large-scale remote-sensing image classification, IEEE Trans. Geosci. Rem. Sens., 55, pp. 645-657, (2017); Mou L., Ghamisi P., Zhu X.X., Deep recurrent neural networks for hyperspectral image classification, IEEE Trans. Geosci. Rem. Sens., 55, pp. 3639-3655, (2017); Nogueira K., Penatti O.A.B., dos Santos J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification, Pattern Recogn., 61, pp. 539-556, (2017); Schultz M., Voss J., Auer M., Carter S., Zipf A., Open land cover from OpenStreetMap and remote sensing, Int. J. Appl. Earth Observ. Geoinform., 63, pp. 206-213, (2017); Sui D., Goodchild M., Elwood S., pp. 1-12, (2013); Tuia D., Volpi M., Dalla Mura M., Rakotomamonjy A., Flamary R., Automatic feature learning for spatio-spectral image classification with sparse SVM, Geosci. Rem. Sens., IEEE Trans., 52, pp. 6062-6074, (2014); Tzotsos A., Argialas D., pp. 663-677, (2008); Walter V., Object-based classification of remote sensing data for change detection, ISPRS J. Photogramm. Rem. Sens., 58, pp. 225-238, (2004); Wan T., Lu H., Lu Q., Luo N., Classification of high-resolution remote-sensing image using openstreetmap information, IEEE Geosci. Rem. Sens. Lett., 14, pp. 2305-2309, (2017); Weng Q., Remote sensing of impervious surfaces in the urban areas: requirements, methods, and trends, Rem. Sens. Environ., 117, pp. 34-49, (2012); Weng Q., Quattrochi D., Gamba P.E., Urban Remote Sensing, (2018); Xiao J., Gerke M., Vosselman G., Building extraction from oblique airborne imagery based on robust façade detection, ISPRS J. Photogramm. Rem. Sens., 68, pp. 56-68, (2012); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 270-279, (2010); Yao Y., Li X., Liu X., Liu P., Liang Z., Zhang J., Mai K., Sensing spatial distribution of urban land use by integrating points-of-interest and Google Word2Vec model, Int. J. Geograph. Inform. Sci., 31, pp. 825-848, (2017); Yi Y., Newsam S., Spatial pyramid co-occurrence for image classification, pp. 1465-1472, (2011); Yu Q., Gong P., Clinton N., Biging G., Kelly M., Schirokauer D., Object-based detailed vegetation classification with airborne high spatial resolution remote sensing imagery, Photogramm. Eng. Rem. Sens., 72, pp. 799-811, (2006); Zhang L., Zhang L., Tao D., Huang X., On combining multiple features for hyperspectral remote sensing image classification, Geosci. Rem. Sensing, IEEE Trans., 50, pp. 879-893, (2012); Zhang X., Du S., Wang Q., Hierarchical semantic cognition for urban functional zones with VHR satellite images and POI data, ISPRS J. Photogramm. Rem. Sens., 132, pp. 170-184, (2017); Zhang Y., Li Q., Huang H., Wu W., Du X., Wang H., The combined use of remote sensing and social sensing data in fine-grained urban land use mapping: a case study in Beijing, China. Rem. Sens., 9, (2017); Zhang Z., Liu Q., Wang Y., Road extraction by deep residual U-Net, IEEE Geosci. Rem. Sens. Lett., 15, pp. 749-753, (2018); Zhao B., Zhong Y., Xia G.-S., Zhang L., Dirichlet-derived multiple topic scene classification model for high spatial resolution remote sensing imagery, IEEE Trans. Geosci. Rem. Sens., 54, pp. 2108-2123, (2016); Zhao B., Zhong Y., Zhang L., A spectral–structural bag-of-features scene classifier for very high spatial resolution remote sensing imagery, ISPRS J. Photogramm. Rem. Sens., 116, pp. 73-85, (2016); Zhao W., Du S., Emery W.J., Object-based convolutional neural network for high-resolution imagery classification, IEEE J. Sel. Top. Appl. Earth Obs. Rem. Sens., 10, pp. 3386-3396, (2017); Zhao W., Du S., Wang Q., Emery W.J., Contextually guided very-high-resolution imagery classification with semantic segments, ISPRS J. Photogramm. Rem. Sens., 132, pp. 48-60, (2017); Zhong Y., Wu S., Zhao B., Scene semantic understanding based on the spatial context relations of multiple objects, Rem. Sens., 9, (2017); Zhong Y., Zhu Q., Zhang L., Scene classification based on the multifeature fusion probabilistic topic model for high spatial resolution remote sensing imagery, IEEE Trans. Geosci. Rem. Sens., 53, pp. 6207-6222, (2015); Zhou F., Hang R., Liu Q., Yuan X., Hyperspectral image classification using spectral-spatial LSTMs, Neurocomputing, 328, pp. 39-47, (2019); Zhu Q., Zhong Y., Zhang L., Li D., Scene classification based on the fully sparse semantic topic model, IEEE Trans. Geosci. Remote Sens., 55, pp. 5525-5538, (2017); Zou Q., Ni L., Zhang T., Wang Q., Deep learning based feature selection for remote sensing scene classification, IEEE Geosci. Rem. Sens. Lett., 12, pp. 2321-2325, (2015)","Y. Bo; State Key Laboratory of Remote Sensing Science, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China; email: boyc@bnu.edu.cn","","Elsevier B.V.","","","","","","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","","Scopus","2-s2.0-85063528424"
"Wu Y.; Wang Y.; Zhang S.; Ogai H.","Wu, Yutian (57208878889); Wang, Yueyu (57221111832); Zhang, Shuwei (57211500543); Ogai, Harutoshi (6603121795)","57208878889; 57221111832; 57211500543; 6603121795","Deep 3D Object Detection Networks Using LiDAR Data: A Review","2021","IEEE Sensors Journal","21","2","9181591","1152","1171","19","41","10.1109/JSEN.2020.3020626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098213929&doi=10.1109%2fJSEN.2020.3020626&partnerID=40&md5=4fa795115fd94cf8fa1ffc3503500cb7","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Horizon Robotics, Beijing, China","Wu Y., Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Wang Y., Horizon Robotics, Beijing, China; Zhang S., Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Ogai H., Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan","As the foundation of intelligent systems, machine vision perceives the surrounding environment and provides a basis for decision-making. Object detection is the core task in machine vision. 3D object detection can provide object steric size and location information. Compared with the 2D object detection widely studied in image coordinates, it can provide more applications of detection systems. Accurate LiDAR data has a stronger spatial capture capability and is insensitive to natural light, which makes LiDAR a potential sensor for 3D detection. Recently, deep neural network has been developed to learn powerful object features from sensor data. However, the sparsity of LiDAR point cloud data poses challenges to the network processing. Plenty of emerged efforts have been made to address this difficulty, but a comprehensive review literature is still lacking. The purpose of this article is to review the challenges and methodologies of 3D object detection networks using LiDAR data. On this account, we first give an outline of 3D detection task and LiDAR sensing techniques. Then we unfold the review of deep 3D detection networks with three kinds of LiDAR point cloud representations and their challenges. We next summarize evaluation metrics and performance of algorithms on three authoritative 3D detection benchmarks. Finally, we provide valuable insights of challenges and open issues.  © 2001-2012 IEEE.","3D object detection; deep learning; LiDAR; neural network; point cloud","Benchmarking; Computer vision; Decision making; Deep neural networks; Intelligent systems; Object recognition; Optical radar; Capture capability; Evaluation metrics; Lidar point cloud datum; Lidar point clouds; Location information; Network processing; Performance of algorithm; Surrounding environment; Object detection","","","","","","","Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 580-587, (2014); Girshick R., Fast R-CNN, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards realtime object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); Amzajerdian F., Role of lidar technology in future nasa space missions, MRS Spring Meeting, (2008); Dubayah R.O., Drake J.B., LiDAR remote sensing for forestry, J. Forestry, 98, 6, pp. 44-46, (2000); Lim K., Treitz P., Wulder M., St-Onge B., Flood M., LiDAR remote sensing of forest structure, Prog. Phys. Geogra., 27, 1, pp. 88-106, (2003); Lefsky M.A., Cohen W.B., Parker G.G., Harding D.J., LiDAR remote sensing for ecosystem studies: LiDAR, an emerging remote sensing technology that directly measures the three-dimensional distribution of plant canopies, can accurately estimate vegetation structural attributes and should be of particular interest to forest, landscape, and global ecologists, BioScience, 52, 1, pp. 19-30, (2002); Dong P., Characterization of individual tree crowns using threedimensional shape signatures derived from LiDAR data, Int. J. Remote Sens., 30, 24, pp. 6621-6628, (2009); Henning J.G., Radtke P.J., Detailed stem measurements of standing trees from ground-based scanning LiDAR, Forest Sci., 52, 1, pp. 67-80, (2006); Amzajerdian F., Petway L., Hines G., Barnes B., Pierrottet D., Lockard G., Doppler LiDAR sensor for precision landing on the moon and Mars, Proc. IEEE Aerosp. Conf., pp. 1-7, (2012); Amzajerdian F., Et al., Imaging flash LiDAR for safe landing on solar system bodies and spacecraft rendezvous and docking, Proc. Spie, 9465, (2015); Jasiobedzki P., Se S., Pan T., Umasuthan M., Greenspan M., Autonomous satellite rendezvous and docking using LiDAR and model based vision, Proc. Spie, 5798, pp. 54-65, (2005); Lindner P., Wanielik G., 3D LiDAR processing for vehicle safety and environment recognition, Proc. IEEE Workshop Comput. Intell. Vehicles Veh. Syst., pp. 66-71, (2009); Asvadi A., Premebida C., Peixoto P., Nunes U., 3D LiDARbased static and moving obstacle detection in driving environments: An approach based on voxels and multi-region ground planes, Robot. Auto. Syst., 83, pp. 299-311, (2016); Larson J., Trivedi M., LiDAR based off-road negative obstacle detection and analysis, Proc. 14th Int. IEEE Conf. Intell. Transp. Syst. (ITSC), pp. 192-197, (2011); Sinha A., Papadakis P., Mind the gap: Detection and traversability analysis of terrain gaps using LiDAR for safe robot navigation, Robotica, 31, 7, pp. 1085-1101, (2013); Moghadam P., Wijesoma W.S., Jun Feng D., Improving path planning and mapping based on stereo vision and LiDAR, Proc. 10th Int. Conf. Control, Autom., Robot. Vis., pp. 384-389, (2008); Kang Y., Roh C., Suh S.-B., Song B., A LiDAR-based decisionmaking method for road boundary detection using multiple Kalman filters, IEEE Trans. Ind. Electron., 59, 11, pp. 4360-4368, (2012); Niclass C.L., Shpunt A., Agranov G.A., Waldon M.C., Rezk M.A., Oggier T., Multi-range time of flight sensing, U.S. Patent 15 586 286, (2018); Gargoum S., El-Basyouny K., Automated extraction of road features using LiDAR data: A review of LiDAR applications in transportation, Proc. 4th Int. Conf. Transp. Inf. Saf. (ICTIS), pp. 563-574, (2017); Arnold E., Al-Jarrah O.Y., Dianati M., Fallah S., Oxtoby D., Mouzakitis A., A survey on 3D object detection methods for autonomous driving applications, IEEE Trans. Intell. Transp. Syst., 20, 10, pp. 3782-3795, (2019); Guo Y., Wang H., Hu Q., Liu H., Liu L., Bennamoun M., Deep Learning for 3D Point Clouds: A Survey, (2019); Feng D., Et al., Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges, IEEE Trans. Intell. Transp. Syst., (2020); Zou Z., Shi Z., Guo Y., Ye J., Object Detection in 20 Years: A Survey, (2019); Qi C.R., Liu W., Wu C., Su H., Guibas L.J., Frustum PointNets for 3D object detection from RGB-D data, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 918-927, (2018); Zaheer M., Kottur S., Ravanbakhsh S., Poczos B., Salakhutdinov R.R., Smola A.J., Deep sets, Proc. Adv. Neural Inf. Process. Syst., pp. 3391-3401, (2017); Wang S., Suo S., Ma W.-C., Pokrovsky A., Urtasun R., Deep parametric continuous convolutional neural networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 2589-2597, (2018); Dean T., Ruzon M.A., Segal M., Shlens J., Vijayanarasimhan S., Yagnik J., Fast, accurate detection of 100, 000 object classes on a single machine, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1814-1821, (2013); Felzenszwalb P.F., Girshick R.B., McAllester D., Ramanan D., Object detection with discriminatively trained part-based models, IEEE Trans. Pattern Anal. Mach. Intell., 32, 9, pp. 1627-1645, (2010); Sermanet P., Kavukcuoglu K., Chintala S., Lecun Y., Pedestrian detection with unsupervised multi-stage feature learning, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3626-3633, (2013); Uijlings J.R.R., Van De Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Process. Syst., pp. 1097-1105, (2012); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Liu W., Et al., SSD: Single shot multibox detector, Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, pp. 21-37, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 779-788, (2016); Li B., Zhang T., Xia T., Vehicle Detection from 3D LiDAR Using Fully Convolutional Network, (2016); Chen X., Ma H., Wan J., Li B., Xia T., Multi-view 3D object detection network for autonomous driving, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1907-1915, (2017); Ku J., Mozifian M., Lee J., Harakeh A., Waslander S.L., Joint 3D proposal generation and object detection from view aggregation, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1-8, (2018); Zhou Y., Tuzel O., VoxelNet: End-to-end learning for point cloud based 3D object detection, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4490-4499, (2018); Geiger A., Lenz P., Urtasun R., Are we ready for autonomous driving? The KITTI vision benchmark suite, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3354-3361, (2012); Huang X., Et al., The ApolloScape dataset for autonomous driving, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 954-960, (2018); Qi C.R., Yi L., Su H., Guibas L.J., PointNet++: Deep hierarchical feature learning on point sets in a metric space, Proc. Adv. Neural Inf. Process. Syst., pp. 5099-5108, (2017); Caesar H., Et al., NuScenes: A Multimodal Dataset for Autonomous Driving, (2019); Sun P., Et al., Scalability in Perception for Autonomous Driving: Waymo Open Dataset, (2019); Patil A., Malla S., Gang H., Chen Y.-T., The H3D dataset for full-surround 3D multi-object detection and tracking in crowded urban scenes, Proc. Int. Conf. Robot. Autom. (ICRA), pp. 9552-9557, (2019); Kesten R., Et al., Lyft level 5 av dataset 2019, Tech. Rep., (2019); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3431-3440, (2015); Minemura K., Liau H., Monrroy A., Kato S., LMNet: Real-time multiclass object detection on CPU using 3D LiDAR, Proc. 3rd Asia-Pacific Conf. Intell. Robot Syst. (ACIRS), pp. 28-34, (2018); Yu F., Koltun V., Multi-scale Context Aggregation by Dilated Convolutions, (2015); Zhou J., Tan X., Shao Z., Ma L., FVNet: 3D Front-view Proposal Generation for Real-time Object Detection from Point Clouds, (2019); Charles R.Q., Su H., Kaichun M., Guibas L.J., PointNet: Deep learning on point sets for 3D classification and segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 652-660, (2017); Meyer G.P., Laddha A., Kee E., Vallespi-Gonzalez C., Wellington C.K., LaserNet: An efficient probabilistic 3D object detector for autonomous driving, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 12669-12678, (2019); Meyer G.P., Charland J., Hegde D., Laddha A., Vallespi-Gonzalez C., Sensor fusion for joint 3D object detection and semantic segmentation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 1230-1237, (2019); Yu F., Wang D., Shelhamer E., Darrell T., Deep layer aggregation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 2403-2412, (2018); Deng J., Czarnecki K., MLOD: A multi-view 3D object detection based on robust feature fusion method, Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 279-284, (2019); Lu H., Chen X., Zhang G., Zhou Q., Ma Y., Zhao Y., Scanet: Spatial-channel attention network for 3D object detection, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), pp. 1992-1996, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2117-2125, (2017); Hu J., Shen L., Sun G., Squeeze-and-excitation networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 7132-7141, (2018); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 2980-2988, (2017); Wang Z., Zhan W., Tomizuka M., Fusing Bird's eye view LiDAR point cloud and front view camera image for 3D object detection, Proc. IEEE Intell. Vehicles Symp. (IV), pp. 1-6, (2018); Liang M., Yang B., Wang S., Urtasun R., Deep continuous fusion for multi-sensor 3D object detection, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 641-656, (2018); Liang M., Yang B., Chen Y., Hu R., Urtasun R., Multi-task multi-sensor fusion for 3D object detection, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7345-7353, (2019); Yu S.-L., Westfechtel T., Hamada R., Ohno K., Tadokoro S., Vehicle detection and localization on bird's eye view elevation images using convolutional neural network, Proc. IEEE Int. Symp. Saf., Secur. Rescue Robot. (SSRR), pp. 102-109, (2017); Beltran J., Guindel C., Moreno F.M., Cruzado D., Garcia F., De La Escalera A., BirdNet: A 3D object detection framework from LiDAR information, Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC), pp. 3517-3523, (2018); Barrera A., Guindel C., Beltran J., Garcia F., BirdNet+: Endto-end 3D Object Detection in LiDAR Bird's Eye View, (2020); Dai J., Li Y., He K., Sun J., R-FCN: Object detection via regionbased fully convolutional networks, Proc. Adv. Neural Inf. Process. Syst., pp. 379-387, (2016); Zeng Y., Et al., RT3D: Real-time 3-D vehicle detection in LiDAR point cloud for autonomous driving, IEEE Robot. Autom. Lett., 3, 4, pp. 3434-3440, (2018); Wirges S., Fischer T., Stiller C., Frias J.B., Object detection and classification in occupancy grid maps using deep convolutional networks, Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC), pp. 3530-3535, (2018); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7263-7271, (2017); Simony M., Milzy S., Amendey K., Gross H.-M., Complex-YOLO: An euler-region-proposal for real-time 3D object detection on point clouds, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 197-209, (2018); Ali W., Abdelkarim S., Zidan M., Zahran M., El Sallab A., YOLO3D: End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 716-728, (2018); Yang B., Luo W., Urtasun R., PIXOR: Real-time 3D object detection from point clouds, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 7652-7660, (2018); Yang B., Liang M., Urtasun R., HDNET: Exploiting HD maps for 3D object detection, Proc. Conf. Robot Learn., pp. 146-155, (2018); Luo W., Yang B., Urtasun R., Fast and furious: Real time endto-end 3D detection, tracking and motion forecasting with a single convolutional net, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 3569-3577, (2018); Wang D.Z., Posner I., Voting for voting in online point cloud object detection, Robot., Sci. Syst., 1, 3, pp. 156071-1560710, (2015); Westin C.-F., Geometrical diffusion measures for MRI from tensor basis analysis, Proc. Ismrm, (1997); Engelcke M., Rao D., Wang D.Z., Tong C.H., Posner I., Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 1355-1361, (2017); Li B., 3D fully convolutional network for vehicle detection in point cloud, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1513-1518, (2017); Graham B., Spatially-sparse Convolutional Neural Networks, (2014); Graham B., Sparse 3D Convolutional Neural Networks, (2015); Graham B., Maaten Der L.Van, Submanifold Sparse Convolutional Networks, (2017); Graham B., Engelcke M., Maaten L.V.D., 3D semantic segmentation with submanifold sparse convolutional networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 9224-9232, (2018); Yan Y., Mao Y., Li B., SECOND: Sparsely embedded convolutional detection, Sensors, 18, 10, (2018); Lang A.H., Vora S., Caesar H., Zhou L., Yang J., Beijbom O., PointPillars: Fast encoders for object detection from point clouds, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 12697-12705, (2019); Lehner J., Mitterecker A., Adler T., Hofmarcher M., Nessler B., Hochreiter S., Patch Refinement-localized 3D Object Detection, (2019); Chen Y., Liu S., Shen X., Jia J., Fast point R-CNN, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9775-9784, (2019); Kuang H., Wang B., An J., Zhang M., Zhang Z., Voxel-FPN: Multi-scale voxel feature aggregation for 3D object detection from LiDAR point clouds, Sensors, 20, 3, (2020); Ye Y., Chen H., Zhang C., Hao X., Zhang Z., SARPNET: Shape attention regional proposal network for LiDAR-based 3D object detection, Neurocomputing, 379, pp. 53-63, (2020); Zhou Y., Et al., End-to-end Multi-view Fusion for 3D Object Detection in LiDAR Point Clouds, (2019); Liu Z., Zhao X., Huang T., Hu R., Zhou Y., Bai X., TANet: Robust 3D Object Detection from Point Clouds with Triple Attention, (2019); Shi S., Wang Z., Shi J., Wang X., Li H., From points to parts: 3D object detection from point cloud with part-aware and partaggregation network, IEEE Trans. Pattern Anal. Mach. Intell., (2020); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent. Cham, pp. 234-241, (2015); Shi S., Wang X., Li H., PointRCNN: 3D object proposal generation and detection from point cloud, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-779, (2019); Chen Q., Sun L., Wang Z., Jia K., Yuille A., Object As Hotspots: An Anchor-free 3D Object Detection Approach Via Firing of Hotspots, (2019); Yi H., Et al., SegVoxelNet: Exploring Semantic Context and Depthaware Features for 3D Vehicle Detection from Point Cloud, (2020); Sindagi V.A., Zhou Y., Tuzel O., MVX-Net: Multimodal Voxelnet for 3D object detection, Proc. Int. Conf. Robot. Autom. (ICRA), pp. 7276-7282, (2019); Jaderberg M., Simonyan K., Zisserman A., Spatial transformer networks, Proc. Adv. Neural Inf. Process. Syst., pp. 2017-2025, (2015); Wu Z., Et al., 3D ShapeNets: A deep representation for volumetric shapes, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1912-1920, (2015); Maturana D., Scherer S., VoxNet: A 3D convolutional neural network for real-time object recognition, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 922-928, (2015); Wang Z., Jia K., Frustum ConvNet: Sliding frustums to aggregate local point-wise features for amodal 3D object detection, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1742-1749, (2019); Shin K., Kwon Y.P., Tomizuka M., RoarNet: A robust 3D object detection based on RegiOn approximation refinement, Proc. IEEE Intell. Vehicles Symp. (IV), pp. 2510-2515, (2019); Xu D., Anguelov D., Jain A., PointFusion: Deep sensor fusion for 3D bounding box estimation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 244-253, (2018); Zhao X., Liu Z., Hu R., Huang K., 3D object detection using scale invariant and feature reweighting networks, Proc. Aaai Conf. Artif. Intell., 33, pp. 9267-9274, (2019); Jiang M., Wu Y., Zhao T., Zhao Z., Lu C., PointSIFT: A SIFTlike Network Module for 3D Point Cloud Semantic Segmentation, (2018); Tian Y., Wang K., Wang Y., Tian Y., Wang Z., Wang F.-Y., Adaptive and Azimuth-aware Fusion Network of Multimodal Local Features for 3D Object Detection, (2019); Yang Z., Sun Y., Liu S., Shen X., Jia J., IPOD: Intensive Point-based Object Detector for Point Cloud, (2018); Ngiam J., Et al., StarNet: Targeted Computation for Object Detection in Point Clouds, (2019); Xu K., Hu W., Leskovec J., Jegelka S., How Powerful Are Graph Neural Networks?, (2018); Qi C.R., Litany O., He K., Guibas L., Deep Hough voting for 3D object detection in point clouds, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9277-9286, (2019); Yang Z., Sun Y., Liu S., Jia J., 3DSSD: Point-based 3D Single Stage Object Detector, (2020); Yang Z., Sun Y., Liu S., Shen X., Jia J., STD: Sparse-to-dense 3D object detector for point cloud, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 1951-1960, (2019); Shi S., Et al., PV-RCNN: Point-voxel Feature Set Abstraction for 3D Object Detection, (2019); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal visual object classes (VOC) challenge, Int. J. Comput. Vis., 88, 2, pp. 303-338, (2010); Lin T.-Y., Et al., Microsoft COCO: Common objects in context, Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, pp. 740-755, (2014); Salton G., McGill M.J., Introduction to Modern Information Retrieval, (1983); Simonelli A., Bulo S.R., Porzi L., Lopez-Antequera M., Kontschieder P., Disentangling monocular 3D object detection, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 1991-1999, (2019); Zhu B., Jiang Z., Zhou X., Li Z., Yu G., Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection, (2019); Yuan W., Khot T., Held D., Mertz C., Hebert M., PCN: Point completion network, Proc. Int. Conf. 3D Vis. (3DV), pp. 728-737, (2018); Liu M., Sheng L., Yang S., Shao J., Hu S.-M., Morphing and Sampling Network for Dense Point Cloud Completion, (2019); Groueix T., Fisher M., Kim V.G., Russell B.C., Aubry M., A papier-mache approach to learning 3D surface generation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 216-224, (2018); Huang Z., Yu Y., Xu J., Ni F., Le X., PF-Net: Point fractal network for 3D point cloud completion, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7662-7670, (2020); Zarzar J., Giancola S., Ghanem B., PointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement, (2019); Shi W., Rajkumar R., Point-GNN: Graph neural network for 3D object detection in a point cloud, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1711-1719, (2020); Vora S., Lang A.H., Helou B., Beijbom O., PointPainting: Sequential Fusion for 3D Object Detection, (2019); Gao F., Wang C., Hybrid strategy for traffic light detection by combining classical and self-learning detectors, Iet Intell. Transp. Syst., 14, 7, pp. 735-741, (2020)","Y. Wu; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; email: wuyutian@fuji.waseda.jp","","Institute of Electrical and Electronics Engineers Inc.","","","","","","1530437X","","","","English","IEEE Sensors J.","Review","Final","","Scopus","2-s2.0-85098213929"
"Perez H.; Tah J.H.M.; Mosavi A.","Perez, Husein (57210797227); Tah, Joseph H. M. (6701862415); Mosavi, Amir (57191408081)","57210797227; 6701862415; 57191408081","Deep learning for detecting building defects using convolutional neural networks","2019","Sensors (Switzerland)","19","16","3556","","","","124","10.3390/s19163556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071457857&doi=10.3390%2fs19163556&partnerID=40&md5=0c5bce6af99227b0b165074ea18be7a9","Oxford Institute for Sustainable Development, School of the Built Environment, Oxford Brookes University, Oxford, OX30BP, United Kingdom","Perez H., Oxford Institute for Sustainable Development, School of the Built Environment, Oxford Brookes University, Oxford, OX30BP, United Kingdom; Tah J.H.M., Oxford Institute for Sustainable Development, School of the Built Environment, Oxford Brookes University, Oxford, OX30BP, United Kingdom; Mosavi A., Oxford Institute for Sustainable Development, School of the Built Environment, Oxford Brookes University, Oxford, OX30BP, United Kingdom","Clients are increasingly looking for fast and e_ective means to quickly and frequently survey and communicate the condition of their buildings so that essential repairs and maintenance work can be done in a proactive and timely manner before it becomes too dangerous and expensive. Traditional methods for this type of work commonly comprise of engaging building surveyors to undertake a condition assessment which involves a lengthy site inspection to produce a systematic recording of the physical condition of the building elements, including cost estimates of immediate and projected long-term costs of renewal, repair and maintenance of the building. Current asset condition assessment procedures are extensively time consuming, laborious, and expensive and pose health and safety threats to surveyors, particularly at height and roof levels which are di_cult to access. This paper aims at evaluating the application of convolutional neural networks (CNN) towards an automated detection and localisation of key building defects, e.g., mould, deterioration, and stain, from images. The proposed model is based on pre-trained CNN classifier of VGG-16 (later compaired with ResNet-50, and Inception models), with class activation mapping (CAM) for object localisation. The challenges and limitations of the model in real-life applications have been identified. The proposed model has proven to be robust and able to accurately detect and localise building defects. The approach is being developed with the potential to scale-up and further advance to support automated detection of defects and deterioration of buildings in real-time using mobile devices and drones. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Building defects; Class activation mapping (CAM); Convolutional neural networks (CNN); Deep learning; Structural-health monitoring; Transfer learning","Aircraft detection; Cams; Chemical activation; Convolution; Cost benefit analysis; Cost estimating; Deep learning; Deep neural networks; Defects; Deterioration; Health risks; Mapping; Meteorological problems; Neural networks; Repair; Structural health monitoring; Surveying; Activation mapping; Automated detection; Building defects; Condition assessments; Convolutional neural network; Real-life applications; Repair and maintenance; Transfer learning; article; classifier; convolutional neural network; deep learning; deterioration; drone; human; male; mold; nonhuman; scale up; stain; transfer of learning; Buildings","","","","","","","Mohseni H., Setunge S., Zhang G.M., Wakefield R., In Condition monitoring and condition aggregation for optimised decision making in management of buildings, Appl. Mech. Mater, 438, (2013); Agdas D., Rice J.A., Martinez J.R., Lasa I.R., Comparison of visual inspection and structural-health monitoring as bridge condition assessment methods, J. Perform. Constr. Facil, (2015); Shamshirband S., Mosavi A., Rabczuk T., Particle Swarm Optimization Model to Predict Scour Depth around Bridge Pier; Zhang Y., Anderson N., Bland S., Nutt S., Jursich G., Joshi S., All-printed strain sensors: Building blocks of the aircraft structural health monitoring system. Sens, Actuators a Phys, 253, pp. 165-172, (2017); Noel A.B., Abdaoui A., Elfouly T., Ahmed M.H., Badawy A., Shehata M.S., Structural health monitoring using wireless sensor networks: A comprehensive survey, IEEE Commun. Surv. Tutor, 19, pp. 1403-1423, (2017); Kong Q., Allen R.M., Kohler M.D., Heaton T.H., Bunn J., Structural health monitoring of buildings using smartphone sensors., Seismol. Res. Lett, 89, pp. 594-602, (2018); Song G., Wang C., Wang B., Structural Health Monitoring (SHM) of Civil Structures., Appl. Sci, 7, (2017); Annamdas V.G.M., Bhalla S., Soh C.K., Applications of structural health monitoring technology in Asia., Struct. Health Monit, 16, pp. 324-346, (2017); Lorenzoni F., Casarin F., Caldon M., Islami K., Modena C., Uncertainty quantification in structural health monitoring: Applications on cultural heritage buildings, Mech. Syst. Signal Process., 66, pp. 261-268, (2016); Oh B.K., Kim K.J., Kim Y., Park H.S., Adeli H., Evolutionary learning based sustainable strain sensing model for structural health monitoring of high-rise buildings, Appl. Soft Comput, 58, pp. 576-585, (2017); Mita A., Gap between technically accurate information and socially appropriate information for structural health monitoring system installed into tall buildings, Proceedings of the Health Monitoring of Structural and Biological Systems, (2016); Mimura T., Mita A., Automatic estimation of natural frequencies and damping ratios of building structures, Procedia Eng, 188, pp. 163-169, (2017); Zhang F.L., Yang Y.P., Xiong H.B., Yang J.H., Yu Z., Structural health monitoring of a 250-m super-tall building and operational modal analysis using the fast Bayesian FFT method., Struct. Control Health Monit, (2019); Davoudi R., Miller G.R., Kutz J.N., Structural load estimation using machine vision and surface crack patterns for shear-critical RC beams and slabs, J. Comput. Civ. Eng, 32, (2018); Hoang N.D., Image Processing-Based Recognition of Wall Defects Using Machine Learning Approaches and Steerable Filters, Comput. Intell. Neurosci, (2018); Jo J., Jadidi Z., Stantic B.A., Drone-based building inspection system using software-agents., In Studies in Computational Intelligence, 737, pp. 115-121, (2017); Pahlberg T., Thurley M., Popovic D., Hagman O., Crack detection in oak flooring lamellae using ultrasound-excited thermography., Infrared Phys. Technol, 88, pp. 57-69, (2018); Pragalath H., Seshathiri S., Rathod H., Esakki B., Gupta R., Deterioration assessment of infrastructure using fuzzy logic and image processing algorithm, J. Perform. Constr. Facil, 32, (2018); Valero E., Forster A., Bosche F., Hyslop E., Wilson L., Turmel A., Automated defect detection and classification in ashlar masonry walls using machine learning, Autom. Constr, 106; Valero E., Forster A., Bosche F., Renier C., Hyslop E., Wilson L., High Level-of-Detail BIM and Machine Learning for Automated Masonry Wall Defect Surveying, Proceedings of the International Symposium on Automation and Robotics in Construction, (2018); Lee B.J., Lee H., David Position-invariant neural network for digital pavement crack analysis, Comput. Aided Civ. Infrastruct. Eng., 19, pp. 105-118, (2004); Koch C., Brilakis I., Pothole detection in asphalt pavement images. Adv. Eng, Inform, 25, pp. 507-515, (2011); Cord A., Chambon S., Automatic road defect detection by textural pattern recognition based on AdaBoost, Comput. Aided Civ. Infrastruct. Eng, 27, pp. 244-259, (2012); Jahanshahi M.R., Jazizadeh F., Masri S.F., Becerik-Gerber B., Unsupervised approach for autonomous pavement-defect detection and quantification using an inexpensive depth sensor, J. Comput. Civ. Eng., 27, pp. 743-754, (2012); Radopoulou S.C., Brilakis I., Automated detection of multiple pavement defects, J. Comput. Civ. Eng, 31, (2016); Abdel-Qader I., Abudayyeh O., Kelly M.E., Analysis of edge-detection techniques for crack identification in bridges, J. Comput. Civ. Eng, 17, pp. 255-263, (2003); Duran O., Althoefer K., Seneviratne L.D., State of the art in sensor technologies for sewer inspection, IEEE Sens. J, 2, pp. 73-81, (2002); Sinha S.K., Fieguth P.W., Automated detection of cracks in buried concrete pipe images., Autom. Constr, 15, pp. 58-72, (2006); Sinha S.K., Fieguth P.W., Neuro-fuzzy network for the classification of buried pipe defects., Autom. Constr, 15, pp. 73-83, (2006); Guo W., Soibelman L., Garrett J.H., Visual Pattern Recognition Supporting Defect Reporting and Condition Assessment of Wastewater Collection Systems, J. Comput. Civ. Eng, 23, pp. 160-169, (2009); Huang G., Liu Z., Weinberger K.Q., Densely Connected Convolutional Networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); German S., Brilakis I., Desroches R., Rapid entropy-based detection and properties measurement of concrete spalling with machine vision for post-earthquake safety assessments, Adv. Eng. Inform, 26, pp. 846-858, (2012); Ji M., Liu L., Buchroithner M., Identifying Collapsed Buildings Using Post-Earthquake Satellite Imagery and Convolutional Neural Networks: A Case Study of the 2010 Haiti Earthquake, Remote Sens, 10, (2018); (2016); Defects in Buildings: Symptoms, Investigation, Diagnosis and Cure, (2001); Seeley I.H., Building Maintenance, (1987); Richardson B., Defects and Deterioration in Buildings: A Practical Guide to the Science and Technology of Material Failure, (2002); Wood B.J., Building Maintenance, (2009); Riley M., Cotgrave A., (2011); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition; Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A., Learning deep features for discriminative localization, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Trotman P.M., Harrison H., Understanding Dampness, (2004); Burkinshaw R., Parrett M., Diagnosing Damp, (2003); Thomas A.R., Treatment of Damp in Old Buildings, (1986); Lourenco P.B., Luso E., Almeida M.G., Defects and moisture problems in buildings from historical city centres: A case study in Portugal, Build. Environ, 41, pp. 223-234, (2006); Bakri N.N.O., Mydin M.A.O., General building defects: Causes, symptoms and remedial work, Eur. J. Technol. Des, 34, pp. 4-17, (2014); Wang W., Wu B., Yang S., Wang Z., Road Damage Detection and Classification with Faster R-CNN, Proceedings of the 2018 IEEE International Conference on Big Data (Big Data), (2018); Cha Y.-J., Choi W., Suh G., Mahmoudkhani S., Buyukozturk O., Autonomous structural visual inspection using region-based deep learning for detecting multiple damage types, Comput. Aided Civ. Infrastruct. Eng, 33, pp. 731-747, (2018); Roth H., Farag A., Lu L.B., Turkbey E., Summers R., Deep convolutional networks for pancreas segmentation in CT imaging, Proceedings of the Image Processing. International Society for Optics and Photonics, (2015); Fukushima K., A neural network for visual pattern recognition, Computer, pp. 65-75, (1988); Rawat W., Wang Z., Deep convolutional neural networks for image classification, A Comprehensive Review. Neural Comput., 29, pp. 2352-2449, (2017); Mosavi A.; Stathakis D., How many hidden layers and nodes?, Int. J. Remote Sens, 30, pp. 2133-2147, (2009); Smithson S.C., Yang G., Gross W.J., Meyer B.H., Neural Networks Designing Neural Networks: Multi-Objective Hyper-Parameter Optimization, Proceedings of the 35Th International Conference on Computer-Aided Design, (2016); Heaton J., Introduction to Neural Networks with Java; Lee C.-Y., Gallagher P.W., Tu Z., Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree, Proceedings of the Artificial Intelligence and Statistics, (2016); Giusti A., Ciresan D.C., Masci J., Gambardella L.M., Schmidhuber J., Fast image scanning with deep max-pooling convolutional neural networks, Proceedings of the 2013 IEEE International Conference on Image Processing, (2013); Yu D., Wang H., Chen P., Wei Z., Mixed Pooling for Convolutional Neural Networks, Proceedings of the Rough Sets and Knowledge Technology, pp. 364-375, (2014); Zhu Z., German S., Brilakis I., Visual retrieval of concrete crack properties for automated post-earthquake structural safety evaluation., Autom. Constr, 20, pp. 874-883, (2011); Pan S.J., Yang Q.A., Survey on Transfer Learning, IEEE Trans. Knowl. Data Eng, 22, pp. 1345-1359, (2010); Deng J., Dong W., Socher R., Li L.-J., Li L., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, (2009); Ge W., Yu Y., Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); Hu J., Lu J., Tan Y.-P., Deep Transfer Metric Learning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2015); Zhao Z.-Q., Zheng P., Xu S., Wu X., Object Detection with Deep Learning: A Review., IEEE Trans. Neural Netw. Learn. Syst, (2019); Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba, A. Object Detectors Emerge in Deep Scene Cnns; He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Nguyen A., Yosinski J., Clune J., Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2015); Lee K., Lee H., Lee K., Shin, J. Training Confidence-Calibrated Classifiers for Detecting Out-Of-Distribution Samples; Guo C., Pleiss G., Sun Y., Weinberger K.Q., On Calibration of Modern Neural Networks, Proceedings of the 34Th International Conference on Machine Learning, (2017); Amodei D., Olah C., Steinhardt J., Christiano P., Schulman J., Mané, D. Concrete Problems in AI Safety; Mosavi A., Shamshirband S., Salwana E., Chau K.W., Tah J.H., Prediction of multi-inputs bubble column reactor using a novel hybrid model of computational fluid dynamics and machine learning, Eng. Appl. Comput. Fluid Mech., 13, pp. 482-498, (2019); Cha Y.-J., Choi W., Buyukozturk O., Deep learning-based crack damage detection using convolutional neural networks, Comput. Aided Civ. Infrastruct. Eng, 32, pp. 361-378, (2017); Mohammadzadeh S., Kazemi S.F., Mosavi A., Nasseralshariati E., Tah J.H., Prediction of Compression Index of Fine-Grained Soils Using a Gene Expression Programming Model, Infrastructures, 4, (2019); Maguire M., Dorafshan S., Thomas R., SDNET2018: A Concrete Crack Image Dataset for Machine Learning Applications, (2018)","H. Perez; Oxford Institute for Sustainable Development, School of the Built Environment, Oxford Brookes University, Oxford, OX30BP, United Kingdom; email: hperez@brookes.ac.uk","","MDPI AG","","","","","","14248220","","","31443244","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85071457857"
"Aburasain R.Y.; Edirisinghe E.A.; Albatay A.","Aburasain, R.Y. (57218716991); Edirisinghe, E.A. (6701576984); Albatay, Ali (57218719072)","57218716991; 6701576984; 57218719072","Drone-Based Cattle Detection Using Deep Neural Networks","2021","Advances in Intelligent Systems and Computing","1250 AISC","","","598","611","13","10","10.1007/978-3-030-55180-3_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090178320&doi=10.1007%2f978-3-030-55180-3_44&partnerID=40&md5=dd186002ee0a1465ffe1d2926efc82a3","Department of Computer Science, Loughborough University, Loughborough, United Kingdom; Zayed University, Dubai, United Arab Emirates","Aburasain R.Y., Department of Computer Science, Loughborough University, Loughborough, United Kingdom; Edirisinghe E.A., Department of Computer Science, Loughborough University, Loughborough, United Kingdom; Albatay A., Zayed University, Dubai, United Arab Emirates","Cattle form an important source of farming in many countries. In literature, several attempts have been conducted to detect farm animals for different applications and purposes. However, these approaches have been based on detecting animals from images captured from ground level and most approaches use traditional machine learning approaches for their automated detection. In this modern era, Drones facilitate accessing images in challenging environments and scanning large-scale areas with minimum time, which enables many new applications to be established. Considering the fact that drones typically are flown at high altitude to facilitate coverage of large areas within a short time, the captured object size tend to be small and hence this significantly challenges the possible use of traditional machine learning algorithms for object detection. This research proposes a novel methodology to detect cattle in farms established in desert areas using Deep Neural Networks. We propose to detect animals based on a ‘group-of-animals’ concept and associated features in which different group sizes and animal density distribution are used. Two state-of-the-art Convolutional Neural Network (CNN) architectures, SSD-500 and YOLO V-3, are effectively configured, trained and used for the purpose and their performance efficiencies are compared. The results demonstrate the capability of the two generated CNN models to detect groups-of-animals in which the highest accuracy recorded was when using SSD-500 giving a F-score of 0.93, accuracy of 0.89 and mAP rate of 84.7. © 2021, Springer Nature Switzerland AG.","Convolution Neural Networks; Drones; Object detection; Unmanned aerial vehicles","Animals; Convolutional neural networks; Deep learning; Deep neural networks; Drones; Intelligent systems; Learning algorithms; Learning systems; Object detection; Associated feature; Automated detection; Density distributions; Machine learning approaches; Minimum time; New applications; Novel methodology; Performance efficiency; Aircraft detection","","","","","","","Otto A., Agatz N., Campbell J., Golden B., Pesch E., Optimization approaches for civil applications of unmanned aerial vehicles (UAVs) or aerial drones: a survey, Networks, 72, 4, pp. 411-458, (2018); Kellenberger B., Volpi M., Tuia D., Fast animal detection in UAV images using convolutional neural networks, 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 866-869, (2017); Voulodimos A., Doulamis N., Doulamis A., Protopapadakis E., Deep learning for computer vision: a brief review, Comput. Intell. Neurosci, 2018, pp. 1-13, (2018); Girshick R., Fast R-CNN, 2015 Presented at the Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448; Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, 28, pp. 91-99, (2015); Liu W., Et al., SSD: Single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, (2016); Redmon J., Farhadi A., Yolov3: an incremental improvement, (2018); Sharma S.U., Shah D.J., A practical animal detection and collision avoidance system using computer vision technique, IEEE Access, 5, pp. 347-358, (2016); Nasirahmadi A., Edwards S.A., Sturm B., Implementation of machine vision for detecting behaviour of cattle and pigs, Livest. Sci, 202, pp. 25-38, (2017); Yousif H., Yuan J., Kays R., He Z., Fast human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification, 2017 IEEE International Symposium on Circuits and Systems (ISCAS), pp. 1-4, (2017); Gomez Villa A., Salazar A., Vargas F., Towards automatic wild animal monitoring: identification of animal species in camera-trap images using very deep convolutional neural networks, Ecol. Inform, 41, pp. 24-32, (2017); Norouzzadeh M.S., Et al., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proc. Natl. Acad. Sci, 115, 25, pp. E5716-E5725, (2018); Rivas A., Chamoso P., Gonzalez-Briones A., Corchado J.M., Detection of cattle using drones and convolutional neural networks, Sensors, 18, 7, (2018); Shao W., Kawakami R., Yoshihashi R., You S., Kawase H., Naemura T., Cattle detection and counting in UAV images based on convolutional neural networks, Int. J. Remote Sens, 41, 1, pp. 31-52, (2020); Xia M., Li W., Fu H., Yu L., Dong R., Zheng J., Fast and robust detection of oil palm trees using high-resolution remote sensing images, Automatic Target Recognition XXIX, 10988, (2019); Hollings T., Burgman M., van Andel M., Gilbert M., Robinson T., Robinson A., How do you find the green sheep? A critical review of the use of remotely sensed imagery to detect and count animals, Methods Ecol. Evol, 9, 4, pp. 881-892, (2018); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014)","R.Y. Aburasain; Department of Computer Science, Loughborough University, Loughborough, United Kingdom; email: r.aburasain@lboro.ac.uk","Arai K.; Kapoor S.; Bhatia R.","Springer","","Intelligent Systems Conference, IntelliSys 2020","3 September 2020 through 4 September 2020","London","244279","21945357","978-303055179-7","","","English","Adv. Intell. Sys. Comput.","Conference paper","Final","","Scopus","2-s2.0-85090178320"
"Pan X.; Zhao J.; Xu J.","Pan, Xin (35422588500); Zhao, Jian (57188561431); Xu, Jun (57210253499)","35422588500; 57188561431; 57210253499","An object-based and heterogeneous segment filter convolutional neural network for high-resolution remote sensing image classiﬁcation","2019","International Journal of Remote Sensing","40","15","","5892","5916","24","19","10.1080/01431161.2019.1584687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062362438&doi=10.1080%2f01431161.2019.1584687&partnerID=40&md5=23c6737fb8d6f8966bf0a0f06d75e483","School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, China; The Key Laboratory of Changbai Mountain Historical Culture and VR Technology Reconfiguration, Changchun Institute of Technology, Changchun, China","Pan X., School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, China, The Key Laboratory of Changbai Mountain Historical Culture and VR Technology Reconfiguration, Changchun Institute of Technology, Changchun, China; Zhao J., School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, China; Xu J., The Key Laboratory of Changbai Mountain Historical Culture and VR Technology Reconfiguration, Changchun Institute of Technology, Changchun, China","In recent years, object-based segmentation methods and shallow-model classification algorithms have been widely integrated for remote sensing image supervised classification. However, as the image resolution increases, remote sensing images contain increasingly complex characteristics, leading to higher intraclass heterogeneity and interclass homogeneity and thus posing substantial challenges for the application of segmentation methods and shallow-model classification algorithms. As important methods of deep learning technology, convolutional neural networks (CNNs) can hierarchically extract higher-level spatial features from images, providing CNNs with a more powerful recognition ability for target detection and scene classification in high-resolution remote sensing images. However, the input of the traditional CNN is an image patch, the shape of which is scarcely consistent with a given segment. This inconsistency may lead to errors when directly using CNNs in object-based remote sensing classification: jagged errors may appear along the land cover boundaries, and some land cover areas may overexpand or shrink, leading to many obvious classification errors in the resulting image. To address the above problem, this paper proposes an object-based and heterogeneous segment filter convolutional neural network (OHSF-CNN) for high-resolution remote sensing image classiﬁcation. Before the CNN processes an image patch, the OHSF-CNN includes a heterogeneous segment filter (HSF) to process the input image. For the segments in the image patch that are obviously different from the segment to be classified, the HSF can differentiate them and reduce their negative influence on the CNN training and decision-making processes. Experimental results show that the OHSF-CNN not only can take full advantage of the recognition capabilities of deep learning methods but also can effectively avoid the jagged errors along land cover boundaries and the expansion/shrinkage of land cover areas originating from traditional CNN structures. Moreover, compared with the traditional methods, the proposed OHSF-CNN can achieve higher classification accuracy. Furthermore, the OHSF-CNN algorithm can serve as a bridge between deep learning technology and object-based segmentation algorithms thereby enabling the application of object-based segmentation methods to more complex high-resolution remote sensing images. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","","Complex networks; Convolution; Data mining; Decision making; Deep learning; Errors; Image resolution; Neural networks; Object recognition; Positive ions; Remote sensing; Classification accuracy; Complex characteristics; Convolutional neural network; Decision making process; High resolution remote sensing images; Remote sensing classification; Segmentation algorithms; Supervised classification; accuracy assessment; algorithm; artificial neural network; image classification; image processing; image resolution; remote sensing; segmentation; Image segmentation","","","","","National Natural Science Foundation of China, NSFC, (41871236); Department of Science and Technology of Jilin Province, (20180101020JC,20180622006JC)","This work was jointly supported by the National Natural Science Foundation of China [41871236]; Foundation of Jilin Provincial Science & Technology Department [20180101020JC,20180622006JC].","Akcay H.G., Aksoy S., Building Detection Using Directional Spatial Constraints, Honolulu, Hawaii, United States, pp. 1932-1935, (2010); Blaschke T., Object Based Image Analysis for Remote Sensing, Isprs Journal of Photogrammetry and Remote Sensing, 65, 1, pp. 2-16, (2010); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., Et al., Geographic Object-Based Image Analysis–Towards a New Paradigm, Isprs Journal of Photogrammetry and Remote Sensing, 87, pp. 180-191, (2014); Fu G., Liu C.J., Zhou R., Sun T., Zhang Q.J., Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network, Remote Sensing, 9, (2017); Garcia-Pedrero A., Gonzalo-Martin C., Fonseca-Luengo D., Lillo-Saavedra M., A GEOBIA Methodology for Fragmented Agricultural Landscapes, Remote Sensing, 7, 1, pp. 767-787, (2015); Hinton G.E., Osindero S., Teh Y.W., A Fast Learning Algorithm for Deep Belief Nets, Neural Computation, 18, 7, pp. 1527-1554, (2006); Hu F., Xia G.S., Hu J.W., Zhang L.P., Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery, Remote Sensing, 7, 11, pp. 14680-14707, (2015); Ioffe S., Szegedy C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, In Proceedings of the 32nd International Conference Machine Learning, 1, pp. 448-456, (2015); Johnson B., Xie Z.X., Classifying a High Resolution Image of an Urban Area Using Super-Object Information, Isprs Journal of Photogrammetry and Remote Sensing, 83, pp. 40-49, (2013); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, Communications of the ACM, 60, 6, pp. 84-90, (2017); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Liu Y., Fang B., Wang L., Bai J., Xiang S., Pan C., Semantic Labeling in Very High Resolution Images via a Self-Cascaded Convolutional Neural Network, ISPRS Journal of Photogrammetry and Remote Sensing, 145, pp. 78-95, (2018); Long J., Shelhamer E., Darrell T., Fully Convolutional Networks for Semantic Segmentation, Boston, MA, pp. 3431-3440, (2015); Lv X., Ming D., Chen Y.Y., Wang M., Very High Resolution Remote Sensing Image Classification with SEEDS-CNN and Scale Effect Analysis for Superpixel CNN Classification, International Journal of Remote Sensing, (2018); Ma L., Li M.C., Ma X.X., Cheng L., Du P.J., Liu Y.X., A Review of Supervised Object-Based Land-Cover Image Classification, Isprs Journal of Photogrammetry and Remote Sensing, 130, pp. 277-293, (2017); Machala M., Zejdova L., Forest Mapping through Object-Based Image Analysis of Multispectral and LiDAR Aerial Data, European Journal of Remote Sensing, 47, pp. 117-131, (2014); Maggiori E., Charpiat G., Tarabalka Y., Alliez P., Recurrent Neural Networks to Correct Satellite Image Classification Maps, Ieee Transactions on Geoscience and Remote Sensing, 55, 9, pp. 4962-4971, (2017); Mullerova J., Bruna J., Bartalos T., Dvorak P., Vitkova M., Pysek P., Timing Is Important: Unmanned Aircraft vs. Satellite Imagery in Plant Invasion Monitoring, Frontiers in Plant Science, 8, (2017); Nogueira K., Penatti O.A.B., Dos Santos J.A., Towards Better Exploiting Convolutional Neural Networks for Remote Sensing Scene Classification, Pattern Recognition, 61, pp. 539-556, (2017); Pan X., Zhao J., A Central-Point-Enhanced Convolutional Neural Network for High-Resolution Remote-Sensing Image Classification, International Journal of Remote Sensing, 38, 23, pp. 6554-6581, (2017); Pan X., Zhao J., High-Resolution Remote Sensing Image Classification Method Based on Convolutional Neural Network and Restricted Conditional Random Field, Remote Sensing, 10, (2018); Pena-Barragan J.M., Ngugi M.K., Plant R.E., Six J., Object-Based Crop Identification Using Multiple Vegetation Indices, Textural Features and Crop Phenology, Remote Sensing of Environment, 115, 6, pp. 1301-1316, (2011); Qayyum A., Malik A.S., Saad N.M., Iqbal M., Abdullah M.F., Rasheed W., Abdullah T.A.B.R., Bin Jafaar M.Y., Scene Classification for Aerial Images Based on CNN Using Sparse Coding Technique, International Journal of Remote Sensing, 38, 8-10, pp. 2662-2685, (2017); Ranzato M., Huang F.J., Boureau Y.L., LeCun Y., Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition, Minneapolis, MI, 1-8, (2007); Rawat W., Wang Z.H., Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review, Neural Computation, 29, 9, pp. 2352-2449, (2017); Sainath T.N., Kingsbury B., Saon G., Soltau H., Mohamed A.R., Dahl G., Ramabhadran B., Deep Convolutional Neural Networks for Large-Scale Speech Tasks, Neural Networks, 64, pp. 39-48, (2015); Salem M., Ibrahim A.F., Ali H.A., Automatic Quick-Shift Method for Color Image Segmentation, Cairo, Egypt, pp. 245-251, (2013); Sebari I., He D.C., Automatic Fuzzy Object-Based Analysis of VHSR Images for Urban Objects Extraction, Isprs Journal of Photogrammetry and Remote Sensing, 79, pp. 171-184, (2013); Szegedy C., Liu W., Jia Y.Q., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going Deeper with Convolutions, Boston, MA, pp. 1-9, (2015); Vedaldi A., Soatto S., Quick Shift and Kernel Methods for Mode Seeking, Marseille, France, 5305, pp. 705-718, (2008); Wang Q., Gao J.Y., Yuan Y., A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling, Ieee Transactions on Intelligent Transportation Systems, 19, 5, pp. 1457-1470, (2018); Wu G.M., Shao X.W., Guo Z.L., Chen Q., Yuan W., Shi X.D., Xu Y.W., Shibasaki R., Automatic Building Segmentation of Aerial Imagery UsingMulti-Constraint Fully Convolutional Networks, Remote Sensing, 10, (2018); Xia G.S., Hu J.W., Hu F., Shi B.G., Bai X., Zhong Y.F., Zhang L.P., Lu X.Q., AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification, Ieee Transactions on Geoscience and Remote Sensing, 55, 7, pp. 3965-3981, (2017); Yu Y.L., Liu F.X., Dense Connectivity Based Two-Stream Deep Feature Fusion Framework for Aerial Scene Classification, Remote Sensing, 10, 7, (2018); Zanotta D.C., Zortea M., Ferreira M.P., A Supervised Approach for Simultaneous Segmentation and Classification of Remote Sensing Images, Isprs Journal of Photogrammetry and Remote Sensing, 142, pp. 162-173, (2018); Zeiler M.D., Fergus R., Visualizing and Understanding Convolutional Networks, Zurich, Switzerland, pp. 818-833, (2014); Zhang C., Pan X., Li H.P., Gardiner A., Sargent I., Hare J., Atkinson P.M., A Hybrid MLP-CNN Classifier for Very Fine Resolution Remotely Sensed Image Classification, Isprs Journal of Photogrammetry and Remote Sensing, 140, pp. 133-144, (2018); Zhang C., Sargent I., Li H., Gardiner A., Hare J., Atkinson P.M., An Object-Based Convolutional Neural Network (OCNN) for Urban Land Use Classification, Remote Sensing of Environment, 216, pp. 57-70, (2018); Zhang F., Du B., Zhang L.P., Scene Classification via a Gradient Boosting Random Convolutional Network Framework, Ieee Transactions on Geoscience and Remote Sensing, 54, 3, pp. 1793-1802, (2016); Zhao W.Z., Du S.H., Emery W.J., Object-Based Convolutional Neural Network for High-Resolution Imagery Classification, Ieee Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 10, 7, pp. 3386-3396, (2017)","X. Pan; School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, China; email: panxinpc@163.com","","Taylor and Francis Ltd.","","","","","","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Final","","Scopus","2-s2.0-85062362438"
"Jing R.; Liu S.; Gong Z.; Wang Z.; Guan H.; Gautam A.; Zhao W.","Jing, Ran (57189875510); Liu, Shuang (57273719200); Gong, Zhaoning (23491851600); Wang, Zhiheng (36563161000); Guan, Hongliang (57201065463); Gautam, Atul (57211652998); Zhao, Wenji (12787367300)","57189875510; 57273719200; 23491851600; 36563161000; 57201065463; 57211652998; 12787367300","Object-based change detection for VHR remote sensing images based on a Trisiamese-LSTM","2020","International Journal of Remote Sensing","41","16","","6209","6231","22","12","10.1080/01431161.2020.1734253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086074087&doi=10.1080%2f01431161.2020.1734253&partnerID=40&md5=ca96046c4accd7e542db81c0a334fae2","College of Geospatial Information Science and Technology, Capital Normal University, Beijing, China; College of Resource Environment and Tourism, Capital Normal University, Beijing, China; School of Geology and Geomatics, Tianjin Chengjian University, Tianjin, China","Jing R., College of Geospatial Information Science and Technology, Capital Normal University, Beijing, China; Liu S., College of Geospatial Information Science and Technology, Capital Normal University, Beijing, China; Gong Z., College of Resource Environment and Tourism, Capital Normal University, Beijing, China; Wang Z., School of Geology and Geomatics, Tianjin Chengjian University, Tianjin, China; Guan H., College of Geospatial Information Science and Technology, Capital Normal University, Beijing, China; Gautam A., College of Geospatial Information Science and Technology, Capital Normal University, Beijing, China; Zhao W., College of Resource Environment and Tourism, Capital Normal University, Beijing, China","Change detection has been a research hotspot in remote sensing fields for decades. However, the increasing use of very high-resolution (VHR) remote sensing images have introduced more difficulties in change detection because of the complex details these images contain. In this paper, we propose a novel deep learning architecture for change detection composed of a Trisiamese subnetwork and a long short-term memory (LSTM) subnetwork that fully utilizes the spatial, spectral and multiphase information and improves the change detection capabilities for VHR remote sensing images. Multiscale simple linear iterative clustering (SLIC)-based image segmentation is first performed on multitemporal images at different image scales to obtain edge information-based objects. A Trisiamese subnetwork with six inputs can extract abundant spectral-spatial feature representations; the LSTM subnetwork then uses the extracted image features to effectively analyse the multiphase information in bitemporal images. The proposed method has the following advantages: 1) it can fully utilize the significant spatial information to improve the detection task; 2) it combines the advantages of convolutional architectures for image feature representation and recurrent neural network (RNN) architectures for sequential data representation, unlike most of the algorithms that use either method or that merely use image differencing or stacking operations. The controlled experiments reveal that the multiphase information extracted by the LSTM subnetwork is important to improve the accuracy of the change detection results. The influence of the Trisiamese subnetwork on change detection is even more significant than that of the LSTM subnetwork. Comparisons with other state-of-the-art change detection methods indicate that in areas with clear surface features and limited interference, the proposed method obtains more competitive results compared to state-of-the-art methods, and in regions where the changed objects occur in complex patterns, the proposed method exhibited an ideal performance. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","","Complex networks; Convolutional neural networks; Deep learning; Feature extraction; Image enhancement; Image segmentation; Iterative methods; Network architecture; Object detection; Remote sensing; Controlled experiment; Image feature representation; Iterative clustering; Learning architectures; Object based change detections; Recurrent neural network (RNN); Remote sensing images; State-of-the-art methods; accuracy assessment; artificial neural network; detection method; image resolution; remote sensing; satellite imagery; segmentation; spatial analysis; spectral analysis; Long short-term memory","","","","","National Key Research and Development Program of China, NKRDPC, (2017YFC1502901)","This work was supported by the National Key Research and Development Plan [2017YFC1502901].","Achanta R., Appu S., Kevin S., Aurelien L., Pascal F., Sabine S., SLIC Superpixels Compared to State-of-the-art Superpixel Methods, IEEE Transactions on Pattern Analysis and Machine Intelligence, 34, 11, pp. 2274-2282, (2012); Bezdek J., Robert E., William F., FCM: The Fuzzy C-means Clustering Algorithm, Computers & Geosciences, 10, 2, pp. 191-203, (1984); Bovolo F., Lorenzo B., A Theoretical Framework for Unsupervised Change Detection Based on Change Vector Analysis in the Polar Domain, IEEE Transactions on Geoscience and Remote Sensing, 45, 1, pp. 218-236, (2006); Chen Z., Cheng W., Chenglu W., Xiuhua T., Yiping C., Haiyan G., Huan L., Liujuan C., Jonathan L., Vehicle Detection in High-resolution Aerial Images via Sparse Representation and Superpixels, IEEE Transactions on Geoscience and Remote Sensing, 54, 1, pp. 103-116, (2015); Cheng G., Peicheng Z., Junwei H., Learning Rotation-invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images, IEEE Transactions on Geoscience and Remote Sensing, 54, pp. 7405-7415, (2016); Collobert R., Samy B., Links between Perceptrons, MLPs and SVMs, In Proceedings of the Twenty-first International Conference on Machine Learning, (2004); Deng J.S., Wang K., Deng Y.H., Qi G.J., PCA-based Land-use Change Detection and Analysis Using Multitemporal and Multisensor Satellite Data, International Journal of Remote Sensing, 29, pp. 4823-4838, (2008); Desclee B., Patrick B., Defourny P., Forest Change Detection by Statistical Object-based Method, Remote Sensing of Environment, 102, 1, pp. 1-11, (2006); Dozat T., Incorporating Nesterov Momentum into Adam, International conference on learning representations, (2016); Dragut L., Csillik O., Eisank C., Tiede D., Automated Parameterisation for Multi-scale Image Segmentation on Multiple Layers, ISPRS Journal of Photogrammetry and Remote Sensing, 88, 100, pp. 119-127, (2014); Dragut L., Tiede D., Levick S.R., ESP: A Tool to Estimate Scale Parameter for Multiresolution Image Segmentation of Remotely Sensed Data, International Journal of Geographical Information Science, 24, 6, pp. 859-871, (2010); Fu K., Chen G., Jie Y., Yue Z., Gu I.-H., Superpixel Based Color Contrast and Color Distribution Driven Salient Object Detection, Signal Processing: Image Communication, 28, 10, pp. 1448-1463, (2013); Gong J., Haigang S., Kaimin S., Guorui M., Junyi L., Object-level Change Detection Based on Full-scale Image Segmentation and Its Application to Wenchuan Earthquake, Science in China Series E: Technological Sciences, 51, 2, pp. 110-122, (2008); Gueguen L., Raffay H., Toward a Generalizable Image Representation for Large-scale Change Detection: Application to Generic Damage Analysis, IEEE Transactions on Geoscience and Remote Sensing, 54, 6, pp. 3378-3387, (2016); Haklay M., Patrick W., Openstreetmap: User-generated Street Maps, IEEE Pervasive Computing, 7, 4, pp. 12-18, (2008); Hebel M., Michael A., Uwe S., Change Detection in Urban Areas by Object-based Analysis and On-the-fly Comparison of Multi-view ALS Data, ISPRS Journal of Photogrammetry and Remote Sensing, 86, pp. 52-64, (2013); Hinton G.E., Osindero S., Teh Y.W., A Fast Learning Algorithm for Deep Belief Nets, Neural Computation, 18, 7, pp. 1527-1554, (2006); Hochreiter S., Schmidhuber J., Long Short-term Memory, Neural Computation, 9, 8, pp. 1735-1780, (1997); Hou B., Yunhong W., Qingjie L., Change Detection Based on Deep Features and Low Rank, IEEE Geoscience and Remote Sensing Letters, 14, 12, pp. 2418-2422, (2017); Im J., Jensen J.R., Tullis J.A., Object‐based Change Detection Using Correlation Image Analysis and Image Segmentation, International Journal of Remote Sensing, 29, 2, pp. 399-423, (2008); Jensen J.R., Introductory Digital Image Processing: A Remote Sensing Perspective, Geocarto International, 15, 1, (2015); Kittler J., Illingworth J., Minimum Error Thresholding, Pattern Recognition, 19, 1, pp. 41-47, (1986); Krizhevsky A., Ilya S., Geoffrey H., Imagenet Classification with Deep Convolutional Neural Networks, Advances in Neural Information Processing Systems, 25, 2, pp. 1097-1105, (2012); Langkvist M., Andrey K., Marjan A., Amy L., Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks, Remote Sensing, 8, 4, pp. 1-21, (2016); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based Learning Applied to Document Recognition, Proceedings of the IEEE, 86, 11, pp. 2278-2324, (1998); Li J., Xin H., Jianya G., Deep Neural Network for Remote-sensing Image Interpretation: Status and Perspectives, National Science Review, (2019); Linke J., Greg M., David L., Adam M., Alysha P., Jerome C., Mryka B., Steven F., A Disturbance-inventory Framework for Flexible and Reliable Landscape Monitoring, Photogrammetric Engineering and Remote Sensing, 75, 8, pp. 981-995, (2009); Liu B., Hao H., Huanyu W., Kaizhi W., Xingzhao L., Wenxian Y., Superpixel-based Classification with an Adaptive Number of Classes for Polarimetric SAR Images, IEEE Transactions on Geoscience and Remote Sensing, 51, 2, pp. 907-924, (2013); Lu T., Dongping M., Xiangguo L., Zhaoli H., Xueding B., Fang J., Detecting Building Edges from High Spatial Resolution Remote Sensing Imagery Using Richer Convolution Features Network, Remote Sensing, 10, 9, (2018); Lv X., Dongping M., Tingting L., Keqi Z., Min W., Hanqing B., A New Method for Region-Based Majority Voting CNNs for Very High Resolution Image Classification, Remote Sensing, 10, 12, (2018); Lv X., Dongping M., Yangyang C., Min W., Very High Resolution Remote Sensing Image Classification with SEEDS-CNN and Scale Effect Analysis for Superpixel CNN Classification, International Journal of Remote Sensing, 40, 2, pp. 506-531, (2019); Lyu H., Hui L., Lichao M., Learning a Transferable Change Rule from a Recurrent Neural Network for Land Cover Change Detection, Remote Sensing, 8, 6, (2016); Ma J., Guoliang T., Changyao W., Shouxun Y., Review of the Development of Remote Sensing Change Detection Technology, Advance in Earth Sciences, 19, 2, pp. 192-196, (2004); Meinel G., Marco N., A Comparison of Segmentation Programs for High Resolution Remote Sensing Data, International Archives of Photogrammetry and Remote Sensing, 35, pp. 1097-1105, (2004); Nielsen A.A., The Regularized Iteratively Reweighted MAD Method for Change Detection in Multi- and Hyperspectral Data, IEEE Trans. Image Process, 16, 2, pp. 463-478, (2007); Otsu N., A Threshold Selection Method from Gray-level Histograms, IEEE Transactions on Systems, Man, and Cybernetics, 9, 1, pp. 62-66, (1979); Ruder S., An Overview of Gradient Descent Optimization Algorithms, (2016); Russakovsky O., Jia D., Hao S., Jonathan K., Sanjeev S., Sean M., Zhiheng H., Et al., Imagenet Large Scale Visual Recognition Challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); Russwurm M., Marco K., Temporal Vegetation Modelling Using Long Short-term Memory Networks for Crop Identification from Medium-resolution Multi-spectral Satellite Images, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 11-19, (2017); Simonyan K., Andrew Z., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); Szegedy C., Liu W., Yangqing J., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Andrew R., Going Deeper with Convolutions, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, (2015); Tom M., Machine Learning, (2003); Wang Q., Zhenghang Y., Qian D., Xuelong L., GETNET: A General End-to-end 2-D CNN Framework for Hyperspectral Image Change Detection, IEEE Transactions on Geoscience and Remote Sensing, 57, 1, pp. 3-13, (2018); Wang Q., Peter A., Wenzhong S., Fast Subpixel Mapping Algorithms for Subpixel Resolution Change Detection, IEEE Transactions on Geoscience and Remote Sensing, 53, 4, pp. 1692-1706, (2015); Woodcock C., Alan S., The Factor of Scale in Remote Sensing, Remote Sensing of Environment, 21, 3, pp. 311-332, (1987); Zhu X., Devis T., Lichao M., Guisong X., Liangpei Z., Feng X., Friedrich F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geoscience and Remote Sensing Magazine, 5, 4, pp. 8-36, (2017)","Z. Gong; College of Resource Environment and Tourism, Capital Normal University, Beijing, China; email: gongzhn@163.com","","Taylor and Francis Ltd.","","","","","","01431161","","IJSED","","English","Int. J. Remote Sens.","Article","Final","","Scopus","2-s2.0-85086074087"
"Polewski P.; Shelton J.; Yao W.; Heurich M.","Polewski, Przemyslaw (25825460700); Shelton, Jacquelyn (38362455600); Yao, Wei (55420675800); Heurich, Marco (23568273000)","25825460700; 38362455600; 55420675800; 23568273000","Instance segmentation of fallen trees in aerial color infrared imagery using active multi-contour evolution with fully convolutional network-based intensity priors","2021","ISPRS Journal of Photogrammetry and Remote Sensing","178","","","297","313","16","6","10.1016/j.isprsjprs.2021.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109099799&doi=10.1016%2fj.isprsjprs.2021.06.016&partnerID=40&md5=4ba3b458f6cc6cf2327ba3df788a5959","Dept. of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Dept. for Visitor Management and National Park Monitoring, Bavarian Forest National Park, Grafenau, 94481, Germany","Polewski P., Dept. of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Shelton J., Dept. of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Yao W., Dept. of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong; Heurich M., Dept. for Visitor Management and National Park Monitoring, Bavarian Forest National Park, Grafenau, 94481, Germany","Over the last several years, semantic image segmentation based on deep neural networks has been greatly advanced. On the other hand, single-instance segmentation still remains a challenging problem. In this paper, we introduce a framework for segmenting instances of a common object class by multiple active contour evolution over semantic segmentation maps of images obtained through fully convolutional networks. The contour evolution is cast as an energy minimization problem, where the aggregate energy functional incorporates a data fit term, an explicit shape model, and accounts for object overlap. Efficient solution neighborhood operators are proposed, enabling optimization through metaheuristics such as simulated annealing. We instantiate the proposed framework in the context of segmenting individual fallen stems from high-resolution aerial multispectral imagery, providing problem-specific energy potentials. We validated our approach on 3 real-world scenes of varying complexity, using 730 manually labeled polygon outlines as ground truth. The test plots were situated in regions of the Bavarian Forest National Park, Germany, which sustained a heavy bark beetle infestation. Evaluations were performed on both the polygon and line segment level, showing that the multi-contour segmentation can achieve up to 0.93 precision and 0.82 recall. An improvement of up to 7 percentage points (pp) in recall and 6 in precision compared to an iterative sample consensus line segment detection baseline was achieved. Despite the simplicity of the applied shape parametrization, an explicit shape model incorporated into the energy function improved the results by up to 4 pp of recall. Finally, we show the importance of using a high-quality semantic segmentation method (e.g. U-net) as the basis for individual stem detection, as the quality of the results degraded dramatically in our baseline experiment utilizing a simpler method. Our method is a step towards increased accessibility of automatic fallen tree mapping in forests, due to higher cost efficiency of aerial imagery acquisition compared to laser scanning. The precise fallen tree maps could be further used as a basis for plant and animal habitat modeling, studies on carbon sequestration as well as soil quality in forest ecosystems. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","energy minimization; precision forestry; sample consensus; simulated annealing; U-net","Bavaria; Bavarian Forest National Park; Germany; Coleoptera; Aerial photography; Antennas; Convolution; Deep neural networks; Ecosystems; Forestry; Image segmentation; Iterative methods; Parks; Remote sensing; Simulated annealing; Contour evolution; Convolutional networks; Energy minimization; Fallen tree; Percentage points; Precision forestry; Sample consensus; Semantic segmentation; Shape model; U-net; artificial neural network; carbon sequestration; detection method; forest ecosystem; laser method; satellite imagery; segmentation; soil quality; street vendor; tree; Semantics","","","","","Research Grants Council, University Grants Committee, RGC, UGC, (PolyU 25211819); Hong Kong Polytechnic University, PolyU, (G-YBZ9)","The work described in this paper was substantially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU 25211819). The work was also partially supported by grants from The Hong Kong Polytechnical University (Project No.1-ZE8E and G-YBZ9).","Akeret J., Chang C., Lucchi A., Refregier A., Radio frequency interference mitigation using deep convolutional neural networks, Astronomy and Computing, 18, pp. 35-39, (2017); Arnab A., (2017); Cremers D., Rousson M., Efficient kernel density estimation of shape and intensity priors for level set segmentation, pp. 447-460, (2007); Cremers D., Rousson M., Deriche R., A review of statistical approaches to level set segmentation: Integrating color, texture, motion and shape, Int. J. Comput. Vision, 72, pp. 195-215, (2007); Dong H., Yang G., Liu F., Mo Y., Guo Y., Automatic brain tumor detection and segmentation using u-net based fully convolutional networks, Medical Image Understanding and Analysis, pp. 506-517, (2017); Douglas D.H., Peucker T.K., Algorithms for the reduction of the number of points required to represent a digitized line or its caricature, Cartographica: The International Journal for Geographic Information and Geovisualization, 10, pp. 112-122, (1973); Duan F., Wan Y., Deng L., A novel approach for coarse-to-fine windthrown tree extraction based on unmanned aerial vehicle images, Remote Sensing 9., (2017); Einzmann K., Immitzer M., Bock S., Bauer O., Schmitt A., Atzberger C., Windthrow detection in european forests with very high-resolution optical data, Forests., 8, (2017); Fischler M.A., Bolles R.C., Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography, Commun. ACM, 24, pp. 381-395, (1981); Freeman M., Stow D., Roberts D., Object-based image mapping of conifer tree mortality in san diego county based on multitemporal aerial ortho-imagery, Photogrammetric Engineering & Remote Sensing, 82, pp. 571-580, (2016); He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2980-2988, (2017); Jensen J.R., Remote Sensing of the Environment: An Earth Resource Perspective, (2006); Kingma D.P., Ba J., (2017); Kirkpatrick S., Gelatt C.D., Vecchi M.P., Optimization by simulated annealing, Science, 220, 4598, pp. 671-680, (1983); Latifi H., Dahms T., Beudert B., Heurich M., Kubert C., Dech S., Synthetic rapideye data used for the detection of area-based spruce tree mortality induced by bark beetles, GIScience & Remote Sensing, 55, pp. 839-859, (2018); Lausch A., Heurich M., Fahse L., Spatio-temporal infestation patterns of Ips typographus (L.) in the Bavarian Forest National Park, Germany. Ecological Indicators, 31, pp. 73-81, (2013); (2017); Li Y., Qi H., Dai J., Ji X., Wei Y., Fully convolutional instance-aware semantic segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4438-4446, (2017); Li Z., Scheraga H.A., (1987); Long J., Shelhamer E., Darrell T., (2015); Lopes Queiroz G., McDermid G.J., Castilla G., Linke J., Rahman M.M., Mapping coarse woody debris with random forest classification of centimetric aerial imagery, Forests, 10, (2019); Lorensen W.E., Cline H.E., Marching cubes: A high resolution 3d surface construction algorithm, Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques, pp. 163-169, (1987); Marchi N., Pirotti F., Lingua E., (2018); Marcos D., Tuia D., Kellenberger B., Zhang L., Bai M., Liao R., Urtasun R., Learning deep structured active contours end-to-end, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2018); Maturana D., Scherer S., Voxnet: A 3d convolutional neural network for real-time object recognition, 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 922-928, (2015); Milan A., Roth S., Schindler K., Continuous energy minimization for multitarget tracking, IEEE Trans. Pattern Anal. Mach. Intell., 36, pp. 58-72, (2014); Muller J., Butler R., A review of habitat thresholds for dead wood: a baseline for management recommendations in european forests, Eur. J. Forest Res., 129, pp. 981-992, (2010); Nievergelt J., Preparata F.P., Plane-sweep algorithms for intersecting geometric figures, Commun. ACM, 25, pp. 739-747, (1982); Ostovar A., Talbot B., Puliti S., Astrup R., Ringdahl O., Detection and classification of root and butt-rot (rbr) in stumps of norway spruce using rgb images and machine learning, Sensors, 19, (2019); Panagiotidis D., Abdollahnejad A., Surovy P., Kuzelka K., Detection of fallen logs from high-resolution uav images, New Zealand Journal of Forestry, 49, (2019); Polewski P., Shelton J., Yao W., Heurich M., (2020); Polewski P., Yao W., Heurich M., Krzystek P., Stilla U., Detection of fallen trees in ALS point clouds using a Normalized Cut approach trained by simulation, ISPRS Journal of Photogrammetry and Remote Sensing, 105, pp. 252-271, (2015); Polewski P., Yao W., Heurich M., Krzystek P., Stilla U., Detection of single standing dead trees from aerial color infrared imagery by segmentation with shape and intensity priors. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial, Inf. Sci., II-3/W4, pp. 181-188, (2015); Polewski P., Yao W., Heurich M., Krzystek P., Stilla U., A voting-based statistical cylinder detection framework applied to fallen tree mapping in terrestrial laser scanning point clouds, ISPRS Journal of Photogrammetry and Remote Sensing, 129, pp. 118-130, (2017); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1137-1149, (2017); Ronneberger O., pp. 234-241, (2015); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A.C., Fei-Fei L., ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision (IJCV), 115, pp. 211-252, (2015); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of fir trees (abies sibirica) damaged by the bark beetle in unmanned aerial vehicle images with deep learning, Remote Sensing, (2019); Seibold S., Thorn S., The Importance of Dead-Wood Amount for Saproxylic Insects and How It Interacts with Dead-Wood Diversity and Other Habitat Factors, pp. 607-637, (2018); Seidl R., Thom D., Kautz M., Martin-Benito D., Peltoniemi M., Vacchiano G., Wild J., Ascoli D., Petr M., Honkaniemi J., Lexer M.J., Trotsiuk V., Mairota P., Svoboda M., Fabrika M., Nagel T.A., Reyer C.P.O., Forest disturbances under climate change, Nature Climate Change, 7, pp. 395-402, (2017); Shi J., Malik J., Normalized cuts and image segmentation, IEEE T. Pattern Anal., 22, pp. 888-905, (2000); Siarry P., Berthiau G., Fitting of tabu search to optimize functions of continuous variables, Int. J. Numer. Meth. Eng., 40, pp. 2449-2457, (1997); Thiel C., Mueller M.M., Epple L., Thau C., Hese S., Voltersen M., Henkel A., Uas imagery-based mapping of coarse wood debris in a natural deciduous forest in central germany (hainich national park), Remote Sensing, 12, (2020); Tucker C.J., Red and photographic infrared linear combinations for monitoring vegetation, Remote Sens. Environ., 8, pp. 127-150, (1979); Wales D.J., Doye J.P.K., Global optimization by basin-hopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms, The Journal of Physical Chemistry A, 101, pp. 5111-5116, (1997); Wand M.P., Jones C., Multivariate plug-in bandwidth selection, Comput. Statistics, 9, pp. 97-116, (1994); Watson J.E.M., Evans T., Venter O., Williams B., Tulloch A., Stewart C., Thompson I., Ray J.C., Murray K., Salazar A., McAlpine C., Potapov P., Walston J., Robinson J.G., Painter M., Wilkie D., Filardi C., Laurance W.F., Houghton R.A., Maxwell S., Grantham H., Samper C., Wang S., Laestadius L., Runting R.K., Silva-Chavez G.A., Ervin J., Lindenmayer D., The exceptional value of intact forest ecosystems, Nature Ecology & Evolution, 2, pp. 599-610, (2018); Zalik B., Two efficient algorithms for determining intersection points between simple polygons, Computers & Geosciences, 26, pp. 137-151, (2000); Zhao Z., Zheng P., Xu S., Wu X., (2018)","W. Yao; Dept. of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Kowloon, Hung Hom, Hong Kong; email: wei.hn.yao@polyu.edu.hk","","Elsevier B.V.","","","","","","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85109099799"
"Ruf B.; Thiel L.; Weinmann M.","Ruf, B. (56536293400); Thiel, L. (57204607330); Weinmann, M. (55818523800)","56536293400; 57204607330; 55818523800","DEEP CROSS-DOMAIN BUILDING EXTRACTION for SELECTIVE DEPTH ESTIMATION from OBLIQUE AERIAL IMAGERY","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","125","132","7","1","10.5194/isprs-annals-IV-1-125-2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056300936&doi=10.5194%2fisprs-annals-IV-1-125-2018&partnerID=40&md5=66b4808e2af8246f993852684c5375db","Fraunhofer IOSB, Video Exploitation Systems, Karlsruhe, 76131, Germany; Institute of Photogrammetry and Remote Sensing, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany","Ruf B., Fraunhofer IOSB, Video Exploitation Systems, Karlsruhe, 76131, Germany, Institute of Photogrammetry and Remote Sensing, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Thiel L., Fraunhofer IOSB, Video Exploitation Systems, Karlsruhe, 76131, Germany; Weinmann M., Institute of Photogrammetry and Remote Sensing, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany","With the technological advancements of aerial imagery and accurate 3d reconstruction of urban environments, more and more attention has been paid to the automated analyses of urban areas. In our work, we examine two important aspects that allow online analysis of building structures in city models given oblique aerial image sequences, namely automatic building extraction with convolutional neural networks (CNNs) and selective real-time depth estimation from aerial imagery. We use transfer learning to train the Faster R-CNN method for real-time deep object detection, by combining a large ground-based dataset for urban scene understanding with a smaller number of images from an aerial dataset. We achieve an average precision (AP) of about 80 % for the task of building extraction on a selected evaluation dataset. Our evaluation focuses on both dataset-specific learning and transfer learning. Furthermore, we present an algorithm that allows for multi-view depth estimation from aerial image sequences in real-time. We adopt the semi-global matching (SGM) optimization strategy to preserve sharp edges at object boundaries. In combination with the Faster R-CNN, it allows a selective reconstruction of buildings, identified with regions of interest (RoIs), from oblique aerial imagery. © Authors 2018.","aerial oblique imagery; building extraction; convolutional neural networks; deep learning; depth estimation; object detection; semi-global matching; transfer learning","","","","","","","","Alexe B., Deselaers T., Ferrari V., Measuring the objectness of image windows, IEEE Transactions on Pattern Analysis and Machine Intelligence, 34, 11, pp. 2189-2202, (2012); Bodla N., Singh B., Chellappa R., Davis L.S., Soft-nms-improving object detection with one line of code, Proc. IEEE International Conference on Computer Vision, pp. 5562-5570, (2017); Cavegn S., Haala N., Nebiker S., Rothermel M., Tutzauer P., Benchmarking high density image matching for oblique airborne imagery, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XL-3, pp. 45-52, (2014); Cordts M., Omran M., Ramos S., Rehfeld T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The cityscapes dataset for semantic urban scene understanding, Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223, (2016); D'Angelo P., Kuschk G., Dense multi-view stereo from satellite imagery, Proc. IEEE International Geoscience and Remote Sensing Symposium, pp. 6944-6947, (2012); Felzenszwalb P.F., Girshick R.B., McAllester D., Ramanan D., Object detection with discriminatively trained part-based models, IEEE Transactions on Pattern Analysis and Machine Intelligence, 32, 9, pp. 1627-1645, (2010); Girshick R., Fast r-cnn, Proc. IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Haala N., Rothermel M., Cavegn S., Extracting 3d urban models from oblique aerial images, Proc. IEEE Joint Urban Remote Sensing Event, pp. 1-4, (2015); Han S., Pool J., Narang S., Mao H., Gong E., Tang S., Elsen E., Vajda P., Paluri M., Tran J., Et al., DSD: Dense-sparse-dense Training for Deep Neural Networks, (2017); He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proc. IEEE International Conference on Computer Vision, pp. 2980-2988, (2017); Hirschmueller H., Stereo processing by semiglobal matching and mutual information, IEEE Transactions on Pattern Analysis and Machine Intelligence, 30, 2, pp. 328-341, (2008); Hosang J., Benenson R., Dollar P., Schiele B., What makes for effective detection proposals, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 4, pp. 814-830, (2016); Kang S.B., Szeliski R., Chai J., Handling occlusions in dense multi-view stereo, Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1, pp. 103-110, (2001); Kelly M., Urban trees and the green infrastructure agenda, Proc. Urban Trees Research Conference, pp. 166-180, (2011); Kolbe T.H., Representing and exchanging 3d city models with citygml, 3D Geo-Information Sciences, pp. 15-31, (2009); Mayer H., Object extraction in photogrammetric computer vision, ISPRS Journal of Photogrammetry and Remote Sensing, 63, 2, pp. 213-222, (2008); Newcombe R.A., Davison A.J., Live dense reconstruction with a single moving camera, Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1498-1505, (2010); Newcombe R.A., Lovegrove S.J., Davison A.J., Dtam: Dense tracking and mapping in real-time, Proc. IEEE International Conference on Computer Vision, pp. 2320-2327, (2011); Nex F., Gerke M., Remondino F., Przybilla H., Baumker M., Zurhorst A., Isprs benchmark for multi-platform photogrammetry, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, II-3, W4, pp. 135-142, (2015); Palazzolo E., Stachniss C., Change detection in 3d models based on camera images, Proc. 9th Workshop on Planning, Perception and Navigation for Intelligent Vehicles at the IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 1-6, (2017); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, pp. 1137-1149, (2017); Rothermel M., Wenzel K., Fritsch D., Haala N., Sure: Photogrammetric surface reconstruction from imagery, Proc. LC3D Workshop, pp. 1-9, (2012); Rottensteiner F., Sohn G., Jung J., Gerke M., Baillard C., Benitez S., Breitkopf U., The isprs benchmark on urban object classification and 3d building reconstruction, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, I-3, pp. 293-298, (2012); Ruf B., Schuchert T., Towards real-time change detection in videos based on existing 3d models, Proc. SPIE 10004, Image and Signal Processing for Remote Sensing XXII, (2016); Ruf B., Erdnuess B., Weinmann M., Determining planesweep sampling points in image space using the cross-ratio for imagebased depth estimation, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2, W6, pp. 325-332, (2017); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A.C., Fei-Fei L., Imagenet large scale visual recognition challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); Schuffert S., Voegtle T., Tate N., Ramirez A., Quality assessment of roof planes extracted from height data for solar energy systems by the eagle platform, Remote Sensing, 7, 12, pp. 17016-17034, (2015); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); Sommer L.W., Schuchert T., Beyerer J., A comprehensive study on object proposals methods for vehicle detection in aerial images, Proc. 9th IAPR Workshop on Pattern Recognition in Remote Sensing, pp. 1-6, (2016); Stuhmer J., Gumhold S., Cremers D., Parallel generalized thresholding scheme for live dense geometry from a handheld camera, Trends and Topics in Computer Vision, pp. 450-462, (2012); Taneja A., Ballan L., Pollefeys M., Geometric change detection in urban environments using images, IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, 11, pp. 2193-2206, (2015); Uijlings J.R.R., Van De Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, International Journal of Computer Vision, 104, 2, pp. 154-171, (2013); Von Gioi R.G., Jakubowicz J., Morel J.-M., Randall G., Lsd: A line segment detector, Image Processing on Line, 2, pp. 35-55, (2012); Wu C., Agarwal S., Curless B., Seitz S.M., Multicore bundle adjustment, Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 3057-3064, (2011); Zabih R., Woodfill J., Non-parametric local transforms for computing visual correspondence, Proc. European Conference on Computer Vision, pp. 151-158, (1994); Zitnick C.L., Dollar P., Edge boxes: Locating object proposals from edges, Proc. 13th European Conference on Computer Vision, 5, pp. 391-405, (2014)","","Hinz S.; Jutzi B.; Weinmann M.","Copernicus GmbH","","2018 ISPRS Technical Commission I Midterm Symposium on Innovative Sensing - From Sensors to Methods and Applications","10 October 2018 through 12 October 2018","Karlsruhe","140824","21949042","","","","English","ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056300936"
"Chen F.; Zhou R.; Van de Voorde T.; Chen X.; Bourgeois J.; Gheyle W.; Goossens R.; Yang J.; Xu W.","Chen, Fen (55714427600); Zhou, Rui (57189379291); Van de Voorde, Tim (23026266400); Chen, Xingzhuang (57215548684); Bourgeois, Jean (8054759400); Gheyle, Wouter (8054759200); Goossens, Rudi (7006683515); Yang, Jian (58372534400); Xu, Wenbo (56237288700)","55714427600; 57189379291; 23026266400; 57215548684; 8054759400; 8054759200; 7006683515; 58372534400; 56237288700","Automatic detection of burial mounds (kurgans) in the Altai Mountains","2021","ISPRS Journal of Photogrammetry and Remote Sensing","177","","","217","237","20","8","10.1016/j.isprsjprs.2021.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107038472&doi=10.1016%2fj.isprsjprs.2021.05.010&partnerID=40&md5=271f9fb57d9a82b13576f11f42035f90","School of Resources and Environment, University of Electronic Science and Technology of China, 2006 Xiyuan Avenue, West Hi-tech Zone, Chengdu, 611731, Sichuan, China; Department of Geography, Ghent University, Krijgslaan 281, S8, Ghent, 9000, Belgium; Department of Archaeology, Ghent University, Sint-Pieternieuwstraat, 35, Ghent, 9000, Belgium; Fellow President's International Fellowship Initiative, China; Aerospace Information Research Institute, Chinese Academy of Sciences, 9 Dengzhuang South Road, Haidan District, Beijing, 100094, China","Chen F., School of Resources and Environment, University of Electronic Science and Technology of China, 2006 Xiyuan Avenue, West Hi-tech Zone, Chengdu, 611731, Sichuan, China; Zhou R., School of Resources and Environment, University of Electronic Science and Technology of China, 2006 Xiyuan Avenue, West Hi-tech Zone, Chengdu, 611731, Sichuan, China; Van de Voorde T., Department of Geography, Ghent University, Krijgslaan 281, S8, Ghent, 9000, Belgium; Chen X., School of Resources and Environment, University of Electronic Science and Technology of China, 2006 Xiyuan Avenue, West Hi-tech Zone, Chengdu, 611731, Sichuan, China; Bourgeois J., Department of Archaeology, Ghent University, Sint-Pieternieuwstraat, 35, Ghent, 9000, Belgium, Fellow President's International Fellowship Initiative, China; Gheyle W., Department of Archaeology, Ghent University, Sint-Pieternieuwstraat, 35, Ghent, 9000, Belgium; Goossens R., Department of Geography, Ghent University, Krijgslaan 281, S8, Ghent, 9000, Belgium; Yang J., Aerospace Information Research Institute, Chinese Academy of Sciences, 9 Dengzhuang South Road, Haidan District, Beijing, 100094, China; Xu W., School of Resources and Environment, University of Electronic Science and Technology of China, 2006 Xiyuan Avenue, West Hi-tech Zone, Chengdu, 611731, Sichuan, China","The Altai Mountains are one of the most impressive and valuable archaeological areas in the world. Kurgans (burial mounds) of ancient civilizations, which are scattered across the vast Altai area, are an exceptionally valuable source of information for archaeology. These precious archaeological resources, which sometimes have been preserved intact in the permafrost underground for over two millennia, are now under various threats, such as natural disasters, farmland expansion, touristic development, and most notably global warming. A detailed map or inventory of the mounds is essential but is still not available. In this study, we test the deep convolutional neural network (CNN) technique for automatic detection of stone mounds from high-resolution satellite images in four regions in the Altai Mountains. We propose three improvement techniques to increase the performance of off-the-shelf object detection methods that are originally proposed for daily-life objects. Our results demonstrate that it is feasible to apply CNN to detect stone mounds, and the detection results are good enough to capture their spatial distribution. CNN-based object detection can largely narrow down the search area for archaeologists in yet un-surveyed regions, and is therefore useful for preparing field survey campaigns and directing archaeological fieldwork. We also applied the method to an un-surveyed Altai Mountain area and successfully discovered stone mounds that are yet undocumented. Our method can potentially be applied to construct an inventory for all stone mounds present in the whole Altai Mountain region. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Altai Mountains; Kurgans; Object detection; Remote sensing archaeology; Stone mounds","Altai Mountains; agricultural land; archaeology; artificial neural network; automation; civilization; detection method; field survey; fieldwork; global warming; permafrost; satellite imagery; spatial distribution","","","","","Chinese Academy of Sciences, CAS, (2020VCA0015); National Key Research and Development Program of China, NKRDPC, (2017YFC0821900)","The authors would like to thank the anonymous reviewers for their valuable and helpful comments. This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFC0821900, and in part by the Chinese Academy of Sciences President’s International Fellowship Initiative under Grant 2020VCA0015.","Baumer C., (2012); Bourgeois J., Cheremisin D.V., Plets G., Dvornikov E.P., Ebel A.V., Stichelbaut B., Van Hoof L., Gheyle W., An archaeological landscape in the Dzhazator valley (Altai Mountains): Surface monuments and petroglyphs from the Chalcolithic to the Ethnographic period, Archaeol., Ethnol. Anthropology of Eurasia, 42, pp. 106-119, (2014); Bourgeois J., De Langhe K., Ebel A.V., Dvornikov E.P., Konstantinov N.A., Gheyle W., Geometric stone settings in the Yustyd valley and its surroundings (Altai Mountains, Russia): Bronze age ‘virtual dwellings’ and associated structures, Archaeol. Res. Asia, 10, pp. 17-31, (2017); Bourgeois J., De Wulf A., Ebel A.V., Gheyle W., Goossens R., van Hoof L., (2007); Bourgeois J., De Wulf A., Goossens R., Gheyle W., Saving the frozen Scythian tombs of the Altai Mountains (Central Asia), World Archaeol., 39, pp. 458-474, (2007); Bourgeois J., Gheyle W., Babin V., Lukyanenko V.N., (2007); Bourgeois J., Gheyle W., Goossens R.; Byeon W., Dominguez-Rodrigo M., Arampatzis G., Baquedano E., Yravedra J., Mate-Gonzalez M.A., Koumoutsakos P., Automated identification and deep classification of cut marks on bones and its paleoanthropological implications, J. Comput. Sci., 32, pp. 36-43, (2019); Cai Z., Vasconcelos N., Cascade R-CNN: Delving into high quality object detection, IEEE Conference on Computer Vision and Pattern Recognition, pp. 6254-16162, (2018); Caspari G., Mapping and damage assessment of “royal” burial mounds in the Siberian Valley of the Kings, Remote Sens., 12, (2020); Caspari G., Crespo P., Convolutional neural networks for archaeological site detection – Finding ‘Princely’ tombs, J. Archaeol. Sci., 110, (2019); Chen F., Ren R., Van de Voorde T., Xu W., Zhou G., Zhou Y., Fast automatic airport detection in remote sensing images using convolutional neural networks, Remote Sens., 10, (2018); Chen L., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell., 40, pp. 834-848, (2018); Chen Z., Zhang T., Ouyang C., End-to-end airplane detection using transfer learning in remote sensing images, Remote Sens., 10, (2018); Chetouani A., Debroutelle T., Treuillet S., Exbrayat M., Jesset S., pp. 1038-1042, (2018); Cugunov K.V., Parzinger H., Nagler A., Der Goldschatz von Aržan. Ein Fürstengrab der Skythenzeit in der südsibirischen Steppe, (2006); Cunliffe B., By steppe, desert, and ocean: The birth of Eurasia, (2015); De Laet V., Paulissen E., Meuleman K., Waelkens M., Effects of image characteristics on the identification and extraction of archaeological features from Ikonos-2 and Quickbird-2 imagery: case study Sagalassos (southwest Turkey), Int. J. Remote Sens., 30, pp. 5655-5668, (2009); De Laet V., Paulissen E., Waelkens M., Methods for the extraction of archaeological features from very high-resolution Ikonos-2 remote sensing imagery, Hisar (southwest Turkey), J. Archaeol. Sci., 34, pp. 830-841, (2007); Figorito B., Tarantino E., Semi-automatic detection of linear archaeological traces from orthorectified aerial images, Int. J. Appl. Earth Obs. Geoinf., 26, pp. 458-463, (2014); Gheyle W., (2009); Gheyle W., De Wulf A., Dvornikov E.P., Ebel A.V., Goossens R., Bourgeois J., (2016); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Goossens R., De Wulf A., Bourgeois J., Gheyle W., Willems T., Satellite imagery and archaeology: the example of CORONA in the Altai Mountains, J. Archaeol. Sci., 33, pp. 745-755, (2006); Han J., Frank L., Treselian D., The preservation of the frozen tombs of the Altai Mountains, (2008); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, IEEE International Conference on Computer Vision, pp. 2980-2988, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Kong T., Sun F., Liu H., Jiang Y., Shi J., (2019); Krizhevsky A., Sutskever I., Hinton G., ImageNet classification with deep convolutional neural networks, 25th International Conference on Neural Information Processing Systems, pp. 1097-1105, (2012); Kubarev V.D., Кypгaны Улaндpыкa - Kurgany Ulandryka, (1987); Kubarev V.D., Кypгaны Юcтыдa - Kurgany Yustyda, (1991); Kubarev V.D., Кypгaны Caйлюгeмa - Kurgany Saylyugema, (1992); Laben C.A., Brower B.V., (2000); Lasaponara R., Masini N., Detection of archaeological crop marks by using satellite Quickbird multispectral imagery, J. Archaeol. Sci., 34, pp. 214-221, (2007); Li H., Lin Z., Shen X., Brandt J., Hua G., A convolutional neural network cascade for face detection, IEEE Conference on Computer Vision and Pattern Recognition, pp. 5325-5334, (2015); Li Q., Zou Q., Ma D., Wang Q., Wang S., Dating ancient paintings of Mogao Grottoes using deeply learnt visual codes, Sci. China Inform. Sci., 61, pp. 1-14, (2018); Lin T., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Featured pyramid networks for object detection, IEEE Conference on Computer Vision and Pattern Recognition, pp. 936-944, (2017); Lin T., Goyal P., Girshick R.B., He K., Dollar P., Focal loss for dense object detection, IEEE International Conference on Computer Vision, pp. 2999-3007, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C., Berg A., SSD: Single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, (2015); Molodin V.I., Polosmak N.V., Novikov A.V., Bogdanov E.S., Sljusarenko I.Y., Cheremisin D.V., Apxeoлoгичecкиe пaмятники плocкoгopья Укoк (Гopный Aлтaй) - Archeologicheskie pamyatniki ploskogor'ya Ukok (Gornyy Altay). Maтepиaлы пo apxeoлoгии Cибиpи - Materialy po arkheologii Sibiri, 3, (2004); Nawroth M., Menghin W., Parzinger H., Graichen G., Prestel P., (2007); Orengo H.A., Conesa F.C., Garcia-Molsosa A., Lobo A., Green A.S., Madella M., Petrie C.A., Automated detection of archaeological mounds using machine-learning classification of multisensor and multitemporal satellite data, PNAS, 117, pp. 18240-18250, (2020); Pang J., Chen K., Shi J., Feng H., Ouyang W., Lin D., Libra R-CNN: Towards balanced learning for object detection, IEEE Conference on Computer Vision and Pattern Recognition, pp. 821-830, (2019); Parzinger H., Die Frühen Völker Eurasiens Vom Neolithikum Bis Zum Mittelalter, (2006); Plets G., Gheyle W., Plets R., Dvornikov E.P., Bourgeois J., A line through the sacred lands of the Altai Mountains: Perspectives on the Altai pipeline project, Mt. Res. Dev., 31, pp. 372-379, (2011); Polosmak N.V., The Ak-Alakh “Frozen Grave” Barrow, Ancient Civilizations from Scythia to Siberia, 1, pp. 346-354, (1994); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, IEEE Conference on Computer Vision and Pattern Recognition, pp. 6517-6525, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1137-1149, (2017); Rudenko S.I., Thompson M.W., Frozen Tombs of Siberia: The Pazyryk Burials of Iron-Age Horsemen, (1970); Schuetter J., Goel P., McCorriston J., Park J., Senn M.J., Harrower M., Autodetection of ancient Arabian tombs in high-resolution satellite imagery, Int. J. Remote Sens., 34, pp. 6611-6635, (2013); Silverman B.W., Density estimation for statistics and data analysis, (1986); Simpson S.J., Pankova S., Scythians, Warriors of ancient Siberia, (2017); Singh B., Davis L.S., An analysis of scale invariance in object detection-SNIP, IEEE Conference on Computer Vision and Pattern Recognition, pp. 3578-3587, (2018); Tian Z., Shen C., Chen H., He T., FCOS: Fully convolutional one-stage object detection, IEEE Conference on Computer Vision, pp. 9627-9636, (2019); Veit A., Wilber M., Belongie S., (2016); Wang H., He Z., Huang Y., Chen D., Zhou Z., Bodhisattva head images modeling style recognition of Dazu rock carvings based on deep convolutional network, J. Cult. Heritage, 27, pp. 60-71, (2017); Zhang L., Lin L., Liang X., He K., Is Faster RCNN doing well for pedestrian detection?, European Conference on Computer Vision, 9906, pp. 443-457, (2016); Zhang Z., Guo W., Zhu S., Yu W., Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks, IEEE Geosci. Remote Sens. Lett., 15, pp. 1745-1749, (2018); Zhou Y., Liu L., Shao L., Mellor M., DAVE: A unified framework for fast vehicle detection and annotation, European Conference on Computer Vision, 9906, pp. 278-293, (2016); Zingman I., Saupe D., Penatti O.A.B., Lambers K., Detection of fragmented rectangular enclosures in very high resolution remote sensing images, IEEE Trans. Geosci. Remote Sens., 54, pp. 4580-4593, (2016)","F. Chen; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, 2006 Xiyuan Avenue, West Hi-tech Zone, 611731, China; email: chenfen@uestc.edu.cn","","Elsevier B.V.","","","","","","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","","Scopus","2-s2.0-85107038472"
"Koeshidayatullah A.; Morsilli M.; Lehrmann D.J.; Al-Ramadan K.; Payne J.L.","Koeshidayatullah, Ardiansyah (56387553100); Morsilli, Michele (6507958441); Lehrmann, Daniel J. (6602378436); Al-Ramadan, Khalid (57204422977); Payne, Jonathan L. (7403333496)","56387553100; 6507958441; 6602378436; 57204422977; 7403333496","Fully automated carbonate petrography using deep convolutional neural networks","2020","Marine and Petroleum Geology","122","","104687","","","","34","10.1016/j.marpetgeo.2020.104687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090602397&doi=10.1016%2fj.marpetgeo.2020.104687&partnerID=40&md5=48789da49f3149e6f2288e0fd34c9fd7","Department of Geological Sciences, Stanford University, Stanford, 94305, CA, United States; Dipartimento di Fisica e Scienze Della Terra, Università di Ferrara, Ferrara, 44122, Italy; Department of Geosciences, Trinity University, San Antonio, 78212, TX, United States; College of Petroleum Engineering & Geosciences, King Fahd University of Petroleum & Minerals, Dhahran, 31261, Saudi Arabia","Koeshidayatullah A., Department of Geological Sciences, Stanford University, Stanford, 94305, CA, United States; Morsilli M., Dipartimento di Fisica e Scienze Della Terra, Università di Ferrara, Ferrara, 44122, Italy; Lehrmann D.J., Department of Geosciences, Trinity University, San Antonio, 78212, TX, United States; Al-Ramadan K., College of Petroleum Engineering & Geosciences, King Fahd University of Petroleum & Minerals, Dhahran, 31261, Saudi Arabia; Payne J.L., Department of Geological Sciences, Stanford University, Stanford, 94305, CA, United States","Carbonate rocks are important archives of past ocean conditions as well as hosts of economic resources such as hydrocarbons, water, and minerals. Geologists typically perform compositional analysis of grain, matrix, cement and pore types in order to interpret depositional environments, diagenetic modification, and reservoir quality of carbonate strata. Such information can be obtained primarily from petrographic analysis, a task that is costly, labor-intensive, and requires in-depth knowledge of carbonate petrology and micropaleontology. Recent studies have leveraged machine learning-based image analysis, including Deep Convolutional Neural Networks (DCNN), to automate description, classification and interpretation of thin sections, subsurface core images and seismic facies, which would accelerate data acquisition and reproducibility for these tasks. In carbonate rocks, this approach has been applied primarily to recognize carbonate lithofacies, and no attempt has been made to individually identify and quantify various types of carbonate grains, matrix, and cement. In this study, the applicability and performance of DCNN-based object detection and image classification approaches are assessed with respect to carbonate compositional analysis. The training data comprised of more than 13,000 individually labelled objects from nearly 4000 carbonate petrographic images. The dataset is grouped into six and nine different classes for the image classification and object detection tasks, respectively. Even with a small and relatively imbalanced training set, the DCNN was able to achieve an F1 score of 92% for image classification and 84% mean precision for object detection by combining one-cycle policy, class weight, and label mixup-smoothing methods. This study highlights the inefficiency of image classification as an approach to replicating human description and classification of carbonate petrography. By contrast, DCNN-based object detection appears capable of approaching human speed and accuracy in the area of carbonate petrography because it is able to individually locate and identify different carbonate components with greater cost-efficiency, speed, and reproducibility than conventional (human) petrographic analysis. © 2020 Elsevier Ltd","Automated analysis; Carbonate; Deep learning; Object detection; Petrography","Carbonates; Cements; Classification (of information); Convolution; Convolutional neural networks; Data acquisition; Deep neural networks; Image classification; Matrix algebra; Object detection; Object recognition; Quality control; Sedimentary rocks; Carbonate petrologies; Classification and interpretation; Classification approach; Compositional analysis; Depositional environment; Economic resources; Human descriptions; Petrographic analysis; artificial neural network; carbonate rock; computer simulation; host rock; hydrocarbon exploration; machine learning; petrography; source rock; Carbonation","","","","","","","Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Kudlur M., Tensorflow: a system for large-scale machine learning, 12th {USENIX} Symposium on Operating Systems Design and Implementation (OSDI), pp. 265-283, (2016); Adams A., Mackenzie I., Carbonate Sediments and Rocks under the Microscope: a Colour Atlas, (1998); Al-Ramadan K., Koeshidayatullah A., Cantrell D., Swart P.K., Impact of basin architecture on diagenesis and dolomitization in a fault-bounded carbonate platform: outcrop analogue of a pre-salt carbonate reservoir, Red Sea rift, NW Saudi Arabia, Petrol. Geosci., 26, 3, pp. 448-461, (2019); Alejo R., Sotoca J.M., Valdovinos R.M., Toribio P., June. Edited nearest neighbor rule for improving neural networks classifications, International Symposium on Neural Networks, pp. 303-310, (2010); Amao A.O., Al-Ramadan K., Koeshidayatullah A., Automated mineralogical methodology to study carbonate grain microstructure: an example from oncoids, Environ. Earth Sci., 75, 8, (2016); Asmussen P., Conrad O., Gunther A., Kirsch M., Riller U., Semi-automatic segmentation of petrographic thin section images using a “seeded-region growing algorithm” with an application to characterize wheathered subarkose sandstone, Comput. Geosci., 83, pp. 89-99, (2015); Ashena R., Thonhauser G., Application of artificial neural networks in geoscience and petroleum industry, Artificial Intelligent Approaches in Petroleum Geosciences, pp. 127-166, (2015); Baraboshkin E.E., Ismailova L.S., Orlov D.M., Zhukovskaya E.A., Kalmykov G.A., Khotylev O.V., Baraboshkin E.Y., Koroteev D.A., Deep convolutions for in-depth automated rock typing, Comput. Geosci., 135, (2020); Bengio Y., June. Deep learning of representations for unsupervised and transfer learning, Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pp. 17-36, (2012); Bianco S., Cadene R., Celona L., Napoletano P., Benchmark analysis of representative deep neural network architectures, IEEE Access, 6, pp. 64270-64277, (2018); Bradski G., The OpenCV library, Dr. Dobb's J. Softw. Tools, 120, pp. 122-125, (2000); Bridle J.S., Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition, Neurocomputing, pp. 227-236, (1990); Buda M., Maki A., Mazurowski M.A., A systematic study of the class imbalance problem in convolutional neural networks, Neural Network., 106, pp. 249-259, (2018); Burchette T.P., Are carbonate reservoirs ‘difficult’?, GEOExPro, 16, 5, (2019); Celik Y., Talo M., Yildirim O., Karabatak M., Acharya U.R., Automated invasive ductal carcinoma detection based using deep transfer learning with whole-slide images, Pattern Recogn. Lett., 133, pp. 232-239, (2020); Canchumuni S.W., Emerick A.A., Pacheco M.A.C., History matching geological facies models based on ensemble smoother and deep generative models, J. Petrol. Sci. Eng., 177, pp. 941-958, (2019); Chatterjee S., Dutta R.K., Ganguly D., Chatterjee K., Roy S., Bengali Handwritten Character Classification Using Transfer Learning on Deep Convolutional Neural Network, (2019); Chawla N.V., Bowyer K.W., Hall L.O., Kegelmeyer W.P., SMOTE: synthetic minority over-sampling technique, J. Artif. Intell. Res., 16, pp. 321-357, (2002); Choi Y., Choi M., Kim M., Ha J.W., Kim S., Choo J., Stargan: unified generative adversarial networks for multi-domain image-to-image translation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8789-8797, (2018); Chollet F., Keras, (2015); Dai J., Li Y., He K., Sun J., R-FCN: object detection via region-based fully convolutional networks, Advances in Neural Information Processing Systems, pp. 379-387, (2016); de Lima R.P., Suriamin F., Marfurt K.J., Pranter M.J., Convolutional neural networks as aid in core lithofacies classification, Interpretation, 7, 3, pp. SF27-SF40, (2019); de Lima R.P., Duarte D., Nicholson C., Slatt R., Marfurt K.J., Petrographic microfacies classification with deep convolutional neural networks, Comput. Geosci., (2020); Della Porta G., Wright P.V., (2009); Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L., Imagenet: a large-scale hierarchical image database, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, (2009); Dunham R.J., Classification of Carbonate Rocks According to Depositional Textures. AAPG Special Volumes: Classification of Carbonate Rocks—A Symposium, pp. 108-121, (1962); Eberli G.P., Baechle G.T., Anselmetti F.S., Incze M.L., Factors controlling elastic properties in carbonate sediments and rocks, Lead. Edge, 22, 7, pp. 654-660, (2003); Eltom H.A., Abdullatif O.M., Babalola L.O., Bashari M.A., Yassin M., Osman M.S., Abdulraziq A.M., Integration of facies architecture, ooid granulometry and morphology for prediction of reservoir quality, Lower Triassic Khuff Formation, Saudi Arabia, Petrol. Geosci., 23, 2, pp. 177-189, (2017); Fawcett T., An introduction to ROC analysis, Pattern Recogn. Lett., 27, pp. 861-874, (2006); Flugel E., Microfacies of Carbonate Rocks: Analysis, Interpretation and Application, (2013); Geng Z., Wu X., Shi Y., Fomel S., Deep learning for relative geologic time and seismic horizons, Geophysics, 85, 4, pp. 87-100, (2020); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., Generative adversarial nets, Advances in Neural Information Processing Systems, pp. 2672-2680, (2014); Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); Grove C., Jerram D.A., jPOR: an ImageJ macro to quantify total optical porosity from blue-stained thin sections, Comput. Geosci., 37, 11, pp. 1850-1859, (2011); Grzymala-Busse J.W., Goodwin L.K., Grzymala-Busse W.J., Zheng X., An approach to imbalanced data sets based on changing rule strength, Rough-neural Computing, pp. 543-553, (2004); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 37, 9, pp. 1904-1916, (2015); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, pp. 2961-2969, (2017); Hossain M.Z., Sohel F., Shiratuddin M.F., Laga H., A comprehensive survey of deep learning for image captioning, ACM Comput. Surv., 51, 6, pp. 1-36, (2019); Howard J., Gugger S., Fastai: a layered API for deep learning, Information, 11, 2, (2020); Hu G.X., Yang Z., Hu L., Huang L., Han J.M., Small object detection with multiscale features, Int. J. Data Min. Bioinf., 2018, pp. 1-10, (2018); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I., Wojna Z., Song Y., Guadarrama S., Murphy K., Speed/accuracy trade-offs for modern convolutional object detectors, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7310-7311, (2017); Huang L., Dong X., Clee T.E., A scalable deep learning platform for identifying geologic features from seismic attributes, Lead. Edge, 36, 3, pp. 249-256, (2017); Hunter J.D., Matplotlib: a 2D graphics environment, Comput. Sci. Eng., 9, 3, pp. 90-95, (2007); Izadi H., Sadri J., Bayati M., An intelligent system for mineral identification in thin sections based on a cascade approach, Comput. Geosci., 99, pp. 37-49, (2017); John C., Kanagandran S., AI to improve the reliability and reproducibility of descriptive data: a case study using convolutional neural networks to recognize carbonate facies in cores, AAPG Annual Convention and Exhibition, San Antonio, TX, (2019); Johnson B.A., Tateishi R., Hoan N.T., A hybrid pansharpening approach and multiscale object-based image analysis for mapping diseased pine and oak trees, Int. J. Rem. Sens., 34, 20, pp. 6969-6982, (2013); Johnson J.M., Khoshgoftaar T.M., Survey on deep learning with class imbalance, J. Big Data, 6, 1, (2019); King G., Zeng L., Logistic regression in rare events data, Polit. Anal., 9, 2, pp. 137-163, (2001); Kingma D.P., Ba J., Adam: A Method for Stochastic Optimization, (2014); Koeshidayatullah A., Al-Ramadan K., Collier R., Hughes G.W., Variations in architecture and cyclicity in fault-bounded carbonate platforms: early miocene red sea rift, NW Saudi Arabia, Mar. Petrol. Geol., 70, pp. 77-92, (2016); Koeshidayatullah A., Corlett H., Stacey J., Swart P.K., Boyce A., Robertson H., Whitaker F., Hollis C., Evaluating new fault-controlled hydrothermal dolomitization models: Insights from the Cambrian Dolomite, Western Canadian Sedimentary Basin, Sedimentology, (2020); Kohavi R., A study of cross-validation and bootstrap for accuracy estimation and model selection, Proc., 14th Int. Joint Conf. On Artificial Intelligence, pp. 1137-1143, (1995); Krizhevsky A., Nair V., Hinton G., Cifar-10 and Cifar-100 Datasets, (2009); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); LeCun Y., Boser B., Denker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation applied to handwritten zip code recognition, Neural Comput., 1, pp. 541-551, (1989); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Lemaitre G., Nogueira F., Aridas C.K., Imbalanced-learn: a python toolbox to tackle the curse of imbalanced datasets in machine learning, J. Mach. Learn. Res., 18, 1, pp. 559-563, (2017); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., September. Microsoft coco: common objects in context, European Conference on Computer Vision, pp. 740-755, (2014); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, pp. 2980-2988, (2017); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117-2125, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Liu Y., Sun W., Durlofsky L.J., A deep-learning-based geological parameterization for history matching complex models, Math. Geosci., 51, 6, pp. 725-766, (2019); Lokier S.W., Al Junaibi M., The petrographic description of carbonate facies: are we all speaking the same language?, Sedimentology, 63, 7, pp. 1843-1885, (2016); Lynda N.O., Systematic survey of convolutional neural network in satellite image classification for geological mapping, 15th International Conference on Electronics, pp. 1-6, (2019); Mosser L., Dubrule O., Blunt M.J., Reconstruction of three-dimensional porous media using generative adversarial neural networks, Phys. Rev., 96, 4, (2017); Nanjo T., Tanaka S., January. Carbonate lithology identification with generative adversarial networks, International Petroleum Technology Conference. International Petroleum Technology Conference, (2020); Oliphant T.E., A Guide to NumPy, 1, (2006); Pan Z., Yu W., Yi X., Khan A., Yuan F., Zheng Y., Recent progress on generative adversarial networks (GANs): a survey, IEEE Access, 7, pp. 36322-36333, (2019); Pang J., Li C., Shi J., Xu Z., Feng H., R2-CNN: fast Tiny object detection in large-scale remote sensing images, IEEE Trans. Geosci. Rem. Sens., 57, 8, pp. 5512-5524, (2019); Paszke A., Gross S., Massa F., Lerer A., Bradbury J., Chanan G., Killeen T., Lin Z., Gimelshein N., Antiga L., Desmaison A., PyTorch: an imperative style, high-performance deep learning library, Advances in Neural Information Processing Systems, pp. 8024-8035, (2019); Payne J.L., Lehrmann D.J., Wei J., Knoll A.H., The pattern and timing of biotic recovery from the end-Permian extinction on the Great Bank of Guizhou, Guizhou Province, China, Palaios, 21, 1, pp. 63-85, (2006); Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay E., Scikit-learn: machine learning in Python, J. Mach. Learn. Res., 12, pp. 2825-2830, (2011); Perez L., Wang J., The Effectiveness of Data Augmentation in Image Classification Using Deep Learning, (2017); Qian F., Yin M., Liu X.Y., Wang Y.J., Lu C., Hu G.M., Unsupervised seismic facies analysis via deep convolutional autoencoders, Geophysics, 83, 3, pp. A39-A43, (2018); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A., Yolov3: an Incremental Improvement, (2018); Reichstein M., Camps-Valls G., Stevens B., Jung M., Denzler J., Carvalhais N., Prabhat, Deep learning and process understanding for data-driven Earth system science, Nature, 566, pp. 195-204, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, pp. 91-99, (2015); Ronneberger O., Fischer P., Brox T., October. U-net: convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Samek W., Binder A., Montavon G., Lapuschkin S., Muller K.R., Evaluating the visualization of what a deep neural network has learned, IEEE Trans. Neur. Network. Learn. Syst., 28, 11, pp. 2660-2673, (2016); Saporetti C.M., da Fonseca L.G., Pereira E., de Oliveira L.C., Machine learning approaches for petrographic classification of carbonate-siliciclastic rocks using well logs and textural information, J. Appl. Geophys., 155, pp. 217-225, (2018); Scholle P.A., Ulmer-Scholle D.S., A Color Guide to the Petrography of Carbonate Rocks: Grains, Textures, Porosity, Diagenesis, (2003); Shin H.C., Roth H.R., Gao M., Lu L., Xu Z., Nogues I., Yao J., Mollura D., Summers R.M., Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning, IEEE Trans. Med. Imag., 35, 5, pp. 1285-1298, (2016); Silva A.A., Tavares M.W., Carrasquilla A., Missagia R., Ceia M., Petrofacies classification using machine learning algorithms, Geophysics, 85, 4, pp. WA101-WA113, (2020); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2014); Smith L.N., A Disciplined Approach to Neural Network Hyper-Parameters: Part 1–learning Rate, Batch Size, Momentum, and Weight Decay, (2018); Soviany P., Ionescu R.T., Optimizing the trade-off between single-stage and two-stage deep object detectors using image difficulty prediction, 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, pp. 209-214, (2018); Sun X., Dong K., Ma L., Sutcliffe R., He F., Chen S., Feng J., Drug-drug interaction extraction via recurrent hybrid convolutional neural networks with an improved focal loss, Entropy, 21, 1, (2019); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., Inception-v4, inception-resnet and the impact of residual connections on learning, Thirty-first AAAI Conference on Artificial Intelligence, (2017); Tan C., Sun F., Kong T., Zhang W., Yang C., Liu C., October. A survey on deep transfer learning, International Conference on Artificial Neural Networks, pp. 270-279, (2018); Tzutalin L., Git Code, (2015); Wang Q., Bi S., Sun M., Wang Y., Wang D., Yang S., Deep learning approach to peripheral leukocyte recognition, PloS One, 14, 6, (2019); Wu X., Xu K., Hall P., A survey of image synthesis and editing with generative adversarial networks, Tsinghua Sci. Technol., 22, 6, pp. 660-674, (2017); Xu T., Zhang P., Huang Q., Zhang H., Gan Z., Huang X., He X., Attngan: fine-grained text to image generation with attentional generative adversarial networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1316-1324, (2018); Zhang Z., He T., Zhang H., Zhang Z., Xie J., Li M., Bag of Freebies for Training Object Detection Neural Networks, (2019); Zhong Z., Sun A.Y., Yang Q., Ouyang Q., A deep learning approach to anomaly detection in geological carbon sequestration sites using pressure measurements, J. Hydrol., 573, pp. 885-894, (2019)","A. Koeshidayatullah; Stanford University, Stanford, 450 Jane Stanford Way, 94305, United States; email: ardikoes@stanford.edu","","Elsevier Ltd","","","","","","02648172","","","","English","Mar. Pet. Geol.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090602397"
"Gonzalez M.E.; Catlett A.; Jabour J.E.; Patel R.R.; Price S.R.","Gonzalez, Megan E. (57224521145); Catlett, Amanda (57224517939); Jabour, Joseph E. (57224516925); Patel, Reena R. (35249230400); Price, Stanton R. (55810815000)","57224521145; 57224517939; 57224516925; 35249230400; 55810815000","Leveraging synthetic imagery to train deep learning algorithms for the detection of objects of interest in radiant energy imagery","2021","Proceedings of SPIE - The International Society for Optical Engineering","11728","","1172803","","","","0","10.1117/12.2585770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107764990&doi=10.1117%2f12.2585770&partnerID=40&md5=e054917ce9af25a23c41866e988416f8","U.S. Army Engineer and Research Development Center, 3909 Halls Ferry Road, Vicksburg, MS, United States","Gonzalez M.E., U.S. Army Engineer and Research Development Center, 3909 Halls Ferry Road, Vicksburg, MS, United States; Catlett A., U.S. Army Engineer and Research Development Center, 3909 Halls Ferry Road, Vicksburg, MS, United States; Jabour J.E., U.S. Army Engineer and Research Development Center, 3909 Halls Ferry Road, Vicksburg, MS, United States; Patel R.R., U.S. Army Engineer and Research Development Center, 3909 Halls Ferry Road, Vicksburg, MS, United States; Price S.R., U.S. Army Engineer and Research Development Center, 3909 Halls Ferry Road, Vicksburg, MS, United States","Radiant energy object detection deep learning algorithms require large training sets with site-specific images, often from locations that are difficult to access, while also remaining diverse enough to encourage a robust model. Of particular interest is the detection of buried and partially buried objects which have a widely varying behavior profile dependent on factors such as depth, soil composition, time of day, moisture level, target composition, etc. The variety associated with these variables increases the difficulty of acquiring an adequately diverse data set. Synthetic imagery offers a potential solution to limited accessibility to data as images can be created on demand with diversity limited only by the parameters of the simulation. The goal of this study is to create custom models using SSD (Single Shot MultiBox Detector), YOLOv3 (You Only Look Once), and Faster R-CNN (Region- based Convolutional Neural Networks) to detect buried objects in real images by leveraging synthetic radiant energy imagery. Custom training is done on a synthetic data set (made in-house) using pre-trained models from Tensor ow's model zoo and ImageAI's YOLOv3 pre-trained model. Model training leverages high performance computing (HPC) resources and utilizes GPU to optimize training speed. Proof-of-concept models for SSD, YOLOv3, and Faster R-CNN have been trained on preliminary synthetic imagery and analyzed. Preliminary results for these models will be discussed.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Automated Target Recognition; High Performance Computing; Machine Learning; Synthetic In- frared Imagery; Tensor ow 2","Convolutional neural networks; Deep learning; Learning systems; Object detection; Object recognition; Radar imaging; Synthetic aperture radar; Tracking radar; High-performance computing resources; Moisture level; Proof of concept; Robust modeling; Soil composition; Synthetic datasets; Synthetic imagery; Target composition; Learning algorithms","","","","","AVCC; University of New South Wales, Australia","This work is supported by AVCC India Exchange program grant, The University of New South Wales, Australia.","DiMarzio C. A., Vo-Dinh T., Scott H. E., Some approaches to infrared spectroscopy for detection of buried objects, Detection and Remediation Technologies for Mines and Minelike Targets III ], 3392, pp. 158-166, (1998); DelGrande N., Durbin P. F., Gorvad M. R., Perkins D. E., Clark G. A., Hernandez J. D., Sherwood R. J., Dual-band infrared capabilities for imaging buried-object sites, Underground and Obscured Object Imaging and Detection], pp. 166-177, (1993); Thanh N. T., Sahli H., Hao D. N., Detection and characterization of buried landmines using infrared thermography, Inverse Problems in Science and Engineering, 19, 3, pp. 281-307, (2011); Galmiche F., Maldague X. P., Active infrared thermography for land mine detection, Diagnostic Imaging Technologies and Industrial Applications], 3827, pp. 146-154, (1999); Price S. R., Anderson D. T., Stone K., Keller J. M., Investigation of context, soft spatial, and spatial frequency domain features for buried explosive hazard detection in-lwir, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XIX], (2014); Price S. R., Murray B., Hu L., Anderson D. T., Havens T. C., Luke R. H., Keller J. M., Multiple kernel based feature and decision level fusion of ieco individuals for explosive hazard detection in ir im-agery, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XXI], 9823, (2016); Stone K., Keller J., Anderson D., Barclay D., An automatic detection system for buried explosive hazards in-lwir and-gpr data, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XVII], (2012); Stone K., Keller J., Convolutional neural network approach for buried target recognition in-lwir imagery, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XIX], (2014); Popescu M., Paino A., Stone K., Keller J. M., Detection of buried objects in ir imaging using mathematical morphology and svm, 2012 IEEE Symposium on Computational Intelligence for Security and Defence Applications], pp. 1-5, (2012); Khanafer K., Vafai K., Baertlein B. A., Effects of thin metal outer case and top air gap on thermal ir images of buried antitank and antipersonnel land mines, IEEE Transactions on Geoscience and Remote Sensing, 41, 1, pp. 123-135, (2003); Khanafer K., Vafai K., Thermal analysis of buried land mines over a diurnal cycle, IEEE Transactions on Geoscience and Remote Sensing, 40, 2, pp. 461-473, (2002); Rodger I., Connor B., Robertson N. M., Classifying objects in lwir imagery via cnns, Electro-Optical and Infrared Systems: Technology and Applications XIII ], 9987, (2016); Rajpura P. S., Bojinov H., Hegde R. S., Object detection using deep cnns trained on synthetic images, (2017); Carlson A., Skinner K. A., Vasudevan R., Johnson-Roberson M., Modeling camera Effects to improve visual learning from synthetic data, Proceedings of The European Conference on Computer Vision (ECCV) Workshops], (2018); Ren Y., Zhu C., Xiao S., Small object detection in optical remote sensing images via modied faster r-cnn, Applied Sciences, 8, 5, (2018); Yang J., Wang W., Lin G., Li Q., Sun Y., Sun Y., Infrared thermal imaging-based crack detection using deep learning, IEEE Access, 7, pp. 182060-182077, (2019); Li M., Zhang Z., Lei L., Wang X., Guo X., Agricultural greenhouses detection in high-resolution satellite images based on convolutional neural networks: Comparison of faster r-cnn, yolo v3 and ssd, Sensors, 20, 17, (2020); Nowruzi F. E., Kapoor P., Kolhatkar D., Hassanat F. A., Laganiere R., Rebut J., How much real data do we actually need: Analyzing object detection performance using synthetic and real data, (2019); Hinterstoisser S., Pauly O., Heibel H., Martina M., Bokeloh M., An annotation saved is an an-notation earned: Using fully synthetic training for object detection, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops], (2019); Price S. R., Price S. R., ieco learned matched filters for automatic target recognition in synthetic midwave infrared imagery, Automatic Target Recognition XXIX], (2019); Gagnon M.-A., Lagueux P., Gagnon J.-P., Savary S., Tremblay P., Farley V., Guyot E., Chamber-land M., Airborne thermal infrared hyperspectral imaging of buried objects, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XX], 9454, (2015); Uppsall M. S., Pettersson L. M., Georgson M., Sjokvist S., Temporal ir contrast variation of buried land mines, Detection and Remediation Technologies for Mines and Minelike Targets V], 4038, pp. 146-155, (2000); Carlo S., Ir thermography for the detection of buried objects: A short review, (2009); Simard J.-R., Improved landmine detection capability (ildc): systematic approach to the detection of buried mines using passive ir imaging, Detection and Remediation Technologies for Mines and Minelike Targets], 2765, pp. 489-500, (1996); Nguyen T. T., Hao D. N., Lopez P., Cremer F., Sahli H., Thermal infrared identication of buried landmines, Detection and Remediation Technologies for Mines and Minelike Targets X], 5794, pp. 198-208, (2005); Thanh N. T., Sahli H., Hao D. N., Infrared thermography for buried landmine detection: Inverse problem setting, IEEE Transactions on Geoscience and Remote Sensing, 46, 12, pp. 3987-4004, (2008); Simard P., Link N. K., Kruk R. V., Evaluation of algorithms for fusing infrared and synthetic imagery, Enhanced and Synthetic Vision 2000], 4023, pp. 127-138, (2000); Howington S., Peters J. F., Ballard J., Berry T., Lynch L., Kees C., A suite of models for producing synthetic, small-scale thermal imagery of vegetated soil surfaces, Proceedings of CMWR XVI International Conference for Computational Methods in Water Resources, (2006); Kniaz V., Gorbatsevich V., Mizginov V., Thermalnet: a deep convolutional network for synthetic thermal image generation, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, (2017); Schmidt J. H., A 3-d adaptive fnite element method for transport processes, (1995); Ballard J, A three dimensional heat and mass transport model for a tree within a forest, (2011); Peters J. F., Howington S., Lynch L., Signature evaluation for thermal infrared countermine and ied detection systems, DoD High Performance Computing Modernization Program Users Group Conference, pp. 238-246, (2007); Kala R. V., Fairley J. R., Price S. J., Ballard J. R., Carrillo A. R., Howington S. E., Eslinger O. J., Hines A. M., Goodson R. A., Overview of computational testbed for evaluating electro-optical/infrared sensor systems, Proc. SPIE 8357, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XVII, 83570J], (2012); Hunter R. H., White B. C., Patel R. R., Ballard J. R., Partitioning terabyte-scale faceted geometry models for efficient parallel ray tracing using out-of-core memory, Proceedings of the 2020 ACM Southeast Conference (ACM SE '20), pp. 256-259, (2020); Hunter R. H., White B. C., Patel R. R., Ballard J. R., Using morton codes to partition faceted geometry, (2020); Hunter R. H., White B. C., Patel R. R., Ballard J. R., Parallel i/o for geometric models, (2020); White B. C., Hunter R. H., Valoroso A. A., Patel R. R., Ballard J. R., A terabyte-scale geometry query engine for the generation of synthetic radiative transfer sensor imagery, Proc. SPIE 11419, Disruptive Technologies in Information Sciences IV, 1141902], (2020); Dodge S., Karam L., Understanding how image quality aects deep neural networks, 2016 eighth international conference on quality of multimedia experience (QoMEX)], pp. 1-6, (2016); Hussin R., Juhari M. R., Kang N. W., Ismail R., Kamarudin A., Digital image processing techniques for object detection from complex background image, Procedia Engineering, 41, pp. 340-344, (2012); Roh M.-C., Lee J.-y., Rening faster-rcnn for accurate object detection, 2017 fteenth IAPR international conference on machine vision applications (MVA)], pp. 514-517, (2017); Xie Y., Richmond D., Pre-training on grayscale imagenet improves medical image classication, Proceedings of the European Conference on Computer Vision (ECCV) Workshops], (2018); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A. C., Ssd: Single shot multibox detector, European conference on computer vision], pp. 21-37, (2016); Kim C. E., Oghaz M. M. D., Fajtl J., Argyriou V., Remagnino P., A comparison of embedded deep learning methods for person detection, (2018); Ghoury S., Sungur C., Durdu A., Real-time diseases detection of grape and grape leaves using faster r-cnn and ssd mobilenet architectures, International Conference on Advanced Technologies, Computer Engineering and Science (ICATCES 2019)], (2019); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I., Wojna Z., Song Y., Guadarrama S., Et al., Speed/accuracy trade-os for modern convolutional object detectors, Proceed-ings of the IEEE conference on computer vision and pattern recognition], pp. 7310-7311, (2017); Jeong J., Park H., Kwak N., Enhancement of ssd by concatenating feature maps for object detection, (2017); Nguyen N.-D., Do T., Ngo T. D., Le D.-D., An evaluation of deep learning methods for small object detection, Journal of Electrical and Computer Engineering 2020, 2020; Yadav N., Binay U., Comparative study of object detection algorithms, International Research Journal of Engineering and Technology (IRJET), 4, 11, pp. 586-591, (2017); Rezaei M., Ravanbakhsh E., Namjoo E., Haghighat M., Assessing the effect of image quality on ssd and faster r-cnn networks for face detection, 2019 27th Iranian Conference on Electrical Engineering (ICEE)], pp. 1589-1594, (2019); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unied, real-time object detection, Proceedings of the IEEE conference on computer vision and pattern recognition], pp. 779-788, (2016); Redmon J., Farhadi A., Yolo9000: Better, faster, stronger, (2016); Redmon J., Farhadi A., Yolov3: An incremental improvement, (2018); Ammar A., Koubaa A., Ahmed M., Saad A., Aerial images processing for car detection using con-volutional neural networks: Comparison between faster r-cnn and yolov3, (2019); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, (2015); Ren S., He K., Girshick R., Sun J., Faster r-cnn: towards real-time object detection with re-gion proposal networks, IEEE transactions on pattern analysis and machine intelligence, 39, 6, pp. 1137-1149, (2016); Lee C., Kim H. J., Oh K. W., Comparison of faster r-cnn models for object detection, 2016 16th International Conference on Control, Automation and Systems (ICCAS)], pp. 107-110, (2016); Zhou Y., Liu L., Shao L., Mellor M., Dave: A unied framework for fast vehicle detection and annotation, European Conference on Computer Vision], pp. 278-293, (2016); Chen Y., Li W., Sakaridis C., Dai D., Van Gool L., Domain adaptive faster r-cnn for object detection in the wild, Proceedings of the IEEE conference on computer vision and pattern recognition], pp. 3339-3348, (2018); Perez L., Wang J., The effectiveness of data augmentation in image classication using deep learning, (2017); Moses, Olafenwa J., Imageai, an open source python library built to empower developers to build applications and systems with self-contained computer vision capabilities, (2018); Abadi M., Agarwal A., Barham P., Brevdo E., Chen Z., Citro C., Corrado G. S., Davis A., Dean J., Devin M., Ghemawat S., Goodfellow I., Harp A., Irving G., Isard M., Jia Y., Jozefowicz R., Kaiser L., Kudlur M., Levenberg J., Mane D., Monga R., Moore S., Murray D., Olah C., Schuster M., Shlens J., Steiner B., Sutskever I., Talwar K., Tucker P., Vanhoucke V., Vasudevan V., Viegas F., Vinyals O., Warden P., Wattenberg M., Wicke M., Yu Y., Zheng X., TensorFlow: Large-scale machine learning on heterogeneous systems, (2015); Keskar N. S., Mudigere D., Nocedal J., Smelyanskiy M., Tang P. T. P., On large-batch training for deep learning: Generalization gap and sharp minima, (2016)","M.E. Gonzalez; U.S. Army Engineer and Research Development Center, Vicksburg, 3909 Halls Ferry Road, United States; email: megan.e.gonzalez@erdc.dren.mil","Zelnio E.; Garber F.D.","SPIE","The Society of Photo-Optical Instrumentation Engineers (SPIE)","Algorithms for Synthetic Aperture Radar Imagery XXVIII 2021","12 April 2021 through 16 April 2021","Virtual, Online","169240","0277786X","978-151064293-5","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85107764990"
"Ghorbanzadeh O.; Blaschke T.","Ghorbanzadeh, Omid (57200006495); Blaschke, Thomas (7005427503)","57200006495; 7005427503","Optimizing sample patches selection of CNN to improve the MIOU on landslide detection","2019","GISTAM 2019 - Proceedings of the 5th International Conference on Geographical Information Systems Theory, Applications and Management","","","","33","40","7","30","10.5220/0007675300330040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067483355&doi=10.5220%2f0007675300330040&partnerID=40&md5=261e3cee1e7d2187c35a85d0c4bcd623","Department of Geoinformatics - Z-GIS, University of Salzburg, Salzburg, Austria","Ghorbanzadeh O., Department of Geoinformatics - Z-GIS, University of Salzburg, Salzburg, Austria; Blaschke T., Department of Geoinformatics - Z-GIS, University of Salzburg, Salzburg, Austria","Remarkable improvement has been made in object detection and image classification, mainly due to the availability of large-scale labelled data and also the progress of deep convolutional neural networks (CNNs). Thus, this amount of training data enables CNNs to learn data-driven image features. However, generating the efficient sample patches from the satellite images for training the CNNs remains a challenge. In this study, we use a CNN for the case of landslide detection based on the optical data from the Rapid Eye satellite. We separate the image into training and test areas of the highly landslide-prone Rasuwa district in Nepal. Thus, the sample patches were extracted from the training area of the Rapid Eye image. Although the approach of random sample patches is considered as the most common for feeding the CNNs, it is not the best solution for all object detection aims. We feed our structured CNN with the randomly selected sample patches as our first approach. For the second approach, the same CNN architecture is trained by the patches that selected based on only the central areas of any landslide. The trained CNNs based on both approaches were used to detection the landslides in an area where considered as our test zone. The detection results are compared against a precise inventory dataset of landslide polygons through a mean intersection-over-union (mIOU). The mIOU value of the first approach is 53.56%. However, that of the second one is 56.24%, which shows an approximately 3% improvement in the resulting accuracy of the landslide detection using the sample patches generated by the second approach. Rather, the current performance of CNNs in object detection domain they strongly depend on the quality of the training data and augmentation strategies. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Convolutional Neural Network; Mean Intersection Over Union; RapidEye; Training Data Set","Convolution; Deep neural networks; Geographic information systems; Image enhancement; Information systems; Information use; Landslides; Neural networks; Object detection; Object recognition; System theory; Convolutional neural network; Current performance; Image features; Landslide detection; Mean Intersection Over Union; Rapideye; Satellite images; Training data sets; Information management","","","","","GIScience Doctoral College, (DK W 1237-N23); Austrian Science Fund, FWF","This research is partly funded by the Austrian Science Fund (FWF) through the GIScience Doctoral College (DK W 1237-N23). Special thanks are owed to Sansar Raj Meena, Department of Geoinformatics, University of Salzburg, Austria.","Amit S.N.K.B., Aoki Y., Disaster detection from aerial imagery with convolutional neural network, Knowledge Creation and Intelligent Computing (IES-KCIC), 2017 International Electronics Symposium on, pp. 239-245, (2017); Bui D.T., Tuan T.A., Klempe H., Pradhan B., Revhaug I., Spatial prediction models for shallow landslide hazards: A comparative assessment of the efficacy of support vector machines, artificial neural networks, kernel logistic regression, and logistic model tree, Landslides, 13, 2, pp. 361-378, (2016); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks, Drones, 2, 4, (2018); Depeursinge A., Vargas A., Platon A., Geissbuhler A., Poletti P.-A., Muller H., Building a reference multimedia database for interstitial lung diseases, Computerized Medical Imaging and Graphics, 36, 3, pp. 227-238, (2012); Ding A., Zhang Q., Zhou X., Dai B., Automatic recognition of landslide based on CNN and texture change detection, Chinese Association of Automation (YAC), Youth Academic Annual Conference of, pp. 444-448, (2016); Dong W., Sun S., Paul J.-C., Optimal sample patches selection for tile-based texture synthesis, Computer Aided Design and Computer Graphics, 2005. Ninth International Conference on, (2005); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of different machine learning methods and deep-learning convolutional neural networks for landslide detection, Remote Sensing, 11, 2, (2019); Ghorbanzadeh O., Tiede D., Dabiri Z., Sudmanns M., Lang S., Dwelling extraction in refugee camps using CNn-first experiences and lessons learnt, International Archives of the Photogrammetry, Remote Sensing, Spatial Information Sciences, 42, 1, (2018); Guirado E., Tabik S., Alcaraz-Segura D., Cabello J., Herrera F., Deep-Learning Convolutional Neural Networks for Scattered Shrub Detection with Google Earth Imagery, (2017); Guzzetti F., Mondini A.C., Cardinali M., Fiorucci F., Santangelo M., Chang K.-T., Landslide inventory maps: New tools for an old problem, Earth-Science Reviews, 112, 1-2, pp. 42-66, (2012); Hong H., Chen W., Xu C., Youssef A.M., Pradhan B., Tien Bui D., Rainfall-induced landslide susceptibility assessment at the Chongren area (China) using frequency ratio, certainty factor, and index of entropy, Geocarto International, 32, 2, pp. 139-154, (2017); Lang S., Schoepfer E., Zeil P., Riedler B., Earth observation for humanitarian assistance, GI Forum-J Geogr Inf Sci, pp. 157-165, (2017); Langkvist M., Alirezaie M., Kiselev A., Loutfi A., Interactive learning with convolutional neural networks for image labeling, International Joint Conference on Artificial Intelligence (IJCAI), (2016); Liu B., Dixit M., Kwitt R., Vasconcelos N., Feature space transfer for data augmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9090-9098, (2018); Mahdianpari M., Salehi B., Rezaee M., Mohammadimanesh F., Zhang Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery, Remote Sensing, 10, 7, (2018); Mezaal M.R., Pradhan B., Sameen M.I., Mohd Shafri H.Z., Yusoff Z.M., Optimized neural architecture for automatic landslide detection from high‐resolution airborne laser scanning data, Applied Sciences, 7, 7, (2017); Modzelewska A., Sterenczak K., Mierczyk M., Maciuk S., Balazy R., Zawila-Niedzwiecki T., Sensitivity of vegetation indices in relation to parameters of Norway spruce stands, Folia Forestalia Polonica, 59, 2, pp. 85-98, (2017); Moosavi V., Talebi A., Shirmohammadi B., Producing a landslide inventory map using pixel-based and object-oriented approaches optimized by Taguchi method, Geomorphology, 204, pp. 646-656, (2014); Qayyum A., Malik A.S., Saad N.M., Iqbal M., Faris Abdullah M., Rasheed W., Rashid Abdullah T.A., Bin Jafaar M.Y., Scene classification for aerial images based on CNN using sparse coding technique, International Journal of Remote Sensing, 38, 8-10, pp. 2662-2685, (2017); Radovic M., Adarkwa O., Wang Q., Object recognition in aerial images using convolutional neural networks, Journal of Imaging, 3, 2, (2017); Wei Y., Xia W., Huang J., Ni B., Dong J., Zhao Y., Yan S., CNN: Single-Label to Multi-Label, (2014); Yu H., Ma Y., Wang L., Zhai Y., Wang X., A landslide intelligent detection method based on CNN and RSG_R, Mechatronics and Automation (ICMA), 2017 IEEE International Conference on, pp. 40-44, (2017); Zhang C., Sargent I., Pan X., Li H., Gardiner A., Hare J., Atkinson P.M., An object-based convolutional neural network (OCNN) for urban land use classification, Remote Sensing of Environment, 216, pp. 57-70, (2018); Zhu X.X., Tuia D., Mou L., Xia G.-S., Zhang L., Xu F., Fraundorfer F., Deep learning in remote sensing: A comprehensive review and list of resources, IEEE Geoscience and Remote Sensing Magazine, 5, 4, pp. 8-36, (2017)","","Grueau C.; Laurini R.; Ragia L.","SciTePress","Institute for Systems and Technologies of Information, Control and Communication (INSTICC)","5th International Conference on Geographical Information Systems Theory, Applications and Management, GISTAM 2019","3 May 2019 through 5 May 2019","Heraklion, Crete","148434","","978-989758371-1","","","English","GISTAM - Proc. Int. Conf. Geograph. Inf. Syst. Theory, Appl. Manag.","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85067483355"
"Wang X.; Cheng P.; Liu X.; Uzochukwu B.","Wang, Xiaoliang (57221484319); Cheng, Peng (57203585199); Liu, Xinchuan (56820383500); Uzochukwu, Benedict (56646202200)","57221484319; 57203585199; 56820383500; 56646202200","Fast and accurate, convolutional neural network based approach for object detection from UAV","2018","Proceedings: IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society","","","8592805","3171","3175","4","27","10.1109/IECON.2018.8592805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061546734&doi=10.1109%2fIECON.2018.8592805&partnerID=40&md5=3ba8b1f911cff296a5b1da88f2f56d63","Department of Technology, College of Engineering and Technology, Virginia State University, Petersburg, VA, United States","Wang X., Department of Technology, College of Engineering and Technology, Virginia State University, Petersburg, VA, United States; Cheng P., Department of Technology, College of Engineering and Technology, Virginia State University, Petersburg, VA, United States; Liu X., Department of Technology, College of Engineering and Technology, Virginia State University, Petersburg, VA, United States; Uzochukwu B., Department of Technology, College of Engineering and Technology, Virginia State University, Petersburg, VA, United States","Unmanned Aerial Vehicles (UAVs), have intrigued different people from all walks of life, because of their pervasive computing capabilities. UAV equipped with vision techniques, could be leveraged to establish navigation autonomous control for UAV itself. Also, object detection from UAV could be used to broaden the utilization of drone to provide ubiquitous surveillance and monitoring services towards military operation, urban administration and agriculture management. As the data-driven technologies evolved, machine learning algorithm, especially the deep learning approach has been intensively utilized to solve different traditional computer vision research problems. Modern Convolutional Neural Networks based object detectors could be divided into two major categories: one-stage object detector and two-stage object detector. In this study, we utilize some representative CNN based object detectors to execute the computer vision task over Stanford Drone Dataset (SDD). State-of-the-art performance has been achieved in utilizing focal loss dense detector RetinaNet based approach for object detection from UAV in a fast and accurate manner. © 2018 IEEE.","Convolutional neural network; Focal loss; Object detection; UAV","Aircraft detection; Antennas; Computer vision; Convolution; Deep learning; Drones; Industrial electronics; Learning algorithms; Machine learning; Military operations; Neural networks; Object recognition; Ubiquitous computing; Unmanned aerial vehicles (UAV); Visual servoing; Agriculture management; Computing capability; Convolutional neural network; Monitoring services; State-of-the-art performance; Traditional computers; Ubiquitous surveillances; Urban administrations; Object detection","","","","","","","Frew E., McGee T., Kim Z., Xiao X., Jackson S., Morimoto M., Rathinam S., Padial J., Sengupta R., Vision-based road-following using a small autonomous aircraft, Aerospace Conference, 2004. Proceedings. 2004 IEEE, 5, pp. 3006-3015, (2004); Fowers S.G., Lee D.J., Tippetts B.J., Lillywhite K.D., Dennis A.W., Archibald J.K., Vision aided stabilization and the development of a quad-rotor micro UAV, Computational Intelligence in Robotics and Automation, 2007. CIRA 2007. International Symposium on, pp. 143-148, (2007); Xu G., Zhang Y., Ji S., Cheng Y., Tian Y., Research on computer vision-based for UAV autonomous landing on a ship, Pattern Recognition Letters, 30, 6, pp. 600-605, (2009); Haddal C.C., Gertler J., Homeland security: Unmanned aerial vehicles and border surveillance, Library of Congress Washington DC Congressional Research Service, (2010); Ma'sum, Anwar M., Et al., Simulation of intelligent unmanned aerial vehicle (uav) for military surveillance, Advanced Computer Science and Information Systems (ICACSIS), 2013 International Conference on, (2013); Chen N., Chen Y., You Y., Ling H., Liang P., Zimmermann R., Dynamic urban surveillance video stream processing using fog computing, Multimedia Big Data (BigMM), 2016 IEEE Second International Conference on, pp. 105-112, (2016); Honkavaara E., Saari H., Kaivosoja J., Polonen I., Hakala T., Litkey P., Makynen J., Pesonen L., Processing and assessment of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral camera for precision agriculture, Remote Sensing, 5, 10, pp. 5006-5039, (2013); Saari H., Pellikka I., Pesonen L., Tuominen S., Heikkila J., Holmlund C., Makynen J., Ojala K., Antila T., Unmanned Aerial Vehicle (UAV) operated spectral camera system for forest and agriculture applications, Remote Sensing for Agriculture, Ecosystems, and Hydrology, 8174, (2011); Rovira-Mas F., Zhang Q., Reid J.F., Stereo vision three-dimensional terrain maps for precision agriculture, Computers and Electronics in Agriculture, 60, 2, pp. 133-143, (2008); Shafique S., Sheikh N.M., Simple algorithm for detection of elliptical objects in remotely sensed images for UAV applications, Applied Sciences and Technology (IBCAST), 2009 6th International Bhurban Conference on, (2009); Ibrahim A.W.N., Ching P.W., Seet G.L.G., Lau W.S.M., Czajewski W., Moving objects detection and tracking framework for UAV-based surveillance, Image and Video Technology (PSIVT), 2010 Fourth Pacific-Rim Symposium on, (2010); Marti M., Barekatain A., Shih H.F., Murray S., Matsuo Y., Prendinger H., Situation Awareness for UAVs Using Deep Learning Techniques; Bejiga, Belete M., Zeggada A., Nouffidj A., Melgani F., A convolutional neural network approach for assisting avalanche search and rescue operations with uav imagery, Remote Sensing, 9, 2, (2017); Zeggada A., Melgani F., Bazi Y., A deep learning approach to UAV image multilabeling, IEEE Geoscience and Remote Sensing Letters, 14, 5, pp. 694-698, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., SSD: Single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, pp. 1137-1149, (2017); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, (2017); Robicquet A., Sadeghian A., Alahi A., Savarese S., Learning social etiquette: Human trajectory understanding in crowded scenes, European Conference on Computer Vision, pp. 549-565, (2016); Saripalli S., Montgomery J.F., Sukhatme G.S., Vision-based autonomous landing of an unmanned aerial vehicle, Robotics and Automation, 2002. Proceedings. ICRA'02. IEEE International Conference on, 3, pp. 2799-2804, (2002); Scaramuzza D., Achtelik M.C., Doitsidis L., Friedrich F., Kosmatopoulos E., Martinelli A., Achtelik M.W., Chli M., Chatzichristofis S., Kneip L., Gurdan D., Vision-controlled micro flying robots: From system design to autonomous navigation and mapping in GPS-denied environments, IEEE Robotics & Automation Magazine, 21, 3, pp. 26-40, (2014); Meier L., Tanskanen P., Heng L., Lee G.H., Fraundorfer F., Pollefeys M., Pixhawk: A micro aerial vehicle design for autonomous flight using onboard computer vision, Autonomous Robots, 33, 1-2, pp. 21-39, (2012); Kendoul F., Fantoni I., Nonami K., Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles, Robotics and Autonomous Systems, 57, 6-7, pp. 591-602, (2009); Ryan A., Zennaro M., Howell A., Sengupta R., Hedrick J.K., An overview of emerging results in cooperative UAV control, Decision and Control, 2004. CDC. 43rd IEEE Conference on, 1, pp. 602-607, (2004); Schneiderman R., Unmanned drones are flying high in the military/aerospace sector [Special reports], IEEE Signal Processing Magazine, 29, 1, pp. 8-11, (2012); Avola D., Foresti G.L., Martinel N., Micheloni C., Pannone D., Piciarelli C., Aerial video surveillance system for small-scale UAV environment monitoring, Advanced Video and Signal Based Surveillance (AVSS), 2017 14th IEEE International Conference on, pp. 1-6, (2017); Horton R., Cano E., Bulanon D., Fallahi E., Peach flower monitoring using aerial multispectral imaging, Journal of Imaging, 3, 1, (2017); Vanegas F., Bratanov D., Powell K., Weiss J., Gonzalez F., A novel methodology for improving plant pest surveillance in vineyards and crops using UAV-based hyperspectral and spatial data, Sensors, 18, 1, (2018); Albani D., IJsselmuiden J., Haken R., Trianni V., Monitoring and mapping with robot swarms for agricultural applications, Advanced Video and Signal Based Surveillance (AVSS), 2017 14th IEEE International Conference on, pp. 1-6, (2017); Chen N., Chen Y., Song S., Huang C.T., Ye X., Smart urban surveillance using fog computing, Edge Computing (SEC), IEEE/ACM Symposium on, pp. 95-96, (2016); Chen N., Chen Y., Blasch E., Ling H., You Y., Ye X., Enabling smart urban surveillance at the edge, Smart Cloud (SmartCloud), 2017 IEEE International Conference on, pp. 109-119, (2017); Dollar P., Tu Z., Perona P., Belongie S., Integral Channel Features, (2009); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, 1, pp. 886-893, (2005); Felzenszwalb P.F., Girshick R.B., McAllester D., Cascade object detection with deformable part models, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 2241-2248, (2010); Teutsch M., Kruger W., Detection, segmentation, and tracking of moving objects in UAV videos, Advanced Video and Signal-Based Surveillance (AVSS), 2012 IEEE Ninth International Conference on, (2012); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Girshick R., Fast R-CNn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Redmon J., Farhadi A., Yolo9000: Better, Faster, Stronger, (2016); Han S., Pool J., Tran J., Dally W., Learning both weights and connections for efficient neural network, Advances in Neural Information Processing Systems, pp. 1135-1143, (2015); Han S., Mao H., Dally W.J., Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, (2015); Hinton G., Vinyals O., Dean J., Distilling the Knowledge in a Neural Network, (2015); Bonomi F., Milito R., Zhu J., Addepalli S., Fog computing and its role in the internet of things, Proceedings of the First Edition of the MCC Workshop on Mobile Cloud Computing, pp. 13-16, (2012); Allaire F.C., Tarbouchi M., Labonte G., Fusina G., FPGA implementation of genetic algorithm for UAV real-time path planning, Unmanned Aircraft Systems, pp. 495-510, (2008); Christopherson H., Pickell W., Koller A., Kannan S., Johnson E., Small adaptive flight control systems for UAVs using FPGA/DSP technology, AIAA 3rd"" Unmanned Unlimited"" Technical Conference, Workshop and Exhibit, (2004); Kok J., Gonzalez L.F., Kelson N., FPGA implementation of an evolutionary algorithm for autonomous unmanned aerial vehicle onboard path planning, IEEE Transactions on Evolutionary Computation, 17, 2, pp. 272-281, (2013); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature Pyramid Networks for Object Detection, (2016); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248-255, (2009); Chen T., Li M., Li Y., Lin M., Wang N., Wang M., Xiao T., Xu B., Zhang C., Zhang Z., Mxnet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems, (2015)","","","Institute of Electrical and Electronics Engineers Inc.","IEEE Industrial Electronics Society (IES); The Institute of Electrical and Electronics Engineers (IEEE)","44th Annual Conference of the IEEE Industrial Electronics Society, IECON 2018","20 October 2018 through 23 October 2018","Washington","144100","","978-150906684-1","","","English","Proceedings: IECON - Annu. Conf. IEEE Ind. Electron. Soc.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85061546734"
"Osco L.P.; de Arruda M.D.S.; Marcato Junior J.; da Silva N.B.; Ramos A.P.M.; Moryia É.A.S.; Imai N.N.; Pereira D.R.; Creste J.E.; Matsubara E.T.; Li J.; Gonçalves W.N.","Osco, Lucas Prado (57196329154); de Arruda, Mauro dos Santos (57221930407); Marcato Junior, José (55640064500); da Silva, Neemias Buceli (57212415066); Ramos, Ana Paula Marques (56198690100); Moryia, Érika Akemi Saito (57212414724); Imai, Nilton Nobuhiro (36720764000); Pereira, Danillo Roberto (37041692000); Creste, José Eduardo (57224100393); Matsubara, Edson Takashi (23393072700); Li, Jonathan (57235557700); Gonçalves, Wesley Nunes (23396539500)","57196329154; 57221930407; 55640064500; 57212415066; 56198690100; 57212414724; 36720764000; 37041692000; 57224100393; 23393072700; 57235557700; 23396539500","A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery","2020","ISPRS Journal of Photogrammetry and Remote Sensing","160","","","97","106","9","102","10.1016/j.isprsjprs.2019.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076641413&doi=10.1016%2fj.isprsjprs.2019.12.010&partnerID=40&md5=b2101be56ec5ec4913a4683bc46efbf8","Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil; Faculty of Agronomy, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Faculty of Engineering and Architecture, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Department of Cartographic Science, São Paulo State University, Mailbox: 19060-900, Presidente Prudente, SP, Brazil; Faculty of Computer Science, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Department of Geography and Environmental Management and Department of Systems Design Engineering, University of Waterloo, Waterloo, N2L 3G1, ON, Canada","Osco L.P., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; de Arruda M.D.S., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil; Marcato Junior J., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; da Silva N.B., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil; Ramos A.P.M., Faculty of Engineering and Architecture, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Moryia É.A.S., Department of Cartographic Science, São Paulo State University, Mailbox: 19060-900, Presidente Prudente, SP, Brazil; Imai N.N., Department of Cartographic Science, São Paulo State University, Mailbox: 19060-900, Presidente Prudente, SP, Brazil; Pereira D.R., Faculty of Computer Science, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Creste J.E., Faculty of Agronomy, University of Western São Paulo, Presidente Prudente, São Paulo, Brazil; Matsubara E.T., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil; Li J., Department of Geography and Environmental Management and Department of Systems Design Engineering, University of Waterloo, Waterloo, N2L 3G1, ON, Canada; Gonçalves W.N., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil, Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, MS, Brazil","Visual inspection has been a common practice to determine the number of plants in orchards, which is a labor-intensive and time-consuming task. Deep learning algorithms have demonstrated great potential for counting plants on unmanned aerial vehicle (UAV)-borne sensor imagery. This paper presents a convolutional neural network (CNN) approach to address the challenge of estimating the number of citrus trees in highly dense orchards from UAV multispectral images. The method estimates a dense map with the confidence that a plant occurs in each pixel. A flight was conducted over an orchard of Valencia-orange trees planted in linear fashion, using a multispectral camera with four bands in green, red, red-edge and near-infrared. The approach was assessed considering the individual bands and their combinations. A total of 37,353 trees were adopted in point feature to evaluate the method. A variation of σ (0.5; 1.0 and 1.5) was used to generate different ground truth confidence maps. Different stages (T) were also used to refine the confidence map predicted. To evaluate the robustness of our method, we compared it with two state-of-the-art object detection CNN methods (Faster R-CNN and RetinaNet). The results show better performance with the combination of green, red and near-infrared bands, achieving a Mean Absolute Error (MAE), Mean Square Error (MSE), R2 and Normalized Root-Mean-Squared Error (NRMSE) of 2.28, 9.82, 0.96 and 0.05, respectively. This band combination, when adopting σ = 1 and a stage (T = 8), resulted in an R2, MAE, Precision, Recall and F1 of 0.97, 2.05, 0.95, 0.96 and 0.95, respectively. Our method outperforms significantly object detection methods for counting and geolocation. It was concluded that our CNN approach developed to estimate the number and geolocation of citrus trees in high-density orchards is satisfactory and is an effective strategy to replace the traditional visual inspection method to determine the number of plants in orchards trees. © 2019 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Citrus tree counting; Deep learning; Multispectral image; Object detection; Orchard; UAV-borne sensor","Valencia; Citrus; Citrus sinensis; Antennas; Convolution; Deep learning; Errors; Forestry; Infrared devices; Learning algorithms; Mean square error; Neural networks; Object detection; Object recognition; Orchards; Unmanned aerial vehicles (UAV); Citrus tree; Convolutional neural network; Multi-spectral cameras; Multi-spectral imagery; Multispectral images; Object detection method; Root mean squared errors; Visual inspection method; algorithm; artificial neural network; deciduous tree; detection method; orchard; pixel; satellite imagery; sensor; spectral analysis; unmanned vehicle; Aircraft detection","","","","","Nvidia; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, (59/300.066/2015, 88881.311850/2018-01); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (304173/2016-9, 433783/2018-4)","Funding text 1: This research was funded by CNPq (p: 433783/2018-4 and 304173/2016-9 ), CAPES Print (p: 88881.311850/2018-01 ) and Fundect (p: 59/300.066/2015 ). The authors acknowledge the donation of a Titan X and a Titan V by NVIDIA.; Funding text 2: This research was funded by CNPq (p: 433783/2018-4 and 304173/2016-9), CAPES Print (p: 88881.311850/2018-01) and Fundect (p: 59/300.066/2015). The authors acknowledge the donation of a Titan X and a Titan V by NVIDIA.","Alshehhi R., Marpu P.R., Woon W.L., Mura M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks, ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149, (2017); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sensing, 11, 4, (2019); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Ball J.E., Anderson D.T., Chan C.S., A comprehensive survey of deep learning in remote sensing: Theories, tools and challenges for the community, J. Appl. Remote Sens., 11, 4, pp. 1-64, (2017); Cao Z., Simon T., Wei S.E., Sheikh Y., Realtime multi-person 2D pose estimation using part affinity fields, CVPR, 2017, pp. 1302-1310, (2017); Chen S.W., Shivakumar S.S., Dcunha S., Das J., Okon E., Qu C., Kumar V., Counting apples and oranges with deep learning: a data-driven approach, IEEE Rob. Autom. Lett., 2, 2, pp. 781-788, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks, Drones, 2, 4, (2018); Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens., 146, pp. 124-136, (2018); Dian Bah M., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sensing, 10, 11, (2018); Dijkstra K., van de Loosdrecht J., Schomaker L.R.B., Wiering M.A., Centroidnet: a deep neural network for joint object localization and counting, Machine Learning and Knowledge Discovery in Databases, pp. 585-601, (2019); Djerriri K., Ghabi M., Karoui M.S., Adjoudj R., Palm trees counting in remote sensing imagery using regression convolutional neural network, IGARSS, 2018, pp. 2627-2630, (2018); Fan Z., Lu J., Gong M., Xie H., Goodman E.D., Automatic tobacco plant detection in UAV images via deep neural networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, 3, pp. 876-887, (2018); Ghamisi P., Plaza J., Chen Y., Li J., Plaza A.J., Advanced spectral classifiers for hyperspectral images: A review, IEEE Geosci. Remote Sens. Mag., 5, 1, pp. 8-32, (2017); Goldbergs G., Maier S.W., Levick S.R., Edwards A., Efficiency of individual tree detection approaches based on light-weight and low-cost UAS imagery in Australian Savannas, Remote Sensing, 10, 2, (2018); Goldman E., Herzig R., Eisenschtat A., Goldberger Hassner J.T., Precise Detection in Densely Packed Scenes, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5227-5236, (2019); Guo Y., Liu Y., Oerlemans A., Lao S., Wu S., Lew M.S., Deep learning for visual understanding: a review, Neurocomputing, 187, pp. 27-48, (2016); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a worldview-2/3 and LiDAR data fusion approach and deep learning, Sensors, 19, 6, pp. 1-23, (2019); Hasan M.M., Chopin J.P., Laga H., Miklavcic S.J., Detection and analysis of wheat spikes using convolutional neural networks, Plant Methods, 14, 1, pp. 1-13, (2018); Hassanein M., Khedr M., El-Sheimy N., Crop row detection procedure using low-cost UAV imagery system, ISPRS Archives, 42, 2/W13, pp. 349-356, (2019); Ho Tong Minh D., Ienco D., Gaetano R., Lalande N., Ndikumana E., Osman F., Maurel P., Deep recurrent neural networks for winter vegetation quality mapping via multitemporal SAR Sentinel-1, IEEE Geosci. Remote Sens. Lett., 15, 3, pp. 465-468, (2018); Hsieh M.R., Lin Y.L., Hsu W.H., Drone-based object counting by spatially regularized regional proposal network, In: Proceedings of the IEEE International Conference on Computer Vision, pp. 4145-4153, (2017); Hunt E.R., Daughtry C.S.T., What good are unmanned aircraft systems for agricultural remote sensing and precision agriculture?, Int. J. Remote Sens., 39, 15-16, pp. 5345-5376, (2018); Index S., Xu N., Tian J., Tian Q., Xu K., Tang S., Analysis of Vegetation red edge with different illuminated/shaded canopy proportions and to construct normalized difference canopy, Remote Sensing, 11, 10, (2019); Jakubowski M.K., Li W., Guo Q., Kelly M., Delineating individual trees from lidar data: a comparison of vector- and raster-based segmentation approaches, Remote Sensing, 5, 9, pp. 4163-4186, (2013); Jiang H., Chen S., Li D., Wang C., Yang J., Papaya tree detection with UAV images using a GPU-accelerated scale-space filtering method, Remote Sensing, 9, 7, (2017); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: a survey, Comput. Electron. Agric., 147, pp. 70-90, (2018); Kang D., Ma Z., Chan A.B., Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking, IEEE Trans. Circuits Syst. Video Technol., 29, 5, pp. 1408-1422, (2019); Larsen M., Eriksson M., Descombes X., Perrin G., Brandtberg T., Gougeon F.A., Comparison of six individual tree crown detection algorithms evaluated under varying forest conditions, Int. J. Remote Sens., 32, 20, pp. 5827-5852, (2011); Lecun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Leiva J.N., Robbins J., Saraswat D., She Y., Ehsani R., Evaluating remotely sensed plant count accuracy with differing unmanned aircraft system altitudes, physical canopy separations, and ground covers, J. Appl. Remote Sens., 11, 3, (2017); Li D., Guo H., Wang C., Li W., Chen H., Zuo Z., Individual tree delineation in windbreaks using airborne-laser-scanning data and unmanned aerial vehicle stereo images, IEEE Geosci. Remote Sens. Lett., 13, 9, pp. 1330-1334, (2016); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sensing, 9, 1, (2017); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, In: Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, (2017); Liu T., Abd-Elrahman A., Morton J., Wilhelm V.L., Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system, GI Sci. Remote Sens., 55, 2, pp. 243-264, (2018); Liu T., Abd-Elrahman A., Deep convolutional neural network training enrichment using multi-view object-based analysis of Unmanned Aerial systems imagery for wetlands classification, ISPRS J. Photogramm. Remote Sens., 139, pp. 154-170, (2018); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177, (2019); Madec S., Jin X., Lu H., De Solan B., Liu S., Duyme F., Baret F., Ear density estimation from high resolution RGB imagery using deep learning technique, Agric. For. Meteorol., 264, pp. 225-234, (2019); Mathews A.J., Jensen J.L.R., Visualizing and quantifying vineyard canopy LAI using an unmanned aerial vehicle (UAV) collected high density structure from motion point cloud, Remote Sensing, 5, 5, pp. 2164-2183, (2013); Ndikumana E., Minh D.H.T., Baghdadi N., Courault D., Hossard L., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France. Remote Sensing, 10, 8, (2018); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., Tommaselli A.M.G., Individual tree detection and classification with UAV-Based photogrammetric point clouds and hyperspectral imaging, Remote Sensing, 9, 3, (2017); Oliveira H.C., Guizilini V.C., Nunes I.P., Souza J.R., Failure detection in row crops from UAV Images using morphological operators, IEEE Geosci. Remote Sens. Lett., 15, 7, pp. 991-995, (2018); Ozcan A.H., Hisar D., Sayar Y., Unsalan C., Tree crown detection and delineation in satellite images using probabilistic voting, Remote Sens. Lett., 8, 8, pp. 761-770, (2017); Ozdarici-Ok A., Automatic detection and delineation of citrus trees from VHR satellite imagery, Int. J. Remote Sens., 36, 17, pp. 4275-4296, (2015); Paoletti M.E., Haut J.M., Plaza J., Plaza A., A new deep convolutional neural network for fast hyperspectral image classification, ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147, (2018); Puletti N., Perria R., Storchi P., Unsupervised classification of very high remotely sensed images for grapevine rows detection, Eur. J. Remote Sens., 47, 1, pp. 45-54, (2014); Ramesh K.N., Chandrika N., Omkar S.N., Meenavathi M.B., Rekha V., Detection of Rows in Agricultural Crop Images Acquired by Remote Sensing from a UAV, Int. J. Image, Graph. Signal Proce., 8, 11, pp. 25-31, (2016); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Adv. Neural Inf. Proce. Syst., pp. 91-99, (2015); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of fir trees (Abies sibirica) Damaged by the bark beetle in unmanned aerial vehicle images with deep learning, Remote Sensing, 11, 6, (2019); Salami E., Gallardo A., Skorobogatov G., Barrado C., On-the-fly olive tree counting using a UAS and cloud services, Remote Sensing, 11, 3, (2019); Santos A., Marcato Junior J., Araujo M.S., Martini D.R., Tetila E.C., Siqueira H.L., Aoki C., Eltner A., Matsubara E.T., Pistori H., Feitosa R., Liesenberg V., Goncalves W.N., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVs, Sensors, 19, 16, (2019); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2014); Surovy P., Almeida Ribeiro N., Panagiotidis D., Estimation of positions and heights from UAV-sensed imagery in tree plantations in agrosilvopastoral systems, Int. J. Remote Sens., 39, 14, pp. 4786-4800, (2018); Tao S., Wu F., Guo Q., Wang Y., Li W., Xue B., Fang J., Segmenting tree crowns from terrestrial and mobile LiDAR data by exploring ecological theories, ISPRS J. Photogramm. Remote Sens., 110, pp. 66-76, (2015); Varela S., Dhodda P.R., Hsu W.H., Prasad P.V.V., Assefa Y., Peralta N.R., Griffin T., Sharda A., Ferguson A., Ciampitti I.A., Early-season stand count determination in corn via integration of imagery from unmanned aerial systems (UAS) and supervised learning techniques, Remote Sensing, 10, 2, (2018); Verma N.K., Lamb D.W., Reid N., Wilson B., Comparison of canopy volume measurements of scattered eucalypt farm trees derived from high spatial resolution imagery and LiDAR, Remote Sensing, 8, 5, (2016); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks, Remote Sensing, 11, 11, (2019); Wu B., Yu B., Wu Q., Huang Y., Chen Z., Wu J., Individual tree crown delineation using localized contour tree method and airborne LiDAR data in coniferous forests, Int. J. Appl. Earth Observ. Geoinf., 52, pp. 82-94, (2016); Wu H., Prasad S., Semi-supervised deep learning using pseudo labels for hyperspectral image classification, IEEE Trans. Image Process., 27, 3, pp. 1259-1270, (2018); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network, Remote Sensing, 11, 6, (2019); Zhang H., Li Y., Zhang Y., Shen Q., Spectral-spatial classification of hyperspectral imagery using a dual-channel convolutional neural network, Remote Sensing Letters, 8, 5, pp. 438-447, (2017); Zhang L., Zhang L., Kumar V., Deep learning for remote sensing data: a technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag., 4, 2, pp. 22-40, (2016); Zhang P., Gong M., Su L., Liu J., Li Z., Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images, ISPRS J. Photogramm. Remote Sens., 116, pp. 24-41, (2016)","L.P. Osco; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Brazil; email: pradoosco@gmail.com","","Elsevier B.V.","","","","","","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","","Scopus","2-s2.0-85076641413"
"Chen L.; Li H.; Zhu G.; Li Q.; Zhu J.; Huang H.; Peng J.; Zhao L.","Chen, Li (57202348994); Li, Haifeng (57189334346); Zhu, Guowei (57219380309); Li, Qi (56047846900); Zhu, Jiawei (57195128854); Huang, Haozhe (57219385371); Peng, Jian (57215095598); Zhao, Lin (57051675300)","57202348994; 57189334346; 57219380309; 56047846900; 57195128854; 57219385371; 57215095598; 57051675300","Attack selectivity of adversarial examples in remote sensing image scene classification","2020","IEEE Access","8","","9146660","137477","137489","12","7","10.1109/ACCESS.2020.3011639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092502915&doi=10.1109%2fACCESS.2020.3011639&partnerID=40&md5=e6df5ffca8e9fd480b4b315375a05b60","School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; School of Computer Science and Engineering, Central South University, Changsha, 410083, China","Chen L., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; Li H., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; Zhu G., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; Li Q., School of Computer Science and Engineering, Central South University, Changsha, 410083, China; Zhu J., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; Huang H., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; Peng J., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; Zhao L., School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China","Remote sensing image (RSI) scene classification is the foundation and important technology of ground object detection, land use management and geographic analysis. During recent years, convolutional neural networks (CNNs) have achieved significant success and are widely applied in RSI scene classification. However, crafted images that serve as adversarial examples can potentially fool CNNs with high confidence and are hard for human eyes to interpret. For the increasing security and robust requirements of RSI scene classification, the adversarial example problem poses a serious problem for the classification results derived from systems using CNN models, which has not been fully recognized by previous research. In this study, to explore the properties of adversarial examples of RSI scene classification, we create different scenarios by testing two major attack algorithms (i.e., the fast gradient sign method (FGSM) and basic iterative method (BIM)) trained on different RSI benchmark datasets to fool CNNs (i.e., InceptionV1, ResNet and a simple CNN). In the experiment, our results show that CNNs of RSI scene classification are also vulnerable to adversarial examples, and some of them have a fooling rate of over 80%. These adversarial examples are affected by the architecture of CNNs and the type of RSI dataset. InceptionV1 has a fooling rate of less than 5%, which is lower than the others. Adversarial examples generated on the UCM dataset are easier than other datasets. Importantly, we also find that the classes of adversarial examples have an attack selectivity property. Misclassifications of adversarial examples of RSIs are related to the similarity of the original classes in the CNN feature space. Attack selectivity reveals potential classes of adversarial examples and provides insights into the design of defensive algorithms in future research.  © 2013 IEEE.","adversarial example; convolutional neural network; deep learning; Remote sensing image","Architectural design; Convolutional neural networks; Image classification; Iterative methods; Land use; Object detection; Remote sensing; Space optics; Benchmark datasets; Classification results; Feature space; High confidence; Land-use management; Misclassifications; Remote sensing images; Scene classification; Classification (of information)","","","","","National Natural Science Foundation of China, NSFC, (41771458, 41861048, 41871276, 41871302, 41871364)","This work was supported by the National Natural Science Foundation of China under Grant 41871364, Grant 41871302, Grant 41871276, Grant 41861048, and Grant 41771458.","Lillesand T., Kiefer R.W., Chipman J., Remote Sensing and Image Interpretation, (2015); Han J., Zhang D., Cheng G., Guo L., Ren J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning, IEEE Trans. Geosci. Remote Sens., 53, 6, pp. 3325-3337, (2015); Forestier G., Puissant A., Wemmert C., Gancarski P., Knowledgebased region labeling for remote sensing image interpretation, Comput. , Environ. Urban Syst., 36, 5, pp. 470-480, (2012); Zhang J., Multi-source remote sensing data fusion: Status and trends, Int. J. Image Data Fusion, 1, 1, pp. 5-24, (2010); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci. Remote Sens. Lett., 14, 5, pp. 778-782, (2017); Jia K., Li Q., Tian Y., Wu B., Zhang F., Meng J., Crop classification using multi-configuration SAR data in the North China plain, Int. J. Remote Sens., 33, 1, pp. 170-183, (2012); Conrad C., Fritsch S., Zeidler J., Rucker G., Dech S., Per-field irrigated crop classification in arid Central Asia using SPOT and ASTER data, Remote Sens., 2, 4, pp. 1035-1056, (2010); Yuan C., Zhang Y., Liu Z., A survey on technologies for automatic forest fire monitoring, detection, and fighting using unmanned aerial vehicles and remote sensing techniques, Can. J. Forest Res., 45, 7, pp. 783-792, (2015); White J.C., Coops N.C., Wulder M.A., Vastaranta M., Hilker T., Tompalski P., Remote sensing technologies for enhancing forest inventories: A review, Can. J. Remote Sens., 42, 5, pp. 619-641, (2016); Tang L., Shao G., Drone remote sensing for forestry research and practices, J. Forestry Res., 26, 4, pp. 791-797, (2015); Scott G.J., England M.R., Starms W.A., Marcum R.A., Davis C.H., Training deep convolutional neural networks for Land Cover classification of high-resolution imagery, IEEE Geosci. Remote Sens. Lett., 14, 4, pp. 549-553, (2017); Jia K., Liang S., Zhang N., Wei X., Gu X., Zhao X., Yao Y., Xie X., Land cover classification of finer resolution remote sensing data integrating temporal features from time series coarser resolution data, Isprs J. Photogramm. Remote Sens., 93, pp. 49-55, (2014); Awrangjeb M., Ravanbakhsh M., Fraser C.S., Automatic detection of residential buildings using LIDAR data and multispectral imagery, Isprs J. Photogramm. Remote Sens., 65, 5, pp. 457-467, (2010); Vakalopoulou M., Karantzalos K., Komodakis N., Paragios N., Building detection in very high resolution multispectral data with deep learning features, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 1873-1876, (2015); Manno-Kovacs A., Sziranyi T., Orientation-selective building detection in aerial images, Isprs J. Photogramm. Remote Sens., 108, pp. 94-112, (2015); Cheng G., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens., 54, 12, pp. 7405-7415, (2016); Li X., Li W., Middel A., Harlan S.L., Brazel A.J., Turner B.L., Remote sensing of the surface urban heat island and land architecture in Phoenix, Arizona: Combined effects of land composition and configuration and cadastral_demographic_economic factors, Remote Sens. Environ., 174, pp. 233-243, (2016); Shrivastava R.J., Gebelein J.L., Land cover classification and economic assessment of citrus groves using remote sensing, Isprs J. Photogramm. Remote Sens., 61, 5, pp. 341-353, (2007); Ke Y., Quackenbush L.J., A review of methods for automatic individual tree-crown detection and delineation from passive remote sensing, Int. J. Remote Sens., 32, 17, pp. 4725-4747, (2011); Hu J., Razdan A., Femiani J.C., Cui M., Wonka P., Road network extraction and intersection detection from aerial images by tracking road footprints, IEEE Trans. Geosci. Remote Sens., 45, 12, pp. 4144-4157, (2007); Fu G., Zhao H., Li C., Shi L., A method by improved circular projection matching of tracking twisty road from remote sensing imagery, Acta Geodaetica et Cartographica Sinica, 43, 7, pp. 724-730, (2014); Crippen R., Calculating the vegetation index faster, Remote Sens. Envi-ron., 34, 1, pp. 71-73, (1990); McFEETERS S.K., The use of the normalized difference water index (NDWI) in the delineation of open water features, Int. J. Remote Sens., 17, 7, pp. 1425-1432, (1996); Huang X., Zhang L., Morphological building/shadow index for building extraction from high-resolution imagery over urban areas, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 5, 1, pp. 161-172, (2012); Chen J., Deng M., Mei X., Chen T., Shao Q., Hong L., Optimal segmentation of a high-resolution remote-sensing image guided by area and boundary, Int. J. Remote Sens., 35, 19, pp. 6914-6939, (2014); Liasis G., Stavrou S., Building extraction in satellite images using active contours and colour features, Int. J. Remote Sens., 37, 5, pp. 1127-1153, (2016); Cheng G., Han J., A survey on object detection in optical remote sensing images, Isprs J. Photogramm. Remote Sens., 117, pp. 11-28, (2016); Li Y., Zhang H., Xue X., Jiang Y., Shen Q., Deep learning for remote sensing image classification: A survey, Wiley Interdiscipl. Rev. , Data Mining Knowl. Discovery, 8, 6, (2018); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, Isprs J. Photogramm. Remote Sens., 152, pp. 166-177, (2019); Deng L., A tutorial survey of architectures, algorithms, and applications for deep learning, Apsipa Trans. Signal Inf. Process., 3, pp. 1-29, (2014); Chen Y., Lin Z., Zhao X., Wang G., Gu Y., Deep learning-based classification of hyperspectral data, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 7, 6, pp. 2094-2107, (2014); Chen Y., Jiang H., Li C., Jia X., Ghamisi P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 54, 10, pp. 6232-6251, (2016); Cheng G., Yang C., Yao X., Guo L., Han J., When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs, IEEE Trans. Geosci. Remote Sens., 56, 5, pp. 2811-2821, (2018); Zhang F., Du B., Zhang L., Scene classification via a gradient boosting random convolutional network framework, IEEE Trans. Geosci. Remote Sens., 54, 3, pp. 1793-1802, (2016); Chaib S., Liu H., Gu Y., Yao H., Deep feature fusion for VHR remote sensing scene classification, IEEE Trans. Geosci. Remote Sens., 55, 8, pp. 4775-4784, (2017); Akhtar N., Mian A., Threat of adversarial attacks on deep learning in computer vision: A survey, IEEE Access, 6, pp. 14410-14430, (2018); Goodfellow I.J., Shlens J., Szegedy C., Explaining and harnessing adversarial examples, Proc. 3rd Int. Conf. Learn. Representations (ICLR), (2015); Kurakin A., Goodfellow I., Bengio S., Adversarial Examples in the Physical World, (2016); Athalye A., Engstrom L., Ilyas A., Kwok K., Synthesizing Robust Adversarial Examples, (2017); Szegedy C., Zaremba W., Sutskever I., Bruna J., Erhan D., Goodfellow I., Fergus R., Intriguing Properties of Neural Networks, (2013); Carlini N., Wagner D., Adversarial examples are not easily detected: Bypassing ten detection methods, Proc. 10th Acm Workshop Artif. Intell. Secur. (AISec), pp. 3-14, (2017); Gilmer J., Adams R.P., Goodfellow I., Andersen D., Dahl G.E., Motivating the Rules of the Game for Adversarial Example Research, (2018); Ranjan R., Patel V.M., Chellappa R., HyperFace: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition, IEEE Trans. Pattern Anal. Mach. Intell., 41, 1, pp. 121-135, (2019); Castelluccio M., Poggi G., Sansone C., Verdoliva L., Land Use Classification in Remote Sensing Images by Convolutional Neural Networks, (2015); Wen D., Huang X., Zhang L., Benediktsson J.A., A novel automatic change detection method for urban high-resolution remotely sensed imagery based on multiindex scene representation, IEEE Trans. Geosci. Remote Sens., 54, 1, pp. 609-625, (2016); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag., 4, 2, pp. 22-40, (2016); Zhao W., Guo Z., Yue J., Zhang X., Luo L., On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery, Int. J. Remote Sens., 36, 13, pp. 3368-3379, (2015); Xu Z., Guan K., Casler N., Peng B., Wang S., A 3D convolutional neural network method for land cover classification using LiDAR and multi-temporal landsat imagery, Isprs J. Photogramm. Remote Sens., 144, pp. 423-434, (2018); Ren S., Deng Y., He K., Che W., Generating natural language adversarial examples through probability weighted word saliency, Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, pp. 1085-1097, (2019); Premlatha B., Reddy K.P., Someswar G.M., Security evaluation of pattern classifier under attack, Compusoft, 7, 2, pp. 2664-2692, (2018); Chakraborty A., Alam M., Dey V., Chattopadhyay A., Mukhopadhyay D., Adversarial Attacks and Defences: A Survey, (2018); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, (2015); Xu B., Wang N., Chen T., Li M., Empirical Evaluation of Rectified Activations in Convolutional Network, (2015); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Pro-cess. Syst., pp. 1097-1105, (2012); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1-9, (2015); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A.C., Fei-Fei L., ImageNet large scale visual recognition challenge, Int. J. Comput. Vis., 115, 3, pp. 211-252, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Everingham M., Eslami S.M.A., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal visual object classes challenge: A retrospective, Int. J. Comput. Vis., 111, 1, pp. 98-136, (2015); Lin T.-Y., Maire M., Belongie S.J., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common objects in context, Proc. 13th Eur. Conf. , Comput. Vis. (ECCV), in Lecture Notes in Computer Science, 8693, (2014); Li Z., Peng C., Yu G., Zhang X., Deng Y., Sun J., DetNet: A Backbone Network for Object Detection, (2018); Tayara H., Chong K., Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network, Sensors, 18, 10, (2018); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Proc. 18th Sigspatial Int. Conf. Adv. Geographic Inf. Syst. (GIS), pp. 270-279, (2010); Cheng G., Han J., Lu X., Remote sensing image scene classification: Benchmark and state of the art, Proc. Ieee, 105, 10, pp. 1865-1883, (2017); Li H., A Large-Scale Remote Sensing Image Scene Classification Database., (2019); Abadi M., Et al., TensorFlow: Large-scale machine learning on heterogeneous systems, CoRR, (2016); Maaten Der L.Van, Hinton G., Visualizing data using t-SNE, J. Mach. Learn. Res., 9, pp. 2579-2605, (2008); Su D., Zhang H., Chen H., Yi J., Chen P.-Y., Gao Y., Is robustness the cost of accuracy? A comprehensive study on the robustness of 18 deep image classification models, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 631-648, (2018); Zhang J., Li C., Adversarial examples: Opportunities and challenges, IEEE Trans. Neural Netw. Learn. Syst., 31, 7, pp. 2578-2593, (2020); Sun L., Tan M., Zhou Z., A survey of practical adversarial example attacks, Cybersecurity, 1, 1, (2018); Zawistowski P., Adversarial examples: A survey, Proc. Baltic Ursi Symp. (URSI), pp. 295-298, (2018); Szegedy C., Zaremba W., Sutskever I., Bruna J., Erhan D., Goodfellow I., Fergus R., Intriguing properties of neural networks, Proc. Int. Conf. Learn. Represent., pp. 1-10, (2014); Tramer F., Kurakin A., Papernot N., Goodfellow I., Boneh D., McDaniel P., Ensemble Adversarial Training: Attacks and Defenses, (2017); Guo C., Rana M., Cisse M., Maaten Der L.Van, Countering Adversarial Images Using Input Transformations, (2017); Xie C., Wang J., Zhang Z., Ren Z., Yuille A., Mitigating Adversarial Effects through Randomization, (2017); Shao Z., Cai J., Remote sensing image fusion with deep convolutional neural network, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 11, 5, pp. 1656-1669, (2018); Shao Z., Pan Y., Diao C., Cai J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network, IEEE Trans. Geosci. Remote Sens., 57, 6, pp. 4062-4076, (2019); Yuan X., He P., Zhu Q., Li X., Adversarial examples: Attacks and defenses for deep learning, IEEE Trans. Neural Netw. Learn. Syst., 30, 9, pp. 2805-2824, (2019)","L. Zhao; School of Geosciences and Info-Physics, Central South University, Changsha, 61872446, China; email: ling_dang@163.com","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092502915"
"Alidoost F.; Arefi H.","Alidoost, F. (55681278800); Arefi, H. (14031194500)","55681278800; 14031194500","Knowledge based 3d building model recognition using convolutional neural networks from lidar and aerial imageries","2016","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41","","","833","840","7","21","10.5194/isprsarchives-XLI-B3-833-2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978062704&doi=10.5194%2fisprsarchives-XLI-B3-833-2016&partnerID=40&md5=8849924744b2c5930cd10cd35821b67d","School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran","Alidoost F., School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; Arefi H., School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran","In recent years, with the development of the high resolution data acquisition technologies, many different approaches and algorithms have been presented to extract the accurate and timely updated 3D models of buildings as a key element of city structures for numerous applications in urban mapping. In this paper, a novel and model-based approach is proposed for automatic recognition of buildings' roof models such as flat, gable, hip, and pyramid hip roof models based on deep structures for hierarchical learning of features that are extracted from both LiDAR and aerial ortho-photos. The main steps of this approach include building segmentation, feature extraction and learning, and finally building roof labeling in a supervised pre-Trained Convolutional Neural Network (CNN) framework to have an automatic recognition system for various types of buildings over an urban area. In this framework, the height information provides invariant geometric features for convolutional neural network to localize the boundary of each individual roofs. CNN is a kind of feed-forward neural network with the multilayer perceptron concept which consists of a number of convolutional and subsampling layers in an adaptable structure and it is widely used in pattern recognition and object detection application. Since the training dataset is a small library of labeled models for different shapes of roofs, the computation time of learning can be decreased significantly using the pre-Trained models. The experimental results highlight the effectiveness of the deep learning approach to detect and extract the pattern of buildings' roofs automatically considering the complementary nature of height and RGB information.","3D Building Model; Convolutional Neural Network; Deep Learning; LiDAR; Pattern Recognition","Aerial photography; Algorithms; Buildings; Convolution; Data acquisition; Feature extraction; Knowledge based systems; Neural networks; Object detection; Optical radar; Pattern recognition; Remote sensing; Roofs; 3D building models; Adaptable structures; Automatic recognition; Automatic recognition system; Convolutional neural network; Deep learning; Hierarchical learning; Model based approach; Three dimensional computer graphics","","","","","","","Menegatti E., Michael N., Berns K., Yamaguchi H., Intelligent Autonomous Systems 13, (2016); Alexandre L.A., 301 of Advances in Intelligent Systems and Computing Series, pp. 889-898, (2016); Arefi H., Reinartz P., Building reconstruction using DSM and orthorectified images, Remote Sensing, 5, 4, pp. 1681-1703, (2013); Awrangjeb M., Zhang C., Fraser C.S., Automatic extraction of building roofs using LIDAR data and multispectral imagery, ISPRS Journal of Photogrammetry and Remote Sensing, 83, pp. 1-18, (2013); Bastien F., Lamblin P., Pascanu R., Bergstra J., Goodfellow J.I., Bergeron A., Bouchard N., Bengio Y., Theano: New features and speed improvements deep learning and unsupervised feature learning, NIPS 2012 Workshop, (2012); Bengio Y., Learning deep architectures for ai, Foundations and Trends in Machine Learning, 2, 1, pp. 1-127, (2009); Chatfield K., Simonyan K., Vedaldi A., Zisserman A., Return of the devil in the details: Delving deep into convolutional nets, Proc. BMVC, (2014); Dahl G.E., Yu D., Deng L., Acero A., Context-dependent pre-Trained deep neural networks for large-vocabulary speech recognition, IEEE Transactions on Audio, Speech, and Language Processing, 20, 1, pp. 30-42, (2012); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, Proc. CVPR, (2009); Filipe S., Alexandre L.A., From the human visual system to the computational models of visual attention: A survey, Artificial Intelligence Review IEEE Transactions on Geoscience and Remote Sensing, 52, 11, pp. 1-47, (2014); Girshick R., Donahue J., Darrell T., Malik J., Region-based convolutional networks for accurate object detection and semantic segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 1, pp. 142-158, (2015); Guo L., Chehata N., Mallet C., Boukir S., Relevance of airborne LiDAR and multispectral image data for urban scene classification using, Random Forests ISPRS Journal of Photogrammetry and Remote Sensing, 66, 1, pp. 56-66, (2011); Haala N., Brenner C., Extraction of buildings and trees in urban environments, ISPRS Journal of Photogrammetry & Remote Sensing, 54, pp. 130-137, (1999); Hermosilla T., Ruiz L.A., Recio J.A., Estornell J., Evaluation of automatic building detection approaches combining high resolution images and lidar data, Remote Sensing, 3, 6, pp. 1188-1210, (2011); Jia Y., Caffe: An Open Source Convolutional Architecture for Fast Feature Embedding, (2013); Karantzalos K., Koutsourakis P., Kalisperakis I., Grammatikopoulos L., Model based building detection from low-cost optical sensors on-board unmanned aerial vehicles, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 293-297, (2015); Khurana M., Wadhwa V., Automatic building detection using modified grab cut algorithm from high resolution satellite image, International Journal of Advanced Research in Computer and Communication Engineering, 4, (2015); Kim C., Habib A., Object-based integration of photogrammetric and lidar data for automated generation of complex polyhedral building models, Sensors, 9, 7, pp. 5679-5701, (2009); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, NIPS, pp. 1106-1114, (2012); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc IEEE, 86, 11, pp. 2278-2324, (1998); Liu T., Fang S., Zhao Y., Wang P., Zhang J., Implementation of training convolutional neural networks, Computer Vision and Pattern Recognition, (2015); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. CVPR, (2015); Maitra D.S., Bhattacharya U., Parui S.K., CNN based common approach to handwritten character recognition of multiple scripts, 13th International Conference on Document Analysis and Recognition, Tunis, pp. 1021-1025, (2015); Maltezos E., Ioannidis C., Automatic detection of building points from LiDAR and dense image matching point clouds, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, (2015); Matikainen L., Hyyppa J., Kaartinen H., Automatic detection of changes from laser scanner and aerial image data for updating building maps, International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 35, pp. 434-439, (2004); Mongus D., Lukac N., Zalik B., Ground and building extraction from LiDAR data based on differential morphological profiles and locally fitted surfaces, ISPRS Journal of Photogrammetry and Remote Sensing, 93, pp. 145-156, (2014); Ngo T.T., Collet C., Mazet V., Automatic rectangular building detection from VHR aerial imagery using shadow and image segmentation, IEEE International Conference on Image Processing, (2015); Partovi T., Krauss T., Arefi H., Omidalizarandi M., Reinartz P., Model-driven 3D building reconstruction based on integration of DSM and spectral information of satellite images, 2014 IEEE Geoscience and Remote Sensing Symposium (IGARSS), Quebec, Canada, pp. 3168-3171, (2014); Phung S.L., Bouzerdoum A., MATLAB Library for Convolutional Neural Networks, (2009); Salah M., Trinder J., Shaker A., Evaluation of the self organizing map classifier for building detection from LiDAR data and multispectral aerial images, Journal of Spatial Science, 54, 2, pp. 15-34, (2009); Schwalbe E., Maas H.-G., Seidel F., 3D building generation from airborne laser scanner data using 2D GIS data and orthogonal point cloud projections, ISPRS Workshop of Laser Scanning, the Netherlands, (2005); Singh G., Jouppi M., Zhang Z., Zakhor A., Shadow based building extraction from single satellite image, Proceedings of SPIE, the International Society for Optical Engineering, 9401, (2015); Socher R., Bengio Y., Manning C., Deep Learning for NLP Tutorial at Association of Computational Logistics (ACL, (2013); Sohn G., Dowman I., Data fusion of high-resolution satellite imagery and LiDAR data for automatic building extraction, ISPRS Journal of Photogrammetry & Remote Sensing, 62, pp. 43-63, (2007); Uijlings J., Van De Sande K., Gevers T., Smeulders A., Selective search for object recognition, International Journal of Computer Vision, 104, 2, pp. 154-171, (2013); Vakalopoulou M., Karantzalos K., Komodakis N., Paragios N., Building detection in very high resolution multispectral data with deep learning features, IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Milan, Italy, pp. 1873-1876, (2015); Vedaldi A., Lenc K., Matconvnet: Convolutional neural networks for matlab, Proceeding of the ACM International Conference on Multimedia, (2015); Vu T.T., Yamazaki F., Matsuok M., Multi-scale solution for building extraction from LiDAR and image data, International Journal of Applied Earth Observation and Geoinformation, 11, 4, pp. 281-289, (2009); Wichmann A., Jung J., Sohn G., Kada M., Ehlers M., Integration of building knowledge into binary space partitioning for the reconstruction of regularized building models, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, (2015); Yang X., Li J., Advances in Mapping from Remote Sensor Imagery, Techniques and Applications, pp. 33-68, (2012); Yuan J., Automatic Building Extraction in Aerial Scenes Using Convolutional Networks, (2016); Yu B., Liu H., Wu J., Hu Y., Zhang L., Automated derivation of urban building density information using airborne LiDAR data and object-based method, Landscape and Urban Planning, 98, 3-4, pp. 210-219, (2010); Zeiler M.D., Fergus R., Computer Vision, ECCV 2014, pp. 818-833, (2014); Zhang Y., Sohn K., Villegas R., Pan G., Lee H., Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2015); Zhou G.Q., Zhou X., Seamless fusion of LiDAR and aerial imagery for building extraction, IEEE Transactions on Geoscience and Remote Sensing, 52, 11, pp. 7393-7407, (2014); Zou Q., Ni L., Zhang T., Wang Q., Deep learning based feature selection for remote sensing scene classification, IEEE Geoscience and Remote Sensing Letters, 12, (2015)","F. Alidoost; School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; email: Falidoost@ut.ac.ir","Halounova L.; Bredif M.; Pajdla T.; Oude Elberink S.; Safar V.; Skaloud J.; Rottensteiner F.; Stilla U.; Limpouch A.; Schindler K.; ETH Zurich, Stefano-Franscini-Platz 5, Zurich; Mayer H.; Mallet C.","International Society for Photogrammetry and Remote Sensing","","23rd International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences Congress, ISPRS 2016","12 July 2016 through 19 July 2016","Prague","122053","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84978062704"
"Avola D.; Cinque L.; Diko A.; Fagioli A.; Foresti G.L.; Mecca A.; Pannone D.; Piciarelli C.","Avola, Danilo (15134991900); Cinque, Luigi (56213232800); Diko, Anxhelo (57223282460); Fagioli, Alessio (57211169159); Foresti, Gian Luca (7006427233); Mecca, Alessio (55890993200); Pannone, Daniele (57189389409); Piciarelli, Claudio (9039137600)","15134991900; 56213232800; 57223282460; 57211169159; 7006427233; 55890993200; 57189389409; 9039137600","MS-faster R-CNN: Multi-stream backbone for improved faster R-CNN object detection and aerial tracking from UAV images","2021","Remote Sensing","13","9","1670","","","","33","10.3390/rs13091670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105436695&doi=10.3390%2frs13091670&partnerID=40&md5=c214c9ed5c9a924df72b8f372c828b37","Department of Computer Science, Sapienza University, Rome, 00198, Italy; Department of Mathematics, Computer Science and Physics, University of Udine, Udine, 33100, Italy","Avola D., Department of Computer Science, Sapienza University, Rome, 00198, Italy; Cinque L., Department of Computer Science, Sapienza University, Rome, 00198, Italy; Diko A., Department of Computer Science, Sapienza University, Rome, 00198, Italy; Fagioli A., Department of Computer Science, Sapienza University, Rome, 00198, Italy; Foresti G.L., Department of Mathematics, Computer Science and Physics, University of Udine, Udine, 33100, Italy; Mecca A., Department of Computer Science, Sapienza University, Rome, 00198, Italy; Pannone D., Department of Computer Science, Sapienza University, Rome, 00198, Italy; Piciarelli C., Department of Mathematics, Computer Science and Physics, University of Udine, Udine, 33100, Italy","Tracking objects across multiple video frames is a challenging task due to several difficult issues such as occlusions, background clutter, lighting as well as object and camera view-point variations, which directly affect the object detection. These aspects are even more emphasized when analyzing unmanned aerial vehicles (UAV) based images, where the vehicle movement can also impact the image quality. A common strategy employed to address these issues is to analyze the input images at different scales to obtain as much information as possible to correctly detect and track the objects across video sequences. Following this rationale, in this paper, we introduce a simple yet effective novel multi-stream (MS) architecture, where different kernel sizes are applied to each stream to simulate a multi-scale image analysis. The proposed architecture is then used as backbone for the well-known Faster-R-CNN pipeline, defining a MS-Faster R-CNN object detector that consistently detects objects in video sequences. Subsequently, this detector is jointly used with the Simple Online and Real-time Tracking with a Deep Association Metric (Deep SORT) algorithm to achieve real-time tracking capabilities on UAV images. To assess the presented architecture, extensive experiments were performed on the UMCD, UAVDT, UAV20L, and UAV123 datasets. The presented pipeline achieved state-of-the-art performance, confirming that the proposed multi-stream method can correctly emulate the robust multi-scale image analysis paradigm. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aerial images; Deep learning; Object detection; Tracking; UAV","Aircraft detection; Antennas; Convolutional neural networks; Object detection; Object recognition; Object tracking; Pipelines; Signal detection; Unmanned aerial vehicles (UAV); Video recording; Association metric; Background clutter; Object detectors; Proposed architectures; Real time tracking; State-of-the-art performance; Tracking objects; Vehicle movements; Image enhancement","","","","","Department of Computer Science of Sapienza University; TRAAA; Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR","Acknowledgments: This work was partially supported by both the ONRG project N62909-20-1-2075 “Target Re-Association for Autonomous Agents” (TRAAA) and MIUR under grant “Departments of Excellence 2018–2022” of the Department of Computer Science of Sapienza University.","Avola D., Cinque L., Pannone D., Design of a 3D Platform for Immersive Neurocognitive Rehabilitation, Information, 11, pp. 1-19, (2020); Manca M., Paterno F., Santoro C., Zedda E., Braschi C., Franco R., Sale A., The impact of serious games with humanoid robots on mild cognitive impairment older adults, Int. J. Hum.-Comput. Stud, 145, (2021); Avola D., Cinque L., Foresti G.L., Marini M.R., Pannone D., VRheab: a fully immersive motor rehabilitation system based on recurrent neural network, Multimed. Tools Appl, 77, pp. 24955-24982, (2018); Ladakis. I., Kilintzis., V.; Xanthopoulou., D.; Chouvarda., I. Virtual Reality and Serious Games for Stress Reduction with Application in Work Environments, Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies, 5, pp. 541-548; Torner J., Skouras S., Molinuevo J.L., Gispert J.D., Alpiste F., Multipurpose virtual reality environment for biomedical and health applications, IEEE Trans. Neural Syst. Rehabil. Eng, 27, pp. 1511-1520, (2019); Avola D., Cinque L., Foresti G.L., Mercuri C., Pannone D., A Practical Framework for the Development of Augmented Reality Applications by Using ArUco Markers, Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods, pp. 645-654; Ikbal M.S., Ramadoss V., Zoppi M., Dynamic Pose Tracking Performance Evaluation of HTC Vive Virtual Reality System, IEEE Access, 9, pp. 3798-3815, (2021); Blut C., Blankenbach J., Three-dimensional CityGML building models in mobile augmented reality: A smartphone-based pose tracking system, Int. J. Digit. Earth, 14, pp. 32-51, (2021); Choy S.M., Cheng E., Wilkinson R.H., Burnett I., Austin M.W., Quality of Experience Comparison of Stereoscopic 3D Videos in Different Projection Devices: Flat Screen, Panoramic Screen and Virtual Reality Headset, IEEE Access, 9, pp. 9584-9594, (2021); Izard S.G., Mendez J.A.J., Palomera P.R., Garcia-Penalvo F.J., Applications of virtual and augmented reality in biomedical imaging, J. Med. Syst, 43, pp. 1-5, (2019); Avola D., Cinque L., Foresti G.L., Pannone D., Automatic Deception Detection in RGB Videos Using Facial Action Units, Proceedings of the 13th International Conference on Distributed Smart Cameras, pp. 1-6; Khan W., Crockett K., O'Shea J., Hussain A., Khan B.M., Deception in the eyes of deceiver: A computer vision and machine learning based automated deception detection, Expert Syst. Appl, 169, (2021); Avola D., Cinque L., De Marsico M., Fagioli A., Foresti G.L., LieToMe: Preliminary study on hand gestures for deception detection via Fisher-LSTM, Pattern Recognit. Lett, 138, pp. 455-461, (2020); Wu Z., Singh B., Davis L., Subrahmanian V., Deception detection in videos, Proceedings of the AAAI Conference on Artificial Intelligence, 32, (2018); Avola D., Cinque L., Foresti G.L., Pannone D., Visual Cryptography for Detecting Hidden Targets by Small-Scale Robots, Proceedings of the Pattern Recognition Applications and Methods, pp. 186-201; Roy S., Hazera C.T., Das D., Rahman Pir R.M.S., Ahmed A.S., A computer vision and artificial intelligence based cost-effective object sensing robot, Int. J. Intell. Robot. Appl, 3, pp. 457-470, (2019); Avola D., Cinque L., Foresti G.L., Pannone D., Homography vs similarity transformation in aerial mosaicking: Which is the best at different altitudes?, Multimed. Tools Appl, 79, pp. 18387-18404, (2020); Manzanilla A., Reyes S., Garcia M., Mercado D., Lozano R., Autonomous Navigation for Unmanned Underwater Vehicles: Real-Time Experiments Using Computer Vision, IEEE Robot. Autom. Lett, 4, pp. 1351-1356, (2019); Viejo C.G., Fuentes S., Howell K., Torrico D., Dunshea F.R., Robotics and computer vision techniques combined with non-invasive consumer biometrics to assess quality traits from beer foamability using machine learning: A potential for artificial intelligence applications, Food Control, 92, pp. 72-79, (2018); Lauterbach H.A., Koch C.B., Hess R., Eck D., Schilling K., Nuchter A., The Eins3D project — Instantaneous UAV-Based 3D Mapping for Search and Rescue Applications, Proceedings of the 2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR), pp. 1-6; Ruetten L., Regis P.A., Feil-Seifer D., Sengupta S., Area-Optimized UAV Swarm Network for Search and Rescue Operations, Proceedings of the 2020 10th Annual Computing and Communication Workshop and Conference (CCWC), pp. 613-618; Alotaibi E.T., Alqefari S.S., Koubaa A., Lsar: Multi-uav collaboration for search and rescue missions, IEEE Access, 7, pp. 55817-55832, (2019); Zhou S., Yang L., Zhao L., Bi G., Quasi-polar-based FFBP algorithm for miniature UAV SAR imaging without navigational data, IEEE Trans. Geosci. Remote Sens, 55, pp. 7053-7065, (2017); Lopez A., Jurado J.M., Ogayar C.J., Feito F.R., A framework for registering UAV-based imagery for crop-tracking in Precision Agriculture, Int. J. Appl. Earth Obs. Geoinf, 97, (2021); Mazzia V., Comba L., Khaliq A., Chiaberge M., Gay P., UAV and Machine Learning Based Refinement of a Satellite-Driven Vegetation Index for Precision Agriculture, Sensors, 20, pp. 25-29, (2020); Mesas-Carrascosa F.J., Clavero Rumbao I., Torres-Sanchez J., Garcia-Ferrer A., Pena J., Lopez Granados F., Accurate ortho-mosaicked six-band multispectral UAV images as affected by mission planning for precision agriculture proposes, Int. J. Remote Sens, 38, pp. 2161-2176, (2017); Popescu D., Stoican F., Stamatescu G., Ichim L., Dragana C., Advanced UAV–WSN system for intelligent monitoring in precision agriculture, Sensors, 20, (2020); Tsouros D.C., Bibi S., Sarigiannidis P.G., A review on UAV-based applications for precision agriculture, Information, 10, (2019); Avola D., Cinque L., Fagioli A., Foresti G.L., Pannone D., Piciarelli C., Automatic estimation of optimal UAV flight parameters for real-time wide areas monitoring, Multimed. Tools Appl, pp. 1-23, (2021); Avola D., Foresti G.L., Martinel N., Micheloni C., Pannone D., Piciarelli C., Aerial video surveillance system for small-scale UAV environment monitoring, Proceedings of the 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pp. 1-6; Piciarelli C., Foresti G.L., Drone swarm patrolling with uneven coverage requirements, IET Comput. Vis, 14, pp. 452-461, (2020); Padro J.C., Munoz F.J., Planas J., Pons X., Comparison of four UAV georeferencing methods for environmental monitoring purposes focusing on the combined use with airborne and satellite remote sensing platforms, Int. J. Appl. Earth Obs. Geoinf, 75, pp. 130-140, (2019); Avola D., Cinque L., Fagioli A., Foresti G.L., Massaroni C., Pannone D., Feature-based SLAM algorithm for small scale UAV with nadir view, Proceedings of the International Conference on Image Analysis and Processing, pp. 457-467; Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2016); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, Proceedings of the International Conference on Learning Representations, pp. 1-14; He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778; Wojke N., Bewley A., Paulus D., Simple online and realtime tracking with a deep association metric, Proceedings of the IEEE International Conference on Image Processing (ICIP), pp. 3645-3649; Du D., Qi Y., Yu H., Yang Y., Duan K., Li G., Zhang W., Huang Q., Tian Q., The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking, Proceedings of the European Conference on Computer Vision (ECCV), pp. 1-17; Mueller M., Smith N., Ghanem B., A Benchmark and Simulator for UAV Tracking, Proceedings of the Computer Vision— ECCV 2016, pp. 445-461, (2016); Avola D., Cinque L., Foresti G.L., Martinel N., Pannone D., Piciarelli C., A UAV Video Dataset for Mosaicking and Change Detection From Low-Altitude Flights, IEEE Trans. Syst. Man Cybern. Syst, 50, pp. 2139-2149, (2020); Yao R., Lin G., Xia S., Zhao J., Zhou Y., Video object segmentation and tracking: A survey, ACM Trans. Intell. Syst. Technol. (TIST), 11, pp. 1-47, (2020); Zhou Q., Zhong B., Zhang Y., Li J., Fu Y., Deep alignment network based multi-person tracking with occlusion and motion reasoning, IEEE Trans. Multimed, 21, pp. 1183-1194, (2018); Chen L., Ai H., Zhuang Z., Shang C., Real-time multiple people tracking with deeply learned candidate selection and person re-identification, Proceedings of the 2018 IEEE International Conference on Multimedia and Expo (ICME), pp. 1-6; Tang Z., Wang G., Xiao H., Zheng A., Hwang J.N., Single-camera and inter-camera vehicle tracking and 3D speed estimation based on fusion of visual and semantic features, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 108-115; Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271; Liu S., Wang S., Shi W., Liu H., Li Z., Mao T., Vehicle tracking by detection in UAV aerial video, Sci. China Inf. Sci, 62, (2019); Zhu M., Zhang H., Zhang J., Zhuo L., Multi-level prediction Siamese network for real-time UAV visual tracking, Image Vis. Comput, 103, (2020); Huang W., Zhou X., Dong M., Xu H., Multiple objects tracking in the UAV system based on hierarchical deep high-resolution network, Multimed. Tools Appl, pp. 1-19, (2021); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1440-1448; Girshick R., Donahue J., Darrell T., Malik J., Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Paszke A., Gross S., Massa F., Lerer A., Bradbury J., Chanan G., Killeen T., Lin Z., Gimelshein N., Antiga L., Et al., PyTorch: An Imperative Style, High-Performance Deep Learning Library, Proceedings of the Advances in Neural Information Processing Systems, 32, pp. 8024-8035; Feng W., Han R., Guo Q., Zhu J., Wang S., Dynamic Saliency-Aware Regularization for Correlation Filter-Based Object Tracking, IEEE Trans. Image Process, 28, pp. 3232-3245, (2019); Danelljan M., Bhat G., Shahbaz Khan F., Felsberg M., ECO: Efficient Convolution Operators for Tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9; Li F., Tian C., Zuo W., Zhang L., Yang M., Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4904-4913; Mueller M., Smith N., Ghanem B., Context-Aware Correlation Filter Tracking, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1387-1395; Danelljan M., Hager G., Khan F.S., Felsberg M., Learning Spatially Regularized Correlation Filters for Visual Tracking, Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 4310-4318; Danelljan M., Hager G., Khan F.S., Felsberg M., Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1430-1438; Galoogahi H.K., Fagg A., Lucey S., Learning Background-Aware Correlation Filters for Visual Tracking, Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1144-1152; Wang C., Zhang L., Xie L., Yuan J., Kernel Cross-Correlator, Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4179-4186; Danelljan M., Hager G., Khan F.S., Felsberg M., Discriminative Scale Space Tracking, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1561-1575, (2017); Li Y., Zhu J., A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration, Proceedings of the Computer Vision—ECCV Workshops, pp. 254-265, (2014); Danelljan M., Hager G., Shahbaz Khan F., Felsberg M., Accurate Scale Estimation for Robust Visual Tracking, Proceedings of the British Machine Vision Conference, pp. 1-11; Henriques J.F., Caseiro R., Martins P., Batista J., High-Speed Tracking with Kernelized Correlation Filters, IEEE Trans. Pattern Anal. Mach. Intell, 37, pp. 583-596, (2015); Fu C., Xu J., Lin F., Guo F., Liu T., Zhang Z., Object Saliency-Aware Dual Regularized Correlation Filter for Real-Time Aerial Tracking, IEEE Trans. Geosci. Remote Sens, 58, pp. 8940-8951, (2020); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I., Wojna Z., Song Y., Guadarrama S., Et al., Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-10","D. Avola; Department of Computer Science, Sapienza University, Rome, 00198, Italy; email: avola@di.uniroma1.it","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85105436695"
"Blaga B.-C.-Z.; Nedevschi S.","Blaga, Bianca-Cerasela-Zelia (57200514208); Nedevschi, Sergiu (6602876540)","57200514208; 6602876540","Weakly Supervised Semantic Segmentation Learning on UAV Video Sequences","2021","European Signal Processing Conference","2021-August","","","731","735","4","2","10.23919/EUSIPCO54536.2021.9616055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123173948&doi=10.23919%2fEUSIPCO54536.2021.9616055&partnerID=40&md5=c3cc319ca1d59b0b3e8ac16d0f0e228d","Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","Blaga B.-C.-Z., Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Nedevschi S., Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","The domain of scene understanding from Unmanned Aerial Vehicles (UAVs) is of high interest for researchers in the computer vision domain, since it can be used for object detection and tracking in scenarios like deforestation monitoring, traffic surveillance, or for civil engineering tasks. However, the topic of dense video segmentation from drones has been insufficiently explored due to the lack of annotated ground truth data. We propose a solution based on a framework composed of a deep neural network for semantic segmentation and an optical flow generator, linked together by a spatio-temporal GRU component to efficiently solve the problem of weakly supervised semantic segmentation of video sequences recorded from UAVs. The novelty of our work comes from the employment of depthwise separable convolutions for the GRU component, which decrease the computation time and increase the segmentation accuracy. We test our methodology on the synthetic dataset Mid-Air, for low-altitude drone flight, and report results that prove the usefulness of the proposed system. © 2021 European Signal Processing Conference. All rights reserved.","Optical flow; Unmanned aerial vehicles; Video semantic segmentation; Weakly supervised learning","Aircraft detection; Antennas; Deep neural networks; Deforestation; Drones; Object detection; Optical flows; Security systems; Semantics; Statistical tests; Video recording; Engineering tasks; Object detection and tracking; Scene understanding; Semantic segmentation; Traffic surveillance; Video segmentation; Video semantic segmentation; Video semantics; Video sequences; Weakly supervised learning; Semantic Segmentation","","","","","Ministry of Education and Research, Romania, (PN-III-P4-ID-PCCF-2016-0180)","ACKNOWLEDGMENT This work was supported by the “SEPCA-Integrated Semantic Visual Perception and Control for Autonomous Systems” grant funded by the Romanian Ministry of Education and Research, code PN-III-P4-ID-PCCF-2016-0180.","Morgan J. L., Gergel S. E., Coops N. C., Aerial photography: A rapidly evolving tool for ecological management, BioScience, 60, 1, pp. 47-59, (2010); Suarez J. C., Ontiveros C., Smith S., Snape S., Use of airborne LiDAR and aerial photography in the estimation of individual tree heights in forestry, Computers & Geosciences, 31, 2, pp. 253-262, (2005); Nguyen V. N., Jenssen R., Roverso D., Automatic autonomous vision-based power line inspection: A review of current status and the potential role of deep learning, International Journal of Electrical Power & Energy Systems, 99, pp. 107-120, (2018); Erdelj M., Natalizio E., Chowdhury K. R., Akyildiz I. F., Help from the sky: Leveraging UAVs for disaster management, IEEE Pervasive Computing, 16, 1, pp. 24-32, (2017); Blaga B.-C.-Z., Nedevschi S., Semantic segmentation learning for autonomous uavs using simulators and real data, 2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP), pp. 303-310, (2019); Schmarje L., Santarossa M., Schroder S.-M., Koch R., A survey on semi-, self-and unsupervised learning for image classification; Blaga B.-C.-Z., Nedevschi S., Exploring deep learning solutions for scene perception from UAVs using simulators and real data, 2020 IEEE 16th International Conference on Intelligent Computer Communication and Processing (ICCP), pp. 353-360, (2020); Giusti A., Guzzi J., Ciresan D. C., He F.-L., Rodriguez J. P., Fontana F., Faessler M., Forster C., Schmidhuber J., Di Caro G., Et al., A machine learning approach to visual perception of forest trails for mobile robots, IEEE Robotics and Automation Letters, 1, 2, pp. 661-667, (2015); Fayyaz M., Saffar M. H., Sabokrou M., Fathy M., Huang F., Klette R., STFCN: spatio-temporal fully convolutional neural network for semantic segmentation of street scenes, Asian Conference on Computer Vision, pp. 493-509, (2016); He Y., Chiu W.-C., Keuper M., Fritz M., STD2P: RGBD semantic segmentation using spatio-temporal data-driven pooling, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4837-4846, (2017); Qiu Z., Yao T., Mei T., Learning deep spatio-temporal dependence for semantic video segmentation, IEEE Transactions on Multimedia, 20, 4, pp. 939-949, (2017); Wu J., Wen Z., Zhao S., Huang K., Video semantic segmentation via feature propagation with holistic attention, Pattern Recognition, (2020); Yan M., Wang J., Li J., Zhang K., Yang Z., Traffic scene semantic segmentation using self-attention mechanism and bi-directional GRU to correlate context, Neurocomputing, 386, pp. 293-304, (2020); Zhang T., Lin G., Cai J., Shen T., Shen C., Kot A. C., Decoupled spatial neural attention for weakly supervised semantic segmentation, IEEE Transactions on Multimedia, 21, 11, pp. 2930-2941, (2019); Zhang K., Wang L., Liu D., Liu B., Liu Q., Li Z., Dual temporal memory network for efficient video object segmentation, Proceedings of the 28th ACM International Conference on Multimedia, pp. 1515-1523, (2020); Redondo-Cabrera C., Baptista-Rios M., Lopez-Sastre R. J., Learning to exploit the prior network knowledge for weakly supervised semantic segmentation, IEEE Transactions on Image Processing, 28, 7, pp. 3649-3661, (2019); Nilsson D., Sminchisescu C., Semantic video segmentation by gated recurrent flow propagation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6819-6828, (2018); Lup V., Nedevschi S., Video semantic segmentation leveraging dense optical flow, 2020 IEEE 16th International Conference on Intelligent Computer Communication and Processing (ICCP), pp. 369-376, (2020); Romera E., Alvarez J. M., Bergasa L. M., Arroyo R., ERFNet: Efficient residual factorized convnet for real-time semantic segmentation, IEEE Transactions on Intelligent Transportation Systems, 19, 1, pp. 263-272, (2017); Sun D., Yang X., Liu M.-Y., Kautz J., PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2018); Ilg E., Mayer N., Saikia T., Keuper M., Dosovitskiy A., Brox T., Flownet 2.0: Evolution of optical flow estimation with deep networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2462-2470, (2017); Yang G., Ramanan D., Volumetric correspondence networks for optical flow, NeurIPS, 5, (2019); Dosovitskiy A., Fischer P., Ilg E., Hausser P., Hazirbas C., Golkov V., Van Der Smagt P., Cremers D., Brox T., Flownet: Learning optical flow with convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, pp. 2758-2766, (2015); Mayer N., Ilg E., Hausser P., Fischer P., Cremers D., Dosovitskiy A., Brox T., A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4040-4048, (2016); Menze M., Geiger A., Object scene flow for autonomous vehicles, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3061-3070, (2015); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258, (2017); Pfeuffer A., Dietmayer K., Separable convolutional LSTMs for faster video segmentation, 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pp. 1072-1078, (2019); Fonder M., Van Droogenbroeck M., Mid-Air: A multi-modal dataset for extremely low altitude drone flights, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, (2019); Wang J., Sun K., Cheng T., Jiang B., Deng C., Zhao Y., Liu D., Mu Y., Tan M., Wang X., Et al., Deep high-resolution representation learning for visual recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, (2020)","","","European Signal Processing Conference, EUSIPCO","","29th European Signal Processing Conference, EUSIPCO 2021","23 August 2021 through 27 August 2021","Dublin","175283","22195491","978-908279706-0","","","English","European Signal Proces. Conf.","Conference paper","Final","","Scopus","2-s2.0-85123173948"
"Zhang J.; Wan G.; Zhang H.; Li S.; Feng X.","Zhang, Junjun (57202419899); Wan, Guangtong (57219005774); Zhang, Hongqun (49865034700); Li, Shanshan (56239889900); Feng, Xuxiang (56707870500)","57202419899; 57219005774; 49865034700; 56239889900; 56707870500","Rapid road extraction from quick view imagery of high-resolution satellites with transfer learning; [迁移学习下高分快视数据道路快速提取]","2020","Journal of Image and Graphics","25","7","","1501","1512","11","3","10.11834/jig.190441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091046245&doi=10.11834%2fjig.190441&partnerID=40&md5=07d328bc127348380ecf0ef218828d95","Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China; University of Chinese Academy of Sciences, Beijing, 100049, China","Zhang J., Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China, University of Chinese Academy of Sciences, Beijing, 100049, China; Wan G., Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China; Zhang H., Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China; Li S., Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China; Feng X., Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China","Objective: Quick view data generated by high-resolution satellites provide real-time reception and full resolution for quick view imaging. Such imaging offers a timely source of data for practical applications, such as fire detection, moving window display, disaster observation, and military information acquisition. Road extraction from remote sensing images has been a popular research topic in the field of remote sensing image analysis. Traditional object-oriented methods are not highly automated, and road features require prior knowledge for manual selection and design. These conditions lead to problems in real-time road information acquisition. The popular deep learning road extraction method mainly focuses on the improvement of precision and lacks research on the timeliness of road information extraction. Transfer learning can rapidly complete the task in the target area through weight sharing among different fields and make the model algorithm highly personalized. A transfer learning deep network for rapidly extracting roads is constructed to utilize quick view data from high-resolution satellites. Method: First, we propose a least-square fitting method of devignetting to solve the most serious radiation problem of TDICCD (time delay and integration charge coupled devices) vignetting phenomenon appearing in raw quick view data. The results of the preprocessing of the quick view data serve as our training dataset. Then, we choose LinkNet as the target network after comparing the performance among different real-time semantic segmentation networks, such as ENet, U-Net, LinkNet, and D-LinkNet. LinkNet is efficient in computation memory, can learn from a relatively small training set, and allows residual unit ease training of deep networks. The rich bypass links each encoder with decoder. Thus, the networks can be designed with few parameters. The encoder starts with a kernel of size 7 × 7. In the next encoder block, its contracting path to capture context uses 3 × 3 full convolution. We use batch normalization in each convolutional layer, followed by ReLU nonlinearity. Reflection padding is used to extrapolate the missing context in the training data for predicting the pixels in the border region of the input image. The input of each encoder layer of LinkNet is bypassed to the output of its corresponding decoder. Lost spatial information about the max pooling can then be recovered by the decoder and its upsampling operations. Finally, we modify LinkNet to keep it consistent with ResNet34 network layer features, the so-called fine tuning, for accelerating LinkNet network training process. Fine tuning is a useful efficient method of transfer learning. The use of ResNet34 weight parameter pretrained on ImageNet initializing LinkNet34 can accelerate the network convergence and lead to improved performance with almost no additional cost. Result: In the process of devignetting quick view data, the least-square linear fitting method proposed in this study can efficiently remove the vignetting strip of the original image, which meets practical applications. In our road extraction experiment, LinkNet34 using the pretrained ResNet34 as encoder has a 6% improvement in Dice accuracy compared with that when using ResNet34 not pretrained on the valid dataset. The time consumption of a single test feature map is reduced by 39 ms, and the test Dice accuracy can reach 88.3%. Pretrained networks substantially reduce training time that also helps prevent overfitting. Consequently, we achieve over 88 % test accuracy and 40 ms test time on the quick view dataset. With an input feature map size of 3×256×256 pixels, the data of Tianjin Binhai with a size of 7 304 × 6 980 pixels take 54 s. The original LinkNet using ResNet18 as its encoder only has a Dice coefficient of 85.7%. We evaluate ResNet50 and ResNet101 as pretrained encoders. The Dice accuracy of the former is not improved, whereas the latter takes too much test time. We compare the performance of LinkNet34 with those of three other popular deep transfer models for classification, namely, U-Net; two modifications of TernausNet and AlubNet using VGG11 (visual geometry group) and ResNet34 as encoders separately; and a modification of D-LinkNet. The two U-Net modifications are likely to incorrectly recognize roads as background or recognize something nonroad, such as tree, as road. D-LinkNet has higher Dice than LinkNet34 on the validation set, but the testing time takes 59 ms more than that of LinkNet34. LinkNet34 avoids the weaknesses of TernuasNet and AlubNet and makes better predictions than them. The small nonroad gap between two roads can also be avoided. Many methods mix the two roads into one. The method proposed in this study generally achieves good connectivity, accurate edge, and clear outline in the case of complete extraction of the entire road and fine location. It is especially suitable for rural linear roads and the extraction of area roads in towns. However, the extraction effect for complex road networks in urban areas is incomplete. Conclusion: In this study, we build a deep transfer learning neural network, LinkNet34, which uses a pretrained network, ResNet34, as an encoder. ResNet34 allows LinkNet34 to learn without any significant increase in the number of parameters, solves the problem that the bottom layer features randomly initialized with weights of neural networks are inadequately rich, and accelerates network convergence. Our approach demonstrates the improvement in LinkNet34 by the use of the pretrained encoder and the better performance of LinkNet34 than other real-time segmentation architecture. The experimental results show that LinkNet34 can handle road properties, such as narrowness, connectivity, complexity, and long span, to some extent. This architecture proves useful for binary classification with limited data and realizes fast and accurate acquisition of road information. Future research should consider increasing the quick view database. The pretrained network LinkNet34 trains on the expanded quick view database and then transfers. The ""semantic gap"" between the source and target networks is reduced, and the data distribute similarly. These features are conducive to model initialization. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","Fast road extraction; Fine-tuning; High-resolution satellite; Quick view data; Transfer learning","","","","","","","","Badrinarayanan V, Kendall A, Cipolla R., SegNet: a deep convolutional encoder-decoder architecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 12, pp. 2481-2495, (2017); Chaurasia A, Culurciello E., LinkNet: exploiting encoder representations for efficient semantic segmentation, Proceedings of 2017 IEEE Visual Communications and Image Processing, pp. 1-4, (2017); Deng J, Dong W, Socher R, Li L J, Li K, Li F F., ImageNet: a large-scale hierarchical image database, Proceedings of 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, (2009); Gan J Y, Qi L, Qin C B, He G H., Lightweight fingerprint classification model combined with transfer learning, Journal of Image and Graphics, 24, 7, pp. 1086-1095, (2019); Gu J X, Yang R Z, Shi L, Wei H W., HJ-1C real-time image processing technology based on GPU, Journal of University of Chinese Academy of Sciences, 31, 5, pp. 708-713, (2014); He K M, Zhang X Y, Ren S Q, Sun J., Deep residual learning for image recognition, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); He K M, Zhang X Y, Ren S Q, Sun J., Identity mappings in deep residual networks, Proceedings of the 14th European Conference on Computer Vision, pp. 630-645, (2016); Iglovikov V, Shvets A., TernausNet: u-net with VGG11 encoder pre-trained on imagenet for image segmentation, (2018); Mnih V., Machine learning for aerial image labeling, (2013); Paszke A, Chaurasia A, Kim S, Culurciello E., ENet: a deep neural network architecture for real-time semantic segmentation, (2016); Ronneberger O, Fischer P, Brox T., U-Net: convolutional networks for biomedical image segmentation, Proceedings of the 18th International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241, (2015); Shelhamer E, Long J, Darrell T., Fully convolutional networks for semantic segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 4, pp. 640-651, (2017); Shi W Z, Zhu C Q, Wang Y., Road feature extraction from remotely sensed image: review and prospects, Acta Geodaetica et Cartographica Sinica, 30, 3, pp. 257-262, (2001); Shvets A A, Rakhlin A, Kalinin A A, Iglovikov V I., Automatic instrument segmentation in robot-assisted surgery using deep learning, Proceedings of the 17th IEEE International Conference on Machine Learning and Applications, pp. 624-628, (2018); Su T., Research on the Registration and Mosaic Technology of TDICCD Stitching Images Based on Reflectors, (2014); Tan C Q, Sun F C, Kong T, Zhang W C, Yang C, Liu C F., A survey on deep transfer learning, Proceedings of the 27th International Conference on Artificial Neural Networks, pp. 270-279, (2018); Yosinski J, Clune J, Bengio Y, Lipson H., How transferable are features in deep neural networks?, (2014); Zhang Z X, Liu Q J, Wang Y H., Road extraction by deep residual u-net, IEEE Geoscience and Remote Sensing Letters, 15, 5, pp. 749-753, (2018); Zhou L C, Zhang C, Wu M., D-LinkNet: LinkNet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction, Proceedings of 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 192-196, (2018)","G. Wan; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, 100094, China; email: wangt@aircas.ac.cn","","Editorial and Publishing Board of JIG","","","","","","10068961","","","","Chinese","J. Image and Graphics","Article","Final","","Scopus","2-s2.0-85091046245"
"Xie Q.; Li D.; Yu Z.; Zhou J.; Wang J.","Xie, Qian (57158331500); Li, Dawei (57919061800); Yu, Zhenghao (57212930537); Zhou, Jun (57216892216); Wang, Jun (55878876500)","57158331500; 57919061800; 57212930537; 57216892216; 55878876500","Detecting Trees in Street Images via Deep Learning with Attention Module","2020","IEEE Transactions on Instrumentation and Measurement","69","8","8930566","5395","5406","11","30","10.1109/TIM.2019.2958580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088252100&doi=10.1109%2fTIM.2019.2958580&partnerID=40&md5=d9f21339ed9c0a3786d712f71b08df38","College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China","Xie Q., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; Li D., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; Yu Z., College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; Zhou J., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; Wang J., College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China","Although object detection techniques have been widely employed in various practical applications, automatic tree detection is still a difficult challenge, especially for street-view images. In this article, we propose a unified end-to-end trainable network for automatic street tree detection based on a state-of-the-art deep learning-based object detector. We tackle low illumination and heavy occlusion conditions in tree detection, which have not been extensively studied until now, due to clear challenges. Existing generic object detectors cannot be directly applied to this task due to aforementioned challenges. To address these issues, we first present a simple, yet effective image brightness adjustment method to handle low illuminance cases. Moreover, we propose a novel loss and a tree part-attention module to reduce false detections caused by heavy occlusion, inspired by the previously proposed occlusion-aware region-convolutional neural network (R-CNN) work. We train and evaluate several versions of the proposed network and validate the importance of each component. It is demonstrated that the resulting framework, part attention network for tree detection (PANTD), can efficiently detect trees in street-view images. The experimental results show that our approach achieves high accuracy and robustness under various conditions. © 1963-2012 IEEE.","Convolutional neural network (CNN); deep learning; occlusion; part attention; tree detection","Convolutional neural networks; Forestry; Object detection; Object recognition; False detections; Heavy occlusion; High-accuracy; Image brightness; Low illuminations; Object detectors; State of the art; Tree detections; Deep learning","","","","","National Natural Science Foundation of China, NSFC, (61772267); Natural Science Foundation of Jiangsu Province, (BK20190016); Fundamental Research Funds for the Central Universities, (NE2016004)","Manuscript received July 29, 2019; revised October 7, 2019; accepted November 26, 2019. Date of publication December 10, 2019; date of current version June 24, 2020. This work was supported in part by the National Natural Science Foundation of China under Grant 61772267, in part by the Fundamental Research Funds for the Central Universities under Grant NE2016004, and in part by the Natural Science Foundation of Jiangsu Province under Grant BK20190016. The Associate Editor coordinating the review process was Amitava Chatterjee. (Corresponding author: Jun Wang.) Q. Xie, D. Li, J. Zhou, and J. Wang are with the College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China (e-mail: qianxie@nuaa.edu.cn; mdaweili@outlook.com; junzhou@nuaa.edu.cn; wjun@nuaa.edu.cn).","Shah U., Khawad R., Krishna K.M., Detecting, localizing, and recognizing trees with a monocular MAV: Towards preventing deforesta-tion, Proc. Ieee Int. Conf. Robot. Automat. (ICRA), May/Jun., pp. 1982-1987, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards realtime object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); Ouyang W., Wang X., Joint deep learning for pedestrian detection, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), Dec., pp. 2056-2063, (2013); Litjens G., Et al., A survey on deep learning in medical image analysis, Med. Image Anal., 42, pp. 60-88, (2017); Gibert X., Patel V.M., Chellappa R., Deep multitask learning for railway track inspection, Ieee Trans. Intell. Transp. Syst., 18, 1, pp. 153-164, (2017); Zhang H., Jin X., Wu Q.M.J., Wang Y., He Z., Yang Y., Automatic visual detection system of railway surface defects with curvature filter and improved Gaussian mixture model, Ieee Trans. Instrum. Meas., 67, 7, pp. 1593-1608, (2018); Yu H., Et al., A coarse-to-fine model for rail surface defect detection, Ieee Trans. Instrum. Meas., 68, 3, pp. 656-666, (2018); Zhang S., Wen L., Bian X., Lei Z., Li S.Z., Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd, (2018); Wang X., Xiao T., Jiang Y., Shao S., Sun J., Shen C., Repulsion loss: Detecting pedestrians in a crowd, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun., pp. 7774-7783, (2018); Kaartinen H., Et al., An international comparison of individual tree detection and extraction using airborne laser scanning, Remote Sens., 4, 4, pp. 950-974, (2012); Guo B., Huang X., Zhang F., Sohn G., Classification of airborne laser scanning data using JointBoost, Isprs J. Photogramm. Remote Sens., 100, pp. 71-83, (2015); Tianyang D., Jian Z., Sibin G., Ying S., Jing F., Single-tree detection in high-resolution remote-sensing images based on a cascade neural network, Isprs Int. J. Geo-Inf., 7, 9, (2018); Secord J., Zakhor A., Tree detection in urban regions using aerial LiDAR and image data, Ieee Geosci. Remote Sens. Lett., 4, 2, pp. 196-200, (2007); Malek S., Bazi Y., Alajlan N., AlHichri H., Melgani F., Efficient framework for palm tree detection in UAV images, Ieee J. Sel. Topics Appl. Earth Observ. Remote Sens., 7, 12, pp. 4692-4703, (2014); Ozcan A.H., Sayar Y., Hisar D., Unsalan C., Multiscale tree analysis from satellite images, Proc. 7th Int. Conf. Recent Adv. Space Technol. (RAST), pp. 265-269, (2015); Nevalainen O., Et al., Individual tree detection and classification with UAV-based photogrammetric point clouds and hyperspectral imaging, Remote Sens., 9, 3, (2017); Strimbu V.F., Strimbu B.M., A graph-based segmentation algorithm for tree crown extraction using airborne LiDAR data, Isprs J. Photogramm. Remote Sens., 104, pp. 30-43, (2015); Aval J., Et al., Individual street tree detection from airborne data and contextual information, Proc. Geobia from Pixels Ecosyst. Global Sustainability, (2018); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens., 9, 1, (2016); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks, Remote Sens., 11, 1, (2019); Zhang Z., Fidler S., Urtasun R., Instance-level segmentation for autonomous driving with deep densely connected MRFs, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun., pp. 669-677, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun., pp. 779-788, (2016); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proc. Ieee Comput. Soc. Conf. Comput. Vis. Pattern Recognit., Jun., 1, 1, pp. 886-893, (2005); Dollar P., Tu Z., Perona P., Belongie S., Integral channel features, Proc. Brit. Mach. Vis. Conf., London, U.K., (2009); Felzenszwalb P.F., Girshick R.B., McAllester D., Cascade object detection with deformable part models, Proc. Ieee Comput. Soc. Conf. Comput. Vis. Pattern Recognit., Jun., pp. 2241-2248, (2010); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Process. Syst., pp. 1097-1105, (2012); Uijlings J.R.R., van de Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun., pp. 580-587, (2014); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, Proc. Eur. Conf. Comput. Vis, pp. 346-361, (2014); Erhan D., Szegedy C., Toshev A., Anguelov D., Scalable object detection using deep neural networks, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun., pp. 2147-2154, (2014); Pinheiro P.O.O., Collobert R., Dollar P., Learning to segment object candidates, Proc. Adv. Neural Inf. Process. Syst., pp. 1990-1998, (2015); Liu W., Et al., SSD: Single shot multibox detector, Proc. Eur. Conf. Comput. Vis, pp. 21-37, (2016); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, (2017); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Chen J., Liu Z., Wang H., Nunez A., Han Z., Automatic defect detection of fasteners on the catenary support device using deep con-volutional neural network, Ieee Trans. Instrum. Meas., 67, 2, pp. 257-269, (2018); Kang G., Gao S., Yu L., Zhang D., Deep architecture for highspeed railway insulator surface defect detection: Denoising autoencoder with multitask learning, Ieee Trans. Instrum. Meas., 68, 8, pp. 2679-2690, (2018); Zhong J., Liu Z., Han Z., Han Y., Zhang W., A CNN-based defect inspection method for catenary split pins in high-speed railway, Ieee Trans. Instrum. Meas., 68, 8, pp. 2849-2860, (2018); Girshick R., Fast R-CNN, Proc. Ieee Int. Conf. Comput. Vis., Dec., pp. 1440-1448, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., Jun., pp. 770-778, (2016); Huang J., Et al., Speed/accuracy trade-offs for modern convolutional object detectors, Proc. Ieee CVPR, Jul., pp. 7310-7311, (2017); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); Zeiler M.D., Fergus R., Visualizing and understanding convo-lutional networks, Proc. Eur. Conf. Comput. Vis, pp. 818-833, (2014); Russakovsky O., Et al., ImageNet large scale visual recognition challenge, Int. J. Comput. Vis., 115, 3, pp. 211-252, (2015); Zhang L., Lin L., Liang X., He K., Is faster R-CNN doing well for pedestrian detection?, Proc. Eur. Conf. Comput. Vis, pp. 443-457, (2016); Zhou C., Yuan J., Multi-label learning of part detectors for heavily occluded pedestrian detection, Proc. Ieee Int. Conf. Comput. Vis. (ICCV), Oct., pp. 3486-3495, (2017); Ouyang W., Zhou H., Li H., Li Q., Yan J., Wang X., Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection, Ieee Trans. Pattern Anal. Mach. Intell., 40, 8, pp. 1874-1887, (2018); Chorowski J.K., Bahdanau D., Serdyuk D., Cho K., Bengio Y., Attention-based models for speech recognition, Proc. Adv. Neural Inf. Process. Syst., pp. 577-585, (2015); Tzutalin Labelimg. [Online], (2015); Dollar P., Wojek C., Schiele B., Perona P., Pedestrian detection: An evaluation of the state of the art, Ieee Trans. Pattern Anal. Mach. Intell., 34, 4, pp. 743-761, (2012)","J. Wang; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; email: wjun@nuaa.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","00189456","","IEIMA","","English","IEEE Trans. Instrum. Meas.","Article","Final","","Scopus","2-s2.0-85088252100"
"Culman M.; Rodríguez A.C.; Wegner J.D.; Delalieux S.; Somers B.","Culman, Mariá (56205876200); Rodríguez, Andrés C. (57208004602); Wegner, Jan Dirk (35099690000); Delalieux, Stephanie (8213218200); Somers, Ben (12645025400)","56205876200; 57208004602; 35099690000; 8213218200; 12645025400","Deep learning for sub-pixel palm tree classification using spaceborne Sentinel-2 imagery","2021","Proceedings of SPIE - The International Society for Optical Engineering","11856","","118560E","","","","1","10.1117/12.2599861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118700151&doi=10.1117%2f12.2599861&partnerID=40&md5=a42c0128b9adb03037f64e9cc0d381de","Division of Forest, Nature and Landscape, KU Leuven, Leuven, 3001, Belgium; EcoVision Lab - Photogrammetry and Remote Sensing, ETH, Zurich, 8093, Switzerland; Flemish Institute for Technological Research-VITO NV, Mol, 2400, Belgium; Institute for Computational Science, Zurich, 8057, Switzerland","Culman M., Division of Forest, Nature and Landscape, KU Leuven, Leuven, 3001, Belgium, Flemish Institute for Technological Research-VITO NV, Mol, 2400, Belgium; Rodríguez A.C., EcoVision Lab - Photogrammetry and Remote Sensing, ETH, Zurich, 8093, Switzerland; Wegner J.D., EcoVision Lab - Photogrammetry and Remote Sensing, ETH, Zurich, 8093, Switzerland, Institute for Computational Science, Zurich, 8057, Switzerland; Delalieux S., Flemish Institute for Technological Research-VITO NV, Mol, 2400, Belgium; Somers B., Division of Forest, Nature and Landscape, KU Leuven, Leuven, 3001, Belgium","The challenge of classifying and locating Phoenix palm trees in different scenes with different appearances and varied ages has been addressed with deep learning object detection over aerial images. Nevertheless, an explicit limitation hereof is that palms should be visually identifiable in the image - i.e., palm crowns should be larger than the pixel size. Unfortunately, high-spatial resolution imagery is not widely and directly available in the Phoenix palm growing regions of the Mediterranean, Middle East, and North Africa. This study, therefore, presents the re-implementation of a semantic segmentation architecture to train a model able to classify Phoenix palm pixels. This is applied to freely available medium resolution space-borne Sentinel-2 images over the Spanish island of La Gomera (Canary Islands). At the study site, a total of 116,330 Phoenix palms had been inventoried by the local government. Palms appear in multiple, heterogeneous environments, which implies a background variability that is a persistent challenge for palm pixel classification. The re-implemented architecture is a novelty in deep semantic segmentation and density estimation initially developed for counting objects of sub-pixel size. And it proved to be successful for creating a model of palm classification, thereby compensating for the limited spatial resolution of the Sentinel-2 images. The palm tree sub-pixel classification model achieved an overall accuracy of 0.921, with a recall and precision of 0.438 and 0.522. These results demonstrate the potential of remote sensing data of medium-spatial resolution for vegetation mapping in applications where trees are scattered over extensive areas. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Convolutional Neural Network; Deep Learning; Ecosystem Mapping; Remote Sensing; Semantic Segmentation; Sub-pixel classification; Tree Inventory; Tree mapping","Antennas; Deep learning; Image classification; Image resolution; Image segmentation; Mapping; Network architecture; Neural networks; Object detection; Pixels; Remote sensing; Semantic Web; Semantics; Space optics; Convolutional neural network; Deep learning; Ecosystem mapping; PaLM-tree; Remote-sensing; Semantic segmentation; Sub-pixel classification; Sub-pixels; Tree inventory; Tree mapping; Semantic Segmentation","","","","","","","Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS Journal of Photogrammetry and Remote Sensing, 117, pp. 11-28, (2016); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS Journal of Photogrammetry and Remote Sensing, 159, pp. 296-307, (2019); Rodriguez A. C., Wegner J. D., Counting the Uncountable: Deep Semantic Density Estimation from Space, Pattern Recognition. GCPR 2018. Lecture Notes in Computer Science, pp. 351-362, (2019); Corbane C., Syrris V., Sabo F., Politis P., Melchiorri M., Pesaresi M., Soille P., Kemper T., Convolutional neural networks for global human settlements mapping from Sentinel-2 satellite imagery, Neural Computing and Applications, 33, 12, pp. 6697-6720, (2021); de Carvalho O. L. F., de Carvalho O. A., de Albuquerque A. O., de Bem P. P., Silva C. R., Ferreira P. H. G., de Moura R. D. S., Gomes R. A. T., Guimar-aes R. F., Borges D. L., Instance segmentation for large, multi-channel remote sensing imagery using mask-RCNN and a mosaicking approach, Remote Sensing, 13, 1, pp. 1-24, (2021); Culman M., Delalieux S., Van Tricht K., Individual palm tree detection using deep learning on RGB imagery to support tree inventory, Remote Sensing, 12, 21, pp. 1-31, (2020); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sensing, 9, 1, (2017); Mubin N. A., Nadarajoo E., Shafri H. Z. M., Hamedianfar A., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method, International Journal of Remote Sensing, 40, 19, pp. 7500-7515, (2019); Sentinel-2; Cartogrfiafica de Canarias S.A., Mapa de palmeras canarias, (2017); Rodrfiguez A. C., D'Aronco S., Schindler K., Wegner J. D., Mapping oil palm density at country scale: An active learning approach, Remote Sensing of Environment, 261, May, (2021)","S. Delalieux; Flemish Institute for Technological Research-VITO NV, Mol, 2400, Belgium; email: stephanie.delalieux@vito.be","Neale C.M.U.; Maltese A.","SPIE","The Society of Photo-Optical Instrumentation Engineers (SPIE)","Remote Sensing for Agriculture, Ecosystems, and Hydrology XXIII 2021","13 September 2021 through 17 September 2021","Virtual, Online","173094","0277786X","978-151064556-1","PSISD","","English","Proc SPIE Int Soc Opt Eng","Conference paper","Final","","Scopus","2-s2.0-85118700151"
"Jones E.G.; Wong S.; Milton A.; Sclauzero J.; Whittenbury H.; McDonnell M.D.","Jones, Eriita G. (55448385300); Wong, Sebastien (9839957400); Milton, Anthony (55270576000); Sclauzero, Joseph (57215896359); Whittenbury, Holly (57215897123); McDonnell, Mark D. (7102564709)","55448385300; 9839957400; 55270576000; 57215896359; 57215897123; 7102564709","The impact of pan-sharpening and spectral resolution on vineyard segmentation through machine learning","2020","Remote Sensing","12","6","934","","","","19","10.3390/rs12060934","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082294071&doi=10.3390%2frs12060934&partnerID=40&md5=ad9174b03a553f4e021f38041deeb8f4","Computational Learning Systems Laboratory, School of Information Technology and Mathematical Sciences, University of South Australia, Adealide, 5095, SA, Australia; Consilium Technology, Adelaide, 5000, SA, Australia; School of Natural and Built Environments, University of South Australia, Adelaide, 5095, SA, Australia","Jones E.G., Computational Learning Systems Laboratory, School of Information Technology and Mathematical Sciences, University of South Australia, Adealide, 5095, SA, Australia, Consilium Technology, Adelaide, 5000, SA, Australia, School of Natural and Built Environments, University of South Australia, Adelaide, 5095, SA, Australia; Wong S., Consilium Technology, Adelaide, 5000, SA, Australia; Milton A., Consilium Technology, Adelaide, 5000, SA, Australia; Sclauzero J., Consilium Technology, Adelaide, 5000, SA, Australia; Whittenbury H., Consilium Technology, Adelaide, 5000, SA, Australia, School of Natural and Built Environments, University of South Australia, Adelaide, 5095, SA, Australia; McDonnell M.D., Computational Learning Systems Laboratory, School of Information Technology and Mathematical Sciences, University of South Australia, Adealide, 5095, SA, Australia, Consilium Technology, Adelaide, 5000, SA, Australia","Precision viticulture benefits from the accurate detection of vineyard vegetation from remote sensing, without a priori knowledge of vine locations. Vineyard detection enables efficient, and potentially automated, derivation of spatial measures such as length and area of crop, and hence required volumes of water, fertilizer, and other resources. Machine learning techniques have provided significant advancements in recent years in the areas of image segmentation, classification, and object detection, with neural networks shown to perform well in the detection of vineyards and other crops. However, what has not been extensively quantitatively examined is the extent to which the initial choice of input imagery impacts detection/segmentation accuracy. Here, we use a standard deep convolutional neural network (CNN) to detect and segment vineyards across Australia using DigitalGlobeWorldview-2 images at~50 cm(panchromatic) and~2m(multispectral) spatial resolution. A quantitative assessment of the variation in model performance with input parameters during model training is presented from a remote sensing perspective, with combinations of panchromatic, multispectral, pan-sharpened multispectral, and the spectral Normalised Difference Vegetation Index (NDVI) considered. The impact of image acquisition parameters-namely, the off-nadir angle and solar elevation angle-on the quality of pan-sharpening is also assessed. The results are synthesised into a 'recipe' for optimising the accuracy of vineyard segmentation, which can provide a guide to others aiming to implement or improve automated crop detection and classification. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image fusion; Machine learning; Pan-sharpening; Precision viticulture; Semantic segmentation","Convolutional neural networks; Crops; Deep learning; Deep neural networks; Image fusion; Learning systems; Object detection; Remote sensing; Semantics; Vegetation; Acquisition parameters; Machine learning techniques; Normalised difference vegetation index; Pan-sharpening; Precision viticulture; Quantitative assessments; Semantic segmentation; Spatial resolution; Image segmentation","","","","","Australian Federal Government's Department of Industry, Innovation, and Science; Department of Industry, Innovation and Science, Australian Government","Funding text 1: This research received no external funding. Innovation Connections grants ICG000351 and ICG000357 from the Australian Federal Government's Department of Industry, Innovation, and Science are gratefully acknowledged.; Funding text 2: Acknowledgments: Innovation Connections grants ICG000351 and ICG000357 from the Australian Federal Government’s Department of Industry, Innovation, and Science are gratefully acknowledged.","Bramley R.G.V., Pearse B., Chamberlain P., Being Profitable Precisely-A Case Study of Precision Viticulture from Margaret River, Aust. N. Z. Grapegrow. Winemak, 473, pp. 84-87, (2003); Arno J., Martinez Casasnovas J.A., Dasi M.R., Rosell J.R., Precision Viticulture. Research Topics, Challenges and Opportunities in Site-Specific Vineyard Management, Span. J. Agric. Res, 7, (2009); Matese A., di Gennaro S.F., Technology in Precision Viticulture: A State of the Art Review, Int. J. Wine Res, 7, pp. 69-81, (2015); Karakizi C., Oikonomou M., Karantzalos K., Vineyard Detection and Vine Variety Discrimination from Very High Resolution Satellite Data, Remote Sens, 8, (2016); Sertel E., Yay I., Vineyard parcel identification from Worldview-2 images using object-based classification model, J. Appl. Remote Sens, 8, pp. 1-17, (2014); Poblete-Echeverria C., Olmedo G.F., Ingram B., Bardeen M., Detection and segmentation of vine canopy in ultra-high spatial resolution RGB imagery obtained from Unmanned Aerial Vehicle (UAV): A case study in a commercial vineyard, Remote Sens, 9, (2017); Shanmuganathan S., Sallis P., Pavesi L., Munoz M.C.J., Computational intelligence and geo-informatics in viticulture, Proceedings of the Second Asia International Conference on Modelling & Simulation (AMS), 2, pp. 480-485, (2008); Rodriguez-Perez J.R., Alvarez-Lopez C.J., Miranda D., Alvarez M.F., Vineyard Area Estimation Using Medium Spatial Resolution, Span. J. Agric. Res, 6, pp. 441-452, (2006); Hall A., Lamb D.W., Holzapfel B., Louis J., Optical Remote Sensing Applications in Viticulture-a Review, Aust. J. Grape Wine Res, 8, pp. 36-47, (2002); Khaliq A., Comba L., Biglia A., Aimonino D., Chiaberge M., Gay P., Comparison of Satellite and UAV-Based Multispectral Imagery for Vineyard Variability Assessment, Remote Sens, 11, (2019); Delenne C., Rabatel G., Agurto V., Deshayes M., Vine Plot Detection in Aerial Images Using Fourier Analysis, Proceedings of the 1st International Conference on Object-Based Image Analysis, 1, pp. 1-6, (2006); Delenne C., Durrieu S., Rabatel G., Deshayes M., From Pixel to Vine Parcel: A Complete Methodology for Vineyard Delineation and Characterization Using Remote-Sensing Data, Comput. Electron. Agric, 70, pp. 78-83, (2010); Kaplan G., Avdan U., Sentinel-2 Pan Sharpening-Comparative Analysis, Proceedings, 2, (2018); Vivone G., Alparone L., Chanussot J., Dalla Mura M., Garzelli A., Giorgio A., Licciardi G.A., Restaino R., Wald L., A Critical Comparison Among Pansharpening Algorithms, IEEE Trans. Geosci. Remote Sens, 53, pp. 2565-2586, (2015); Du Q., King R., On the Performance Evaluation of Pan-Sharpening Techniques, IEEE Geosci. Remote Sens, 4, pp. 518-522, (2007); Alparone L., Wald L., Chanussot J., Thomas C., Gamba P., Bruce L.M., Comparison of Pansharpening Algorithms: Outcome of the 2006 GRS-S Data-Fusion Contest, IEEE Trans. Geosci. Remote Sens, 45, pp. 3012-3021, (2007); Amro I., Mateos J., Vega M., Molina R., Katsaggelos A., A survey of classical methods and new trends in pansharpening of multispectral images, EURAS1P J. Adv. Signal Process, 79, pp. 1-22, (2011); Sertel E., Seker D., Yay I., Ozelkan E., Saglan M., Boz Y., Gunduz A., Vineyard mapping using remote sensing technologies, Proceedings of the FIG Working Week 2012: Knowing to Manage the Territory, pp. 1-8, (2012); Smit J.L., Sithole G., Strever A.E., Vine Signal Extraction-an Application of Remote Sensing in Precision Viticulture, S. Afr. J. Enol. Vitic, 31, pp. 65-73, (2010); Comba L., Gay P., Primicerio J., Aimonino D., Vineyard detection from unmanned aerial systems images, Comput. Electron. Agric, 114, pp. 78-87, (2015); Cinat P., di Gennaro S., Berton A., Matese A., Comparison of Unsupervised Algorithms for Vineyard Canopy Segmentation from UAV Multispectral Images, Remote Sens, 11, (2019); Padua L., Marques P., Hruska J., Adao T., Bessa J., Sousa A., Peres E., Morais R., Sousa J., Vineyard properties extraction combining UAS-based RGB imagery with elevation data, 1nt. J. Remote Sens, 39, pp. 5377-5401, (2018); Delenne C., Rabatel G., Deshayes M., An automatized frequency analysis for vine plot detection and delineation in remote sensing, IEEE Geosci. Remote Sens. Lett, 5, pp. 341-345, (2008); Rabatel G., Delenne C., Deshayes M., A Non-Supervised Approach Using Gabor Filters for Vine-Plot Detection in Aerial Images, Comput. Electron. Agric, 62, pp. 159-168, (2008); Ranchin T., Naert B., Albuisson M., Boyer G., Astrand P., An Automatic Method for Vine Detection in Airborne Imagery Using Wavelet Transform and Multiresolution Analysis, Photogramm. Eng. Remote Sens, 67, pp. 88-91, (2001); Gao F., He T., Masek J.G., Shuai Y., Schaaf C.B., Wang Z., Angular effects and correction for medium resolution sensors to support crop monitoring, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 7, pp. 4480-4489, (2014); Poblete T., Ortega-Farias S., Ryu D., Automatic Coregistration Algorithm to Remove Canopy Shaded Pixels in UAV-Borne Thermal Images to Improve the Estimation of Crop Water Stress Index of a Drip-Irrigated Cabernet Sauvignon Vineyard, Sensors, 18, (2018); Hall A., Louis J., Lamb D.A., Method For Extracting Detailed Information From High Resolution Multispectral Images Of Vineyards, Proceedings of the 6th International Conference on Geocomputation, 6, pp. 1-9, (2001); Towers P., Strever A., Poblete-Echeverria C., Comparison of Vegetation Indices for Leaf Area Index Estimation in Vertical Shoot Positioned Vine Canopies With and Without Grenbiule Hail-Protection Netting, Remote Sensing, 11, (2019); Delenne C., Durrieu S., Rabatel G., Deshayes M., Bailly J.S., Lelong C., Couteron P., Textural Approaches for Vineyard Detection and Characterization Using Very High Spatial Resolution Remote Sensing Data, Int. J. Remote Sens, 29, pp. 1153-1167, (2008); Geographical Indications, (2018); Halliday J., Wine Atlas of Australia, (2014); Sun L., Gao F., Anderson M.C., Kustas W.P., Alsina M.M., Sanchez L., Sams B., McKee L., Dulaney W., White W.A., Et al., Daily mapping of 30 m LAI and NDVI for grape yield prediction in California vineyards, Remote Sens, 9, (2017); Lamb D.W., Weedon M.M., Bramley R.G.V., Using remote sensing to predict grape phenolics and colour at harvest in a Cabernet Sauvignon vineyard: Timing observations against vine phenology and optimising image resolution, Aust. J. Grape Wine Res, 10, pp. 46-54, (2008); Webb L.B., Whetton P.H., Bhend J., Darbyshire R., Alsina M.M., Sanchez L., Sams B., McKee L., Dulaney W., White W.A., Et al., Earlier wine-grape ripening driven by climatic warming and drying and management practices, Nat. Clim. Chang, 2, pp. 259-264, (2012); Jackson R.S., Vineyard Practice, pp. 143-306, (2015); Globe D., Advanced Image Preprocessor with AComp, (2018); Accuracy of Worldview Products, (2019); Carper W., Lillesand T., Kiefer R., The use of intensity-hue-saturation transformations for merging SPOT panchromatic and multi-spectral image data, Photogramm. Eng. Remote Sens, 56, pp. 459-467, (1990); Laben C.A., Brower B.V., Process for Enhancing the Spatial Resolution of Multispectral Imagery Using Pan-Sharpening, (2000); Chavez P., Kwarteng A., Extracting spectral contrast in Landsat thematic mapper image data using selective principal component analysis, Photogramm. Eng. Remote Sens, 55, pp. 338-348, (1989); Lui W., Wang Z., A practical pan-sharpening method with wavelet transform and sparse representation, Proceedings of the 2013 IEEE International Conference on Imaging Systems and Techniques (IST), pp. 288-293, (2013); Palubinskas G., Fast, simple, and good pan-sharpening method, J. Appl. Rem. Sens, 7, (2013); Aiazzi B., Alparone L., Baronti S., Lotti F., Lossless image compression by quantization feedback in a content-driven enhanced Laplacian pyramid, IEEE Trans. Image Process, 6, pp. 831-843, (1997); Imani M., Band Dependent Spatial Details Injection Based on Collaborative Representation for Pansharpening, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 11, pp. 4994-5004, (2018); Gram-Schmidt Pan Sharpening, (2018); Song S., Liu J., Pu H., Liu Y., Luo J., The comparison of fusion methods for HSRRSI considering the effectiveness of land cover (Features) object recognition based on deep learning, Remote Sens, 11, (2019); Zhou C., Liang D., Yang X., Xu B., Yang G., Recognition of wheat spike from field based phenotype platform using multi-sensor fusion and improved maximum entropy segmentation algorithms, Remote Sens, 10, (2018); Baronti S., Aiazzi B., Selva M., Garzelli A., Alparone L., A Theoretical Analysis of the Effects of Aliasing and Misregistration on Pansharpened Imagery, IEEE J. Sel. Top. Signal Process, 5, pp. 446-453, (2011); Li H., Jing L., Tang Y., Assessment of pan-sharpening methods applied to WorldView-2 imagery fusion, Sensors, 17, (2017); Loncan L., de Almeida L.B., Bioucas-Dias J.M., Briottet X., Chanussot J., Dobigeon N., Fabre S., Liao W., Licciardi G.A., Simoes M., Et al., Hyperspectral Pansharpening: A Review, IEEE Geosci. Remote Sens. Mag, 3, pp. 1-15, (2015); Basaeed E., Bhaskar H., Al-mualla M., Comparative Analysis of Pan-sharpening Techniques on DubaiSat-1 images, Proceedings of the 16th International Conference on Information Fusion, 16, pp. 227-234, (2013); Garcia-Garcia A., Orts-Escolano S., Oprea S., Villena-Martinez V., Martinez-Gonzalez P., Garcia-Rodriguez J., A survey on deep learning techniques for image and video semantic segmentation, Appl. Soft Comput, 70, pp. 41-65, (2018); Huang B., Zhao B., Song T., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery, Remote Sens. Environ, 214, pp. 73-86, (2018); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Sozzi M., Kayad A., Tomasi D., Lovat L., Marinello F., Sartori L., Assessment of grapevine yield and quality using a canopy spectral index in white grape variety, Proceedings of the Precision Agriculture '19, (2019); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 2481-2495, (2017); Olaf Ronneberger P.F., Brox T., U-Net: Convolutional Networks for Biomedical Image Segmentation, Proceedings of the Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015, (2015); Chen L.C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation, ArXiv, (2018); Kornblith S., Shlens J., Le Q.V., Do Better ImageNet Models Transfer Better?, ArXiv, (2018); Wong S.C., Gatt A., Stamatescu V., McDonnell M.D., Understanding Data Augmentation for Classification: When to Warp?, Proceedings of the 2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA), pp. 1-6, (2016); Neural Netw, 106, pp. 249-259, (2018); Bishop C.M., Pattern Recognition and Machine Learning;, (2006); Google Maps, (2019); Landis J.R., Koch G.G., The Measurement of Observer Agreement for Categorical Data, Biometrics, 33, pp. 159-174, (1977); Congalton R.G., Accuracy assessment and validation of remotely sensed and other spatial information, Int. J. Wildland Fire, 10, pp. 321-328, (2001); Jerri A.J., The Shannon Sampling Theorem-Its Various Extensions and Applications: A Tutorial Review, Proc. IEEE, 65, pp. 1565-1596, (1977); Aswatha S.M., Mukhopadhyay J., Biswas P.K., Spectral Slopes for Automated Classification of Land Cover in Landsat Images, Proceedings of the IEEE International Conference on Image Processing (ICIP), pp. 4354-4358, (2016); Jones E.G., Caprarelli G., Mills F.P., Doran B., Clarke J., An Alternative Approach to Mapping Thermophysical Units from Martian Thermal Inertia and Albedo Data Using a Combination of Unsupervised Classification Techniques, Remote Sens, 6, pp. 5184-5237, (2014); Thomas C., Ranchin T., Wald L., Chanussot J., Synthesis of multispectral images to high spatial resolution: A critical review of fusion methods based on remote sensing physics, IEEE Trans. Geosci. Remote Sens, 46, pp. 1301-1312, (2008); Lord D., Desjardins R.L., Dube P.A., Sun-angle effects on the red and near infrared reflectances of five different crop canopies, Can. J. Remote Sens, 14, pp. 46-55, (1988); Rodriguez-Perez J., Ordonez C., Gonzalez-Fernandez A., Sanz-Ablanedo E., Valenciano J., Marcelo V., Leaf Water Content Estimation By Functional Linear Regression of Field Spectroscopy Data, Biosyst. Eng, 165, pp. 36-46, (2018); Huber S., Tagesson T., Fensholt R., An Automated Field Spectrometer System For Studying VIS, NIR and SWIR Anisotropy For Semi-Arid Savanna, Remote Sens. Environ, 152, pp. 547-556, (2014); Codella N.C.F., Gutman D., Celebi M.E., Helba B., Marchetti M.A., Dusza S.W., Kalloo A., Liopyris K., Mishra N., Kittler H., Et al., Skin lesion analysis toward melanoma detection: A challenge at the 2017 International symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC), ArXiv, (2018); Iglovikov V., Mushinskiy S., Osin V., Satellite Imagery Feature Detection using Deep Convolutional Neural Network: A Kaggle Competition, ArXiv, (2017); Tian C., Li C., Shi J., Dense Fusion Classmate Network for Land Cover Classification, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, (2018)","E.G. Jones; Computational Learning Systems Laboratory, School of Information Technology and Mathematical Sciences, University of South Australia, Adealide, 5095, Australia; email: eriita.jones@unisa.edu.au","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082294071"
"Chen X.; Xiang S.; Liu C.-L.; Pan C.-H.","Chen, Xueyun (56076920300); Xiang, Shiming (8938807200); Liu, Cheng-Lin (36064176500); Pan, Chun-Hong (8558023500)","56076920300; 8938807200; 36064176500; 8558023500","Aircraft detection by deep convolutional neural networks","2015","IPSJ Transactions on Computer Vision and Applications","7","","","10","17","7","16","10.2197/ipsjtcva.7.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930015347&doi=10.2197%2fipsjtcva.7.10&partnerID=40&md5=651e48c01a984d6e216445e12e655cd1","College of Electrical Engineering, Guangxi University, Nanning, 530007, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Chen X., College of Electrical Engineering, Guangxi University, Nanning, 530007, China; Xiang S., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Liu C.-L., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Pan C.-H., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Features play crucial role in the performance of classifier for object detection from high-resolution remote sensing images. In this paper, we implemented two types of deep learning methods, deep convolutional neural network (DNN) and deep belief net (DBN), comparing their performances with that of the traditional methods (handcrafted features with a shallow classifier) in the task of aircraft detection. These methods learn robust features from a large set of training samples to obtain a better performance. The depth of their layers (>6 layers) grants them the ability to extract stable and large-scale features from the image. Our experiments show both deep learning methods reduce at least 40% of the false alarm rate of the traditional methods (HOG, LBP+SVM), and DNN performs a little better than DBN. We also fed some multi-preprocessed images simultaneously to one DNN model, and found that such a practice helps to improve the performance of the model obviously with no extra-computing burden adding. © 2015 Information Processing Society of Japan.","Deep belief nets; Deep convolutional neural networks; Object detection; Remote sensing","Convolution; Feature extraction; Image reconstruction; Learning systems; Neural networks; Object detection; Object recognition; Remote sensing; Training aircraft; Convolutional neural network; Deep belief nets; Deep learning; False alarm rate; High resolution remote sensing images; Performance of classifier; Training sample; Aircraft detection","","","","","National Natural Science Foundation of China, (91338202)","","Hsieh J.-W., Chen J.-M., Chuang C.-H., Fan K.-C., Aircraft type recognition in satellite images, IEE Proceedings Vision, Image and Signal Processing, 152, 3, pp. 307-315, (2005); Yildiz C., Polat E., Detection of stationary aircrafts from satelitte images, 2011 IEEE 19th Conference on Signal Processing and Communications Applications, pp. 515-521, (2011); Liu G., Sun X., Fu K., Wang H., Aircraft recognition in high- resolution satellite images using coarse-to-fine shape prior, IEEE Geoscience and Remote Sensing Letters, 10, 3, pp. 573-577, (2013); Cai K., Shao W., Yin X., Liu G., Co-segmentation of aircrafts from high-resolution satellite images, Proc. ICSP 2012, pp. 993-996, (2012); Sun H., Sun X., Wang H., Li Y., Li X., Automatic target detection in high-resolution remote sensing images using spatial sparse coding bag-of-words model, IEEE Geoscience and Remote Sensing Letters, 9, 1, pp. 109-113, (2012); Li W., Xiang S., Wang H., Pan C., Robust airplane detection in satellite images, Proc. ICIP, pp. 2877-2880, (2011); Filippidis A., Jain L.C., Martin N., Fusion of intelligent agents for the detection of aircraft in sar images, IEEE Trans. PAMI, 22, pp. 378-384, (2000); Tien S.C., Chia T.L., Lu Y., Using cross-ratios to model curve data for aircraft recognition, Pattern Recognit. Lett., 24, 12, pp. 2047-2060, (2003); Xu C.F., Duan H.B., Artificial bee colony (ABC) optimized edge potential function (EPF) approach to target recognition for lowaltitude aircraft, Pattern Recognit. Lett., 31, 13, pp. 1759-1772, (2010); Scott G.J., Klaric M.N., Davis C.H., Shyu C.-R., Entropybalanced bitmap tree for shape-based object retrieval from largescale satellite imagery databases, IEEE Trans. Geosci. Remote Sens., 49, 5, pp. 1603-1616, (2011); Kembhavi A., Harwood D., Davis L.S., Vehicle detection using partial least squares, IEEE Trans. PAMI, 63, 3, pp. 1250-1265, (2011); Ali K., Fleuret F., Hasler D., Fua P., A real-time deformable detector, IEEE Trans. PAMI, 34, 2, pp. 225-239, (2012); Zunic J., Kopanja L., On the orientability of shapes, IEEE Trans. on Image Processing, 15, 11, pp. 3478-3487, (2006); Grabner H., Nguyen T., Gruber B., Bischof H., On-line boosting-based car detection from aerial images, ISPRS J. Photogrammetry and Remote Sensing, 63, 3, pp. 382-396, (2008); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proc. CVPR, 1, pp. 888-893, (2005); Wiesel D.H., Hubel T.N., Receptive fields of single neurones in the cats striate cortex, J. Physiology, 148, pp. 574-591, (1959); Fukushima K., Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position, Biological Cybernetics, 36, 4, pp. 193-202, (1980); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, 11, pp. 2278-2324, (1998); Garcia C., Delakis M., Convolutional face finder: A neural architecture for fast and robust face detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 26, 11, pp. 1408-1423, (2004); Ciresan D.C., Meier U., Schmidhuber J., Multi-column deep neural networks for image classification, Proc. Computer Vision and Pattern Recognition, pp. 3642-3649, (2012); Hinton G.E., Osindero S., A fast learning algorithm for deep belief nets, Neural Computation, 18, pp. 1527-1554, (2006); Hinton G.E., A Practical Guide to Training Restricted Boltzmann Machines, pp. 1-20, (2010); Hinton G.E., Reducing the dimensionality of data with neural networks, Science, 313, pp. 504-507, (2006); Yu D., Deng L., Dahl G., Roles of pretraining and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition, Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, (2010); Mohamed A., Yu D., Deng L., Investigation of full-sequence training of deep belief networks for speech recognition, Proc. Interspeech, 2010, pp. 1692-1695, (2010); Sarikaya R., Hinton G.E., Deep belief nets for natural language callcrouting, Proc. ICASSP, pp. 5680-5683, (2011); Seide F., Li G., Yu D., Conversational speech transcription using context-dependent deep neural networks, Proc. Interspeech 2011, (2011); Mohamed A., Dahl G.E., Hinton G., Acoustic modeling using deep belief networks, IEEE Trans. Audio, Speech, and Language Processing, 20, 1, pp. 14-22, (2012); Dahl G., Yu D., Deng L., Acero A., Context-dependent pretrained deep neural networks for large vocabulary speech recognition, IEEE Trans. Speech and Audio Processing, 20, 1, pp. 30-42, (2012); Yu D., Seide F., Li G., Li J., Seltzer M., Why deep neural networks are promising for large vocabulary speech recognition, IEEE Trans. Audio, Speech, and Language Processing, (2012); Daugman J.G., Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression, IEEE Trans. Acoust., Speech, Signal Processing, 36, 7, pp. 1169-1179, (1988); Ojala T., Pietikainen M., Maenpaa T., Multiresolution gray scale and rotation invariant texture classification with local binary patterns, IEEE Trans. PAMI, 24, 7, pp. 971-987, (2002); Lowe D.G., Distinctive image features from scale-invariant keypoints, International Journal of Computer Vision, 60, 2, pp. 91-110, (2004)","","","Information Processing Society of Japan","","","","","","18826695","","","","English","IPSJ Trans. Comput. Vis. Appl.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84930015347"
"Robson B.A.; Bolch T.; MacDonell S.; Hölbling D.; Rastner P.; Schaffer N.","Robson, Benjamin Aubrey (56900795400); Bolch, Tobias (55901447400); MacDonell, Shelley (24759068600); Hölbling, Daniel (23978173300); Rastner, Philipp (35311691900); Schaffer, Nicole (55183948300)","56900795400; 55901447400; 24759068600; 23978173300; 35311691900; 55183948300","Automated detection of rock glaciers using deep learning and object-based image analysis","2020","Remote Sensing of Environment","250","","112033","","","","64","10.1016/j.rse.2020.112033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089440785&doi=10.1016%2fj.rse.2020.112033&partnerID=40&md5=63ce65d3359e47eb653cd6699f95d410","Department of Geography, University of Bergen, Norway; School of Geography and Sustainable Development, University of St. Andrews, United Kingdom; Centro de Estudios Avanzados en Zonas Áridas (CEAZA), La Serena, Chile; Department of Geoinformatics – Z_GIS, University of Salzburg, Austria; Department of Geography, University of Zurich, Switzerland","Robson B.A., Department of Geography, University of Bergen, Norway; Bolch T., School of Geography and Sustainable Development, University of St. Andrews, United Kingdom; MacDonell S., Centro de Estudios Avanzados en Zonas Áridas (CEAZA), La Serena, Chile; Hölbling D., Department of Geoinformatics – Z_GIS, University of Salzburg, Austria; Rastner P., Department of Geography, University of Zurich, Switzerland; Schaffer N., Centro de Estudios Avanzados en Zonas Áridas (CEAZA), La Serena, Chile","Rock glaciers are an important component of the cryosphere and are one of the most visible manifestations of permafrost. While the significance of rock glacier contribution to streamflow remains uncertain, the contribution is likely to be important for certain parts of the world. High-resolution remote sensing data has permitted the creation of rock glacier inventories for large regions. However, due to the spectral similarity between rock glaciers and the surrounding material, the creation of such inventories is typically conducted based on manual interpretation, which is both time consuming and subjective. Here, we present a novel method that combines deep learning (convolutional neural networks or CNNs) and object-based image analysis (OBIA) into one workflow based on freely available Sentinel-2 optical imagery (10 m spatial resolution), Sentinel-1 interferometric coherence data, and a digital elevation model (DEM). CNNs identify recurring patterns and textures and produce a prediction raster, or heatmap where each pixel indicates the probability that it belongs to a certain class (i.e. rock glacier) or not. By using OBIA we can segment the datasets and classify objects based on their heatmap value as well as morphological and spatial characteristics. We analysed two distinct catchments, the La Laguna catchment in the Chilean semi-arid Andes and the Poiqu catchment in the central Himalaya. In total, our method mapped 108 of the 120 rock glaciers across both catchments with a mean overestimation of 28%. Individual rock glacier polygons howevercontained false positives that are texturally similar, such as debris-flows, avalanche deposits, or fluvial material causing the user's accuracy to be moderate (63.9–68.9%) even if the producer's accuracy was higher (75.0–75.4%). We repeated our method on very-high-resolution Pléiades satellite imagery and a corresponding DEM (at 2 m resolution) for a subset of the Poiqu catchment to ascertain what difference image resolution makes. We found that working at a higher spatial resolution has little influence on the producer's accuracy (an increase of 1.0%), however the rock glaciers delineated were mapped with a greater user's accuracy (increase by 9.1% to 72.0%). By running all the processing within an object-based environment it was possible to both generate the deep learning heatmap and perform post-processing through image segmentation and object reshaping. Given the difficulties in differentiating rock glaciers using image spectra, deep learning combined with OBIA offers a promising method for automating the process of mapping rock glaciers over regional scales and lead to a reduction in the workload required in creating inventories. © 2020 The Author(s)","","Andes; Chile; Himalayas; La Laguna; Catchments; Classification (of information); Convolutional neural networks; Image resolution; Image segmentation; Learning systems; Mapping; Object detection; Remote sensing; Rocks; Runoff; Satellite imagery; Surveying; Textures; Digital elevation model; High resolution remote sensing; Interferometric coherence; Object based image analysis; Object based image analysis (OBIA); Spatial characteristics; Surrounding materials; Very high resolution; cryosphere; detection method; digital elevation model; image analysis; permafrost; rock glacier; satellite imagery; Sentinel; streamflow; Deep learning","","","","","CONICYT-FONDECYT, (3180417, 4000121469/17/I-NB); CONICYT-Programa Regional, (R16A10003); Coquimbo Regional Government, (BIP 40000343); Meltzer foundation; Spatio-Temporal Dynamics of Land Surface Morphology, (FWF-P29461-N29); European Space Agency, ESA; Austrian Science Fund, FWF; Universitetet i Bergen, UiB","Funding text 1: B Robson was supported by the Meltzer foundation and a University of Bergen grant. S MacDonell was supported by CONICYT-Programa Regional ( R16A10003 ) and the Coquimbo Regional Government via FIC-R(2016) BIP 40000343 . D. Hölbling has been supported by the Austrian Science Fund through the project MORPH (Mapping, Monitoring and Modeling the Spatio-Temporal Dynamics of Land Surface Morphology; FWF-P29461-N29 ). N Schaffer was financed by CONICYT-FONDECYT ( 3180417 ) and P Rastner by the ESA Dragon 4 programme ( 4000121469/17/I-NB ). Thank you to Anna Telegina who read an early version of the manuscript. We are thankful to ESA for the provision of Sentinel data and CNES/Airbus DS for the provision of the Pléiades satellite data for a reduced price within the ISIS programme. We are grateful for the constructive comments from three anonymous reviewers. ; Funding text 2: B Robson was supported by the Meltzer foundation and a University of Bergen grant. S MacDonell was supported by CONICYT-Programa Regional (R16A10003) and the Coquimbo Regional Government via FIC-R(2016)BIP 40000343. D. H?lbling has been supported by the Austrian Science Fund through the project MORPH (Mapping, Monitoring and Modeling the Spatio-Temporal Dynamics of Land Surface Morphology; FWF-P29461-N29). N Schaffer was financed by CONICYT-FONDECYT (3180417) and P Rastner by the ESA Dragon 4 programme (4000121469/17/I-NB). Thank you to Anna Telegina who read an early version of the manuscript. We are thankful to ESA for the provision of Sentinel data and CNES/Airbus DS for the provision of the Pl?iades satellite data for a reduced price within the ISIS programme. We are grateful for the constructive comments from three anonymous reviewers.","Alba M., Barazzetti L., Scaioni M., Remondino F., Automatic registration of multiple laser scans using panoramic RGB and intensity images, Int. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., 3812, pp. 49-54, (2011); Alifu H., Tateishi R., Johnson B., A new band ratio technique for mapping debris-covered glaciers using Landsat imagery and a digital elevation model, Int. J. Remote Sens., 36, pp. 2063-2075, (2015); Azocar G.F., Brenning A., Hydrological and geomorphological significance of rock glaciers in the dry Andes, (27 degrees-33 degrees S), Permafr. Periglac. Process., 21, pp. 42-53, (2010); Barboux C., Delaloye R., Lambiel C., Inventorying slope movements in an Alpine environment using DInSAR, Earth Surf. Process. Landf., 39, pp. 2087-2099, (2014); Barcaza G., Nussbaumer S.U., Tapia G., Valdes J., Garcia J.L., Videla Y., Albornoz A., Arias V., Glacier inventory and recent glacier variations in the Andes of Chile, South America, Ann. Glaciol., 58, 75pt2, pp. 166-180, (2017); Barsch D., Rock Glaciers, (1996); Bentes C., Frost A., Velotto D., Tings B., Ship-iceberg discrimination with convolutional neural networks in high resolution SAR images, Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar, pp. 1-4, (2016); Bertone A., Zucca F., Marin C., Notarnicola C., Cuozzo G., Krainer K., Mair V., Riccardi P., Callegari M., Seppi R., An unsupervised method to detect rock glacier activity by using Sentinel-1 SAR interferometric coherence: a regional-scale study in the Eastern European Alps, Remote Sens., 11, 14, (2019); Bianchi F.M., Grahn J., Eckerstorfer M., Malnes E., Vickers H.J.A.P.A., Snow avalanche segmentation in SAR images with fully convolutional neural networks, arXiv preprint arXiv, 1910, (2019); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., Van Der Meer F., Van Der Werff H., Van Coillie F., Geographic object-based image analysis–towards a new paradigm, ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191, (2014); Bodin X., Thomas E., Liaudat D.T., Vivero S., Pitte P., Rock Glacier Activity and Distribution in the Semi-Arid Andes of Chile and Argentina Detected from dInSAR, pp. 20-24, (2016); Bolch T., Gorbunov A.P., Characteristics and origin of rock glaciers in Northern Tien Shan (Kazakhstan/Kyrgyzstan), Permafr. Periglac. Process., 25, pp. 320-332, (2014); Bolch T., Marchenko S., Significance of Glaciers, Rock Glaciers and Ice-Rich Permafrost in the Northern Tien Shan as Water Towers under Climate Change Conditions, (2009); Bolch T., Buchroithner M.F., Kunert A., Kamp U., Automated delineation of debris-covered glaciers based on ASTER data. Geoinformation in Europe, Proceedings of the 27th EARSeL Symposium, pp. 4-6, (2007); Bolch T., Rohrbach N., Kutuzov S., Robson B., Osmonov A., Occurrence, evolution and ice content of ice-debris complexes in the Ak-Shiirak, central Tien Shan revealed by geophysical and remotely-sensed investigations, Earth Surf. Process. Landf., 44, pp. 129-143, (2019); Bolch T., Rastner P., Pronk J.B., Bhattacharya A., Liu L., Hu Y., Zhang G.Q., Yao T.D., Occurrence and characteristics of Ice-Debris landforms in Poiqu Basin – Central Himalaya, EGU General Assembly 2020, (2020); Brardinoni F., Scotti R., Sailer R., Mair V., Evaluating sources of uncertainty and variability in rock glacier inventories, Earth Surf. Process. Landf., 44, 12, pp. 2450-2466, (2019); Brenning A., Benchmarking classifiers to optimally integrate terrain analysis and multispectral remote sensing in automatic rock glacier detection, Remote Sens. Environ., 113, pp. 239-247, (2009); Brenning A., Long S.L., Fieguth P., Detecting rock glacier flow structures using Gabor filters and IKONOS imagery, Remote Sens. Environ., 125, pp. 227-237, (2012); Colucci R.R., Forte E., Zebre M., Maset E., Zanettini C., Guglielmin M., Is that a relict rock glacier?, Geomorphology, 330, pp. 177-189, (2019); Cremonese E., Gruber S., Phillips M., Pogliotti P., Bockli L., Noetzli J., Suter C., Bodin X., Crepaz A., Kellerer-Pirklbauer A., An inventory of permafrost evidence for the European Alps, Cryosphere, 5, pp. 651-657, (2011); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks, Drones, 2, (2018); Dao P., Liou Y.-A., Object-based flood mapping and affected rice field estimation with Landsat 8 OLI and MODIS data, Remote Sens., 7, pp. 5077-5097, (2015); Delaloye R., Barboux C., Echelard T., Bodin X., Brardinoni F., Lambiel C., Wee J., Towards standard guidelines for inventorying rock glaciers (version 2.0), IPA Action Group Rock Glacier Inventories and Kinematics (2018–2020), (2019); Ding A., Zhang Q., Zhou X., Dai B., Automatic recognition of landslide based on CNN and texture change detection, 2016 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC), 2016, pp. 444-448, (2016); Chilean National Glacier Inventory, (2012); Dragut L., Csillik O., Eisank C., Tiede D., Automated parameterisation for multi-scale image segmentation on multiple layers, ISPRS J. Photogramm. Remote Sens., 88, pp. 119-127, (2014); Esper Angillieri M.Y., A preliminary inventory of rock glaciers at 30°S latitude, cordillera frontal of San Juan, Argentina, Quat. Int., 195, pp. 151-157, (2009); Falaschi D., Castro M., Masiokas M., Tadono T., Ahumada A.L., Rock glacier inventory of the Valles Calchaquíes region (~ 25 S), Salta, Argentina, derived from ALOS data, Permafr. Periglac. Process., 25, pp. 69-75, (2014); Favier V., Falvey M., Rabatel A., Praderio E., Lopez D., Interpreting discrepancies between discharge and precipitation in high-altitude area of Chile's Norte Chico region (26–32°S), Water Resour. Res., 45, (2009); Fu Y., Liu K., Shen Z., Deng J., Gan M., Liu X., Lu D., Wang K., Mapping impervious surfaces in town–rural transition belts using China's GF-2 imagery and object-based deep CNNs, Remote Sens., 11, (2019); Gallego A.J., Pertusa A., Gil P., Automatic ship classification from optical aerial images with convolutional neural networks, Remote Sens., 10, (2018); Geiger S.T., Daniels J.M., Miller S.N., Nicholas J.W., Influence of rock glaciers on stream hydrology in the La Sal Mountains, Utah, Arct. Antarct. Alp. Res., 46, pp. 645-658, (2014); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of different machine learning methods and deep-learning convolutional neural networks for landslide detection, Remote Sens., 11, (2019); Gonzalez-Audicana M., Otazu X., Fors O., Seco A., Comparison between Mallat's and the ‘à trous’ discrete wavelet transform based algorithms for the fusion of multispectral and panchromatic images, Int. J. Remote Sens., 26, pp. 595-614, (2005); Gorbunov A., Titkov S., Kamennye Gletchery Gor Srednej Azii (Rock Glaciers of the Central Asian Mountains), (1989); Guirado E., Tabik S., Alcaraz-Segura D., Cabello J., Herrera F., Deep-learning versus OBIA for scattered shrub detection with Google earth imagery: Ziziphus lotus as case study, Remote Sens., 9, (2017); Haeberli W., Hallet B., Arenson L., Elconin R., Humlum O., Kaab A., Kaufmann V., Ladanyi B., Matsuoka N., Springman S., Permafrost creep and rock glacier dynamics, Permafr. Periglac. Process., 17, pp. 189-214, (2006); Hay G.J., Castilla G., Geographic Object-Based Image Analysis (GEOBIA): a new name for a new discipline, Object-Based Image Analysis: Spatial Concepts for Knowledge-Driven Remote Sensing Applications, (2008); Hirschmuller H., Stereo processing by semiglobal matching and mutual information, IEEE Trans. Pattern Anal. Mach. Intell., 30, pp. 328-341, (2007); Holbling D., Fureder P., Antolini F., Cigna F., Casagli N., Lang S., A semi-automated object-based approach for landslidedetection validated by persistent scatterer interferometry measures and landslide inventories, Remote Sens., 4, pp. 1310-1336, (2012); Holbling D., Betts H., Spiekermann R., Phillips C., Identifying spatio-temporal landslide hotspots on North Island, New Zealand, by analyzing historical and recent aerial photography, Geosciences, 6, (2016); Huang L., Luo J., Lin Z., Niu F., Liu L., Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan plateau) from CubeSat images, Remote Sens. Environ., 237, (2020); Huggel C., Allen S., Wymann Von Dach S., Dimri A.P., Mal S., Linbauer A., Salzmann N., Bolch T., An integrative and joint approach to climate impacts, hydrological risks and adaptation in the Indian Himalayan region, Himalayan Weather and Climate and their Impact on the Environment, pp. 553-573, (2020); Huss M., Hock R., Global-scale hydrological response to future glacier mass loss, Nat. Clim. Chang., 8, 2, pp. 135-140, (2018); Immerzeel W.W., Van Beek L.P.H., Bierkens M.F.P., Climate change will affect the Asian water towers, Science, 328, pp. 1382-1385, (2010); Janke J.R., Rock glacier mapping: a method utilizing enhanced TM data and GIS modeling techniques, Geocarto Int., 16, pp. 5-15, (2001); Jones D., Harrison S., Anderson K., Betts R., Mountain rock glaciers contain globally significant water stores, Sci. Rep., 8, (2018); Jones D., Harrison S., Anderson K., Selley H., Wood J., Betts R., The distribution and hydrological significance of rock glaciers in the Nepalese Himalaya, Glob. Planet. Chang., 160, pp. 123-142, (2018); Jozdani S., Chen D., On the versatility of popular and recently proposed supervised evaluation metrics for segmentation quality of remotely sensed images: an experimental case study of building extraction, ISPRS J. Photogramm. Remote Sens., 160, pp. 275-290, (2020); Kaab A., Vollmer M., Surface geometry, thickness changes and flow fields on creeping mountain permafrost: automatic extraction by digital image analysis, Permafr. Periglac. Process., 11, 4, pp. 315-326, (2000); Kehrwald N.M., Thompson L.G., Yao T.D., Mosley-Thompson E., Schotterer U., Alfimov V., Beer J., Eikenberg J., Davis M.E., Mass loss on Himalayan glacier endangers water resources, Geophys. Res. Lett., 35, (2008); Knevels R., Petschko H., Leopold P., Brenning A., Geographic object-based image analysis for automated landslide detection using open source GIS software, ISPRS Int. J. Geo Inf., 8, 12, (2019); Kofler C., Steger S., Mair V., Zebisch M., Comiti F., Schneiderbauer S., An inventory-driven rock glacier status model (intact vs. relict) for South Tyrol, Eastern Italian Alps, Geomorphology, 106887, (2019); Lang S., Object-based image analysis for remote sensing applications: modeling reality–dealing with complexity, Object-Based Image Analysis, pp. 3-27, (2008); Langkvist M., Kiselev A., Alirezaie M., Loutfi A., Classification and segmentation of satellite orthoimagery using convolutional neural networks, Remote Sens., 8, (2016); Li Y., Zhang H., Xue X., Jiang Y., Shen Q., Deep learning for remote sensing image classification: a survey, Wiley Interdisc. Rev. Data Min. Knowl. Discov., 8, (2018); Liu L., Millar C.I., Westfall R.D., Zebker H.A., Surface motion of active rock glaciers in the Sierra Nevada, California, USA: inventory and a case study using InSAR, Cryosphere, 7, pp. 1109-1119, (2013); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: a meta-analysis and review, ISPRS J. Photogramm. Remote Sens., 152, pp. 166-177, (2019); Mahdianpari M., Salehi B., Rezaee M., Mohammadimanesh F., Zhang Y., Very deep convolutional neural networks for complex land cover mapping using multispectral remote sensing imagery, Remote Sens., 10, (2018); Mallinis G., Gitas I.Z., Giannakopoulos V., Maris F., Tsakiri-Strati M., An object-based approach for flood area delineation in a transboundary area using ENVISAT ASAR and LANDSAT TM data, Int. J. Digit. Earth, 6, pp. 124-136, (2013); Mithan H.T., Hales T.C., Cleall P.J., Supervised classification of landforms in Arctic mountains, Permafr. Periglac. Process., 30, 3, pp. 131-145, (2019); Monnier S., Kinnard C., Surazakov A., Bossy W., Geomorphology, internal structure, and successive development of a glacier foreland in the semiarid Chilean Andes (Cerro Tapado, upper Elqui Valley, 30°08′ S., 69°55′ W.), Geomorphology, 207, pp. 126-140, (2014); Necsoiu M., Onaca A., Wigginton S., Urdea P., Rock glacier dynamics in Southern Carpathian Mountains from high-resolution optical and multi-temporal SAR satellite imagery, Remote Sens. Environ., 177, pp. 21-36, (2016); Nicholson L., Marin J., Lopez D., Rabatel A., Bown F., Rivera A., Glacier inventory of the upper Huasco valley, Norte Chico, Chile: glacier characteristics, glacier change and comparison with Central Chile, Ann. Glaciol., 50, 53, pp. 111-118, (2009); Onaca A., Ardelean F., Urdea P., Magori B., Southern Carpathian rock glaciers: inventory, distribution and environmental controlling factors, Geomorphology, 293, pp. 391-404, (2017); Outcalt S.I., Benedict J.B.J., G J.O., Photo-interpretation of two types of rock glacier in the Colorado front range, USA, J. Glaciol., 5, 42, pp. 849-856, (1965); Pandey P., Inventory of rock glaciers in Himachal Himalaya, India using high-resolution google earth imagery, Geomorphology, 340, pp. 103-115, (2019); Paul F., Huggel C., Kaab A., Combining satellite multispectral image data and a digital elevation model for mapping debris-covered glaciers, Remote Sens. Environ., 89, pp. 510-518, (2004); Paul F., Barrand N.E., Baumann S., Berthier E., Bolch T., Casey K., Frey H., Joshi S., Konovalov V., Le Bris R., On the accuracy of glacier outlines derived from remote-sensing data, Ann. Glaciol., 54, pp. 171-182, (2013); Paul F., Bolch T., Kaab A., Nagler T., Nuth C., Scharrer K., Shepherd A., Strozzi T., Ticconi F., Bhambri R., The glaciers climate change initiative: methods for creating glacier area, elevation change and velocity products, Remote Sens. Environ., 162, pp. 408-426, (2015); Pfeffer W.T., Arendt A.A., Bliss A., Bolch T., Cogley J.G., Gardner A.S., Hagen J.-O., Hock R., Kaser G., Kienholz C., The Randolph glacier inventory: a globally complete inventory of glaciers, J. Glaciol., 60, pp. 537-552, (2014); Piao S., Ciais P., Huang Y., Shen Z., Peng S., Li J., Zhou L., Liu H., Ma Y., Ding Y., The impacts of climate change on water resources and agriculture in China, Nature, 467, 7311, pp. 43-51, (2010); Pope A., Rees W.G., Impact of spatial, spectral, and radiometric properties of multispectral imagers on glacier surface classification, Remote Sens. Environ., 141, pp. 1-13, (2014); Pourrier J., Jourde H., Kinnard C., Gascoin S., Monnier S., Glacier meltwater flow paths and storage in a geomorphologically complex glacial foreland: the case of the Tapado glacier, dry Andes of Chile (30°S), J. Hydrol., 519, pp. 1068-1083, (2014); Pritchard H.D., Asia's shrinking glaciers protect large populations from drought stress, Nature, 569, pp. 649-654, (2019); Racoviteanu A., Williams M.W., Decision tree and texture analysis for mapping debris-covered glaciers in the Kangchenjunga area, Eastern Himalaya, Remote Sens., 4, pp. 3078-3109, (2012); Rangecroft S., Harrison S., Anderson K., Magrath J., Castel A.P., Pacheco P., A first rock glacier inventory for the Bolivian Andes, Permafr. Periglac. Process., 25, pp. 333-343, (2014); Rangecroft S., Harrison S., Anderson K., Rock glaciers as water stores in the Bolivian Andes: an assessment of their hydrological importance, Arct. Antarct. Alp. Res., 47, pp. 89-98, (2015); Rastner P., Bolch T., Notarnicola C., Paul F., A comparison of pixel-and object-based glacier classification with optical satellite images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7, pp. 853-862, (2013); Robson B.A., Nuth C., Dahl S.O., Holbling D., Strozzi T., Nielsen P.R., Automated classification of debris-covered glaciers combining optical, SAR and topographic data in an object-based environment, Remote Sens. Environ., 170, pp. 372-387, (2015); Schaffer N., Macdonell S., Rock Glacier Inventory for the La Laguna Catchment (Unpublished Dataset), (2020); Schaffer N., Macdonell S., Reveillet M., Yanez E., Valois R., Rock glaciers as a water resource in a changing climate in the semiarid Chilean Andes, Reg. Environ. Chang., 19, pp. 1263-1279, (2019); Schratz P., Muenchow J., Iturritxa E., Richter J., Brenning A., Hyperparameter tuning and performance assessment of statistical and machine-learning algorithms using spatial data, Ecol. Model., 406, pp. 109-120, (2019); Scotti R., Brardinoni F., Alberti S., Frattini P., Crosta G.B., A regional inventory of rock glaciers and protalus ramparts in the central Italian Alps, Geomorphology, 186, pp. 136-149, (2013); Shean D., High Mountain Asia 8-Meter DEM Mosaics Derived from Optical Imagery, Version 1, (2017); Villarroel C.D., Tamburini Beliveau G., Forte A.P., Monserrat O., Morvillo M., DInSAR for a regional inventory of active rock glaciers in the dry Andes mountains of Argentina and Chile with sentinel-1 data, Remote Sens., 10, (2018); Wahrhaftig C., Cox A., Rock glaciers in the Alaska range, GSA Bull., 70, 4, pp. 383-436, (1959); Wang X.W., Liu L., Zhao L., Wu, T. H., Li, Z. Q. & Liu, G. X., Mapping and inventorying active rock glaciers in the Northern Tien Shan of China using satellite SAR interferometry, Cryosphere, 11, pp. 997-1014, (2017); Xiang Y., Gao Y., Yao T., Glacier change in the Poiqu River basin inferred from Landsat data from 1975 to 2010, Quat. Int., 349, pp. 392-401, (2014); Xu H., Modification of normalised difference water index (NDWI) to enhance open water features in remotely sensed imagery, Int. J. Remote Sens., 27, 14, pp. 3025-3033, (2006); Yu H., Ma Y., Wang L., Zhai Y., Wang X., A landslide intelligent detection method based on CNN and RSG_R, 2017 IEEE International Conference on Mechatronics and Automation (ICMA), 2017, pp. 40-44, (2017); Zemp M., Huss M., Thibert E., Eckert N., Mcnabb R., Huber J., Barandun M., Machguth H., Nussbaumer S.U., Gartner-Roer I., Global glacier mass changes and their contributions to sea-level rise from 1961 to 2016, Nature, 568, 7752, pp. 382-386, (2019); Zhang C., Sargent I., Pan X., Li H.P., Gardiner A., Hare J., Atitinson P.M., An object-based convolutional neural network (OCNN) for urban land use classification, Remote Sens. Environ., 216, pp. 57-70, (2018); Zhang C., Yue P., Tapete D., Shangguan B., Wang M., Wu Z., A multi-level context-guided classification method with object-based convolutional neural network for land cover classification using very high resolution remote sensing images, Int. J. Appl. Earth Obs. Geoinf., 88, (2020); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: a technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40, (2016)","B.A. Robson; Department of Geography, University of Bergen, Norway; email: benjamin.robson@uib.no","","Elsevier Inc.","","","","","","00344257","","RSEEA","","English","Remote Sens. Environ.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85089440785"
"Chang Z.; Yu H.; Zhang Y.; Wang K.","Chang, Zhanyuan (57211800405); Yu, Huiling (23013564000); Zhang, Yizhuo (57205021661); Wang, Keqi (22735182900)","57211800405; 23013564000; 57205021661; 22735182900","Fusion of hyperspectral casi and airborne lidar data for ground object classification through residual network","2020","Sensors (Switzerland)","20","14","3961","1","16","15","5","10.3390/s20143961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088231804&doi=10.3390%2fs20143961&partnerID=40&md5=a00cfbb735e98afb91e8eaac7ca3502f","College of Mechanical and Electrical Engineering, Northeast Forestry University, Harbin, 150040, China; College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, 200234, China; College of Information and Computer Engineering, Northeast Forestry University, Harbin, 150040, China","Chang Z., College of Mechanical and Electrical Engineering, Northeast Forestry University, Harbin, 150040, China, College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, Shanghai, 200234, China; Yu H., College of Information and Computer Engineering, Northeast Forestry University, Harbin, 150040, China; Zhang Y., College of Mechanical and Electrical Engineering, Northeast Forestry University, Harbin, 150040, China; Wang K., College of Mechanical and Electrical Engineering, Northeast Forestry University, Harbin, 150040, China","Modern satellite and aerial imagery outcomes exhibit increasingly complex types of ground objects with continuous developments and changes in land resources. Single remote-sensing modality is not sufficient for the accurate and satisfactory extraction and classification of ground objects. Hyperspectral imaging has been widely used in the classification of ground objects because of its high resolution, multiple bands, and abundant spatial and spectral information. Moreover, the airborne light detection and ranging (LiDAR) point-cloud data contains unique high-precision three-dimensional (3D) spatial information, which can enrich ground object classifiers with height features that hyperspectral images do not have. Therefore, the fusion of hyperspectral image data with airborne LiDAR point-cloud data is an effective approach for ground object classification. In this paper, the effectiveness of such a fusion scheme is investigated and confirmed on an observation area in the middle parts of the Heihe River in China. By combining the characteristics of hyperspectral compact airborne spectrographic imager (CASI) data and airborne LiDAR data, we extracted a variety of features for data fusion and ground object classification. Firstly, we used the minimum noise fraction transform to reduce the dimensionality of hyperspectral CASI images. Then, spatio-spectral and textural features of these images were extracted based on the normalized vegetation index and the gray-level co-occurrence matrices. Further, canopy height features were extracted from airborne LiDAR data. Finally, a hierarchical fusion scheme was applied to the hyperspectral CASI and airborne LiDAR features, and the fused features were used to train a residual network for high-accuracy ground object classification. The experimental results showed that the overall classification accuracy was based on the proposed hierarchical-fusion multiscale dilated residual network (M-DRN), which reached an accuracy of 97.89%. This result was found to be 10.13% and 5.68% higher than those of the convolutional neural network (CNN) and the dilated residual network (DRN), respectively. Spatio-spectral and textural features of hyperspectral CASI images can complement the canopy height features of airborne LiDAR data. These complementary features can provide richer and more accurate information than individual features for ground object classification and can thus outperform features based on a single remote-sensing modality. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Airborne LiDAR; CASI hyperspectral imagery; Data fusion; Deep learning; Residual network","Aerial photography; Antennas; Convolutional neural networks; Data fusion; Data mining; Hyperspectral imaging; Optical radar; Remote sensing; Satellite imagery; Spectroscopy; Classification accuracy; Compact airborne spectrographic imager; Complementary features; Gray-level co-occurrence matrix; Hyperspectral image datas; Light detection and ranging; Minimum noise fraction; Threedimensional (3-d); article; canopy; China; classifier; convolutional neural network; deep learning; human; human experiment; imagery; noise; remote sensing; river; vegetation; Classification (of information)","","","","","Shanghai Sailing Program, (18YF1418600, 19YF1437200); Science and Technology Commission of Shanghai Municipality, STCSM; Fundamental Research Funds for the Central Universities, (2572017CB34)","Funding text 1: Funding: This research was supported by the Fundamental Research Funds for the Central Universities (2572017CB34) and the Shanghai Sailing Program (19YF1437200, 18YF1418600).; Funding text 2: Acknowledgments: The authors sincerely thank the Shanghai Science and Technology Commission for their funding support.","Bhardwaj A., Sam L., Bhardwaj A., Martin-Torres F., LiDAR remote sensing of the cryosphere: Present applications and future prospects, Remote Sens. Environ, 1771, pp. 25-143, (2016); Kolzenburg S., Favalli M., Fornaciai A., Isola I., Harris A.J.L., Nannipieri L., Giordano D., Rapid Updating and Improvement of Airborne LIDAR DEMs Through Ground-Based SfM 3-D Modeling of Volcanic Features, IEEE Trans. Geosci. Remote Sens, 54, pp. 6687-6699, (2016); Sothe C., Dalponte M., Almeida C.M.D., Schimalski M.B., Lima C.L., Liesenberg V., Tommaselli A.M.G., Tree Species Classification in a Highly Diverse Subtropical Forest Integrating UAV-Based Photogrammetric Point Cloud and Hyperspectral Data, Remote Sens, 11, (2019); Abeysinghe T., Simic Milas A., Arend K., Hohman B., Reil P., Gregory A., Vazquez-Ortega A., Mapping Invasive Phragmites Australis in the Old Woman Creek Estuary Using UAV Remote Sensing and Machine Learning Classifiers, Remote Sens, 11, (2019); Cao J., Leng W., Liu K., Liu L., He Z., Zhu Y., Object-Based Mangrove Species Classification Using Unmanned Aerial Vehicle Hyperspectral Images and Digital Surface Models, Remote Sens, 10, (2018); Banerjee B.P., Raval S., Cullen P., UAV-hyperspectral imaging of spectrally complex environments, Int. Remote Sens, 41, pp. 4136-4159, (2020); Tong X., Li X., Xu X., Xie H., Feng T., Sun T., Jin Y., Liu X., A Two-Phase Classification of Urban Vegetation Using Airborne LiDAR Data and Aerial Photography, IEEE Sel. Top. Appl. Earth Obs. Remote Sens, 7, pp. 4153-4166, (2014); Matikainen L., Karila K., Litkey P., Ahokas E., Hyyppa J., Combining single photon and multispectral airborne laser scanning for land cover classification, ISPRS Photogramm. Remote Sens, 1642, pp. 00-216, (2020); Liu X., Bo Y., Object-Based Crop Species Classification Based on the Combination of Airborne Hyperspectral Images and LiDAR Data, Remote Sens, 7, pp. 922-950, (2015); Chu H.J., Wang C.K., Kong S.J., Chen K.C., Integration of full-waveform Li DAR and hyperspectral data to enhance tea and areca classification, GISci. Remote Sens, 53, pp. 542-559, (2016); Dalponte M., Frizzera L., Orka H.O., Gobakken T., Naesset E., Gianelle D., Predicting stem diameters and aboveground biomass of individual trees using remote sensing data, Ecol. Indic, 85, pp. 367-376, (2018); Jahan F., Zhou J., Awrangjeb M., Gao Y., Inverse Coefficient of Variation Feature and Multilevel Fusion Technique for Hyperspectral and LiDAR Data Classification, IEEE Sel. Top. Appl. Earth Obs. Remote Sens, 133, pp. 67-381, (2020); Hardy A.J., Barr S.L., Mills J.P., Miller P.E., Characterising soil moisture in transport corridor environments using airborne LIDAR and CASI data, Hydrol. Process, 26, pp. 1925-1936, (2012); Zhang L., Liu Z., Ren T., Liu D., Ma Z., Tong L., Zhang C., Zhou T., Zhang X., Li S., Identification of Seed Maize Fields with High Spatial Resolution and Multiple Spectral Remote Sensing Using Random Forest Classifier, Remote Sens, 12, (2020); Akar O., Gungor O., Integrating multiple texture methods and NDVI to the Random Forest classification algorithm to detect tea and hazelnut plantation areas in northeast Turkey, Int. Remote Sens, 36, pp. 442-464, (2015); Kang X., Li S., Benediktsson A., Feature Extraction of Hyperspectral Images with Image Fusion and Recursive Filtering, IEEE Trans. Geosci. Remote Sens, 52, pp. 3742-3752, (2014); Onojeghuo A.O., Blackburn G.A., Optimising the use of hyperspectral and LiDAR data for mapping reedbed habitats, Remote Sens. Environ, 115, pp. 2025-2034, (2011); Deng W., Liu H., Xu J., Zhao H., Song Y., An Improved Quantum-Inspired Differential Evolution Algorithm for Deep Belief Network, IEEE Trans. Instrum. Meas, (2020); Mou L., Ghamisi P., Zhu X.X., Unsupervised Spectral-Spatial Feature Learning via Deep Residual Conv-Deconv Network for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens, 56, pp. 391-406, (2018); Aptoula E., Ozdemir M.C., Yanikoglu B., Deep Learning With Attribute Profiles for Hyperspectral Image Classification, IEEE Geosci. Remote Sens. Lett, 13, pp. 1970-1974, (2016); Govender M., Chetty K., Naiken V., Bulcock H., A Comparison of Satellite Hyperspectral and Multispectral Remote Sensing Imagery for Improved Classification and Mapping of Vegetation, Water SA, 34, pp. 147-154, (2008); Hu W., Huang Y., Wei L., Zhang F., Li H., Deep Convolutional Neural Networks for Hyperspectral Image Classification, J. Sensors, 2015, (2015); Zhao W., Du S., Spectral–Spatial Feature Extraction for Hyperspectral Image Classification: A Dimension Reduction and Deep Learning Approach, IEEE Trans. Geosci. Remote Sens, 54, pp. 4544-4554, (2016); Zhong Z., Li J., Luo Z., Chapman M., Spectral-Spatial Residual Network for Hyperspectral Image Classification: A 3-D Deep Learning Framework, IEEE Trans. Geosci. Remote Sens, 56, pp. 847-858, (2018); Garcia-Salgado B.P., Ponomaryov P., Feature extraction scheme for a textural hyperspectral image classification using gray-scaled HSV and NDVI image features vectors fusion, Int. Conf. Electron. Commun. Comput, 1, pp. 86-191, (2016); Vaddi R., Manoharan P., Hyperspectral Image Classification Using CNN with Spectral and Spatial Features Integration, Infrared Phys. Technol, 1071, (2020); Ia X., Kuo B.C., Crawford M.M., Feature Mining for Hyperspectral Image Classification, Proc. IEEE, 101, pp. 676-697, (2013); Nielsen A.A., Kernel maximum autocorrelation factor and minimum noise fraction transformations, IEEE Trans. Image Process, 20, pp. 612-624, (2010); Guan L.X., Xie W.X., Pei H., Segmented minimum noise fraction transformation for efficient feature extraction of hyperspectral images, Pattern Recognit, 48, pp. 3216-3226, (2015); Yi-zhuo Z., Miao-miao X., Xiao-hu W., Ke-qi W., Hyperspectral image classification based on hierarchical fusion of residual networks, Spectrosc. Spectr. Anal, 39, pp. 3501-3507, (2019); Hsiao T.Y., Chang Y.C., Chou H.H., Chiu C.T., Filter-based deep-compression with global average pooling for convolutional networks, Syst. Archit, 95, pp. 9-18, (2019)","Y. Zhang; College of Mechanical and Electrical Engineering, Northeast Forestry University, Harbin, 150040, China; email: nefuzyz@163.com","","MDPI AG","","","","","","14248220","","","32708693","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088231804"
"Scott G.J.; Alex Hurt J.; Yang A.; Islam M.A.; Anderson D.T.; Davis C.H.","Scott, Grant J. (7402930105); Alex Hurt, J. (57219549048); Yang, Alex (57211403694); Islam, Muhammad Aminul (57217039894); Anderson, Derek T. (55483345800); Davis, Curt H. (7404360319)","7402930105; 57219549048; 57211403694; 57217039894; 55483345800; 7404360319","Differential Morphological Profile Neural Network for Object Detection in Overhead Imagery","2020","Proceedings of the International Joint Conference on Neural Networks","","","9207387","","","","3","10.1109/IJCNN48605.2020.9207387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093864274&doi=10.1109%2fIJCNN48605.2020.9207387&partnerID=40&md5=d12f0297422d4c3e7be87382103fc118","University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States","Scott G.J., University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States; Alex Hurt J., University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States; Yang A., University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States; Islam M.A., University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States; Anderson D.T., University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States; Davis C.H., University of Missouri, Department of Electrical Engineering Computer Science, Columbia, MO, United States","Deep convolutional neural networks (DCNN) have been the dominant methodology in the field of computer vision over the last decade, using various architectural organizations of successive convolutional layers to extract and assemble low level image features into visual component detectors. One of the tradeoffs that have been made as the community has migrated to deep neural models is the loss of explainability and understanding of which salient visual components are being recognized by a model for a particular task. However, there exists a significant heritage in the remote sensing community that has developed advanced algorithms to analyze the signal and structural characteristics of anthropogenic features. One such approach is the use of morphological image processing techniques to extract objects from imagery and aid in the structural analysis of shapes. In particular, the differential morphological profile (DMP) has had great success extracting object shapes, while naturally grouping the extracted shapes into scale ranges. In this research, we present a novel architecture that integrates an explicit (definable and explainable) scaled object extraction into the network architecture, allowing shallower convolutional layers and lower complexity neural models. The architecture is evaluated on a challenging remote sensing dataset of object classes, providing insights to this approach and illuminating future directions of integrating morphology into neural architectures for enhanced explainability. © 2020 IEEE.","Convolutional neural network; differential morphological profile; object detection; overhead imagery","Convolution; Convolutional neural networks; Deep neural networks; Network architecture; Object detection; Object recognition; Remote sensing; Architectural organization; Differential morphological profile; Low-level image features; Morphological image processing; Neural architectures; Novel architecture; Structural characteristics; Visual components; Multilayer neural networks","","","","","","","Scott G.J., England M.R., Starms W.A., Marcum R.A., Davis C.H., Training deep convolutional neural networks for land-cover classification of high-resolution imagery, IEEE Geoscience and Remote Sensing Letters, 14, 4, pp. 549-553, (2017); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the Inception Architecture for Computer Vision, (2015); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, (2015); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., Inception-v4, inception-resnet and the impact of residual connections on learning, Thirty-First AAAI Conference on Artificial Intelligence, (2017); Huang G., Liu Z., Weinberger K.Q., Van Der Maaten L., Densely connected convolutional networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1, (2017); Chollet F., Xception: Deep learning with depthwise separable convolutions, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1800-1807, (2017); Sabour S., Frosst N., Hinton G.E., Dynamic routing between capsules, Advances in Neural Information Processing Systems, pp. 3856-3866, (2017); Yang A., Alex Hurt J., Veal C.T., Scott G.J., Remote sensing object localization with deep heterogeneous superpixel features, 2019 IEEE International Conference on Big Data (BIGDATA), (2019); Scott G.J., Marcum R.A., Davis C.H., Nivin T.W., Fusion of deep convolutional neural networks for land cover classification of highresolution imagery, IEEE Geoscience and Remote Sensing Letters, 14, 9, pp. 1638-1642, (2017); Anderson D.T., Scott G.J., Islam M., Murray B., Marcum R., Fuzzy choquet integration of deep convolutional neural networks for remote sensing, Computational Intelligence for Pattern Recognition, (2018); Scott G.J., Hagan K.C., Marcum R.A., Hurt J.A., Anderson D.T., Davis C.H., Enhanced fusion of deep neural networks for classification of benchmark high-resolution image data sets, IEEE Geoscience and Remote Sensing Letters, pp. 1-5, (2018); Hurt J.A., Scott G.J., Davis C.H., Comparison of deep learning model performance between meta-dataset training versus deep neural ensembles, IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1326-1329, (2019); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C., Berg A.C., Ssd: Single shot multibox detector, European Conference on Computer Vision, pp. 21-37, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only Look Once: Unified, Real-time Object Detection 2015, (2015); Redmon J., Farhadi A., Yolov3: An Incremental Improvement, (2018); Van Etten A., You only Look Twice: Rapid Multi-scale Object Detection in Satellite Imagery, (2018); Van Etten A., Satellite imagery multiscale rapid detection with windowed networks, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, pp. 735-743, (2019); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-assisted Intervention. Springer, pp. 234-241, (2015); He K., Gkioxari G., Dollar P., Girshick R.B., Mask R-cnn. Corr abs/1703. 06870 2017, (2017); Pesaresi M., Benediktsson J.A., A new approach for the morphological segmentation of high-resolution satellite imagery, IEEE Transactions on Geoscience and Remote Sensing, 39, 2, pp. 309-320, (2001); Klaric M.N., Scott G.J., Shyu C., Multi-index multi-object content-based retrieval, IEEE Transactions on Geoscience and Remote Sensing, 50, 10, pp. 4036-4049, (2012); Scott G.J., Anderson D.T., Importance-weighted multi-scale texture and shape descriptor for object recognition in satellite imagery, 2012 IEEE International Geoscience and Remote Sensing Symposium, pp. 79-82, (2012); Price S.R., Anderson D.T., England M.R., Scott G.J., Soft segmentation weighted ieco descriptors for object recognition in satellite imagery, 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 4939-4942, (2015); Shackelford A.K., Davis C.H., A combined fuzzy pixelbased and object-based approach for classification of high-resolution multispectral data over urban areas, IEEE Transactions on GeoScience and Remote Sensing, 41, 10, pp. 2354-2363, (2003); Scott G.J., Klaric M.N., Davis C.H., Shyu C., Entropy-balanced bitmap tree for shape-based object retrieval from large-scale satellite imagery databases, IEEE Transactions on Geoscience and Remote Sensing, 49, 5, pp. 1603-1616, (2011); Scott G.J., Anderson D.T., Fusion of differential morphological profiles for multi-scale weighted feature pyramid generation in remotely sensed imagery, 2011 IEEE Applied Imagery Pattern Recognition Workshop (AIPR), pp. 1-8, (2011); Hurt J.A., Scott G.J., Anderson D.T., Davis C.H., Benchmark meta-dataset of high-resolution remote sensing imagery for training robust deep learning models in machine-assisted visual analytics, 2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR), pp. 1-9, (2018)","","","Institute of Electrical and Electronics Engineers Inc.","IEEE Computational Intelligence Society (CIS)","2020 International Joint Conference on Neural Networks, IJCNN 2020","19 July 2020 through 24 July 2020","Virtual, Glasgow","163566","","978-172816926-2","85OFA","","English","Proc Int Jt Conf Neural Networks","Conference paper","Final","","Scopus","2-s2.0-85093864274"
"Biffi L.J.; Mitishita E.; Liesenberg V.; Dos Santos A.A.; Gonçalves D.N.; Estrabis N.V.; Silva J.A.; Osco L.P.; Ramos A.P.M.; Centeno J.A.S.; Schimalski M.B.; Rufato L.; Neto S.L.R.; Junior J.M.; Gonçalves W.N.","Biffi, Leonardo Josoé (55942501700); Mitishita, Edson (12751886500); Liesenberg, Veraldo (15848875300); Dos Santos, Anderson Aparecido (57209615996); Gonçalves, Diogo Nunes (56797665900); Estrabis, Nayara Vasconcelos (57212376607); Silva, Jonathan de Andrade (23396702400); Osco, Lucas Prado (57196329154); Ramos, Ana Paula Marques (56198690100); Centeno, Jorge Antonio Silva (12752460700); Schimalski, Marcos Benedito (36505022100); Rufato, Leo (34573697800); Neto, Sílvio Luís Rafaeli (57194017468); Junior, José Marcato (55640064500); Gonçalves, Wesley Nunes (23396539500)","55942501700; 12751886500; 15848875300; 57209615996; 56797665900; 57212376607; 23396702400; 57196329154; 56198690100; 12752460700; 36505022100; 34573697800; 57194017468; 55640064500; 23396539500","Article atss deep learning-based approach to detect apple fruits","2021","Remote Sensing","13","1","54","1","23","22","30","10.3390/rs13010054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098759179&doi=10.3390%2frs13010054&partnerID=40&md5=f1b5fcf3d122a0872448bf68d1d4a5bb","Department of Environmental and Sanitation Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572—Limoeiro, Pres, Prudente, 19067-175, SP, Brazil; Department of Agronomy, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil","Biffi L.J., Department of Environmental and Sanitation Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil, Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Mitishita E., Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Liesenberg V., Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Dos Santos A.A., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Gonçalves D.N., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Estrabis N.V., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Silva J.A., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Osco L.P., Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572—Limoeiro, Pres, Prudente, 19067-175, SP, Brazil; Ramos A.P.M., Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572—Limoeiro, Pres, Prudente, 19067-175, SP, Brazil; Centeno J.A.S., Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Schimalski M.B., Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Rufato L., Department of Agronomy, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Neto S.L.R., Department of Environmental and Sanitation Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Junior J.M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Gonçalves W.N., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil, Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil","In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. Specifically, in fruit detection problems, several recent works were developed using Deep Learning (DL) methods applied in images acquired in different acquisition levels. However, the increasing use of anti-hail plastic net cover in commercial orchards highlights the importance of terrestrial remote sensing systems. Apples are one of the most highly-challenging fruits to be detected in images, mainly because of the target occlusion problem occurrence. Additionally, the introduction of high-density apple tree orchards makes the identification of single fruits a real challenge. To support farmers to detect apple fruits efficiently, this paper presents an approach based on the Adaptive Training Sample Selection (ATSS) deep learning method applied to close-range and low-cost terrestrial RGB images. The correct identification supports apple production forecasting and gives local producers a better idea of forthcoming management practices. The main advantage of the ATSS method is that only the center point of the objects is labeled, which is much more practicable and realistic than boundingbox annotations in heavily dense fruit orchards. Additionally, we evaluated other object detection methods such as RetinaNet, Libra Regions with Convolutional Neural Network (R-CNN), Cascade RCNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), and High-Resolution Network (HRNet). The study area is a highly-dense apple orchard consisting of Fuji Suprema apple fruits (Malus domestica Borkh) located in a smallholder farm in the state of Santa Catarina (southern Brazil). A total of 398 terrestrial images were taken nearly perpendicularly in front of the trees by a professional camera, assuring both a good vertical coverage of the apple trees in terms of heights and overlapping between picture frames. After, the high-resolution RGB images were divided into several patches for helping the detection of small and/or occluded apples. A total of 3119, 840, and 2010 patches were used for training, validation, and testing, respectively. Moreover, the proposed method’s generalization capability was assessed by applying simulated image corruptions to the test set images with different severity levels, including noise, blurs, weather, and digital processing. Experiments were also conducted by varying the bounding box size (80, 100, 120, 140, 160, and 180 pixels) in the image original for the proposed approach. Our results showed that the ATSS-based method slightly outperformed all other deep learning methods, between 2.4% and 0.3%. Also, we verified that the best result was obtained with a bounding box size of 160 × 160 pixels. The proposed method was robust regarding most of the corruption, except for snow, frost, and fog weather conditions. Finally, a benchmark of the reported dataset is also generated and publicly available. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Object detection; Precision agriculture","Agricultural robots; Convolutional neural networks; Forestry; Fruits; Learning systems; Object detection; Orchards; Pixels; Precipitation (meteorology); Remote sensing; Agriculture-related; Artificial intelligence techniques; Generalization capability; Learning-based approach; Malus domestica Borkh; Management practices; Object detection method; Remote sensing system; Deep learning","","","","","Department of Environmental and Sanitation Engineering; Santa Catarina Research Foundation; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303279/2018-4, 303559/2019-5, 304052/2019-1, 307689/2013-1, 313887/2018-7, 433783/2018-4, 436863/2018-9); Fundação de Amparo à Pesquisa e Inovação do Estado de Santa Catarina, FAPESC, (2017TR1762, 2019TR816); Universidade do Estado de Santa Catarina, UDESC","Funding text 1: Funding: This research was partially funded by the Santa Catarina Research Foundation (FAPESC; 2017TR1762, 2019TR816), the Brazilian National Council for Scientific and Technological Development (CNPq; 307689/2013-1, 303279/2018-4, 433783/2018-4, 313887/2018-7, 436863/2018-9, 303559/2019-5, and 304052/2019-1) and the Coordination for the Improvement of Higher Education Personnel (CAPES; Finance Code 001). APC charges of this manuscript were covered by the Remote Sensing 2019 Outstanding Reviewer Award granted to V.L.; Funding text 2: Acknowledgments: We would like to thank the State University of Santa Catarina (UDESC), and the Department of Environmental and Sanitation Engineering, for supporting the doctoral dissertation of the first author. We would also like to thank the owner of the study site for allowing us access into the rural property, as well as for their availability and generosity. The authors also acknowledge the computational support of the Federal University of Mato Grosso do Sul (UFMS). Finally, we would like to thank the editors and two reviewers for providing very constructive concerns and suggestions. Such feedback has greatly helped us improve the quality of the manuscript.","Dian Bah M., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens, 10, pp. 1-20, (2018); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Tu S., Pang J., Liu H., Zhuang N., Chen Y., Zheng C., Wan H., Xue Y., Passion fruit detection and counting based on multiple scale faster R-CNN using RGB-D images, Precision Agric, 21, pp. 1072-1091, (2020); Chen J., Li F., Wang R., Fan Y., Raza M.A., Liu Q., Wang Z., Cheng Y., Wu X., Yang F., Yang W., Estimation of nitrogen and carbon content from soybean leaf reflectance spectra using wavelet analysis under shade stress, Comput. Electron. Agric, 156, pp. 482-489, (2019); Hasan M.M., Chopin J.P., Laga H., Miklavcic S.J., Detection and analysis of wheat spikes using Convolutional Neural Networks, Plant Methods, 14, pp. 1-13, (2018); Hunt M.L., Blackburn G.A., Carrasco L., Redhead J.W., Rowland C.S., High resolution wheat yield mapping using Sentinel-2, Remote Sens. Environ, 233, (2019); Salami E., Gallardo A., Skorobogatov G., Barrado C., On-the-fly olive tree counting using a UAS and cloud services, Remote Sens, (2019); Ball J.E., Anderson D.T., Sr C.S.C., Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community, J. Appl. Remote Sens, 11, pp. 1-54, (2017); Schmidhuber J., Deep Learning in neural networks: An overview, Neural Netw, 61, pp. 85-117, (2015); Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens, 146, pp. 124-136, (2018); Meng L., Peng Z., Zhou J., Zhang J., Lu Z., Baumann A., Du Y., Real-Time Detection of Ground Objects Based on Unmanned Aerial Vehicle Remote Sensing with Deep Learning: Application in Excavator Detection for Pipeline Safety, Remote Sens, 12, (2020); Zhang X., Han L., Han L., Zhu L., How Well Do Deep Learning-Based Methods for Land Cover Classification and Object Detection Perform on High Resolution Remote Sensing Imagery?, Remote Sens, 12, (2020); Yuan Q., Shen H., Li T., Li Z., Li S., Jiang Y., Xu H., Tan W., Yang Q., Wang J., Gao J., Zhang L., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ, 241, (2020); Chaudhuri U., Banerjee B., Bhattacharya A., Datcu M., CMIR-NET: A deep learning based model for cross-modal retrieval in remote sensing, Pattern Recognit. Lett, 131, pp. 456-462, (2020); Osco L.P., dos Santos de Arruda M., Marcato Junior J., da Silva N.B., Ramos A.P.M., Erika Akemi Saito Moryia, Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Li J., Goncalves W.N., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens, 160, pp. 97-106, (2020); Lobo Torres D., Queiroz Feitosa R., Nigri Happ P., Elena Cue La Rosa L., Marcato Junior J., Martins J., Ola Bressan P., Goncalves W.N., Liesenberg V., Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery, Sensors, 20, (2020); Zhu L., Huang L., Fan L., Huang J., Huang F., Chen J., Zhang Z., Wang Y., Landslide Susceptibility Prediction Modeling Based on Remote Sensing and a Novel Deep Learning Algorithm of a Cascade-Parallel Recurrent Neural Network, Sensors, 20, (2020); Castro W., Marcato Junior J., Polidoro C., Osco L.P., Goncalves W., Rodrigues L., Santos M., Jank L., Barrios S., Valle C., Simeao R., Carromeu C., Silveira E., Jorge L.A.d.C., Matsubara E., Deep Learning Applied to Phenotyping of Biomass in Forages with UAV-Based RGB Imagery, Sensors, 20, (2020); Lecun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Khamparia A., Singh K.M., A systematic review on deep learning architectures and applications, Expert Syst, 36, pp. 1-22, (2019); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS J. Photogramm. Remote Sens, 159, pp. 296-307, (2020); Apolo-Apolo O., Martinez-Guanter J., Egea G., Raja P., Perez-Ruiz M., Deep learning techniques for estimation of the yield and size of citrus fruits using a UAV, Eur. J. Agron, 115, (2020); Apolo-Apolo O.E., Perez-Ruiz M., Martinez-Guanter J., Valente J., A Cloud-Based Environment for Generating Yield Estimation Maps From Apple Orchards Using UAV Imagery and a Deep Learning Technique, Front. Plant Sci, 11, (2020); Veeranampalayam Sivakumar A.N., Li J., Scott S., Psota E., Jhala J., Luck J.D., Shi Y., Comparison of Object Detection and Patch-Based Classification Deep Learning Models on Mid-to Late-Season Weed Detection in UAV Imagery, Remote Sens, 12, (2020); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens, 9, (2016); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Habaragamuwa H., Ogawa Y., Suzuki T., Shiigi T., Ono M., Kondo N., Detecting greenhouse strawberries (mature and immature), using deep convolutional neural network, Eng. Agric. Environ. Food, 11, pp. 127-138, (2018); Kirk R., Cielniak G., Mangan M., L*a*b*Fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks, Sensors, 20, pp. 1-19, (2020); Liu X., Chen S.W., Aditya S., Sivakumar N., Dcunha S., Qu C., Taylor C.J., Das J., Kumar V., Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1045-1052, (2018); Bargoti S., Underwood J.P., Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards, J. Field Robot, 34, pp. 1039-1060, (2017); Kestur R., Meduri A., Narasipura O., MangoNet: A deep semantic segmentation architecture for a method to detect and count mangoes in an open orchard, Eng. Appl. Artif. Intell, 77, pp. 59-69, (2019); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning – Method overview and review of use for fruit detection and yield estimation, Comput. Electron. Agric, 162, pp. 219-234, (2019); Dias P.A., Tabb A., Medeiros H., Apple flower detection using deep convolutional networks, Comput. Ind, 99, pp. 17-28, (2018); Wu D., Lv S., Jiang M., Song H., Using channel pruning-based YOLO v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments, Comput. Electron. Agric, 178, (2020); Jiang P., Chen Y., Liu B., He D., Liang C., Real-Time Detection of Apple Leaf Diseases Using Deep Learning Approach Based on Improved Convolutional Neural Networks, IEEE Access, 7, pp. 59069-59080, (2019); Wang D., Li C., Song H., Xiong H., Liu C., He D., Deep Learning Approach for Apple Edge Detection to Remotely Monitor Apple Growth in Orchards, IEEE Access, 8, pp. 26911-26925, (2020); Tian Y., Yang G., Wang Z., Wang H., Li E., Liang Z., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agric, 157, pp. 417-426, (2019); Kang H., Chen C., Fast implementation of real-time fruit detection in apple orchards using deep learning, Comput. Electron. Agric, 168, (2020); Gene-Mola J., Vilaplana V., Rosell-Polo J.R., Morros J.R., Ruiz-Hidalgo J., Gregorio E., Multi-modal deep learning for Fuji apple detection using RGB-D cameras and their radiometric capabilities, Comput. Electron. Agric, 162, pp. 689-698, (2019); Gao F., Fu L., Zhang X., Majeed Y., Li R., Karkee M., Zhang Q., Multi-class fruit-on-plant detection for apple in SNAP system using Faster R-CNN, Comput. Electron. Agric, 176, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, (2015); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, (2017); Zhang S., Chi C., Yao Y., Lei Z., Li S.Z., Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition CVPR; Srivastava L.M., CHAPTER 17—Fruit Development and Ripening, Plant Growth and Development, pp. 413-429, (2002); Meszaros M., Belikova H., Conka P., Namestek J., Effect of hail nets and fertilization management on the nutritional status, growth and production of apple trees, Sci. Hortic, 255, pp. 134-144, (2019); Brglez Sever M., Tojnko S., Breznikar A., Skendrovic Babojelic M., Ivancic A., Sirk M., Unuk T., The influence of differently coloured anti-hail nets and geomorphologic characteristics on microclimatic and light conditions in apple orchards, J. Cent. Eur. Agric, 21, pp. 386-397, (2020); Bosco L.C., Bergamaschi H., Cardoso L.S., Paula V.A.d., Marodin G.A.B., Brauner P.C., Microclimate alterations caused by agricultural hail net coverage and effects on apple tree yield in subtropical climate of Southern Brazil, Bragantia, 77, pp. 181-192, (2018); Bosco L.C., Bergamaschi H., Marodin G.A., Solar radiation effects on growth, anatomy, and physiology of apple trees in a temperate climate of Brazil, Int. J. Biometeorol, pp. 1969-1980, (2020); Pang J., Chen K., Shi J., Feng H., Ouyang W., Lin D., Libra R-CNN: Towards Balanced Learning for Object Detection, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821-830; Cai Z., Vasconcelos N., Cascade R-CNN: Delving Into High Quality Object Detection, Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, pp. 6154-6162, (2018); Zhu C., He Y., Savvides M., Feature Selective Anchor-Free Module for Single-Shot Object Detection, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 840-849; Wang J., Sun K., Cheng T., Jiang B., Deng C., Zhao Y., Liu D., Mu Y., Tan M., Wang X., Liu W., Xiao B., Deep High-Resolution Representation Learning for Visual Recognition, IEEE Trans. Pattern Anal. Mach. Intell, pp. 1-1, (2020); Alvares C.A., Stape J.L., Sentelhas P.C., de Moraes Goncalves J.L., Sparovek G., Köppen’s climate classification map for Brazil, Meteorol. Z, 22, pp. 711-728, (2013); Soil taxonomy: a basic system of soil classification for making and interpreting soil surveys, 436, (1999); dos Santos H.G., JACOMINE P.K.T., Dos Anjos L., De Oliveira V., LUMBRERAS J.F., COELHO M.R., De Almeida J., de Araujo Filho J., De Oliveira J., CUNHA T.J.F., Sistema Brasileiro de Classificação de Solos, (2018); HIDROWEB V3.1.1—Séries Históricas de Estações; Bittencourt C.C., Barone F.M., A cadeia produtiva da maçã em Santa Catarina: competitividade segundo produção e packing house, Rev. Admin. Pública, 45, pp. 1199-1222, (2011); Censo Agropecuário 2017: Resultados Definitivos, (2019); Denardi F., Kvitschal M.V.A.c., Hawerroth M.C., A brief history of the forty-five years of the Epagri apple breeding program in Brazil, Crop. Breed. Appl. Biotechnol, 19, pp. 347-355, (2019); Geosciences: Continuos Catographic Bases; Liang X., Jaakkola A., Wang Y., Hyyppa J., Honkavaara E., Liu J., Kaartinen H., The use of a hand-held camera for individual tree 3D mapping in forest sample plots, Remote Sens, 6, pp. 6587-6603, (2014); Petri J., Denardi F., SUZUKI A.E., 405-Fuji Suprema: Nova cultivar de macieira, Agropecu. Catarin. Florianópolis, 10, pp. 48-50, (1997); Dutta A., Zisserman A., The VIA Annotation Software for Images, Audio and Video, Proceedings of the 27th ACM International Conference on Multimedia (MM ’19), (2019); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature Pyramid Networks for Object Detection, (2017); Michaelis C., Mitzkus B., Geirhos R., Rusak E., Bringmann O., Ecker A.S., Bethge M., Brendel W., Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming, (2020); Hendrycks D., Dietterich T., Benchmarking Neural Network Robustness to Common Corruptions and Perturbations, Proceedings of the International Conference on Learning Representations, (2019); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning for real-time fruit detection and orchard fruit load estimation: benchmarking of ‘MangoYOLO’, Precis. Agric, 20, pp. 1107-1135, (2019); Underwood J.P., Hung C., Whelan B., Sukkarieh S., Mapping almond orchard canopy volume, flowers, fruit and yield using lidar and vision sensors, Comput. Electron. Agric, 130, pp. 83-96, (2016); Hani N., Roy P., Isler V., A comparative study of fruit detection and counting methods for yield mapping in apple orchards, J. Field Robot, 37, pp. 263-282, (2020); Fachinello J.A.C., Pasa M.d.S., Schmtiz J.D., Betemps D.A.L., Situação e perspectivas da fruticultura de clima temperado no Brasil, Rev. Bras. Frutic, 33, pp. 109-120, (2011); Schotsmans W., East A., Thorp G., Woolf A., 6—Feijoa (Acca sellowiana [Berg] Burret), Postharvest Biology and Technology of Tropical and Subtropical Fruits, pp. 115-135, (2011)","V. Liesenberg; Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Lages, Avenida Luiz de Camões 2090, 88520-000, Brazil; email: veraldo.liesenberg@udesc.br","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85098759179"
"Alganci U.; Soydas M.; Sertel E.","Alganci, Ugur (24281315400); Soydas, Mehmet (57215420909); Sertel, Elif (21934838300)","24281315400; 57215420909; 21934838300","Comparative research on deep learning approaches for airplane detection from very high-resolution satellite images","2020","Remote Sensing","12","3","458","","","","79","10.3390/rs12030458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080957273&doi=10.3390%2frs12030458&partnerID=40&md5=d38aa5d2d787eab7478c135fc0b6bcae","Geomatics Engineering Department, Istanbul Technical University, ITU Ayazaga Campus, Sariyer, Istanbul, 34469, Turkey; Institute of Informatics, Istanbul Technical University, ITU Ayazaga Campus, Institute of Informatics Building, Sariyer, Istanbul, 34469, Turkey","Alganci U., Geomatics Engineering Department, Istanbul Technical University, ITU Ayazaga Campus, Sariyer, Istanbul, 34469, Turkey; Soydas M., Institute of Informatics, Istanbul Technical University, ITU Ayazaga Campus, Institute of Informatics Building, Sariyer, Istanbul, 34469, Turkey; Sertel E., Geomatics Engineering Department, Istanbul Technical University, ITU Ayazaga Campus, Sariyer, Istanbul, 34469, Turkey","Object detection from satellite images has been a challenging problem for many years. With the development of effective deep learning algorithms and advancement in hardware systems, higher accuracies have been achieved in the detection of various objects from very high-resolution (VHR) satellite images. This article provides a comparative evaluation of the state-of-the-art convolutional neural network (CNN)-based object detection models, which are Faster R-CNN, Single Shot Multi-box Detector (SSD), and You Look Only Once-v3 (YOLO-v3), to cope with the limited number of labeled data and to automatically detect airplanes in VHR satellite images. Data augmentation with rotation, rescaling, and cropping was applied on the test images to artificially increase the number of training data from satellite images. Moreover, a non-maximum suppression algorithm (NMS) was introduced at the end of the SSD and YOLO-v3 flows to get rid of the multiple detection occurrences near each detected object in the overlapping areas. The trained networks were applied to five independent VHR test images that cover airports and their surroundings to evaluate their performance objectively. Accuracy assessment results of the test regions proved that Faster R-CNN architecture provided the highest accuracy according to the F1 scores, average precision (AP) metrics, and visual inspection of the results. The YOLO-v3 ranked as second, with a slightly lower performance but providing a balanced trade-off between accuracy and speed. The SSD provided the lowest detection performance, but it was better in object localization. The results were also evaluated in terms of the object size and detection accuracy manner, which proved that large-and medium-sized airplanes were detected with higher accuracy. © 2020 by the authors.","Convolutional neural networks (CNNs); End-to-end detection; Faster RCNN; Remote sensing; Single shot multi-box detector (SSD); Transfer learning; You Look Only Once-v3 (YOLO-v3)","Aircraft; Aircraft detection; Convolution; Convolutional neural networks; Economic and social effects; Labeled data; Learning algorithms; Object detection; Object recognition; Remote sensing; Satellites; Transfer learning; Comparative evaluations; Detection performance; End to end; Faster RCNN; Non-maximum suppression; Single shots; Very high resolution satellite images; You Look Only Once-v3 (YOLO-v3); Deep learning","","","","","Istanbul Technical University-Center for Satellite Communications and Remote Sensing","This research received no external funding., Authors acknowledge the support of Istanbul Technical University-Center for Satellite Communications and Remote Sensing (ITU-CSCRS) by providing the Pleiades satellite images.","Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens, 117, pp. 11-28, (2016); Svatonova H., Analysis of visual interpretation of satellite data, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 41, pp. 675-681, (2016); Wu H., Zhang H., Geofan J., Xu F., Typical target detection in satellite images based on convolutional neural networks, Proceedings of the 2015 IEEE International Conference on Systems, pp. 2956-2961, (2015); Zhou P., Cheng G., Liu Z., Bu S., Hu X., Weakly supervised target detection in remote sensing images based on transferred deep features and negative bootstrapping, Multidim. Syst. Sign. Process, 27, pp. 925-944, (2015); Gidaris S., Komodakis N., Locnet: Improving localization accuracy for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 789-798, (2020); Tello M., Martinez C.L., A novel algorithm for ship detection in sar imagery based on the wavelet transform, IEEE Trans. Geosci. Remote Sens, 2, pp. 201-205, (2005); Sirmacek B., Unsalan C., Urban-area and building detection using sift keypoints and graph theory, IEEE Trans. Geosci. Remote Sens, 47, (2009); Han J., Zhang D., Cheng G., Guo L., Ren J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning, IEEE Trans. Geosci. Remote Sens, 53, pp. 3325-3337, (2014); Ma L., Li M., Ma X., Cheng L., Du P., Liu Y., A review of supervised object-based land-cover image classification, ISPRS J. Photogramm. Remote Sens, 130, pp. 277-293, (2017); Sun H., Sun X., Wang H., Li Y., Li X., Automatic target detection in high-resolution remote sensing images using spatial sparse coding bag-of-words model, IEEE Trans. Geosci. Remote Sens, 9, pp. 109-113, (2012); Polat E., Yildiz C., Stationary aircraft detection from satellite images, Istanb. Univ. J. Electr. Electron. Eng, 12, pp. 1523-1528, (2012); Cheng G., Han J., Guo L., Qian X., Zhou P., You X., Hu X., Object detection in remote sensing imagery using a discriminatively trained mixture model, ISPRS J. Photogramm. Remote Sens, 85, pp. 32-43, (2013); Cheng G., Guo L., Zhao T., Han J., Li H., Fang J., Automatic landslide detection from remote-sensing imagery using a scene classification method based on BoVW and pLSA, Int. J. Remote Sens, 34, pp. 45-59, (2012); Han J., Zhou P., Zhang D., Cheng G., Guo L., Liu Z., Bu S., Wu J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding, ISPRS J. Photogramm. Remote Sens, 89, pp. 37-48, (2014); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proceedings of the Advances in Neural Information Processing Systems 25 (NIPS 2012), pp. 1097-1105, (2012); Deng J., Dong W., Socher R., Li L.J., Li K., Li F.F., Imagenet: A large-scale hierarchical image database, Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, (2020); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, ArXiv, (2014); Szegedy C., Liu W., Jia Y.Q., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, (2015); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-9, (2015); Hu F., Xia G.S., Hu J., Zhang L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Remote Sens, 7, pp. 14680-14707, (2015); Scott G.J., England M.R., Starms W.A., Marcum R.A., Davis C.H., Training deep convolutional neural networks for land-cover classification of high-resolution imagery, IEEE Geosci. Remote Sens. Lett, 14, (2017); Cheng G., Han J., Lu X., Remote sensing image scene classification: Benchmark and state of the art, Proc. IEEE, 105, pp. 1865-1883, (2017); Zhong Y., Fei F., Liu Y., Zhao B., Jiao H., Zhang L., SatCNN: Satellite image dataset classification using agile convolutional neural networks, Remote Sens. Lett, 2, pp. 136-145, (2016); Papadomanolaki M., Vakalopoulou M., Zagoruyko S., Karantzalos K., Benchmarking deep learning frameworks for the classification of very high resolution satellite multispectral data, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 3, pp. 83-88, (2016); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 580-587, (2014); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell, 37, pp. 1904-1916, (2015); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Ren S., He K.M., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proceedings of the Advances in Neural Information Processing Systems 28 (NIPS 2015), pp. 91-99, (2015); Hu G., Yang Z., Han J., Huang L., Gong J., Xiong N., Aircraft detection in remote sensing images based on saliency and convolution neural network, EURASIPJ. Wirel. Commun. Netw, 26, pp. 1-16, (2018); Tang T., Zhou S., Deng Z., Zou H., Lei L., Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining, Sensors, 17, (2017); Zhang R., Yao J., Zhang K., Feng C., Zhang J., S-cnn-based ship detection from high-resolution remote sensing images, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 41, pp. 423-430, (2016); Liu Z., Yuan L., Weng L., Yang Y., A high resolution optical satellite image dataset for ship recognition and some new baselines, Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods, pp. 324-331, (2017); Chen C., Gong W., Hu Y., Chen Y., Ding Y., Learning oriented region-based convolutional neural networks for building detection in satellite remote sensing images, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 42, pp. 461-464, (2017); Cheng G., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens, 54, pp. 7405-7415, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single Shot MultiBox Detector, Proceedings of the European Conference on Computer Vision, pp. 21-37, (2016); Everingham M., Gool L.V., Williams C.K.I., Winn J., Zisserman A., The PASCAL visual object classes (VOC) challenge, Int. J. Comput. Vis, 88, pp. 303-338, (2010); Lin T.Y., Marie M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common Objects in Context, Proceedings of the European Conference on Computer Vision, pp. 740-755, (2014); Radovic M., Adarkwa O., Wang Q., Object recognition in aerial images using convolutional neural networks, J. Imaging, 3, (2017); Nie G.H., Zhang P., Niu X., Dou Y., Xia F., Ship detection using transfer learned single shot multi box detector, Proceedings of the 4th Annual International Conference on Information Technology and Applications (ITA 2017), pp. 1006-1012, (2017); Wang Y., Wang C., Zhang H., Combining single shot multibox detector with transfer learning for ship detection using Sentinel-1 images, Proceedings of the BIGSARDATA, pp. 1-4, (2017); Szegedy C., Vanhoucke C., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture forcomputer vision, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818-2826, (2016); Wojek C., Dorko' G., Schulz A., Schiele B., Sliding-Windows for Rapid Object Class Localization: A Parallel Technique, Joint Pattern Recognition Symposium, (2008); Neubeck A., van Gool L., Efficient Non-Maximum Suppression, Proceedings of the 18th International Conference on Pattern Recognition (ICPR'06), (2006); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, ArXiv, (2018); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Et al., TensorFlow: A System for Large-Scale Machine Learning, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, pp. 265-283, (2016); (2020)","U. Alganci; Geomatics Engineering Department, Istanbul Technical University, ITU Ayazaga Campus, Sariyer, Istanbul, 34469, Turkey; email: alganci@itu.edu.tr","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85080957273"
"Liu Y.; Piramanayagam S.; Monteiro S.T.; Saber E.","Liu, Yansong (56199148200); Piramanayagam, Sankaranarayanan (55211559300); Monteiro, Sildomar T. (7005373503); Saber, Eli (56889316900)","56199148200; 55211559300; 7005373503; 56889316900","Semantic segmentation of multisensor remote sensing imagery with deep ConvNets and higher-order conditional random fields","2019","Journal of Applied Remote Sensing","13","1","016501","","","","30","10.1117/1.JRS.13.016501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062614142&doi=10.1117%2f1.JRS.13.016501&partnerID=40&md5=8b5e80ccb8bdf5400d901a5b92ed5c60","Rochester Institute of Technology, Chester F. Carlson Center for Imaging Science, Rochester, NY, United States; Rochester Institute of Technology, Department of Electrical and Microelectronic Engineering, Rochester, NY, United States","Liu Y., Rochester Institute of Technology, Chester F. Carlson Center for Imaging Science, Rochester, NY, United States; Piramanayagam S., Rochester Institute of Technology, Chester F. Carlson Center for Imaging Science, Rochester, NY, United States; Monteiro S.T., Rochester Institute of Technology, Chester F. Carlson Center for Imaging Science, Rochester, NY, United States, Rochester Institute of Technology, Department of Electrical and Microelectronic Engineering, Rochester, NY, United States; Saber E., Rochester Institute of Technology, Chester F. Carlson Center for Imaging Science, Rochester, NY, United States, Rochester Institute of Technology, Department of Electrical and Microelectronic Engineering, Rochester, NY, United States","Aerial images acquired by multiple sensors provide comprehensive and diverse information of materials and objects within a surveyed area. The current use of pretrained deep convolutional neural networks (DCNNs) is usually constrained to three-band images (i.e., RGB) obtained from a single optical sensor. Additional spectral bands from a multiple sensor setup introduce challenges for the use of DCNN. We fuse the RGB feature information obtained from a deep learning framework with light detection and ranging (LiDAR) features to obtain semantic labeling. Specifically, we propose a decision-level multisensor fusion technique for semantic labeling of the very-high-resolution optical imagery and LiDAR data. Our approach first obtains initial probabilistic predictions from two different sources: one from a pretrained neural network fine-tuned on a three-band optical image, and another from a probabilistic classifier trained on LiDAR data. These two predictions are then combined as the unary potential using a higher-order conditional random field (CRF) framework, which resolves fusion ambiguities by exploiting the spatialcontextual information. We utilize graph cut to efficiently infer the final semantic labeling for our proposed higher-order CRF framework. Experiments performed on three benchmarking multisensor datasets demonstrate the performance advantages of our proposed method. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Conditional random fields; Deep convolutional neural networks; Light detection and ranging; Multisensor remote sensing; Semantic segmentation","Antennas; Benchmarking; Convolution; Deep neural networks; Geometrical optics; Graphic methods; Image segmentation; Neural networks; Random processes; Remote sensing; Semantics; Conditional random field; Convolutional neural network; Light detection and ranging; Multisensor remote sensing; Semantic segmentation; Optical radar","","","","","ISPRS; U.S. Department of Defense, DOD; Bloom's Syndrome Foundation, BSF","The authors would like to acknowledge the Department of Defense for its support of this research as well as the usage of the dataset provided by ISPRS and BSF Swissphoto, released in conjunction with the ISPRS, led by ISPRS WG II/4. The authors also would like to thank the Belgian Royal Military Academy for acquiring and providing the data used in this study, and the IEEE GRSS Image Analysis and Data Fusion Technical Committee.","Xu M., Chen H., Varshney P.K., An image fusion approach based on Markov random fields, IEEE Trans. Geosci. Remote Sens., 49, 12, pp. 5116-5127, (2011); Solberg A.H.S., Taxt T., Jain A.K., A Markov random field model for classification of multisource satellite imagery, IEEE Trans. Geosci. Remote Sens., 34, 1, pp. 100-113, (1996); Moser G., Serpico S.B., Benediktsson J.A., Land-cover mapping by Markov modeling of spatial-contextual information in very-high-resolution remote sensing images, Proc. IEEE, 101, 3, pp. 631-651, (2013); Cianci L., Moser G., Serpico S., Change detection from very highresolution multisensor remote-sensing images by a Markovian approach, Proc. IEEE-GOLD, (2012); Sherrah J., Fully Convolutional Networks for Dense Semantic Labelling of High-resolution Aerial Imagery, (2016); Volpi M., Tuia D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 55, 2, pp. 881-893, (2017); Audebert N., Saux B.L., Lefevre S., Semantic segmentation of earth observation data using multimodal and multi-scale deep networks, Asia Conf. on Computer Vision, pp. 180-196, (2016); Marmanis D., Et al., Deep learning earth observation classification using imagenet pretrained networks, IEEE Geosci. Remote Sens. Lett., 13, 1, pp. 105-109, (2016); Maggiori E., Et al., High-resolution Semantic Labeling with Convolutional Neural Networks, (2016); Campos-Taberner M., Et al., Processing of extremely high-resolution Lidar and RGB data: Outcome of the 2015 IEEE GRSS data fusion contest-part a: 2-D contest, IEEE J. Sel. Topics Appl. Earth Obs. Remote Sens., 9, 12, pp. 5547-5559, (2016); Chen X., Et al., Vehicle detection in satellite images by hybrid deep convolutional neural networks, IEEE Geosci. Remote Sens. Lett., 11, 10, pp. 1797-1801, (2014); Castelluccio M., Et al., Land Use Classification in Remote Sensing Images by Convolutional Neural Networks, (2015); Ugarriza L., Et al., Automatic image segmentation by dynamic region growth and multiresolution merging, IEEE Trans. Image Process., 18, 10, pp. 2275-2288, (2009); Liu Y., Et al., Dense semantic labeling of very-high-resolution aerial imagery and lidar with fully-convolutional neural networks and higher-order CRFs, IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), (2017); Gomez-Chova L., Et al., Multimodal classification of remote sensing images: A review and future directions, Proc. IEEE, 103, 9, pp. 1560-1584, (2015); Debes C., Et al., Hyperspectral and LiDAR data fusion: Outcome of the 2013 GRSS data fusion contest, IEEE J. Sel. Topics Appl. Earth Obs. Remote Sens., 7, 6, pp. 2405-2418, (2014); Penatti O., Nogueira K., Dos Santos J.A., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains, IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 44-51, (2015); Alonzo M., Bookhagen B., Roberts D.A., Urban tree species mapping using hyperspectral and LiDAR data fusion, Remote Sens. Environ., 148, pp. 70-83, (2014); Mnih V., Hinton G., Learning to detect roads in high-resolution aerial images, Lect. Notes Comput. Sci., 6316, pp. 210-223, (2010); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440, (2015); Badrinarayanan V., Kendall A., Cipolla R., Segnet: A Deep Convolutional Encoderdecoder Architecture for Image Segmentation, (2015); Chen L.-C., Et al., Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs, (2014); Romero A., Gatta C., Camps-Valls G., Unsupervised deep feature extraction for remote sensing image classification, IEEE Trans. Geosci. Remote Sens., 54, 3, pp. 1349-1362, (2016); Waske B., Benediktsson J.A., Fusion of support vector machines for classification of multisensor data, IEEE Trans. Geosci. Remote Sens., 45, 12, pp. 3858-3866, (2007); Wegner J.D., Montoya-Zegarra J.A., Schindler K., A higher-order CRF model for road network extraction, IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 1698-1705, (2013); Li E., Et al., Robust rooftop extraction from visible band images using higher order CRF, IEEE Trans. Geosci. Remote Sens., 53, 8, pp. 4483-4495, (2015); Kluckner S., Et al., Semantic classification in aerial imagery by integrating appearance and height information, Lect. Notes Comput. Sci., 5995, pp. 477-488, (2010); Marmanis D., Et al., Semantic segmentation of aerial images with an ensemble of CNSS, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. III, 3, pp. 473-480, (2016); Kohli P., Torr P.H., Robust higher order potentials for enforcing label consistency, Int. J. Comput. Vision, 82, 3, pp. 302-324, (2009); Ladick L., Et al., Associative hierarchical random fields, IEEE Trans. Pattern Anal. Mach. Intell., 36, 6, pp. 1056-1077, (2014); Boix X., Et al., Harmony potentials, Int. J. Comput. Vision, 96, 1, pp. 83-102, (2012); Ladicky L., Et al., Graph cut based inference with co-occurrence statistics, Lect. Notes Comput. Sci., 6315, pp. 239-253, (2010); Philipp K., Koltun V., Efficient inference in fully connected CRFs with Gaussian edge potentials, Adv. Neural Inf. Process. Syst., 2, 3, pp. 109-117, (2011); Marcello J., Medina A., Eugenio F., Evaluation of spatial and spectral effectiveness of pixel-level fusion techniques, IEEE Geosci. Remote Sens. Lett., 10, 3, pp. 432-436, (2013); Li W., Prasad S., Fowler J., Decision fusion in kernel-induced spaces for hyperspectral image classification, IEEE Geosci. Remote Sens. Lett., 52, 6, pp. 3399-3411, (2014); Pohl C., Van Genderen J.L., Review article multisensor image fusion in remote sensing: Concepts, methods and applications, Int. J. Remote Sens., 19, 5, pp. 823-854, (1998); Benediktsson J.A., Swain P.H., Ersoy O.K., Neural network approaches versus statistical methods in classification of multisource remote sensing data, IEEE Trans. Geosci. Remote Sens., 28, 4, pp. 540-552, (1990); Audebert N., Le Saux B., Lefevre S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks, ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32, (2017); Paisitkriangkrai S., Et al., Effective semantic pixel labelling with convolutional networks and conditional random fields, IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 36-43, (2015); Khurshid H., Khan M.F., Segmentation and classification using logistic regression in remote sensing imagery, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8, 1, pp. 224-232, (2015); Platt J., Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods, Adv. Large Margin Classifiers, 10, 3, pp. 61-74, (1999); Panda S.S., Ames D.P., Panigrahi S., Application of vegetation indices for agricultural crop yield prediction using neural network techniques, Remote Sens., 2, 3, pp. 673-696, (2010); Pettorelli N., Et al., Using the satellite-derived NDVI to assess ecological responses to environmental change, Trends Ecol. Evol., 20, 9, pp. 503-510, (2005); Nouri H., Et al., High spatial resolution worldview-2 imagery for mapping NDVI and its relationship to temporal urban landscape evapotranspiration factors, Remote Sens., 6, 1, pp. 580-602, (2014); Achanta R., Et al., Slic superpixels compared to state-of-the-art superpixel methods, IEEE Trans. Pattern Anal. Mach. Intell., 34, 11, pp. 2274-2282, (2012); Domke J., Learning graphical model parameters with approximate marginal inference, IEEE Trans. Pattern Anal. Mach. Intell., 35, 10, pp. 2454-2467, (2013); Kolmogorov V., Zabin R., What energy functions can be minimized via graph cuts, IEEE Trans. Pattern Anal. Mach. Intell., 26, 2, pp. 147-159, (2004); ISPRS 2D Semantic Labeling Contest-Potsdam; ISPRS 2D Semantic Labeling Contest-Vaihingen; IEEE GRSS Data Fusion Contest; Jia Y., Et al., Caffe: Convolutional architecture for fast feature embedding, Proc. of the 22nd ACM Int. Conf. on Multimedia, pp. 675-678, (2014); Marmanis D., Et al., Classification with an edge: Improving semantic image segmentation with boundary detection, ISPRS J. Photogramm. Remote Sens., 135, pp. 158-172, (2018); Vantaram S.R., Saber E., Survey of contemporary trends in color image segmentation, J. Electron. Imaging, 21, 4, (2012); Kemker R., Kanan C., Deep neural networks for semantic segmentation of multispectral remote sensing imagery, CoRR, (2017); Lin G., Et al., Efficient piecewise training of deep structured models for semantic segmentation, IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 3194-3203, (2016)","Y. Liu; Rochester Institute of Technology, Chester F. Carlson Center for Imaging Science, Rochester, United States; email: yxl3624@rit.edu","","SPIE","","","","","","19313195","","JARSC","","English","J. Appl. Remote Sens.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85062614142"
"Shams T.; Desbarats P.","Shams, Tooba (57221502201); Desbarats, Pascal (8582283800)","57221502201; 8582283800","Detection of asian hornet's nest on drone acquired FLIR and color images using deep learning methods","2020","2020 10th International Conference on Image Processing Theory, Tools and Applications, IPTA 2020","","","9286693","","","","1","10.1109/IPTA50016.2020.9286693","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099298316&doi=10.1109%2fIPTA50016.2020.9286693&partnerID=40&md5=b627dd8cab359c1a8b2b79f8c4a3201e","Univ. Bordeaux/Bees for Life, France; Univ. Bordeaux, LaBRI UMR5800 (Univ. Bordeaux/CNRS/Bordeaux INP), Talence cedex, 33405, France","Shams T., Univ. Bordeaux/Bees for Life, France; Desbarats P., Univ. Bordeaux, LaBRI UMR5800 (Univ. Bordeaux/CNRS/Bordeaux INP), Talence cedex, 33405, France","Asian hornets are considered a pest because of their dangerousness and their impact on the ecosystem. Detecting nests of this species is a difficult task, as they are found in the trees, hidden in the leaves. Our goal is to carry out this detection from images acquired by a drone. We propose in this work a new method, based on the advantages of visible spectrum and FLIR images. We compare two models of state-of-the-art neural networks (YOLO and Mask-RCNN) for this task. The results are presented from the two separate image sets, then by combining the network responses. To do this, a third dataset (for ensemble model) was built by simulating a FLIR acquisition simultaneous with the acquisition in the visible spectrum. Preliminary results show that the best strategy is to use Mask-RCNN on the ensemble model (detection rate of 93%). A discussion on the relevant information present in the images and on taking into account of this information by the networks is also proposed. © 2020 IEEE.","Deep Neural Networks; Drone image acquisition; FLIR and visible spectrum images; Object detection","Aircraft detection; Drones; Image acquisition; Learning systems; Color images; Detection rates; Ensemble modeling; FLIR images; Learning methods; Network response; State of the art; Visible spectra; Deep learning","","","","","Erasmus Mundus Association; European Commission, EC","We would like to thank Bees For Life, for collecting all the data and making it available for this work, and European Union and the Erasmus Mundus Association along with Bees For Life for funding this research.","De Haro L., Et al., Medical consequences of the asian black hornet (Vespa velutina) invasion in southwestern France, Toxicon, 55, 2-3, pp. 650-652, (2010); Perrard A., Et al., Observations on the colony activity of the asian hornet vespa velutina lepeletier 1836 (Hymenoptera: Vespidae: Vespinae) in France, Annales de la Société Entomologique de France, 45, 1, (2009); Kennedy P.J., Et al., Searching for nests of the invasive asian hornet (Vespa velutina) using radio-telemetry, Communications Biology, 1, 1, pp. 1-8, (2018); Milanesio D., Et al., Design of an harmonic radar for the tracking of the A sian yellow-legged hornet, Ecology and Evolution, 6, 7, pp. 2170-2178, (2016); Viola P., Jones M., Robust real-time object detection, International Journal of Computer Vision, 4, 34-47, (2001); Piccinini P., Prati A., Cucchiara R., Real-time object detection and localization with SIFT-based clustering, Image and Vision Computing, 30, 8, pp. 573-587, (2012); Zou Z., Et al., Object Detection in 20 Years: A Survey, (2019); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Scholkopf B., Smola A., Support Vector Machines, (2014); Pietrow D., Matuszewski J., Objects detection and recognition system using artificial neural networks and drones, 2017 Signal Processing Symposium (SPSympo), pp. 1-5, (2017); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, (2012); Redmon J., Et al., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); Redmon J., Farhadi A., Yolov3: An Incremental Improvement, (2018); Girshick R., Et al., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2014); Ren S., Et al., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, (2015); He K., Et al., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, (2017); Polikar R., Ensemble Learning in Ensemble Machine Learning: Methods and Applications, pp. 1-34, (2012); Astrom F., Koker R., A parallel neural network approach to prediction of Parkinson's disease, Expert Systems with Applications, 38, 10, pp. 12470-12474, (2011); Bromley J., Et al., Signature verification using a ""siamese"" time delay neural network, Advances in Neural Information Processing Systems, (1994); Aizi K., Ouslim M., Score level fusion in multibiometric identification based on zones of interest, Journal of King Saud University-Computer and Information Sciences, (2019); Dollar P., Et al., Pedestrian detection: An evaluation of the state of the art, IEEE Transactions on Pattern Analysis and Machine Intelligence, 34, 4, pp. 743-761, (2011); Kennedy H.V., Modeling noise in thermal imaging systems, Infrared Imaging Systems: Design, Analysis, Modeling, and Testing IV, 1969, (1993); Moreira M., Fiesler E., Neural Networks with Adaptive Learning Rate and Momentum Terms, (1995); He K., Et al., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Daniel D.M., Mihaela C., Romulus T., Combining feature extraction level and score level fusion in a multimodal biometric system, 2014 11th International Symposium on Electronics and Telecommunications (ISETC), (2014)","","","Institute of Electrical and Electronics Engineers Inc.","","10th International Conference on Image Processing Theory, Tools and Applications, IPTA 2020","9 November 2020 through 12 November 2020","Virtual, Paris","165865","","978-172818750-1","","","English","Int. Conf. Image Process. Theory, Tools Appl., IPTA","Conference paper","Final","","Scopus","2-s2.0-85099298316"
"Ocer N.E.; Kaplan G.; Erdem F.; Kucuk Matci D.; Avdan U.","Ocer, Nuri Erkin (57217729257); Kaplan, Gordana (57196402161); Erdem, Firat (57196018307); Kucuk Matci, Dilek (57200555621); Avdan, Ugur (8356726300)","57217729257; 57196402161; 57196018307; 57200555621; 8356726300","Tree extraction from multi-scale UAV images using Mask R-CNN with FPN","2020","Remote Sensing Letters","11","9","","847","856","9","42","10.1080/2150704X.2020.1784491","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087546830&doi=10.1080%2f2150704X.2020.1784491&partnerID=40&md5=5e7a3c77128f609e46a5034719436fbe","Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey","Ocer N.E., Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey; Kaplan G., Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey; Erdem F., Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey; Kucuk Matci D., Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey; Avdan U., Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey","Tree detection and counting have been performed using conventional methods or high costly remote sensing data. In the past few years, deep learning techniques have gained significant progress in the remote sensing area. Namely, convolutional neural networks (CNNs) have been recognized as one of the most successful and widely used deep learning approaches and they have been used for object detection. In this paper, we employed a Mask R-CNN model and feature pyramid network (FPN) for tree extraction from high-resolution RGB unmanned aerial vehicle (UAV) data. The main aim of this paper is to explore the employed method in images with different scales and tree contents. For this purpose, UAV images from two different areas were acquired and three big-scale test images were created for experimental analysis and accuracy assessment. According to the accuracy analyses, despite the scale and the content changes, the proposed model maintains its detection accuracy to a large extent. To our knowledge, this is the first time a Mask R-CNN model with FPN has been used with UAV data for tree extraction. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","","Antennas; Convolutional neural networks; Data mining; Deep learning; Extraction; Forestry; Object detection; Remote sensing; Unmanned aerial vehicles (UAV); Accuracy analysis; Accuracy assessment; Conventional methods; Detection accuracy; Experimental analysis; Learning approach; Learning techniques; Remote sensing data; accuracy assessment; aerial survey; artificial neural network; detection method; image analysis; numerical model; remote sensing; tree; unmanned vehicle; Trees (mathematics)","","","","","","","Amini Amirkolaee H., Arefi H., CNN-based Estimation of Pre-and Post-earthquake Height Models from Single Optical Images for Identification of Collapsed Buildings, Remote Sensing Letters, 10, 7, pp. 679-688, (2019); Chen S.W., Shivakumar S.S., Dcunha S., Das J., Okon E., Qu C., Taylor C.J., Kumar V., Counting Apples and Oranges with Deep Learning: A Data-driven Approach, IEEE Robotics and Automation Letters, 2, 2, pp. 781-788, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, 4, (2018); Girshick R., Fast R-CNN, Proceedings of the IEEE international conference on computer vision, Las Condes, Chile, (2015); Hanapi S.S., Shukor S., Johari J., A Review on Remote Sensing-based Method for Tree Detection and Delineation, IOP Conference Series: Materials Science and Engineering, (2019); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the IEEE international conference on computer vision, Venice, Italy, (2017); Holmgren J., Lindberg E., Tree Crown Segmentation Based on a Tree Crown Density Model Derived from Airborne Laser Scanning, Remote Sensing Letters, 10, 12, pp. 1143-1152, (2019); Hu Y., Guo F., Building Extraction Using Mask Scoring R-CNN Network, Proceedings of the 3rd International Conference on Computer Science and Application Engineering, Sanya, China, (2019); Janicke C., Okujeni A., Cooper S., Clark M., Hostert P., van der Linden S., Brightness Gradient-corrected Hyperspectral Image Mosaics for Fractional Vegetation Cover Mapping in Northern California, Remote Sensing Letters, 11, 1, pp. 1-10, (2020); Le T.D., Huynh D.T., Pham H.V., Efficient Human-Robot Interaction Using Deep Learning with Mask R-CNN: Detection, Recognition, Tracking and Segmentation, 2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV), (2018); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-resolution Remote Sensing Images, Remote Sensing, 9, 1, (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft Coco: Common Objects in Context, European conference on computer vision, (2014); Liu X., Chen S.W., Aditya S., Sivakumar N., Dcunha S., Qu C., Taylor C.J., Das J., Kumar V., Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), (2018); Mubin N.A., Nadarajoo E., Shafri H.Z.M., Hamedianfar A., Young and Mature Oil Palm Tree Detection and Counting Using Convolutional Neural Network Deep Learning Method, International Journal of Remote Sensing, 40, 19, pp. 7500-7515, (2019); Nie G.-H., Zhang P., Niu X., Dou Y., Xia F., Ship Detection Using Transfer Learned Single Shot Multi Box Detector, ITM Web of Conferences, (2017); Nie S., Jiang Z., Zhang H., Cai B., Yao Y., Inshore Ship Detection Based on Mask R-CNN. IGARSS 2018-2018, IEEE International Geoscience and Remote Sensing Symposium, (2018); Nie X., Duan M., Ding H., Hu B., Wong E.K., Attention Mask R-CNN for Ship Detection and Segmentation from Remote Sensing Images, IEEE Access, 8, pp. 9325-9334, (2020); Peng C., Zhao K., Wiliem A., Zhang T., Hobson P., Jennings A., Lovell B.C., To What Extent Does Downsampling, Compression, and Data Scarcity Impact Renal Image Analysis?, 2019 Digital Image Computing: Techniques and Applications (DICTA), (2019); Qian Y., Liu Q., Zhu H., Fan H., Du B., Liu S., Mask R-CNN for Object Detection in Multitemporal SAR Images, 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), (2019); Ribera J., Chen Y., Boomsma C., Delp E.J., Counting Plants Using Deep Learning, 2017 IEEE global conference on signal and information processing (GlobalSIP), (2017); Sa I., Ge Z., Dayoub F., Upcroft B., Perez T., McCool C., Deepfruits: A Fruit Detection System Using Deep Neural Networks, Sensors, 16, 8, (2016); Santos A.A.D.J., Marcato Junior M.S., Araujo D.R., Di Martini E.C., Tetila H.L., Siqueira C., Aoki A., Eltner E.T., Matsubara, Pistori H., Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs, Sensors, 19, 16, (2019); Tang R., Liu H., Wei J., Tang W., Supervised Learning with Convolutional Neural Networks for Hyperspectral Visualization, Remote Sensing Letters, 11, 4, pp. 363-372, (2020); Waleed A., Working Code Forked from Mask R-CNN for Object Detection and Segmentation, (2018); Wang Z., Underwood J., Walsh K.B., Machine Vision Assessment of Mango Orchard Flowering, Computers and Electronics in Agriculture, 151, pp. 501-511, (2018); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual Tree-crown Detection in RGB Imagery Using Semi-supervised Deep Learning Neural Networks, Remote Sensing, 11, 11, (2019); Zhang R., Cheng C., Zhao X., Li X., Multiscale Mask R-CNN–Based Lung Tumor Detection Using PET Imaging, Molecular Imaging, 18, (2019); Zhao T., Yang Y., Niu H., Wang D., Chen Y., Comparing U-Net Convolutional Network with Mask R-CNN in the Performances of Pomegranate Tree Canopy Segmentation, Multispectral, Hyperspectral, and Ultraspectral Remote Sensing Technology, Techniques and Applications VII, International Society for Optics and Photonics, San Diego, CA, United States, (2018)","N.E. Ocer; Earth and Space Sciences Institute, Eskisehir Technical University, Eskisehir, Turkey; email: neocer@eskisehir.edu.tr","","Taylor and Francis Ltd.","","","","","","2150704X","","","","English","Remote Sens. Lett.","Article","Final","","Scopus","2-s2.0-85087546830"
"Li W.; Dong R.; Fu H.; Yu L.","Li, Weijia (57191833776); Dong, Runmin (57205415789); Fu, Haohuan (8713118400); Yu, Le (55277716700)","57191833776; 57205415789; 8713118400; 55277716700","Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks","2019","Remote Sensing","11","1","11","","","","86","10.3390/rs11010011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059949711&doi=10.3390%2frs11010011&partnerID=40&md5=7d39994d89098681d79bef29ed19c151","Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China; Joint Center for Global Change Studies (JCGCS), Beijing, 100084, China","Li W., Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China, Joint Center for Global Change Studies (JCGCS), Beijing, 100084, China; Dong R., Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China, Joint Center for Global Change Studies (JCGCS), Beijing, 100084, China; Fu H., Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China, Joint Center for Global Change Studies (JCGCS), Beijing, 100084, China; Yu L., Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China, Joint Center for Global Change Studies (JCGCS), Beijing, 100084, China","                             Being an important economic crop that contributes 35% of the total consumption of vegetable oil, remote sensing-based quantitative detection of oil palm trees has long been a key research direction for both agriculture and environmental purposes. While existing methods already demonstrate satisfactory effectiveness for small regions, performing the detection for a large region with satisfactory accuracy is still challenging. In this study, we proposed a two-stage convolutional neural network (TS-CNN)-based oil palm detection method using high-resolution satellite images (i.e. Quickbird) in a large-scale study area of Malaysia. The TS-CNN consists of one CNN for land cover classification and one CNN for object classification. The two CNNs were trained and optimized independently based on 20,000 samples collected through human interpretation. For the large-scale oil palm detection for an area of 55 km                             2                             , we proposed an effective workflow that consists of an overlapping partitioning method for large-scale image division, a multi-scale sliding window method for oil palm coordinate prediction, and a minimum distance filter method for post-processing. Our proposed approach achieves a much higher average F1-score of 94.99% in our study area compared with existing oil palm detection methods (87.95%, 81.80%, 80.61%, and 78.35% for single-stage CNN, Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN), respectively), and much fewer confusions with other vegetation and buildings in the whole image detection results.                          © 2018 by the authors.","Convolutional neural networks; Deep learning; High-resolution satellite imagery; Object detection; Oil palm tree","Convolution; Decision trees; Deep learning; Forestry; Neural networks; Object detection; Palm oil; Remote sensing; Satellite imagery; Support vector machines; Convolutional neural network; High resolution satellite imagery; High resolution satellite images; Land cover classification; Object classification; Oil palm tree; Quantitative detection; Sliding window methods; Palmprint recognition","","","","","National Key R&D Program of China, (2017YFA0604401, 2017YFA0604500); Pilot National Laboratory for Marine Science and Technology (Qingdao); National Natural Science Foundation of China, NSFC, (5171101179)","Funding: This work was supported in part by the National Key R&D Program of China (Grant No. 2017YFA0604500 and 2017YFA0604401), by the National Natural Science Foundation of China (Grant No. 5171101179), and by Center for High Performance Computing and System Simulation, Pilot National Laboratory for Marine Science and Technology (Qingdao).","Cheng Y., Yu L., Xu Y., Lu H., Cracknell A.P., Kanniah K., Gong P., Mapping oil palm extent in Malaysia using ALOS-2 PALSAR-2 data, Int. J. Remote Sens, 39, pp. 432-452, (2018); Lian P.K., Wilcove D.S., Cashing in palm oil for conservation, Nature, 448, pp. 993-994, (2007); Chong K.L., Kanniah K.D., Pohl C., Tan K.P., A review of remote sensing applications for oil palm studies, Geo-Spat. Inf. Sci, 20, pp. 184-200, (2017); Abram N.K., Meijaard E., Wilson K.A., Davis J.T., Wells J.A., Ancrenaz M., Budiharta S., Durrant A., Fakhruzzi A., Runting R.K., Et al., Oil palm-community conflict mapping in Indonesia: A case for better community liaison in planning for development initiatives, Appl. Geogr, 78, pp. 33-44, (2017); Cheng Y., Yu L., Zhao Y., Xu Y., Hackman K., Cracknell A.P., Gong P., Towards a global oil palm sample database: Design and implications, Int. J. Remote Sens, 38, pp. 4022-4032, (2017); Barnes A.D., Jochum M., Mumme S., Haneda N.F., Farajallah A., Widarto T.H., Brose U., Consequences of tropical land use for multitrophic biodiversity and ecosystem functioning, Nat. Commun, 5, (2014); Busch J., Ferretti-Gallon K., Engelmann J., Wright M., Austin K.G., Stolle F., Turubanova S., Potapov P.V., Margono B., Hansen M.C., Et al., Reductions in emissions from deforestation from Indonesia's moratorium on new oil palm, timber, and logging concessions, Proc. Natl. Acad. Sci. USA, 112, pp. 1328-1333, (2015); Cheng Y., Yu L., Xu Y., Liu X., Lu H., Cracknell A.P., Kanniah K., Gong P., Towards global oil palm plantation mapping using remote-sensing data, Int. J. Remote Sens, 39, pp. 5891-5916, (2018); Koh L.P., Miettinen J., Liew S.C., Ghazoul J., Remotely sensed evidence of tropical peatland conversion to oil palm, Proc. Natl. Acad. Sci. USA, 1, (2011); Carlson K.M., Heilmayr R., Gibbs H.K., Noojipady P., Burns D.N., Morton D.C., Walker N.F., Paoli G.D., Kremen C., Effect of oil palm sustainability certification on deforestation and fire in Indonesia, Proc. Natl. Acad. Sci. USA, 115, pp. 121-126, (2018); Cracknell A.P., Kanniah K.D., Tan K.P., Wang L., Evaluation of MODIS gross primary productivity and land cover products for the humid tropics using oil palm trees in Peninsular Malaysia and Google Earth imagery, Int. J. Remote Sens, 34, pp. 7400-7423, (2013); Tan K.P., Kanniah K.D., Cracknell A.P., Use of UK-DMC 2 and ALOS PALSAR for studying the age of oil palm trees in southern peninsular Malaysia, Int. J. Remote Sens, 34, pp. 7424-7446, (2013); Gutierrez-Velez V.H., DeFries R., Annual multi-resolution detection of land cover conversion to oil palm in the Peruvian Amazon, Remote Sens. Environ, 129, pp. 154-167, (2013); Cheng Y., Yu L., Cracknell A.P., Gong P., Oil palm mapping using Landsat and PALSAR: A case study in Malaysia, Int. J. Remote Sens, 37, pp. 5431-5442, (2016); Balasundram S.K., Memarian H., Khosla R., Estimating oil palm yields using vegetation indices derived from Quickbird, Life Sci. J, 10, pp. 851-860, (2013); Thenkabail P.S., Stucky N., Griscom B.W., Ashton M.S., Diels J., Van Der Meer B., Enclona E., Biomass estimations and carbon stock calculations in the oil palm plantations of African derived savannas using IKONOS data, Int. J. Remote Sens, 25, pp. 5447-5472, (2004); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens, 9, (2016); Wulder M., Niemann K.O., Goodenough D.G., Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery, Remote Sens. Environ, 73, pp. 103-114, (2000); Wang L., Gong P., Biging G.S., Individual tree-crown delineation and treetop detection in high-spatial-resolution aerial imagery, Photogramm. Eng. Remote Sens, 70, pp. 351-357, (2004); Jiang H., Chen S., Li D., Wang C., Yang J., Papaya Tree Detection with UAV Images Using a GPU-Accelerated Scale-Space Filtering Method, Remote Sens, 9, (2017); Pitkanen J., Individual tree detection in digital aerial images by combining locally adaptive binarization and local maxima methods, Can. J. For. Res, 31, pp. 832-844, (2001); Daliakopoulos I.N., Grillakis E.G., Koutroulis A.G., Tsanis I.K., Tree crown detection on multispectral VHR satellite imagery, Photogramm. Eng. Remote Sens, 75, pp. 1201-1211, (2009); Ke Y., Quackenbush L.J., A review of methods for automatic individual tree-crown detection and delineation from passive remote sensing, Int. J. Remote Sens, 32, pp. 4725-4747, (2011); Dos Santos A.M., Mitja D., Delaitre E., Demagistri L., de Souza Miranda I., Libourel T., Petit M., Estimating babassu palm density using automatic palm tree detection with very high spatial resolution satellite images, J. Environ. Manag, 193, pp. 40-51, (2017); Yang J., He Y., Caspersen J.P., Jones T.A., Delineating Individual Tree Crowns in an Uneven-Aged, Mixed Broadleaf Forest Using Multispectral Watershed Segmentation and Multiscale Fitting, IEEE J. Sel. Top. Appl. Earth Observ, 10, pp. 1390-1401, (2017); Chemura A., van Duren I., van Leeuwen L.M., Determination of the age of oil palm from crown projection area detected from WorldView-2 multispectral remote sensing data: The case of Ejisu-Juaben district, Ghana, ISPRS J. Photogramm, 100, pp. 118-127, (2015); Pouliot D.A., King D.J., Bell F.W., Pitt D.G., Automated tree crown detection and delineation in high-resolution digital camera imagery of coniferous forest regeneration, Remote Sens. Environ, 82, pp. 322-334, (2002); Shafri H.Z., Hamdan N., Saripan M.I., Semi-automatic detection and counting of oil palm trees from high spatial resolution airborne imagery, Int. J. Remote Sens, 32, pp. 2095-2115, (2011); Srestasathiern P., Rakwatin P., Oil palm tree detection with high resolution multi-spectral satellite imagery, Remote Sens, 6, pp. 9749-9774, (2014); Gomes M.F., Maillard P., Deng H., Individual tree crown detection in sub-meter satellite imagery using Marked Point Processes and a geometrical-optical model, Remote Sens. Environ, 211, pp. 184-195, (2018); Ardila J.P., Bijker W., Tolpekin V.A., Stein A., Multitemporal change detection of urban trees using localized region-based active contours in VHR images, Remote Sens. Environ, 124, pp. 413-426, (2012); Pu R., Landry S., A comparative analysis of high spatial resolution IKONOS and WorldView-2 imagery for mapping urban tree species, Remote Sens. Environ, 124, pp. 516-533, (2012); Hung C., Bryson M., Sukkarieh S., Multi-class predictive template for tree crown detection, ISPRS J. Photogramm, 68, pp. 170-183, (2012); Dalponte M., Orka H.O., Ene L.T., Gobakken T., Naesset E., Tree crown delineation and tree species classification in boreal forests using hyperspectral and ALS data, Remote Sens. Environ, 140, pp. 306-317, (2014); Malek S., Bazi Y., Alajlan N., AlHichri H., Melgani F., Efficient framework for palm tree detection in UAV images, IEEE J. Sel. Top. Appl. Earth Observ, 7, pp. 4692-4703, (2014); Lopez-Lopez M., Calderon R., Gonzalez-Dugo V., Zarco-Tejada P.J., Fereres E., Early detection and quantification of almond red leaf blotch using high-resolution hyperspectral and thermal imagery, Remote Sens, 8, (2016); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., Hyyppa J., Saari H., Polonen I., Imai N.N., Et al., Individual tree detection and classification with UAV-based photogrammetric point clouds and hyperspectral imaging, Remote Sens, 9, (2017); Chen X., Xiang S., Liu C.L., Pan C.H., Vehicle detection in satellite images by hybrid deep convolutional neural networks, IEEE Geosci. Remote Sens. Lett, 11, pp. 1797-1801, (2014); Han W., Feng R., Wang L., Cheng Y., A semi-supervised generative framework with deep learning features for high-resolution remote sensing image scene classification, ISPRS J. Photogramm, 145, pp. 23-43, (2017); Long Y., Gong Y., Xiao Z., Liu Q., Accurate object localization in remote sensing images based on convolutional neural networks, IEEE T. Geosci. Remote, 55, pp. 2486-2498, (2017); Li W., Fu H., Yu L., Gong P., Feng D., Li C., Clinton N., Stacked autoencoder-based deep learning for remote-sensing image classification: A case study of African land-cover mapping, Int. J. Remote Sens, 37, pp. 5632-5646, (2016); Zhang C., Sargent I., Pan X., Li H., Gardiner A., Hare J., Atkinson P.M., An object-based convolutional neural network (OCNN) for urban land use classification, Remote Sens. Environ, 216, pp. 57-70, (2018); Huang B., Zhao B., Song Y., Urban land-use mapping using a deep convolutional neural network with high spatial resolution multispectral remote sensing imagery, Remote Sens. Environ, 214, pp. 73-86, (2018); Mou L., Ghamisi P., Zhu X.X., Unsupervised spectral-spatial feature learning via deep residual Conv-Deconv network for hyperspectral image classification, IEEE T. Geosci. Remote, 56, pp. 391-406, (2018); Chen Y., Lin Z., Zhao X., Wang G., Gu Y., Deep learning-based classification of hyperspectral data, IEEE J. Sel. Top. Appl. Earth Observ, 7, pp. 2094-2107, (2014); Makantasis K., Doulamis A.D., Doulamis N.D., Nikitakis A., Tensor-based classification models for hyperspectral data analysis, IEEE T. Geosci. Remote. Sens, 9, pp. 1-15, (2018); Zou Q., Ni L., Zhang T., Wang Q., Deep Learning Based Feature Selection for Remote Sensing Scene Classification, IEEE Geosci. Remote Sens. Lett, 12, pp. 2321-2325, (2015); Hu F., Xia G.S., Hu J., Zhang L., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Remote Sens, 7, pp. 14680-14707, (2015); Liu Y., Zhong Y., Fei F., Zhu Q., Qin Q., Scene Classification Based on a Deep Random-Scale Stretched Convolutional Neural Network, Remote Sens, 10, (2018); Rey N., Volpi M., Joost S., Tuia D., Detecting animals in African Savanna with UAVs and the crowds, Remote Sens. Environ, 200, pp. 341-351, (2017); Cheng G., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens, 54, pp. 7405-7415, (2016); Ding P., Zhang Y., Deng W.J., Jia P., Kuijper A., A light and faster regional convolutional neural network for object detection in optical remote sensing images, ISPRS J. Photogramm, 141, pp. 208-218, (2018); Tang T., Zhou S., Deng Z., Lei L., Zou H., Arbitrary-oriented vehicle detection in aerial imagery with single convolutional neural networks, Remote Sens, 9, (2017); Han X., Zhong Y., Zhang L., An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery, Remote Sens, 9, (2017); Li W., He C., Fang J., Fu H., Semantic Segmentation based Building Extraction Method using Multi-source GIS Map Datasets and Satellite Imagery, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 238-241, (2018); Fu G., Liu C., Zhou R., Sun T., Zhang Q., Classification for high resolution remote sensing imagery using a fully convolutional network, Remote Sens, 9, (2017); Wang H., Wang Y., Zhang Q., Xiang S., Pan C., Gated convolutional neural network for semantic segmentation in high-resolution images, Remote Sens, 9, (2017); Guirado E., Tabik S., Alcaraz-Segura D., Cabello J., Herrera F., Deep-learning versus OBIA for scattered shrub detection with Google earth imagery: Ziziphus Lotus as case study, Remote Sens, 9, (2017); Pibre L., Chaumont M., Subsol G., Ienco D., Derras M., How to deal with multi-source data for tree detection based on deep learning, Proceedings of the GlobalSIP: Global Conference on Signal and Information Processing, (2017); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proceedings of the Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Li W., Fu H., You Y., Yu L., Fang J., Parallel Multiclass Support Vector Machine for Remote Sensing Data Classification on Multicore and Many-Core Architectures, IEEE J. Sel. Top. Appl. Earth Observ, 10, pp. 4387-4398, (2017); Fassnacht F.E., Latifi H., Sterenczak K., Modzelewska A., Lefsky M., Waser L.T., Straub C., Ghosh A., Review of studies on tree species classification from remotely sensed data, Remote Sens. Environ, 186, pp. 64-87, (2016); Dalponte M., Ene L.T., Marconcini M., Gobakken T., Naesset E., Semi-supervised SVM for individual tree crown species classification, ISPRS J. Photogramm, 110, pp. 77-87, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proceedings of the Advances in Neural Information Processing Systems, pp. 91-99, (2015)","H. Fu; Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, 100084, China; email: haohuan@tsinghua.edu.cn","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85059949711"
"Hashmi H.; Dwivedi R.; Kumar A.","Hashmi, Hina (57465018000); Dwivedi, Rakesh (57224182856); Kumar, Anil (57214420708)","57465018000; 57224182856; 57214420708","An AI & ML Based Detection & Identification in Remote Imagery: State-of-the-Art","2021","Journal of Automation, Mobile Robotics and Intelligent Systems","2021","4","","3","17","14","0","10.14313/JAMRIS/4-2021/22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139113834&doi=10.14313%2fJAMRIS%2f4-2021%2f22&partnerID=40&md5=6be9c96cdd3f3d785f46343fb1aa9c58","CCSIT, Teerthanker Mahaveer University, Moradabad, India; Indian Institute of Remote Sensing, Dehradun, India","Hashmi H., CCSIT, Teerthanker Mahaveer University, Moradabad, India; Dwivedi R., CCSIT, Teerthanker Mahaveer University, Moradabad, India; Kumar A., Indian Institute of Remote Sensing, Dehradun, India","Remotely sensed images and their allied areas of application have been the charm for a long time among researchers. Remote imagery has a vast area in which it is serving and achieving milestones. From the past, after the advent of AL, ML, and DL-based computing, remote imagery is related techniques for processing and ana-lyzing are continuously growing and offering countless services like traffic surveillance, earth observation, land surveying, and other agricultural areas. As Artificial intelligence has become the charm of researchers, machine learning and deep learning have been proven as the most commonly used and highly effective techniques for object detection. AI & ML-based object segmentation & detection makes this area hot and fond to the researchers again with the opportunities of enhanced accuracy in the same. Several researchers have been proposed their works in the form of research papers to highlight the effectiveness of using remotely sensed imagery for commercial purposes. In this article, we have discussed the concept of remote imagery with some preprocess-ing techniques to extract hidden and fruitful information from them. Deep learning techniques applied by various researchers along with object detection, object recognition are also discussed here. This literature survey is also included a chronological review of work done related to detection and recognition using deep learning techniques. © The Authors.","Artificial Intelligence; Convolutional Neural Network; Deep Learning; Feature Extraction; Machine Learning; Object Detection; Remote Sensed Imagery","","","","","","","","Yao Q., Hu X., Lei H., Multiscale Convolu-tional Neural Networks for Geospatial Object Detection in VHR Satellite Images, IEEE Geo-science and Remote Sensing Letters, 18, 1, pp. 23-27, (2021); Mountrakis G., Im J., Ogole C., Support vec-tor machines in remote sensing: A review, ISPRS Journal of Photogrammetry and Remote Sensing, 66, 3, pp. 247-259, (2011); Yao X., Feng X., Han J., Cheng G., Guo L., Au-tomatic Weakly Supervised Object Detection From High Spatial Resolution Remote Sensing Images via Dynamic Curriculum Learning, IEEE Transactions on Geoscience and Remote Sen-sing, 59, 1, pp. 675-685, (2021); Li L., Zhou Z., Wang B., Miao L., Zong H., A Novel CNN-Based Method for Accurate Ship Detection in HR Optical Remote Sensing Images via Rotated Bounding Box, IEEE Transactions on Geoscience and Remote Sensing, 59, 1, pp. 686-699, (2021); Matsuyama T., Knowledge-Based Aerial Image Understanding Systems and Expert Systems for Image Processing, IEEE Transactions on Geo-science and Remote Sensing, GE-25, 3, pp. 305-316, (1987); Garnesson P., Giraudon G., Montesinos P., An image analysis, application for aerial imagery interpretation, Proc. 10th International Conference on Pattern Recognition, 1, pp. 210-212, (1990); Ionescu D., Geling G., Automatic detection of large object features from SAR data, Proc. IGARSS ‘93-IEEE International Geoscience and Remote Sensing Symposium, pp. 1225-1227, (1993); Coppin P. R., Bauer M. E., Processing of multitemporal Landsat TM imagery to opti-mize extraction of forest cover change featu-res, IEEE Transactions on Geoscience and Remote Sensing, 32, 4, pp. 918-927, (1994); Mandal D. P., Murthy C. A., Pal S. K., Analysis of IRS imagery for detecting man-made objects with a multivalued recognition system, IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 26, 2, pp. 241-247, (1996); Ren Hsuan, Chang Chein-I, A computer--aided detection and classification method for concealed targets in hyperspectral imagery, IGARSS ‘98. Sensing and Managing the Environ-ment. 1998 IEEE International Geoscience and Remote Sensing. Symposium Proceedings, pp. 1016-1018, (1998); Shufelt J. A., Performance evaluation and analysis of monocular building extraction from aerial imagery, IEEE Transactions on Pattern Analysis and Machine Intelligence, 21, 4, pp. 311-326, (1999); Shanks J. G., Shetler B. V., Confronting clo-uds: detection, remediation and simulation approaches for hyperspectral remote sensing sys-tems, Proc. 29th Applied Imagery Pattern Recognition Workshop, pp. 25-31, (2000); Haithcoat T. L., Song W., Hipple J. D., Buil-ding footprint extraction and 3-D reconstruc-tion from LIDAR data, IEEE/ISPRS Joint Workshop on Remote Sensing and Data Fusion over Urban Areas, pp. 74-78, (2001); Chen K., Blong R., Extracting building features from high resolution aerial imagery for natural hazards risk assessment, IEEE International Geoscience and Remote Sensing Symposium, 4, pp. 2039-2041, (2002); Duan J., Prinet V., Lu H., Building extraction in urban areas from satellite images using GIS data as prior information, Proc. IEEE International Geoscience and Remote Sensing Sym-posium, 2004. IGARSS ‘04, 7, pp. 4762-4764, (2004); Dongmei Yan, Zhongming Zhao, Zhong Chen, A fused road detection approach in high resolution multi-spectrum remote sensing ima-gery, Proc. 2005 IEEE International Geo-science and Remote Sensing Symposium, 2005. IGARSS ‘05, 3, pp. 1557-1560, (2005); Secord J., Zakhor A., Tree Detection in Urban Regions Using Aerial Lidar and Image Data, IEEE Geoscience and Remote Sensing Letters, 4, 2, pp. 196-200, (2007); Chaudhuri D., Samal A., An Automatic Brid-ge Detection Technique for Multispectral Ima-ges, IEEE Transactions on Geoscience and Remote Sensing, 46, 9, pp. 2720-2727, (2008); Paes R. L., Lorenzzetti J. A., Gherar-di D. F. M., Ship Detection Using TerraSAR-X Images in the Campos Basin (Brazil), IEEE Geoscience and Remote Sensing Letters, 7, 3, pp. 545-548, (2010); Grant C. S., Moon T. K., Gunther J. H., Stites M. R., Williams G. P., Detection of Amorphously Shaped Objects Using Spatial Information Detection Enhancement (SIDE), IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 5, 2, pp. 478-487, (2012); Sevo I., Avramovic A., Convolutional Neural Network Based Automatic Object Detection on Aerial Images, IEEE Geoscience and Remote Sensing Letters, 13, 5, pp. 740-744, (2016); Deng Z., Lei L., Sun H., Zou H., Zhou S., Zhao J., An enhanced deep convolutional neural network for densely packed objects detection in remote sensing images, 2017 International Workshop on Remote Sensing with Intelligent Processing (RSIP), pp. 1-4, (2017); Farooq A., Hu J., Jia X., Efficient object pro-posals extraction for target detection in VHR remote sensing images, 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3337-3340, (2017); Li Y., Jiao L., Tang X., Zhang X., Zhang W., Gao L., Weak Moving Object Detection In Optical Remote Sensing Video With Motion-Drive Fusion Network, IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5476-5479, (2019); Sui B., Xu M., Gao F., Patch-Based Three-Sta-ge Aggregation Network for Object Detection in High Resolution Remote Sensing Images, IEEE Access, 8, pp. 184934-184944, (2020); Guo J., Yang J., Yue H., Tan H., Hou C., Li K., CDnetV2: CNN-Based Cloud Detection for Remote Sensing Imagery With Cloud-Snow Coexi-stence, IEEE Transactions on Geoscience and Remote Sensing, 59, 1, pp. 700-713, (2021); Han Y., Ma S., Xu Y., He L., Li S., Zhu M., Ef-fective Complex Airport Object Detection in Remote Sensing Images Based on Improved End--to-End Convolutional Neural Network, IEEE Access, 8, pp. 172652-172663, (2020); Yang J., Guo J., Yue H., Liu Z., Hu H., Li K., CDnet: CNN-Based Cloud Detection for Remote Sensing Imagery, IEEE Transactions on Geo-science and Remote Sensing, 57, 8, pp. 6195-6211, (2019); Rao Z., He M., Dai Y., Class Attention Network for Semantic Segmentation of Remote Sensing Images, 2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pp. 150-155, (2020); Sharma P., Ding N., Goodman S., Soricut R., Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset for Automatic Image Cap-tioning, Proceedings of the 56th Annual Me-eting of the Association for Computational Lingu-istics (Volume 1: Long Papers), pp. 2556-2565, (2018); Sisodia A., Swati and H. Hashmi, “Incorporation of Non-Fictional Applications in Wireless Sensor Networks, International Journal of Innovative Technology and Exploring Engineering, 9, 11, pp. 42-49, (2020); Hashmi H., Dwivedi R. K., Kumar A., Identifi-cation of Objects using AI & ML Approaches: Sta-te-of-the-Art, 2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART), pp. 1-5, (2021); Kumar A., Hashmi H., Khan S. A., Kazim Naqvi S., SSE: A Smart Framework for Live Video Streaming based Alerting System, 2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART), pp. 193-197, (2021); Belongie S., Malik J., Puzicha J., Shape mat-ching and object recognition using shape conte-xts, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24, 4, pp. 509-522, (2002); Yu D., Guo H., Xu Q., Lu J., Zhao C., Lin Y., Hie-rarchical Attention and Bilinear Fusion for Remote Sensing Image Scene Classification, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13, pp. 6372-6383, (2020); Xue D., Lei T., Jia X., Wang X., Chen T., Nan-di A. K., Unsupervised Change Detection Using Mul-tiscale and Multiresolution Gaussian-Mixture--Model Guided by Saliency Enhancement, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14, pp. 1796-1809, (2021); Peer P., Batagelj B., Computer vision in con-temporary art (introduction to the special ses- sion), 2008 50th International Symposium EL-MAR, 2, pp. 471-474, (2008); Haralick R. M., Kelly G. L., Pattern recognition with measurement space and spatial clustering for multiple images, Proc. of the IEEE, 57, 4, pp. 654-665, (1969); Anuta P. E., Spatial Registration of Multispectral and Multitemporal Digital Imagery Using Fast Fourier Transform Techniques, IEEE Transactions on Geoscience Electronics, 8, 4, pp. 353-368, (1970); Waite W. P., MacDonald H. C., Vegeta-tion Penetration” with K-Band Imaging Ra-dars, IEEE Transactions on Geoscience Elec-tronics, 9, 3, pp. 147-155, (1971); Nagy G., Digital image-processing activities in remote sensing for earth resources, Proc. of the IEEE, 60, 10, pp. 1177-1200, (1972); Roska T., Boros T., Thiran P., Chua L. O., De-tecting simple motion using cellular neural ne-tworks, IEEE International Workshop on Cellular Neural Networks and their Applications, pp. 127-138, (1990); Yang Xiou-Ping, Yang Tao, Yang Lin-Bao, Extracting focused object from defocused background using cellular neural networks, Proc. of the Third IEEE International Workshop on Cellular Neural Networks and Their Applications (CNNA-94), pp. 451-455, (1994); Tolluoglu O., Ucan O. M., Kent S., Kargin S., Edge detection in noise mounted picture, Proc. of the IEEE 12th Signal Processing and Communications Applications Conference, pp. 335-338, (2004); Grassi G., Di Sciascio E., Grieco A. L., Vec-chio P., A New Object-Oriented Segmentation Algorithm based on CNNs-Part I: Edge Extrac-tion, 2005 9th International Workshop on Cellular Neural Networks and Their Applications, pp. 158-161, (2005); Slot K., Korbel P., Gozdzik M., Kim H., Pat-tern detection in spectrograms by means of Cellular Neural Networks, 2006 10th International Workshop on Cellular Neural Networks and Their Applications, pp. 1-6, (2006); Takarli F., Aghagolzadeh A., Seyedarabi H., Robust pedestrian detection using low level and high level features, 2013 21st Iranian Conference on Electrical Engineering (ICEE), pp. 1-6, (2013); Thubsaeng W., Kawewong A., Patanukhom K., Vehicle logo detection using convolutional neural network and pyramid of histogram of oriented gradients, 2014 11th International Joint Conference on Computer Science and Software Engineering (JCSSE), pp. 34-39, (2014); Li Z., Wu Q., Cheng B., Cao L., Yang H., Re-mote Sensing Image Scene Classification Based on Object Relationship Reasoning CNN, IEEE Geoscience and Remote Sensing Letters, 19, pp. 1-5, (2022); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-Based Learning Applied to Document Recognition, Proc. of the IEEE, 86, pp. 2278-2324, (1998); Femin A., Biju K. S., Accurate Detection of Buildings from Satellite Images using CNN, 2020 International Conference on Elec-trical, Communication, and Computer Engineering (ICECCE), pp. 1-5, (2020); Marmanis D., Wegner J. D., Galliani S., Schin-dler K., Datcu M., Stilla U., Semantic Segmentation of Aerial Images With an Ensemble of CNNs, ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, III-3, pp. 473-480, (2016); Cheng L., Trenberth K. E., Palmer M. D., Zhu J., Abraham J. P., Observed and simulated full--depth ocean heat-content changes for 1970– 2005, Ocean Science, 12, 4, pp. 925-935, (2016); Kussul N., Lavreniuk M., Skakun S., She-lestov A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geoscience and Remote Sensing Letters, 14, 5, pp. 778-782, (2017); Elbakary M. I., Iftekharuddin K. M., Shadow Detection of Man-Made Buildings in High-Re-solution Panchromatic Satellite Images, IEEE Transactions on Geoscience and Remote Sen-sing, 52, 9, pp. 5374-5386, (2014); Zhang X., Noda S., Himeno R., Liu H., Car-diovascular disease-induced thermal responses during passive heat stress: an integrated computational study: An integrated bioheat transfer model, International Journal for Numerical Methods in Biomedical Engineering, 32, 11, pp. 5374-5386, (2016); Liu L., Ouyang W., Wang X., Fieguth P., Chen J., Liu X., Pietikainen M., Deep Learning for Generic Object Detection: A Survey, International Journal of Computer Vision, 128, 2, pp. 261-318, (2020); McCulloch W. S., Pitts W., A Logical Calculus of the Ideas Immanent in Nervous Activity, The Bulletin of Mathematical Biophysics, 5, 4, pp. 115-133, (1943); Hsieh Y.-C., Chin C.-L., Wei C.-S., Chen I.-M., Yeh P.-Y., Tseng R.-J., Combining VGG16, Mask R-CNN and Inception V3 to identify the benign and ma-lignant of breast microcalcification clusters, 2020 International Conference on Fuzzy Theory and Its Applications (iFUZZY), (2020); Hinton G. E., Osindero S., Teh Y.-W., A Fast Learning Algorithm for Deep Belief Nets, Neural Computation, 18, 7, pp. 1527-1554, (2006); Dumoulin V., Goodfellow I., Courville A., Bengio Y., On the Challenges of Physical Imple-mentations of RBMs, Proceedings of the AAAI Conference on Artificial Intelligence, 28, 1, (2014); Russakovsky O., Deng J., Su H., Krause J., Sa-theesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A. C., Fei-Fei L., ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); Castelluccio G. M., McDowell D. L., Micro-structure and mesh sensitivities of mesoscale surrogate driving force measures for transgra-nular fatigue cracks in polycrystals, Materials Science and Engineering: A, 639, pp. 626-639, (2015); Krizhevsky A., Sutskever I., Hinton G. E., Ima-geNet classification with deep convolutional neural networks, Communications of the ACM, 60, 6, pp. 84-90, (2017); Tian L., Noore A., Software Reliability Pre-diction Using Recurrent Neural Network with Bayesian Regularization, International Journal of Neural Systems, 14, 3, pp. 165-174, (2004); Benamira A., Devillers B., Lesot E., Ray A. K., Saadi M., Malliaros F. D., Semi-Supervised Learning and Graph Neural Networks for Fake News Detection, Proceedings of the 2019 IEEE/ACM International Conference on Advan-ces in Social Networks Analysis and Mining, pp. 568-569, (2019); Zhou X., Koch M. W., Roberts M. W., A Se-lective Attention Neural Network for Invariant Recognition of Distorted Objects, IJCNN--91-Seattle International Joint Conference on Neural Networks, ii, (1991); Fu L., Zhang D., Ye Q., Recurrent Thrifty Attention Network for Remote Sensing Scene Re-cognition, IEEE Transactions on Geoscience and Remote Sensing, 59, 10, pp. 8257-8268, (2021); Larionov R., Khryashchev V., Pavlov V., Se-paration of Closely Located Buildings on Aerial Images Using U-Net Neural Network, 2020 26th Conference of Open Innovations Association (FRUCT), pp. 256-261, (2020)","H. Hashmi; CCSIT, Teerthanker Mahaveer University, Moradabad, India; email: hinahashmi170@gmail.com","","Industrial Research Institute for Automation and Measurements","","","","","","18978649","","","","English","J. Automation, Mob. Robot. Intell. Syst.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139113834"
"Sarwar F.; Griffin A.; Rehman S.U.; Pasang T.","Sarwar, Farah (57224466004); Griffin, Anthony (55708552200); Rehman, Saeed Ur (57193652261); Pasang, Timotius (56962784100)","57224466004; 55708552200; 57193652261; 56962784100","Detecting sheep in UAV images","2021","Computers and Electronics in Agriculture","187","","106219","","","","10","10.1016/j.compag.2021.106219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107703965&doi=10.1016%2fj.compag.2021.106219&partnerID=40&md5=0f7aab1056c9681faa50d851862bfca1","Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; College of Science and Engineering, Flinders University, South Australia, Australia; Department of Manufacturing and Mechanical Engineering and Technology, Oregon Institute of Technology, Oregon, United States","Sarwar F., Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; Griffin A., Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; Rehman S.U., College of Science and Engineering, Flinders University, South Australia, Australia; Pasang T., Department of Manufacturing and Mechanical Engineering and Technology, Oregon Institute of Technology, Oregon, United States","In the last decade, researchers have focused more on deep convolutional neural networks (CNNs) than other machine learning algorithms for object detection, localization, classification and segmentation. Such CNNs have achieved remarkable results in these fields and use the bounding boxes as the ground truth data. In this research article, we have used a fully connected network (FCN) for livestock detection in aerial images captured by an unmanned aerial vehicle (UAV), that used centroids as ground truth data. For performance evaluation and comparison, we have proposed a single-layered and a seven-layered CNN network in this article. These proposed networks are trained using state-of-the-art method, Region-based CNN. In addition, AlexNet, GoogLeNet, VGG16, VGG19 and ResNet50 were also fine-tuned for livestock detection. The results of the FCN and one of our proposed networks are then merged to improve the recall of the complete system from 90% to 98%. © 2021","Deep learning; Livestock; Object detection; UAV","Agriculture; Aircraft detection; Antennas; Deep neural networks; Learning algorithms; Network layers; Object recognition; Unmanned aerial vehicles (UAV); Aerial vehicle; Convolutional neural network; Deep learning; Fully connected networks; Ground truth data; Machine learning algorithms; Object localization; Objects detection; Objects segmentation; Vehicle images; algorithm; artificial neural network; detection method; image analysis; machine learning; sheep; unmanned vehicle; Object detection","","","","","","","(2016); Anderson K., Gaston K.J., Lightweight unmanned aerial vehicles will revolutionize spatial ecology, Front. Ecol. Environ., 11, 3, pp. 138-146, (2013); Avtar R., Suab S.A., Yunus A.P., Kumar P., Srivastava P.K., Ramaiah M., Juan C.A., Applications of UAVs in plantation health and area management in Malaysia, Unmanned Aerial Vehicle: Applications in Agriculture and Environment, pp. 85-100, (2020); Bai Y., Zhang Y., Ding M., Ghanem B., SOD-MTGAN: Small object detection via multi-task generative adversarial network, The European Conference on Computer Vision (ECCV), (2018); Barbedo J.G.A., Koenigkan L.V., Perspectives on the use of unmanned aerial systems to monitor cattle, Outlook Agric., 47, 3, pp. 214-222, (2018); Blight L.K., Bertram D.F., Kroc E., Evaluating UAV-based techniques to census an urban-nesting gull population on Canada's Pacific coast, J. Unmanned Vehicle Syst., 7, 4, pp. 312-324, (2019); Bo L., Hai T., Qiang F., A small object detection method based on local maxima and SSD, AOPC 2019: AI in Optics and Photonics, 11342, (2019); Burghardt T., Calic J., Real-time face detection and tracking of animals, 8th Seminar on Neural Network Applications in Electrical Engineering, pp. 27-32, (2006); Burry M., Drones and dog combo prove efficient for farmer, (2018); Burry M., (2019); Chamoso P., Raveane W., Parra V., Gonzalez A., UAVs applied to the counting and monitoring of animals, Ambient Intelligence-Software and Applications, pp. 71-80, (2014); Chao L., Ding Y.-D., Wang D.-B., Dense-SSD for detecting small objects, (2018); Chen C., Liu M.-Y., Tuzel O., Xiao J., R-CNN for small object detection, Asian Conference on Computer Vision, pp. 214-230, (2016); Dang-Ngoc H., Nguyen-Trung H., Evaluation of forest fire detection model using video captured by UAVs, 19th International Symposium on Communications and Information Technologies (ISCIT), pp. 513-518, (2019); Desai U.B., Merchant S.N., Zaveri M., Ajishna G., Purohit M., Phanish H., Small object detection and tracking: Algorithm, analysis and application, International Conference on Pattern Recognition and Machine Intelligence, pp. 108-117, (2005); Di Perna M., Rodrigues L., A UAV software flight management system using arinc communication protocols, IEEE Aerosp. Electron. Syst. Mag., 33, 9, pp. 18-28, (2018); Du Z., Yin J., Yang J., 1314, (2019); Eggert C., Brehm S., Winschel A., Zecha D., Lienhart R., A closer look: Small object detection in faster R-CNN, IEEE International Conference on Multimedia and Expo (ICME), pp. 421-426, (2017); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Handcock R., Swain D., Bishop-Hurley G., Patison K., Wark T., Valencia P., Corke P., O'Neill C., Monitoring animal behaviour and environmental interactions using wireless sensor networks, GPS collars and satellite remote sensing, Sensors, 9, 5, pp. 3586-3603, (2009); Han L., Tao P., Martin R.R., Livestock detection in aerial images using a fully convolutional network, Comput. Visual Media, 5, 2, pp. 221-228, (2019); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Hein D., Kraft T., Brauchle J., Berger R., Integrated UAV-based real-time mapping for security applications, ISPRS Int. J. Geo-Informat., 8, 5, (2019); Hogan S., Kelly M., Stark B., Chen Y., Et al., Unmanned aerial systems for agriculture and natural resources, Calif. Agric., 71, 1, pp. 5-14, (2017); Hu G.X., Yang Z., Hu L., Huang L., Han J.M., Small object detection with multiscale features, (2018); Kassler M., (2001); Kawamura K., Lim J., Kurokawa Y., Obitsu T., Yayota M., Ogura S., Monitoring spatial heterogeneity of pasture within paddock scale using a small unanned aerial vehicle sUAV, J. Integrated Field Sci, 14, pp. 61-66, (2017); Kong T., Yao A., Chen Y., Sun F., Hypernet: Towards accurate region proposal generation and joint object detection, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 845-853, (2016); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Li J., Liang X., Wei Y., Xu T., Feng J., Yan S., Perceptual generative adversarial networks for small object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1222-1230, (2017); Li W., Liu K., Yan L., Cheng F., Lv Y., Zhang L., FRD-CNN: Object detection based on small-scale convolutional neural networks and feature reuse, Sci. Rep., 9, pp. 1-12, (2019); Lim J.-S., Astrid M., Yoon H.-J., Lee S., (2019); Luo H.-W., Zhang C.-S., Pan F.-C., Ju X.-M., Contextual-YOLOV3: implement better small object detection based deep learning, International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI), pp. 134-141, (2019); Ma D.W., Wu X.J., Yang H., Efficient small object detection with an improved region proposal networks, IOP Conference Series: Materials Science and Engineering, 533, (2019); Meng W., Jin T., Zhao X., Adaptive method of dim small object detection with heavy clutter, Appl. Opt., 52, 10, pp. D64-D74, (2013); Mufford J.T., Hill D.J., Flood N.J., Church J.S., Use of unmanned aerial vehicles UAVs and photogrammetric image analysis to quantify spatial proximity in beef cattle, J. Unmanned Vehicle Syst., (2019); Mutalib A.H.A., Ruppert N., Akmar S., Kamaruszaman F.F.J., Rosley N.F.N., Feasibility of thermal imaging using unmanned aerial vehicles to detect bornean orangutans, J. Sustainability Sci. Manage., 14, 5, pp. 182-194, (2019); Nasi R., Honkavaara E., Lyytikainen-Saarenmaa P., Blomqvist M., Litkey P., Hakala T., Viljanen N., Kantola T., Tanhuanpaa T., Holopainen M., Using UAV-based photogrammetry and hyperspectral imaging for mapping bark beetle damage at tree-level, Remote Sensing, 7, 11, pp. 15467-15493, (2015); Nawaz H., Ali H.M., Massan S., pp. 85-105, (2019); Noh J., Bae W., Lee W., Seo J., Kim G., Better to follow, follow to be better: Towards precise supervision of feature super-resolution for small object detection, Proceedings of the IEEE International Conference on Computer Vision, pp. 9725-9734, (2019); Parikh M., Patel M., Bhatt D., Animal detection using template matching algorithm, Int. J. Res. Modern Eng. Emerg. Technol., 1, 3, pp. 26-32, (2013); Portas K.; Rahnemoonfar M., Dobbs D., Yari M., Starek M.J., DisCountNet: Discriminating and counting network for real-time counting and localization of sparse objects in high-resolution UAV imagery, Remote Sensing, 11, 9, (2019); Razaak M., Kerdegari H., Argyriou V., Remagnino P., Multi-scale feature fused single shot detector for small object detection in UAV images, International Conference on Computer Vision Systems, pp. 778-786, (2019); Redmon J., Farhadi A., (2018); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Ren Y., Zhu C., Xiao S., Small object detection in optical remote sensing images via modified Faster R-CNN, Appl. Sci., 8, 5, (2018); Ribera J., Guera D., Chen Y., Delp Edward J.D., Weighted Hausdorff distance: A loss function for object localization, (2018); Ribera J., Guera D., Chen Y., Delp E.J., Locating objects without bounding boxes, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2019); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Medical Image Computing and Computer-Assisted Intervention (MICCAI), pp. 234-241, (2015); Sarwar F., Griffin A., Periasamy P., Portas K., Law J., Detecting and counting sheep with a convolutional neural network, 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), (2018); Sarwar F., Griffin A., (2020); Scherer J., Yahyanejad S., Hayat S., Yanmaz E., Andre T., Khan A., Vukadinovic V., Bettstetter C., Hellwagner H., Rinner B., pp. 33-38, (2015); Simonyan K., Zisserman A., (2014); Singh A., Pietrasik M., Natha G., Ghouaiel N., Brizel K., Ray N., (2019); Sun Z., Wang D., Zhong G., Extraction of farmland geographic information using OpenStreetMap data, 7th International Conference on Agro-geoinformatics (Agro-geoinformatics), pp. 1-4, (2018); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); van Gemert J.C., Verschoor C.R., Mettes P., Epema K., Koh L.P., Wich S., Nature conservation drones for automatic localization and counting of animals, Workshop at the European Conference on Computer Vision, pp. 255-270, (2014); Vard A., Jamshidi K., Movahhedinia N., Small object detection in cluttered image using a correlation based active contour model, Pattern Recogn. Lett., 33, 5, pp. 543-553, (2012); Wichmann F.A., Drewes J., Rosas P., Gegenfurtner K.R., Animal detection in natural scenes: critical features revisited, J. Vision, 10, 4, pp. 1-27, (2010); Witczuk J., Pagacz S., Zmarz A., Cypel M., Exploring the feasibility of unmanned aerial vehicles and thermal imaging for ungulate surveys in forests - preliminary results, Int. J. Remote Sensing, 39, 15-16, pp. 5504-5521, (2018); Wu W., Qurishee M.A., Owino J., Fomunung I., Onyango M., Atolagbe B., Coupling deep learning and UAV for infrastructure condition assessment automation, 2018 IEEE International Smart Cities Conference, ISC2, pp. 1-7, (2018); Xu P., Xu W., Luo Y., Zhao Z., Et al., Precise classification of cultivated land based on visible remote sensing image of UAV, J. Agric. Sci. Technol. (Beijing), 21, 6, pp. 79-86, (2019); Yang T., Li Z., Zhang F., Xie B., Li J., Liu L., Panoramic UAV surveillance and recycling system based on structure-free camera array, IEEE Access, 7, pp. 25763-25778, (2019)","A. Griffin; Electrical & Electronic Engineering Department, Auckland University of Technology, Auckland, New Zealand; email: anthony.griffin@aut.ac.nz","","Elsevier B.V.","","","","","","01681699","","CEAGE","","English","Comput. Electron. Agric.","Article","Final","","Scopus","2-s2.0-85107703965"
"Singh M.; Govil M.C.; Pilli E.S.","Singh, Maheep (57189378218); Govil, M.C. (35812070200); Pilli, E.S. (35756126100)","57189378218; 35812070200; 35756126100","V-SIN: Visual Saliency detection in noisy Images using convolutional neural Network","2018","2018 Conference on Information and Communication Technology, CICT 2018","","","8722431","","","","0","10.1109/INFOCOMTECH.2018.8722431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067108294&doi=10.1109%2fINFOCOMTECH.2018.8722431&partnerID=40&md5=8b90d5ac8573af8e24bffc8f30cb21e7","Department Computer Science and Engineering, MNIT, Jaipur, India","Singh M., Department Computer Science and Engineering, MNIT, Jaipur, India; Govil M.C., Department Computer Science and Engineering, MNIT, Jaipur, India; Pilli E.S., Department Computer Science and Engineering, MNIT, Jaipur, India","In the Computer era, the capability of a machine to differentiate salient object from the background has became a critical fact in the domain of Computer Vision. The features in the noisy images get greatly compromised, owing to which the Salient Object Detection (SOD) is difficult. Moreover, the existing research has not yet been matched the performance of humans for detecting the visual saliency in noisy environment. Therefore, this work highlights a novel SOD technique in noisy environment using convolutional neural network (CNN), while the salient object detection accuracy has been well maintained. The denoising of the image is performed using CNN which comprises the coordinate descent as a regularizing function. The performance of our proposed V-SIN technique has been assessed with four evaluation parameters, computing time, recall, precision, and F-measure on two publicly available image datasets. The experimental evaluations on these two dataset shows that the proposed model has been much robust to detect salient object in the presence of noise or mixture of noises in images. © 2018 IEEE.","Coordinate Descent; Deep Learning; Image Denoising; Salient object","Convolution; Deep learning; Neural networks; Object detection; Object recognition; Visualization; Convolutional neural network; Coordinate descent; Evaluation parameters; Experimental evaluation; Noisy environment; Salient object detection; Salient objects; Visual saliency detections; Image denoising","","","","","","","Borji A., Cheng M.M., Jiang H., Li J., Salient object detection: A benchmark, IEEE Transactions on Image Processing, 24, 12, pp. 5706-5722, (2015); Borji A., Itti L., State-of-the-art in visual attention modeling, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 1, pp. 185-207, (2013); Graefe V., Efenberger W., A novel approach for the detection of vehicles on freeways by real-time vision, Intelligent Vehicles Symposium, 1996., Proceedings of the 1996 IEEE. IEEE, (1996); Li Z., Itti L., Saliency and GIST features for target detection in satellite images, IEEE Transactions on Image Processing, 20, 7, pp. 2017-2029, (2011); Gonzalez, Et al., Digital Image Processing, (2002); Krishan, Et al., ESUMM: Event SUMMarization on scale-free network, IETE Technical Review, pp. 1-11, (2018); Sharma, Et al., A-PNR: Automatic plate number recognition, The 7th ACM International Conference on Computer and Communication Technology, ICCCT'17, pp. 1-6, (2017); Krishan, Et al., Deep event learning boost-up approach: Delta, Multimedia Tools and Applications, pp. 1-21, (2018); Anurag, Et al., A novel superpixel based color spatial feature for salient object detection, The Conference on Information and Communication Technology CICT'17, pp. 1-6, (2017); Kumar, Et al., Event BAGGING: A novel event summarization approach in multi-view surveillance videos, IEEE International Conference on Innovations in Electronics, Signal Processing and Communication, IESC'17, pp. 1-6, (2017); Shikhar, Et al., D-fes: Deep facial expression recognition system, The Conference on Information and Communication Technology CICT'17, pp. 1-6, (2017); Shikhar, Et al., GUESS: Genetic uses in video encryption with secret sharing, CVIP, pp. 1-6, (2017); Kumar K., Et al., Eratosthenes sieve based key-frame extraction technique for event summarization in videos, Multimedia Tools and Applications, pp. 1-22, (2017); Kumar K., Et al., F-DES: Fast and deep event summarization, IEEE Transactions on Multimedia, 20, 2, pp. 323-334, (2018); Piyushi, Et al., V-SEE: Video Secret sharing Encryption techniquE, CICT, (2017); Krishan K., Et al., Equal partition based clustering approach for event summarization in videos, IEEE (SITIS), 2016 12th International Conference on, pp. 119-126, (2016); Kumar, Et al., D-CAD: Deep and crowded anomaly detection, The 7th ACM International Conference on Computer and Communication Technology, ICCCT'17, pp. 1-6, (2017); Itti L., Koch C., Niebur E., A model of saliency-based visual attention for rapid scene analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, 20, 11, pp. 1254-1259, (1998); Le Meur O., Et al., A coherent computational approach to model bottom-up visual attention, IEEE Transactions on Pattern Analysis and Machine Intelligence, 28, 5, pp. 802-817, (2006); Liu T., Et al., Learning to detect a salient object, IEEE Computer Vision and Pattern Recognition CVPR'07, pp. 1-8, (2007); Xie Y., Lu H., Yang M.H., Bayesian saliency via low and mid level cues, IEEE Transactions on Image Processing, 22, 5, pp. 1689-1698, (2013); Singh N., Arya R., Agrawal R.K., A novel approach to combine features for salient object detection using constrained particle swarm optimization, Pattern Recognition, 47, 4, pp. 1731-1739, (2014); Singh M., Govil M.C., Pilli E.S., CHACT: Convex Hull Enabled Active Contour Technique for Salient Object Detection, IEEE Access, 6, pp. 22441-22451, (2018); Kim C., Milanfar P., Visual saliency in noisy images, Journal of Vision, 13, 4, (2013); Liu Z., Zou W., Le Meur O., Saliency tree: A novel saliency detection framework, IEEE Transactions on Image Processing, 23, 5, pp. 1937-1952, (2014); Singh N., Agrawal R.K., Combination of KullbackLeibler divergence and Manhattan distance measures to detect salient objects, Signal, Image and Video Processing, 9, 2, pp. 427-435, (2015); Singh N., Arya R., Agrawal R.K., Performance enhancement of salient object detection using superpixel based Gaussian mixture model, Multimedia Tools and Applications, pp. 1-19, (2017); Singh N., Arya R., Agrawal R.K., A novel position prior using fusion of rule of thirds and image center for salient object detection, Multimedia Tools and Applications, 76, 8, pp. 10521-10538, (2017); Arya R., Singh N., Agrawal R.K., A novel hybrid approach for salient object de-tection using local and global saliency in frequency domain, Multimed. Tools Appl; Fu H., Cao X., Tu Z., Cluster-based co-saliency detection, IEEE Transactions on Image Processing, 22, 10, pp. 3766-3778, (2013); Harris C., Stephens M., A combined corner and edge detector, Alvey Vision Conference, 15, 50, pp. 10-5244, (1988); Chan T.F., Vese L.A., Active contours without edges, IEEE Transactions on Image Processing, 10, 2, pp. 266-277, (2001); Kass M., Witkin A., Terzopoulos D., Snakes: Active contour models, International Journal of Computer Vision, 1, 4, pp. 321-331, (1988); Le, Et al., GEMINI: Gradient estimation through matrix inversion after noise injection, Advances in Neural Information Processing Systems, pp. 141-148, (1989); Hinton, Et al., A fast learning algorithm for deep belief nets, Neural Computation, 18, 7, pp. 1527-1554, (2006); McGaffin, Et al., Edge-preserving image denoising via group coordinate descent on the GPU, TIP, 24, 4, pp. 1273-1281, (2015); Kingma, Et al., Adam: A Method for Stochastic Optimization, (2014)","","","Institute of Electrical and Electronics Engineers Inc.","BrahMos Aerospace; M.P. Council of Science and Technology; PDM Indian Institute of Information Technology, Design and Manufacturing Jabalpur","2018 Conference on Information and Communication Technology, CICT 2018","26 October 2018 through 28 October 2018","Jabalpur","148480","","978-153868215-9","","","English","Conf. Inf. Commun. Technol., CICT","Conference paper","Final","","Scopus","2-s2.0-85067108294"
"Wesley A.M.; Matisziw T.C.","Wesley, Aaron M. (57226304455); Matisziw, Timothy C. (7801654268)","57226304455; 7801654268","Methods for measuring geodiversity in large overhead imagery datasets","2021","IEEE Access","9","","9478876","100279","100293","14","0","10.1109/ACCESS.2021.3096034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111125883&doi=10.1109%2fACCESS.2021.3096034&partnerID=40&md5=5452748b098a975b62c0762d416eacf4","Institute for Data Science and Informatics, University of Missouri, Columbia, 65211, MO, United States; National Geospatial-Intelligence Agency, Saint Louis, 63118, MO, United States; Department of Civil and Environmental Engineering, University of Missouri, Columbia, 65211, MO, United States; Department of Geography, University of Missouri, Columbia, 65211, MO, United States","Wesley A.M., Institute for Data Science and Informatics, University of Missouri, Columbia, 65211, MO, United States, National Geospatial-Intelligence Agency, Saint Louis, 63118, MO, United States; Matisziw T.C., Institute for Data Science and Informatics, University of Missouri, Columbia, 65211, MO, United States, Department of Civil and Environmental Engineering, University of Missouri, Columbia, 65211, MO, United States, Department of Geography, University of Missouri, Columbia, 65211, MO, United States","Geographic variation in the appearance of objects on Earth is readily observable in remotely sensed imagery (RSI) and somewhat intuitive to understand for most people - many classes of objects (houses, vehicles, crop fields etc.) simply look different depending on their location. This variation has recently been shown to have important implications when training machine learning models on geotagged image datasets for specific object detection and classification tasks. For example, models trained on datasets with ethnocentric biases in image content have been shown to misclassify objects in under-sampled regions, particularly in least-developed countries. The need to evaluate the growing corpus of RSI datasets for representativeness, heterogeneity and geodiversity is therefore high; yet scalable methods for measuring these concepts are absent in the remote sensing domain. This paper introduces the first dataset analysis methods for detecting and assessing geodiversity problems in large RSI datasets, based on geospatial adaptations of the Fréchet Inception Distance and Inception Score in the deep learning framework. Geospatial Fréchet Distance is proposed as a dissimilarity measure for image features of an object class across geographic regions - useful for comparing differences in object class appearance in different locations and/or spatial scales. A complementary Geospatial Inception Score is proposed to quantify heterogeneity of geographic context present in dataset labels within particular regions/locations, taking into account the labels themselves as well as their immediate surroundings. Rigorous tests of these methods on simulated RSI datasets demonstrate their stability, sensitivity, and the broad range of dataset analyses to which they can be applied. © 2013 IEEE.","Big geodata; Deep convolutional neural networks; Explainable ai; Geographic domain shift; GIS-remote sensing fusion; Imagery interpretation; Spatial data analysis","Classification (of information); Deep learning; Learning systems; Object detection; Remote sensing; Scalability; Statistical tests; Classification tasks; Dissimilarity measures; Geographic contexts; Geographic variation; Learning frameworks; Least-developed countries; Remotely sensed imagery; Training machines; Large dataset","","","","","","","Piaget J., Child's Conception of Space: Selected Works, 4, (2013); Marr D., The philosophy and the approach, Vision. San Francisco, (1982); Ullman E.L., Boyce R.R., Geography As Spatial Interaction, (1980); Klapka P., Halas M., Conceptualising patterns of spatial flows: Five decades of advances in the definition and use of functional regions, Moravian Geograph. Rep., 24, 2, pp. 2-11, (2016); Morrill R.L., The Spatial Organization of Society, (1970); Golledge R.G., Spatial Behavior: A Geographic Perspective, (1997); Lengen C., Kistemann T., Sense of place and place identity: Review of neuroscientific evidence, Health Place, 18, 5, pp. 1162-1171, (2012); Li M., Zang S., Zhang B., Li S., Wu C., A review of remote sensing image classification techniques: The role of spatio-contextual information, Eur. J. Remote Sens., 47, 1, pp. 389-411, (2014); Blaschke T., Object based image analysis for remote sensing, Isprs J. Photogram. Remote Sens., 65, 1, pp. 2-16, (2010); Tobler W.R., A computer movie simulating urban growth in the detroit region, Econ. Geogr., 46, 1, pp. 234-240, (1970); Goodchild M.F., Geographical data modeling, Comput. Geosci., 18, 4, pp. 401-408, (1992); Campbell N.W., Mackeown W.P.J., Thomas B.T., Troscianko T., Interpreting image databases by region classification, Pattern Recog-nit., 30, 4, pp. 555-563, (1997); Aizawa K., Sakaue K., Suenaga Y., Image Processing Technolo-gies: Algorithms, Sensors, and Applications, (2004); Rawat W., Wang Z., Deep convolutional neural networks for image classification: A comprehensive review, Neural Comput., 29, 9, pp. 2352-2449, (2017); Goodchild M., Citizens as sensors: The world of volunteered geography, GeoJournal, 69, 4, pp. 211-221, (2007); Schmidt G.B., Jettinghoff W.M., Using amazon mechanical turk and other compensated crowdsourcing sites, Bus. Horizons, 59, 4, pp. 391-400, (2016); Lam D., Kuzma R., McGee K., Dooley S., Laielli M., Klaric M., Bulatov Y., McCord B., XView: Objects in Context in Overhead Imagery, (2018); Van Etten A., Lindenbaum D., Bacastow T.M., SpaceNet: A Remote Sensing Dataset and Challenge Series, (2018); Marconi S., Graves S.J., Gong D., Nia M.S., Bras M.L., Dorr B.J., Fontana P., Gearhart J., Greenberg C., Harris D.J., Kumar S.A., Nishant A., Prarabdh J., Rege S.U., Bohlman S.A., White E.P., Wang D.Z., A data science challenge for converting airborne remote sensing data into ecological information, PeerJ, 6, (2019); Gupta R., Goodman B., Patel N., Hosfelt R., Sajeev S., Heim E., Doshi J., Lucas K., Choset H., Gaston M., Creating xBD: A dataset for assessing building damage from satellite imagery, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 10-17, (2019); Cardillo R., Small Satellites: Big Data, (2017); Deng X., Liu P., Liu X., Wang R., Zhang Y., He J., Yao Y., Geospatial big data: New paradigm of remote sensing applications, Ieee J. Sel. Topics Appl. Earth Observ. Remote Sens., 12, 10, pp. 3841-3851, (2019); Jiang Z., Shekhar S., Spatial Big Data Science: Classification Tech-niques for Earth Observation Imagery, (2017); Lee J.-G., Kang M., Geospatial big data: Challenges and opportunities, Big Data Res., 2, 2, pp. 74-81, (2015); Stryker T., Colohan P., The national plan for civil Earth observations, Office Sci. Technol. Policy, (2014); Doshi J., Basu S., Pang G., From Satellite Imagery to Disaster Insights, (2018); Weichenthal S., Hatzopoulou M., Brauer M., A picture tells a thousand exposures: Opportunities and challenges of deep learning image analyses in exposure science and environmental epidemiology, Environ. Int., 122, pp. 3-10, (2019); Hruka J., Adao T., Padua L., Marques P., Cunha A., Peres E., Sousa A., Morais R., Sousa J.J., Machine learning classification methods in hyperspectral data processing for agricultural applications, Proc. Int. Conf. Geoinform. Data Anal., pp. 137-141, (2018); Suel E., Polak J.W., Bennett J.E., Ezzati M., Measuring social, environmental and health inequalities using deep learning and street imagery, Sci. Rep., 9, 1, pp. 1-10, (2019); Maire F., Alvarez L.M., Hodgson A., Automating marine mammal detection in aerial images captured during wildlife surveys: A deep learning approach, Proc. Australas. Joint Conf. Artif. Intell. Canberra, pp. 379-385, (2015); Dufresne-Camaro C.-O., Chevalier F., Ahmed S.I., Computer vision applications and their ethical risks in the global south, Proc. Graph. Interface Conf., pp. 1-10, (2020); Hospedales T., Antoniou A., Micaelli P., Storkey A., Meta-learning in Neural Networks: A Survey, (2020); Eraslan G., Avsec, Gagneur J., Theis F.J., Deep learning: New computational modelling techniques for genomics, Nature Rev. Genet., 20, 7, pp. 389-403, (2019); Young T., Hazarika D., Poria S., Cambria E., Recent trends in deep learning based natural language processing, Ieee Comput. Intell. Mag., 13, 3, pp. 55-75, (2018); Zhang L., Li Y., Xiao X., Li X.-Y., Wang J., Zhou A., Li Q., CrowdBuy: Privacy-friendly image dataset purchasing via crowdsourcing, Proc. Ieee Conf. Comput. Commun. (INFOCOM), pp. 2735-2743, (2018); Zhao B., Tang S., Liu X., Zhang X., Chen W.-N., IronM: Privacypreserving reliability estimation of heterogeneous data for mobile crowdsensing, Ieee Internet Things J., 7, 6, pp. 5159-5170, (2020); Nguyen H., Kieu L.-M., Wen T., Cai C., Deep learning methods in transportation domain: A review, Iet Intell. Transp. Syst., 12, 9, pp. 998-1004, (2018); Sherman R.M., Salzberg S.L., Pan-genomics in the human genome era, Nature Rev. Genet., 21, pp. 243-254, (2020); Sirugo G., Williams S.M., Tishkoff S.A., The missing diversity in human genetic studies, Cell, 177, 1, pp. 26-31, (2019); Ballouz S., Dobin A., Gillis J.A., Is it time to change the reference genome?, Genome Biol., 20, 1, pp. 1-9, (2019); Dinan E., Fan A., Williams A., Urbanek J., Kiela D., Weston J., Queens Are Powerful Too: Mitigating Gender Bias in Dialogue Generation, (2019); Swinger N., De-Arteaga M., Heffernan N.T., Leiserson M.D., Kalai A.T., What are the biases in my word embedding?, Proc. AAAI/ACM Conf. AI, Ethics, Soc., pp. 305-311, (2019); Shankar S., Halpern Y., Breck E., Atwood J., Wilson J., Sculley D., No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World, (2017); De Vries T., Misra I., Wang C., Maaten Der L.Van, Does object recognition work for everyone?, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 52-59, (2019); Vodrahalli K., Li K., Malik J., Are All Training Examples Created Equal? An Empirical Study, (2018); Yang K., Qinami K., Fei-Fei L., Deng J., Russakovsky O., Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the ImageNet hierarchy, Proc. Conf. Fairness, Accountability, Transparency, pp. 547-558, (2020); Merler M., Ratha N., Feris R.S., Smith J.R., Diversity in Faces, (2019); Hecht B.J., Stephens M., A tale of cities: Urban biases in volunteered geographic information, Proc. Icwsm, 14, 14, pp. 197-205, (2014); Zhang G., Zhu A.-X., The representativeness and spatial bias of volunteered geographic information: A review, Ann. Gis, 24, 3, pp. 151-162, (2018); Kitchin R., Big data and human geography: Opportunities, challenges and risks, Dialogues Hum. Geogr., 3, 3, pp. 262-267, (2013); Vinuesa R., Azizpour H., Leite I., Balaam M., Dignum V., Domisch S., Fellander A., Langhans S.D., Tegmark M., Nerini F.F., The role of artificial intelligence in achieving the sustainable development goals, Nature Commun., 11, 1, pp. 1-10, (2020); Salem T., Workman S., Jacobs N., Learning a dynamic map of visual appearance, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 12435-12444, (2020); Nachmany Y., Alemohammad H., Detecting roads from satellite imagery in the developing world, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 83-89, (2019); De-Arteaga M., Herlands W., Neill D.B., Dubrawski A., Machine learning for the developing world, ACMTrans. Manage. Inf. Syst., 9, 2, pp. 1-14, (2018); Xia G.-S., Bai X., Ding J., Zhu Z., Belongie S., Luo J., Datcu M., Pelillo M., Zhang L., DOTA: A large-scale dataset for object detection in aerial images, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 3974-3983, (2018); Geodiverse Open Training Data As a Global Good., (2018); Bollinger D., Geo-diversity for Better, Fairer Machine Learning. Development Seed., (2018); Hellstrom T., Dignum V., Bensch S., Bias in Machine Learning What Is It Good for, (2020); Mehrabi N., Morstatter F., Saxena N., Lerman K., Galstyan A., A Survey on Bias and Fairness in Machine Learning, (2019); Strahler A.H., Woodcock C.E., Smith J.A., On the nature of models in remote sensing, Remote Sens. Environ., 20, 2, pp. 121-139, (1986); Castilla G., Hay G.J., Image objects and geographic objects, Object-Based Image Analysis. Berlin, pp. 91-110, (2008); Arvor D., Durieux L., Andres S., Laporte M.-A., Advances in geographic object-based image analysis with ontologies: A review of main contributions and limitations from a remote sensing perspective, Isprs J. Photogram. Remote Sens., 82, pp. 125-137, (2013); Zemel R., Wu Y., Swersky K., Pitassi T., Dwork C., Learning fair representations, Proc. Int. Conf. Mach. Learn., pp. 325-333, (2013); Khanzadi P., Majidi B., Akhtarkavan E., A novel metric for digital image quality assessment using entropy-based image complexity, Proc. Ieee 4th Int. Conf. Knowl.-Based Eng. Innov. (KBEI), pp. 0440-0445, (2017); Salimans T., Goodfellow I., Zaremba W., Cheung V., Radford A., Chen X., Improved techniques for training GANs, Proc. Adv. Neural Inf. Process. Syst., pp. 2234-2242, (2016); Jost L., Entropy and diversity, Oikos, 113, 2, pp. 363-375, (2006); Jain A.K., Murty M.N., Flynn P.J., Data clustering: A review, Acm Comput. Surv., 31, 3, pp. 264-323, (1999); Li T., Ma S., Ogihara M., Entropy-based criterion in categorical clustering, Proc. 21st Int. Conf. Mach. Learn., (2004); Dudewicz E.J., Van Der Meulen E.C., Entropy-based tests of uniformity, J. Amer. Stat. Assoc., 76, 376, pp. 967-974, (1981); Holzinger A., Holzinger A., Hortenhuber M., Mayer C., Bachler M., Wassertheurer S., Pinho A.J., Koslicki D., On entropy-based data mining, Interactive Knowledge Discovery and Data Mining in Biomedical Informatics. Berlin, pp. 209-226, (2014); Ceccarello M., Pietracaprina A., Pucci G., A general coreset-based approach to diversity maximization under matroid constraints, Acm Trans. Knowl. Discovery Data, 14, 5, (2020); Ceccarello M., Pietracaprina A., Pucci G., Upfal E., MapReduce and streaming algorithms for diversity maximization in metric spaces of bounded doubling dimension, Proc. Vldb Endowment, 10, 5, pp. 469-480, (2017); Li A., Zhang L., Qian J., Xiao X., Li X.-Y., Xie Y., TODQA: Efficient task-oriented data quality assessment, Proc. 15th Int. Conf. Mobile Ad-Hoc Sensor Netw. (MSN), pp. 81-88, (2019); Wu T., Chen L., Hui P., Zhang C.J., Li W., Hear the whole story: Towards the diversity of opinion in crowdsourcing markets, Proc. Vldb Endowment, 8, 5, pp. 485-496, (2015); Xiao X., Zhang L., Li X.-Y., Noisy data collection towards diversity maximization, Proc. 5th Int. Conf. Big Data Comput. Commun. (BIGCOM), pp. 283-287, (2019); Ayinde B.O., Inanc T., Zurada J.M., Regularizing deep neural networks by enhancing diversity in feature extraction, Ieee Trans. Neural Netw. Learn. Syst., 30, 9, pp. 2650-2661, (2019); Yang F., Zhu A.-X., Ichii K., White M.A., Hashimoto H., Nemani R.R., Assessing the representativeness of the AmeriFlux network using MODIS and GOES data, J. Geophys. Res., Biogeosci., 113, G4, (2008); Roman M.O., Schaaf C.B., Woodcock C.E., Strahler A.H., Yang X., Braswell R.H., Curtis P.S., Davis K.J., Dragoni D., Goulden M.L., Gu L., Hollinger D.Y., Kolb T.E., Meyers T.P., Munger J.W., Privette J.L., Richardson A.D., Wilson T.B., Wofsy S.C., The MODIS (collection V005) BRDF/albedo product: Assessment of spatial representativeness over forested landscapes, Remote Sens. Environ., 113, 11, pp. 2476-2498, (2009); Schutgens N., Tsyro S., Gryspeerdt E., Goto D., Weigum N., Schulz M., Stier P., On the spatio-temporal representativeness of observations, Atmos. Chem. Phys., 17, 16, pp. 9761-9780, (2017); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 2818-2826, (2016); Vasershtein L.N., Markov processes over denumerable products of spaces, describing large systems of automata, Problemy Peredachi Infor-matsii, 5, 3, pp. 64-72, (1969); Heusel M., Ramsauer H., Unterthiner T., Nessler B., Hochreiter S., GANs trained by a two time-scale update rule converge to a local Nash equilibrium, Proc. Adv. Neural Inf. Process. Syst., pp. 6626-6637, (2017); Lucic M., Kurach K., Michalski M., Gelly S., Bousquet O., Are GANs created equal? A large-scale study, Proc. Adv. Neural Inf. Process. Syst., pp. 700-709, (2018); Christie G., Fendley N., Wilson J., Mukherjee R., Functional map of the world, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 6172-6180, (2018); Openshow S., A million or so correlation coefficients, three experiments on the modifiable areal unit problem, Stat. Appl. Spatial Sci., pp. 127-144, (1979); Josselin D., Louvet R., Impact of the scale on several metrics used in geographical object-based image analysis: Does GEOBIA mitigate the modifiable areal unit problem (MAUP)?, Isprs Int. J. Geo-Inf., 8, 3, (2019); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 248-255, (2009); Barratt S., Sharma R., A Note on the Inception Score, (2018); Chong M.J., Forsyth D., Effectively unbiased FID and inception score and where to find them, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 6070-6079, (2020); Borji A., Pros and cons of GAN evaluation measures, Comput. Vis. Image Understand., 179, pp. 41-65, (2019); Horak D., Yu S., Salimi-Khorshidi G., Topology Distance: A Topology-based Approach for Evaluating Generative Adversarial Networks, (2020); Khrulkov V., Oseledets I., Geometry score: A method for comparing generative adversarial networks, Proc. Int. Conf. Mach. Learn., pp. 2621-2629, (2018); Demir I., Koperski K., Lindenbaum D., Pang G., Huang J., Basu S., Hughes F., Tuia D., Raskar R., DeepGlobe 2018: A challenge to parse the Earth through satellite images, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 172-181, (2018); Brown M., Goldberg H., Foster K., Leichtman A., Wang S., Hagstrom S., Bosch M., Almes S., Large-scale public LiDAR and satellite image data set for urban semantic labeling, Proc. Spie, (2018); Perera A.G., Law Y.W., Chahl J., Drone-action: An outdoor recorded drone video dataset for action recognition, Drones, 3, 4, (2019); Gebru T., Morgenstern J., Vecchione B., Vaughan J.W., Wallach H., Daume H., Crawford K., Datasheets for Datasets, (2018); Miceli M., Yang T., Naudts L., Schuessler M., Serbanescu D., Hanna A., Documenting computer vision datasets: An invitation to reflexive data practices, Proc. Acm Conf. Fairness, Accountability, Transparency, pp. 161-172, (2021); McDuff D., Cheng R., Kapoor A., Identifying Bias in Ai Using Simulation, (2018); Yesson C., Brewer P.W., Sutton T., Caithness N., Pahwa J.S., Burgess M., Gray W.A., White R.J., Jones A.C., Bisby F.A., Culham A., How global is the global biodiversity information facility?, PLoS One, 2, 11, (2007); Creager E., Madras D., Jacobsen J.H., Weis M., Swersky K., Pitassi T., Zemel R., Flexibly fair representation learning by disentanglement, Proc. Int. Conf. Mach. Learn., pp. 1436-1445, (2019); Amini A., Soleimany A.P., Schwarting W., Bhatia S.N., Rus D., Uncovering and mitigating algorithmic bias through learned latent structure, Proc. AAAI/ACM Conf. AI, Ethics, Soc., pp. 289-295, (2019); Wang A., Narayanan A., Russakovsky O., REVISE: A tool for measuring and mitigating bias in visual datasets, Proc. Eur. Conf. Comput. Vis. Glasgow, pp. 733-751, (2020); Liu Y., Gross L., Li Z., Li X., Fan X., Qi W., Automatic building extraction on high-resolution remote sensing imagery using deep convolutional encoder-decoder with spatial pyramid pooling, Ieee Access, 7, pp. 128774-128786, (2019); Kheliff L., Mignotte M., Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis, Ieee Access, 8, pp. 126385-126400, (2020); Rajagopal A., Joshi G.P., Ramachandran A., Subhalakshmi R.T., Khari M., Jha S., Shankar K., You J., A deep learning model based on multi-objective particle swarm optimization for scene classification in unmanned aerial vehicles, Ieee Access, 8, pp. 135383-135393, (2020); Li W., Liu H., Wang Y., Li Z., Jia Y., Gui G., Deep learning-based classification methods for remote sensing images in urban built-up areas, Ieee Access, 7, pp. 36274-36284, (2019); Gao X., Sun X., Zhang Y., Yan M., Xu G., Sun H., Jiao J., Fu K., An end-to-end neural network for road extraction from remote sensing imagery by multiple feature pyramid network, Ieee Access, 6, pp. 39401-39414, (2018); Liu N., Wan L., Zhang Y., Zhou T., Huo H., Fang T., Exploiting convolutional neural networks with deeply local description for remote sensing image classification, Ieee Access, 6, pp. 11215-11228, (2018); Bessinger Z., Jacobs N., A generative model of worldwide facial appearance, Proc. Ieee Winter Conf. Appl. Comput. Vis. (WACV), pp. 1569-1578, (2019); Yang Z., Dan T., Yang Y., Multi-temporal remote sensing image registration using deep convolutional features, Ieee Access, 6, pp. 38544-38555, (2018); Shi Q., Liu X., Li X., Road detection from remote sensing images by generative adversarial networks, Ieee Access, 6, pp. 25486-25494, (2017); Wang Z., Zou C., Cai W., Small sample classification of hyperspectral remote sensing images based on sequential joint deeping learning model, Ieee Access, 8, pp. 71353-71363, (2020); Abdollahi A., Pradhan B., Alamri A., VNet: An end-to-end fully convolutional neural network for road extraction from high-resolution remote sensing data, Ieee Access, 8, pp. 179424-179436, (2020); Hou D., Miao Z., Xing H., Wu H., V-RSIR: An open access webbased image annotation tool for remote sensing image retrieval, Ieee Access, 7, pp. 83852-83862, (2019); Bender E.M., Gebru T., McMillan-Major A., Shmitchell S., On the dangers of stochastic parrots: Can language models be too big?, Proc. Acm Conf. Fairness, Accountability, Transparency, pp. 610-623, (2021); Hinton G.E., To recognize shapes, first learn to generate images, Prog. Brain Res., 165, 6, pp. 535-547, (2007); Chang K.-T., Merghadi A., Yunus A.P., Pham B.T., Dou J., Evaluating scale effects of topographic variables in landslide susceptibility models using GIS-based machine learning techniques, Sci. Rep., 9, 1, pp. 1-21, (2019); Von Neumann-Cosel K., Roth E., Lehmann D., Speth J., Knoll A., Testing of image processing algorithms on synthetic data, Proc. 4th Int. Conf. Softw. Eng. Adv., pp. 169-172, (2009); Redmill K.A., Martin J.I., Ozgliner U., Virtual environment simulation for image processing sensor evaluation, Proc. Ieee Intell. Transp. Syst. (ITSC), pp. 64-70, (2000); Fassnacht F.E., Latiff H., Hartig F., Using synthetic data to evaluate the benefits of large field plots for forest biomass estimation with LiDAR, Remote Sens. Environ., 213, pp. 115-128, (2018); Polidori L., El Hage M., Digital elevation model quality assessment methods: A critical review, Remote Sens., 12, 21, (2020); Frette O., Erga S.R., Stamnes J.J., Stamnes K., Optical remote sensing of waters with vertical structure, Appl. Opt., 40, 9, pp. 1478-1487, (2001)","A.M. Wesley; Institute for Data Science and Informatics, University of Missouri, Columbia, 65211, United States; email: amwvg5@umsystem.edu","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111125883"
"Miyoshi G.T.; Arruda M.D.S.; Osco L.P.; Junior J.M.; Gonçalves D.N.; Imai N.N.; Tommaselli A.M.G.; Honkavaara E.; Gonçalves W.N.","Miyoshi, Gabriela Takahashi (57192677861); Arruda, Mauro dos Santos (57221930407); Osco, Lucas Prado (57196329154); Junior, José Marcato (55640064500); Gonçalves, Diogo Nunes (56797665900); Imai, Nilton Nobuhiro (36720764000); Tommaselli, Antonio Maria Garcia (7003997306); Honkavaara, Eija (14325124400); Gonçalves, Wesley Nunes (23396539500)","57192677861; 57221930407; 57196329154; 55640064500; 56797665900; 36720764000; 7003997306; 14325124400; 23396539500","A novel deep learning method to identify single tree species in UAV-based hyperspectral images","2020","Remote Sensing","12","8","1294","","","","47","10.3390/RS12081294","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084533046&doi=10.3390%2fRS12081294&partnerID=40&md5=7b2ccb11aadaafa9dabe06239f7afbcd","Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Graduate Program in Computer Sciences, Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Faculty of Engineering and Architecture and Urbanism, University ofWestern São Paulo (UNOESTE), Cidade Universitária, R. José Bongiovani, Presidente Prudente, SP, 19050-920, Brazil; Faculty of Engineering, Architecture, and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Finnish Geospatial Research Institute, National Land Survey of Finland, Geodeetinrinne 2, Masala, 02430, Finland","Miyoshi G.T., Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Arruda M.D.S., Graduate Program in Computer Sciences, Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Osco L.P., Faculty of Engineering and Architecture and Urbanism, University ofWestern São Paulo (UNOESTE), Cidade Universitária, R. José Bongiovani, Presidente Prudente, SP, 19050-920, Brazil, Faculty of Engineering, Architecture, and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Junior J.M., Faculty of Engineering, Architecture, and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Gonçalves D.N., Graduate Program in Computer Sciences, Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil; Imai N.N., Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil, Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Tommaselli A.M.G., Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil, Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; Honkavaara E., Finnish Geospatial Research Institute, National Land Survey of Finland, Geodeetinrinne 2, Masala, 02430, Finland; Gonçalves W.N., Graduate Program in Computer Sciences, Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil, Faculty of Engineering, Architecture, and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Av. Costa e Silva, Campo Grande, 79070-900, Brazil","Deep neural networks are currently the focus of many remote sensing approaches related to forest management. Although they return satisfactory results in most tasks, some challenges related to hyperspectral data remain, like the curse of data dimensionality. In forested areas, another common problem is the highly-dense distribution of trees. In this paper, we propose a novel deep learning approach for hyperspectral imagery to identify single-tree species in highly-dense areas. We evaluated images with 25 spectral bands ranging from 506 to 820 nm taken over a semideciduous forest of the Brazilian Atlantic biome. We included in our network's architecture a band combination selection phase. This phase learns from multiple combinations between bands which contributed the most for the tree identification task. This is followed by a feature map extraction and a multi-stage model refinement of the confidence map to produce accurate results of a highly-dense target. Our method returned an f-measure, precision and recall values of 0.959, 0.973, and 0.945, respectively. The results were superior when compared with a principal component analysis (PCA) approach. Compared to other learning methods, ours estimate a combination of hyperspectral bands that most contribute to the mentioned task within the network's architecture. With this, the proposed method achieved state-of-the-art performance for detecting and geolocating individual tree-species in UAV-based hyperspectral images in a complex forest. © 2020 by the authors.","Band selection; Convolutional neural network; Data-reduction; High-density object; Tree species identification","Aircraft detection; Deep neural networks; Forestry; Learning systems; Network architecture; Principal component analysis; Remote sensing; Spectroscopy; Unmanned aerial vehicles (UAV); Data dimensionality; Hyper-spectral imageries; Hyperspectral Data; Multi stage modeling; Precision and recall; Remote sensing approaches; State-of-the-art performance; Tree identification; Deep learning","","","","","","","Aasen H., Honkavaara E., Lucieer A., Zarco-Tejada P.J., Quantitative Remote Sensing at Ultra-High Resolution with UAV Spectroscopy: A Review of Sensor Technology, Measurement Procedures, and Data Correction Workflows, Remote. Sens., 10, (2018); Guimaraes N., Padua L., Marques P., Silva N., Peres E., Sousa J.J., Forestry Remote Sensing from Unmanned Aerial Vehicles: A Review Focusing on the Data, Processing and Potentialities, Remote. Sens., 12, (2020); Nasi R., Honkavaara E., Lyytikainen-Saarenmaa P., Blomqvist M., Litkey P., Hakala T., Viljanen N., Kantola T., Tanhuanpaa T., Holopainen M., Using UAV-Based Photogrammetry and Hyperspectral Imaging for Mapping Bark Beetle Damage at Tree-Level, Remote. Sens., 7, pp. 15467-15493, (2015); Saarinen N., Vastaranta M., Nasi R., Rosnell T., Hakala T., Honkavaara E., Wulder M.A., Luoma V., Tommaselli A.M.G., Imai N.N., Et al., Assessing Biodiversity in Boreal Forests with UAV-Based Photogrammetric Point Clouds and Hyperspectral Imaging, Remote. Sens., 10, (2018); Reis B.P., Martins S.V., Filho E.I.F., Sarcinelli T.S., Gleriani J.M., Marcatti G.E., Leite H.G., Halassy M., Management Recommendation Generation for Areas Under Forest Restoration Process through Images Obtained by UAV and LiDAR, Remote. Sens., 11, (2019); Navarro A., Young M., Allan B., Carnell P., Macreadie P., Ierodiaconou D., The application of Unmanned Aerial Vehicles (UAVs) to estimate above-ground biomass of mangrove ecosystems, Remote. Sens. Environ., 242, (2020); Casapia X.T., Falen L., Bartholomeus H., Cardenas R., Flores G., Herold M., Coronado E.N.H., Baker T.R., Identifying and Quantifying the Abundance of Economically Important Palms in Tropical Moist Forest Using UAV Imagery, Remote. Sens., 12, (2019); Li L., Chen J., Mu X., Li W., Yan G., Xie D., Zhang W., Quantifying Understory and Overstory Vegetation Cover Using UAV-Based RGB Imagery in Forest Plantation, Remote. Sens., 12, (2020); Colgan M.S., Baldeck C.A., Feret J.-B., Asner G.P., Mapping Savanna Tree Species at Ecosystem Scales Using Support Vector Machine Classification and BRDF Correction on Airborne Hyperspectral and LiDAR Data, Remote Sens., 4, pp. 3462-3480, (2012); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., Hyyppa J., Saari H., Polonen I., Imai N.N., Et al., Individual Tree Detection and Classification with UAV-Based Photogrammetric Point Clouds and Hyperspectral Imaging, Remote. Sens., 9, (2017); Tuominen S., Nasi R., Honkavaara E., Balazs A., Hakala T., Viljanen N., Polonen I., Saari H., Ojanen H., Assessment of Classifiers and Remote Sensing Features of Hyperspectral Imagery and Stereo-Photogrammetric Point Clouds for Recognition of Tree Species in a Forest Area of High Species Diversity, Remote. Sens., 10, (2018); Raczko E., Zagajewski B., Comparison of support vector machine, random forest and neural network classifiers for tree species classification on airborne hyperspectral APEX images, Eur. J. Remote. Sens., 50, pp. 144-154, (2017); Xie Z., Chen Y., Lu D., Li G., Chen E., Classification of Land Cover, Forest, and Tree Species Classes with ZiYuan-3 Multispectral and Stereo Data, Remote. Sens., 11, (2019); Maxwell A.E.Warner T.A., Fang F., Implementation of machine-learning classification in remote sensing: An applied review, Int. J. Remote Sens., 39, pp. 2784-2817, (2018); Osco L.P., Ramos A.P.M., Pereira D.R., Moriya E., Imai N.N., Matsubara E., Estrabis N., De Souza M., Marcato J., Goncalves W.N., Et al., Predicting Canopy Nitrogen Content in Citrus-Trees Using Random Forest Algorithm Associated to Spectral Vegetation Indices from UAV-Imagery, Remote. Sens., 11, (2019); Pham T.D., Yokoya N., Bui D.T., Yoshino K., Friess D.A., Remote Sensing Approaches for Monitoring Mangrove Species, Structure, and Biomass: Opportunities and Challenges, Remote. Sens., 11, (2019); Miyoshi G.T., Imai N.N., Tommaselli A.M.G., De Moraes M.V.A., Honkavaara E., Evaluation of Hyperspectral Multitemporal Information to Improve Tree Species Identification in the Highly Diverse Atlantic Forest, Remote. Sens., 12, (2020); Marrs J., Ni-Meister W., Machine Learning Techniques for Tree Species Classification Using Co-Registered LiDAR and Hyperspectral Data, Remote. Sens., 11, (2019); Imangholiloo M., Saarinen N., Markelin L., Rosnell T., Nasi R., Hakala T., Honkavaara E., Holopainen M., Hyyppa J., Vastaranta M., Characterizing Seedling Stands Using Leaf-Off and Leaf-On Photogrammetric Point Clouds and Hyperspectral Imagery Acquired from Unmanned Aerial Vehicle, Forests, 10, (2019); Cao J., Leng W., Liu K., Liu L., He Z., Zhu Y., Object-Based Mangrove Species Classification Using Unmanned Aerial Vehicle Hyperspectral Images and Digital Surface Models, Remote. Sens., 10, (2018); Nasi R., Honkavaara E., Blomqvist M., Lyytikainen-Saarenmaa P., Hakala T., Viljanen N., Kantola T., Holopainen M., Remote sensing of bark beetle damage in urban forests at individual tree level using a novel hyperspectral camera from UAV and aircraft, Urban For. Urban Green., 30, pp. 72-83, (2018); Nezami S., Khoramshahi E., Nevalainen O., Polonen I., Honkavaara E., Tree Species Classification of Drone Hyperspectral and RGB Imagery with Deep Learning Convolutional Neural Networks, Remote. Sens., 12, (2020); Sothe C., Almeida C.M.D., Schimalski M.B., Rosa L.E.C.L., Castro J.D.B., Feitosa R.Q., Dalponte M., Lima C.L., Liesenberg V., Miyoshi G.T., Et al., Comparative performance of convolutional neural network, weighted and conventional support vector machine and random forest for classifying tree species using hyperspectral and photogrammetric data, GIScience Remote. Sens., 57, pp. 369-394, (2020); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of Fir Trees (Abies sibirica) Damaged by the Bark Beetle in Unmanned Aerial Vehicle Images with Deep Learning, Remote. Sens., 11, (2019); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens., 9, (2017); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric., 147, pp. 70-90, (2018); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote. Sens., 152, pp. 166-177, (2019); Khamparia A., Singh K., A systematic review on deep learning architectures and applications, Expert Syst., 36, (2019); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, arXiv, (2018); Lin T.-Y., Goyal P., Girshick R.B., He K., Dollar P., Focal Loss for Dense Object Detection, arXiv, (2017); Ren S., He K., Girshick R.B., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv, (2015); Dos Santos A.A., Marcato Junior J., Araujo M.S., Di Martini D.R., Tetila E.C., Siqueira H.L., Aoki C., Eltner A., Matsubara E.T., Pistori H., Et al., Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs, Sensors, 19, (2019); Lobo Torres D., Feitosa R., Nigri Happ P., Cue L.a.Rosa L., Junior J., Martins J., Bressan P., Goncalves W., Liesenberg V., Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery, Sensors, 20, (2020); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, arXi, (2015); Sylvain J.-D., Drolet G., Brown N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography, ISPRS J. Photogramm. Remote. Sens., 156, pp. 14-26, (2019); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual tree-crown detection in RGB imagery using self-supervised deep learning neural networks, bioRxiv, (2019); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban Tree Species Classification Using a WorldView-2/3 and LiDAR Data Fusion Approach and Deep Learning, Sensors, 19, (2019); Belgiu M., Dragut L., Random forest in remote sensing: A review of applications and future directions, ISPRS J. Photogramm. Remote. Sens., 114, pp. 24-31, (2016); Hennessy A., Clarke K., Lewis M., Hyperspectral Classification of Plants: A Review of Waveband Selection Generalisability, Remote. Sens., 12, (2020); Alshehhi R., Marpu P.R., Woon W.L., Mura M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks, ISPRS J. Photogramm. Remote. Sens., 130, pp. 139-149, (2017); Audebert N., Saux B.L., Lefevre S., Deep Learning for Classification of Hyperspectral Data: A Comparative Review, IEEE Geosci. Remote Sens. Mag., 7, pp. 159-173, (2019); Bioucas-Dias J., Plaza A., Camps-Valls G., Scheunders P., Nasrabadi N.M., Chanussot J., Hyperspectral Remote Sensing Data Analysis and Future Challenges, IEEE Geosci. Remote Sens. Mag., 1, pp. 6-36, (2013); Richards J.A., Jia X., Remote Sensing Digital Image Analysis: An Introduction, 4th ed., (2005); Maschler J., Atzberger C., Immitzer M., Individual Tree Crown Segmentation and Classification of 13 Tree Species Using Airborne Hyperspectral Data, Remote. Sens., 10, (2018); Liu L., Song B., Zhang S., Liu X., A Novel Principal Component Analysis Method for the Reconstruction of Leaf Reflectance Spectra and Retrieval of Leaf Biochemical Contents, Remote. Sens., 9, (2017); Johnson R.A.Wichern D.W., Applied Multivariate Statistical Analysis, (2007); Ozcan A.H., Hisar D., Sayar Y., Unsalan C., Tree crown detection and delineation in satellite images using probabilistic voting, Remote Sens. Lett., 8, pp. 761-770, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Ampatzidis Y., Partel V., UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence, Remote. Sens., 11, (2019); Osco L.P., Arruda M.D.S.D., Junior J.M., Da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote. Sens., 160, pp. 97-106, (2020); Brasil Descreto s/n de 16 de julho de 2002, (2002); Brasil Descreto s/n de 14 de maio de 2004, (2004); Berveglieri A., Tommaselli A.M.G., Imai N.N., Ribeiro E.A.W., Guimaraes R.B., Honkavaara E., Identification of Successional Stages and Cover Changes of Tropical Forest Based on Digital Surface Model Analysis, IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 9, pp. 5385-5397, (2016); Berveglieri A., Imai N.N., Tommaselli A.M.G., Casagrande B., Honkavaara E., Successional stages and their evolution in tropical forests using multi-temporal photogrammetric surface models and superpixels, ISPRS J. Photogramm. Remote. Sens., 146, pp. 548-558, (2018); Giombini M.I., Bravo S.P., Sica Y.V., Tosto D.S., Early genetic consequences of defaunation in a large-seeded vertebrate-dispersed palm (Syagrus romanzoffiana), Heredity, 118, pp. 568-577, (2017); Elias G., Colares R., Rocha Antunes A., Padilha P., Tucker Lima J., Santos R., Palm (Arecaceae) Communities in the Brazilian Atlantic Forest: A Phytosociological Study, Floresta e Ambiente, (2019); Da Silva F.R., Begnini R.M., Lopes B.C., Castellani T.T., Seed dispersal and predation in the palm Syagrus romanzoffiana on two islands with different faunal richness, southern Brazil, Stud. Neotrop. Fauna Environ., 46, pp. 163-171, (2011); Brasil D.F., Espécies Nativas da Flora Brasileira de Valor Econômico Atual ou Potencial: Plantas para o Futuro-Região Centro-Oeste, (2011); Lorenzi H., Árvores Brasileiras, 1, (1992); Mendes C., Ribeiro M., Galetti M., Patch size, shape and edge distance influence seed predation on a palm species in the Atlantic forest, Ecography, (2015); Sica Y., Bravo S.P., Giombini M., Spatial Pattern of Pindó Palm (Syagrus romanzoffiana) Recruitment in Argentinian Atlantic Forest: The Importance of Tapir and Effects of Defaunation, Biotropica, (2014); Honkavaara E., Saari H., Kaivosoja J., Polonen I., Hakala T., Litkey P., Makynen J., Pesonen L., Processing and Assessment of Spectrometric, Stereoscopic Imagery Collected Using a Lightweight UAV Spectral Camera for Precision Agriculture, Remote Sens., 5, pp. 5006-5039, (2013); Honkavaara E., Rosnell T., Oliveira R., Tommaselli A., Band registration of tuneable frame format hyperspectral UAV imagers in complex scenes, ISPRS J. Photogramm. Remote. Sens., 134, pp. 96-109, (2017); Honkavaara E., Khoramshahi E., Radiometric Correction of Close-Range Spectral Image Blocks Captured Using an Unmanned Aerial Vehicle with a Radiometric Block Adjustment, Remote. Sens., 10, (2018); Smith G.M., Milton E.J., The use of the empirical line method to calibrate remotely sensed data to reflectance, Int. J. Remote Sens., 20, pp. 2653-2662, (1999); Miyoshi G.T., Imai N.N., Tommaselli A.M.G., Honkavaara E., Nasi R., Moriya E.A.S., Radiometric block adjustment of hyperspectral image blocks in the Brazilian environment, Int. J. Remote Sens., 39, pp. 4910-4930, (2018); Aich S., Stavness I., Improving Object Counting with Heatmap Regulation, arXi, (2018); Zhao H., Shi J., Qi X.Wang X., Jia J., Pyramid Scene Parsing Network, In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6230-6239, (2017); Story M., Congalton R.G., Accuracy assessment: A user's perspective, Photogramm. Eng. Remote Sens., 52, pp. 397-399, (1986); Jensen J.R., Remote Sensing of the Environment: An Earth Resource Perspective, (2007); Clark M.L., Roberts D.A., Species-Level Differences in Hyperspectral Metrics among Tropical Rainforest Trees as Determined by a Tree-Based Classifier, Remote Sens., 4, pp. 1820-1855, (2012); Dalponte M., Bruzzone L., Gianelle D., Tree species classification in the Southern Alps based on the fusion of very high geometrical resolution multispectral/hyperspectral images and LiDAR data, Remote. Sens. Environ., 123, pp. 258-270, (2012); Natesan S., Armenakis C., Vepakomma U., RESNET-Based tree species classification using UAV images, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci., (2019)","G.T. Miyoshi; Graduate Program in Cartographic Sciences, São Paulo State University (UNESP), Presidente Prudente, SP, 19060-900, Brazil; email: gabriela.t.miyoshi@unesp.br","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084533046"
"Xu F.; Sun W.","Xu, Feng (58156626900); Sun, Wanyan (58155435700)","58156626900; 58155435700","Remote sensing scene recognition using unsymmetrical non-local convolutional neural network; [基于非对称全局卷积神经网络的遥感图像识别方法]","2020","Journal of Forestry Engineering","5","6","","137","142","5","0","10.13360/j.issn.2096-1359.202001037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150805888&doi=10.13360%2fj.issn.2096-1359.202001037&partnerID=40&md5=0fb867abe4f6d21cae9fe6313b740eec","College of Information Sciences and Technology, Nanjing Forestry University, Nanjing, 210037, China","Xu F., College of Information Sciences and Technology, Nanjing Forestry University, Nanjing, 210037, China; Sun W., College of Information Sciences and Technology, Nanjing Forestry University, Nanjing, 210037, China","Compared with traditional shallow learning methods, deep learning based methods have the advantages of high efficiency, outstanding performance and strong expression ability. Among these methods, the image processing technology based on the convolutional neural network is widely used in the fields of image classification, semantic segmentation and object detection. In particular, Non-local convolutional neural network (Non-local CNN) is an en- hanced deep convolutional neural network model, which has attracted much attention in recent years. In image pro- cessing tasks, the Non-local CNN can be used to obtain the global contextual rather than the limited local information of an input image, which is proven to be beneficial for image recognition tasks. In a large number of computer vision tasks, the effectiveness and feasibility of the Non-local CNN have been proved. Considering the fact that there are nu- merous repeated pixel-patches and objects in some categories of remote sensing images (i.e., forest, farmland and beach), the repeated calculations for the same pixel-patches not only would reduce the model efficiency but also may worsen the performance of the model. To address this problem, this study proposed an unsymmetrical non-local con- volution (UNC) module to reduce the calculation time and space complexity of the Non-local CNN, so that the model efficiency can be improved. Specifically, a down-sampling operation on feature maps of Key-value pairs in the classi- cal non-local convolutional module was performed firstly; then, a global object context estimation and weighted fea- ture maps of the input image was obtained by the pixel-wise multiplication of Query and Key-value pair; finally, the residual connection and weighted feature maps were added together as the output for the UNC module. Compared with the classical Non-local CNN, the UNC has the same implementation process as the Non-local CNN, but the proposed UNC was an efficient and general context acquisition module that can be used in a variety of deep learning CNN mod- els. In order to demonstrate its effectiveness, experiments were carried out on four public remote sensing object recog- nition datasets. The experimental results showed that UNC could reduce the model space complexity by up to 24.53% as well as 49.1% on the floating point operations, and obtain the recognition accuracies of 96.63%, 99.16%, 98.90% and 96.28% in RSSCN7, UCML, WHU-RS19, and AID, respectively, which demonstrated that UNC had a strong practicality and universality. © 2020 Nanjing Forestry University. All rights reserved.","convolutional neural network; image recognition; intelligent forestry; remote sensing scene","","","","","","","","MIN W F, JIANG Z H, LI X, Et al., Area measurement for field crops based on UAV platform and image analysis, Journal of Hunan Agricultural University (Natural Sciences), 43, 2, pp. 212-216, (2017); YE Q L, XU D P, ZHANG D., Remote sensing image classifica- tion based on deep learning features and support vector machine, Journal of Forestry Engineering, 4, 2, pp. 119-125, (2019); KRIZHEVSKY A, SUTSKEVER I, HINTON G E., ImageNet classification with deep convolutional neural networks[J], Com- munications of the ACM, 60, 6, pp. 84-90, (2017); SHU X B, TANG J H, QI G J, Et al., Image classification with tailored fine-grained dictionaries [ J ], IEEE Transactions on Circuits and Systems for Video Technology, 28, 2, pp. 454-467, (2018); XU D P, REN Y, YAN Z, Et al., The CNN driven quality evalua- tion for UAV remote sensing images, Journal of Forestry Engi- neering, 3, 5, pp. 121-127, (2018); GAO L M, XU F, LI X, Et al., Board surface defects recognition method based on deep learning features and non-linear support vector machine, Journal of Forestry Engineering, 4, 4, pp. 99-106, (2019); WANG X L, GIRSHICK R, GUPTA A, Et al., Non-local neural networks[ C], 2018 IEEE / CVF Conference on Computer Vision and Pattern Recognition, pp. 7794-7803, (2018); HU T, YANG M., A small object semantic segmentation algorithm combined with object detection, Journal of Nanjing University (Natural Science), 55, 1, pp. 73-84, (2019); FAN J N, LIU Y, HU Z K, Et al., Solid wood panel defect detec- tion and recognition system based on Faster R-CNN, Journal of Forestry Engineering, 4, 3, pp. 112-117, (2019); ZOU Q, NI L H, ZHANG T, Et al., Deep learning based feature selection for remote sensing scene classification, IEEE Geosci- ence and Remote Sensing Letters, 12, 11, pp. 2321-2325, (2015); YANG Y, NEWSAM S., Bag-of-visual-words and spatial extensions for land-use classification [ C ], Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geo- graphic Information Systems-GIS’10, pp. 270-279, (2010); SHENG G F, YANG W, XU T, Et al., High-resolution satellite scene classification using a sparse coding based multiple feature combination, International Journal of Remote Sensing, 33, 8, pp. 2395-2412, (2012); XIA G S, HU J W, HU F, Et al., AID: a benchmark data set for performance evaluation of aerial scene classification, IEEE Transactions on Geoscience and Remote Sensing, 55, 7, pp. 3965-3981, (2017); ZHANG D, LI N, YE Q L., Positional context aggregation network for remote sensing scene classification, IEEE Geoscience and Remote Sensing Letters, pp. 1-5, (2019); HE K M, ZHANG X Y, REN S Q, Et al., Deep residual learning for image recognition[ C], 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); ZHAO F, ZHANG W K, YAN Z Y, Et al., Multi-feature map pyr- amid fusion deep network for semantic segmentation on remote sensing data, Journal of Electronics & Information Technology, 41, 10, pp. 2525-2531, (2019); LI Y J, ZHANG G Q, WANG H J., A method for forestry business images classification based on auto-learning features, Scientia Silvae Sinicae, 54, 5, pp. 78-86, (2018); BOUALLEG Y, FARAH M, FARAH I R., Remote sensing scene classification using convolutional features and deep forest classifier [J], IEEE Geoscience and Remote Sensing Letters, 16, 12, pp. 1944-1948, (2019); MA D, TANG P, ZHAO L J., SiftingGAN: generating and sifting labeled samples to improve the remote sensing image scene classi- fication baseline in vitro, IEEE Geoscience and Remote Sens- ing Letters, 16, 7, pp. 1046-1050, (2019); ZHANG W, TANG P, ZHAO L J., Remote sensing image scene classification using CNN-CapsNet, Remote Sensing, 11, 5, (2019); LU X Q, JI W J, LI X L, Et al., Bidirectional adaptive feature fu- sion for remote sensing scene classification, Neurocomputing, 328, pp. 135-146, (2019); ZENG D, CHEN S J, CHEN B Y, Et al., Improving remote sens- ing scene classification by integrating global-context and local-ob- ject features, Remote Sensing, 10, 5, (2018); SELVARAJU R R, COGSWELL M, DAS A, Et al., Grad-CAM: visual explanations from deep networks via gradient-based locali- zation [ C ], 2017 IEEE International Conference on Computer Vision (ICCV), pp. 618-626, (2017)","F. Xu; College of Information Sciences and Technology, Nanjing Forestry University, Nanjing, 210037, China; email: fufeng_njfu@163.com","","Nanjing Forestry University","","","","","","20961359","","","","Chinese","J. For. Eng.","Article","Final","","Scopus","2-s2.0-85150805888"
"Sariturk B.; Bayram B.; Duran Z.; Seker D.Z.","Sariturk, Batuhan (57193885092); Bayram, Bulent (15130508500); Duran, Zaide (7801693044); Seker, Dursun Zafer (6602931561)","57193885092; 15130508500; 7801693044; 6602931561","FEATURE EXTRACTION FROM SATELLITE IMAGES USING SEGNET AND FULLY CONVOLUTIONAL NETWORKS (FCN)","2020","International Journal of Engineering and Geosciences","5","3","","138","143","5","14","10.26833/ijeg.645426","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102039497&doi=10.26833%2fijeg.645426&partnerID=40&md5=6c840da7ff86a8755fd389d8995fd431","Istanbul Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey; Yildiz Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey","Sariturk B., Istanbul Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey; Bayram B., Yildiz Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey; Duran Z., Istanbul Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey; Seker D.Z., Istanbul Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey","Object detection and classification are among the most popular topics in Photogrammetry and Remote Sensing studies. With technological developments, a large number of high-resolution satellite images have been obtained and it has become possible to distinguish many different objects. Despite all these developments, the need for human intervention in object detection and classification is seen as one of the major problems. Machine learning has been used as a priority option to this day to reduce this need. Although success has been achieved with this method, human intervention is still needed. Deep learning provides a great convenience by eliminating this problem. Deep learning methods carry out the learning process on raw data unlike traditional machine learning methods. Although deep learning has a long history, the main reasons for its increased popularity in recent years are; the availability of sufficient data for the training process and the availability of hardware to process the data. In this study, a performance comparison was made between two different convolutional neural network architectures (SegNet and Fully Convolutional Networks (FCN)) which are used for object segmentation and classification on images. These two different models were trained using the same training dataset and their performances have been evaluated using the same test dataset. The results show that, for building segmentation, there is not much significant difference between these two architectures in terms of accuracy, but FCN architecture is more successful than SegNet by 1%. However, this situation may vary according to the dataset used during the training of the system. © 2020, Murat Yakar. All rights reserved.","Deep Learning; Feature Extraction; Fully Convolutional Networks; Photogrammetry; SegNet","","","","","","","","Akbulut Z., Ozdemir S., Acar H., Dihkan M., Karsli F., Automatic extraction of building boundaries from high resolution images with active contour segmentation, International Journal of Engineering and Geosciences, 3, 1, pp. 37-42, (2018); Badrinarayanan V., Kendall A., Cipolla R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE transactions on pattern analysis and machine intelligence, 39, 12, pp. 2481-2495, (2017); Bozkurt S., Derin Ogrenme Algoritmalari Kullanilarak Cay Alanlarının Otomatik Segmentasyonu, (2018); Chen Q., Wang L., Wu Y., Wu G., Guo Z., Waslander S. L., Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings, ISPRS journal of photogrammetry and remote sensing, 147, pp. 42-55, (2019); Comert R., Kucuk D., Avdan U., Object Based Burned Area Mapping with Random Forest Algorithm, International Journal of Engineering and Geosciences, 4, 2, pp. 78-87, (2019); Cordts M., Omran M., Ramos S., Rehfeld T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The cityscapes dataset for semantic urban scene understanding, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3213-3223, (2016); Deng J., Dong W., Socher R., Li L. J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255, (2009); De Souza W., Semantic Segmentation using Fully Convolutional Neural Networks, (2017); Du Z., Yang J., Huang W., Ou C., Training SegNet for cropland classification of high resolution remote sensing images, AGILE Conference, (2018); Everingham M., Van Gool L., Williams C. K., Winn J., Zisserman A., The pascal visual object classes (voc) challenge, International journal of computer vision, 88, 2, pp. 303-338, (2010); Guo Z., Shao X., Xu Y., Miyazaki H., Ohira W., Shibasaki R., Identification of village building via Google Earth images and supervised machine learning methods, Remote Sensing, 8, 4, (2016); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, (2016); Krizhevsky A., Sutskever I., Hinton G. E., Imagenet classification with deep convolutional neural networks, Advances in neural information processing systems, pp. 1097-1105, (2012); LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E., Hubbard W., Jackel L. D., Backpropagation applied to handwritten zip code recognition, Neural computation, 1, 4, pp. 541-551, (1989); Lin T. Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C. L., Microsoft coco: Common objects in context, European conference on computer vision, pp. 740-755, (2014); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440, (2015); Ma L., Li M., Ma X., Cheng L., Du P., Liu Y., A review of supervised object-based land-cover image classification, ISPRS Journal of Photogrammetry and Remote Sensing, 130, pp. 277-293, (2017); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark, 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229, (2017); Sevgen S. C., Airborne lidar data classification in complex urban area using random forest: a case study of Bergama, Turkey, International Journal of Engineering and Geosciences, 4, 1, pp. 45-51, (2019); Tasdemir S., Ozkan I. A., Ann approach for estimation of cow weight depending on photogrammetric body dimensions, International Journal of Engineering and Geosciences, 4, 1, pp. 36-44, (2019); (2012); (2017); (2020); Vakalopoulou M., Karantzalos K., Komodakis N., Paragios N., Building detection in very high resolution multispectral data with deep learning features, 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1873-1876, (2015); Wu G., Guo Z., Shi X., Chen Q., Xu Y., Shibasaki R., Shao X., A boundary regulated network for accurate roof segmentation and outline extraction, Remote Sensing, 10, 8, (2018); Wu G., Shao X., Guo Z., Chen Q., Yuan W., Shi X., Xu Y., Shibasaki R., Automatic building segmentation of aerial imagery using multi-constraint fully convolutional networks, Remote Sensing, 10, 3, (2018)","B. Sariturk; Istanbul Technical University, Faculty of Civil Engineering, Department of Geomatics Engineering, Istanbul, Turkey; email: sariturkb@itu.edu.tr","","Murat Yakar","","","","","","25480960","","","","English","Int. J. Eng. Geosci.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85102039497"
"Guo H.; Bai H.; Zhou Y.; Li W.","Guo, Hongwei (57206166066); Bai, Hongyang (26428713700); Zhou, Yuxin (57218154795); Li, Weiming (57196302698)","57206166066; 26428713700; 57218154795; 57196302698","DF-SSD: A deep convolutional neural network-based embedded lightweight object detection framework for remote sensing imagery","2020","Journal of Applied Remote Sensing","14","1","014521","","","","12","10.1117/1.JRS.14.014521","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083031521&doi=10.1117%2f1.JRS.14.014521&partnerID=40&md5=d5d1f241a6c1ee2706c047267bf482aa","Nanjing University of Science and Technology, School of Energy and Power Engineering, Nanjing, China; Shandong Institute of Space Electronic Technology, Yantai, China","Guo H., Nanjing University of Science and Technology, School of Energy and Power Engineering, Nanjing, China; Bai H., Nanjing University of Science and Technology, School of Energy and Power Engineering, Nanjing, China; Zhou Y., Nanjing University of Science and Technology, School of Energy and Power Engineering, Nanjing, China; Li W., Shandong Institute of Space Electronic Technology, Yantai, China","In recent years, there has been an increasing interest in object detection in remote sensing imagery with onboard sensors and embedded platform based on deep convolutional neural networks. However, the limited cost, power consumption, compute complexity, and parameter size make the task challenging. The current object detection frameworks are mainly designed on the basis of graphics processing units (GPUs) and require further optimization in power consumption, and calculating quantity and parameter size. To address these issues, we propose an effective single-shot multibox detector (DF-SSD) framework, using the DepthFire module we designed to reform SqueezeNet as the backbone network to reduce the calculating quantity and improve the processing efficiency. To evaluate the effectiveness and superiority of DF-SSD, compared with the other state-of-the-art methods, extensive experiments are conducted on various hardware platforms, including GPU 1080ti, central processing unit i7-7700k, NVidia Jetson TX2, and Cambricon-1H8. Experiment results show that the designed algorithm can achieve a mean average precision of 75.2% on NWPU VHR-10 dataset, with 181, 5.2, 26.3, and 15 frames per second on the above four typical hardware platforms, respectively, which finally demonstrate the effectiveness and high accuracy of the proposed algorithm. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Cambricon-1H8; deep learning; embedded platform; lightweight network; object detection; remote sensing imagery","Computer graphics; Convolution; Deep neural networks; Electric power utilization; Graphics processing unit; Image coding; Object detection; Object recognition; Program processors; Remote sensing; Back-bone network; Detection framework; Embedded platforms; Frames per seconds; Hardware platform; On-board sensors; Remote sensing imagery; State-of-the-art methods; Convolutional neural networks","","","","","National Natural Science Foundation of China, NSFC, (61603189)","The authors would like to thank the anonymous reviewers for the constructive suggestions. This research was funded by National Natural Science Foundation of China (61603189).","Chen Z., Et al., Vehicle detection in high-resolution aerial images based on fast sparse representation classification and multiorder feature, IEEE Trans. Intell. Transp. Syst., 17, 8, pp. 2296-2309, (2016); Tang T., Et al., Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining, Sensors, 17, 2, pp. 336-353, (2017); Tao Y., Et al., GAN-assisted two-stream neural network for high-resolution remote sensing image classification, Remote Sens., 9, 12, (2017); Zhao W., Du S., Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach, IEEE Trans. Geosci. Remote Sens., 54, 8, pp. 4544-4554, (2016); Katia S., He D.C., Building detection in very high spatial resolution multispectral images using the hit-or-miss transform, IEEE Geosci. Remote Sens. Lett., 10, 1, pp. 86-90, (2013); Liu G., Et al., Aircraft recognition in high-resolution satellite images using coarse-to-fine shape prior, IEEE Geosci. Remote Sens. Lett., 10, 3, pp. 573-577, (2013); Hung C., Bryson M., Sukkarieh S., Multi-class predictive template for tree crown detection, ISPRS J. Photogramm. Remote Sens., 68, pp. 170-183, (2012); Marco G., Et al., OBIA ship detection with multispectral and SAR images: A simulation for copernicus security applications, IEEE Int. Geosci. And Remote Sens. Symp., pp. 1229-1232, (2016); Im J., Et al., Optimum scale in object-based image analysis, Scale Issues in Remote Sensing, pp. 197-214, (2014); Diana C., Et al., Monitoring recovery after earthquakes through the integration of remote sensing, GIS, and ground observations: The case of L'Aquila (Italy), Cartogr. Geogr. Inf. Sci., 43, 2, pp. 115-133, (2016); Ok A.O., Automated detection of buildings from single VHR multispectral images using shadow information and graph cuts, ISPRS J. Photogramm. Remote Sens., 86, 12, pp. 21-40, (2013); Shanmugam L., Vani K., Water flow based geometric active deformable model for road network, ISPRS J. Photogramm. Remote Sens., 102, pp. 140-147, (2015); Gong C., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28, (2016); Andreopoulos A., Tsotsos J.K., 50 years of object recognition: Directions forward, Comput. Vision Image Understanding, 117, pp. 827-891, (2013); Huang Y., Et al., Feature coding in image classification: A comprehensive study, IEEE Trans. Pattern Anal. Mach. Intell., 36, 3, pp. 493-506, (2014); Martin L., Et al., Classification and segmentation of satellite orthoimagery using convolutional neural networks, Remote Sens., 8, 4, (2016); Nataliia K., Et al., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci. Remote Sens. Lett., 14, 5, pp. 778-782, (2017); Li W., Et al., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens., 9, 1, (2017); Yang L., Et al., Accurate object localization in remote sensing images based on convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 55, 5, pp. 2486-2498, (2017); Chen Z., Zhang T., Ouyang C., End-to-end airplane detection using transfer learning in remote sensing images, Remote Sens., 10, 1, (2018); Standvoss K., Et al., Cerebral microbleed detection in traumatic brain injury patients using 3D convolutional neural networks, Proc. SPIE, (2018); Zhao C., Et al., Lung nodule detection via 3D U-net and contextual convolutional neural network, Int. Conf. Networking and Network Appl., pp. 356-361, (2018); Roa-Barco L., Et al., A 2D/3D convolutional neural network for brain white matter lesion detection in multimodal MRI, Int. Conf. Comput. Recognit. Syst., pp. 377-385, (2017); Vu H., Et al., On the use of convolutional neural networks for graphical model-based human pose estimation, Int. Conf. Recent Adv. Signal Process., Telecommun. And Comput., pp. 88-93, (2017); Trumble M., Et al., Deep convolutional networks for marker-less human pose estimation from multiple views, Proc. 13th Eur. Conf. Visual Media Production, pp. 1-9, (2016); Ge L., Et al., 3D convolutional neural networks for efficient and robust hand pose estimation from single depth images, Proc. IEEE Conf. Comput. Vision and Pattern Recognit, pp. 1991-2000, (2017); Li B., Et al., Skeleton boxes: Solving skeleton based action detection with a single deep convolutional neural network, IEEE Int. Conf. Multimedia and Expo Workshops, pp. 613-616, (2017); Nguyen V.Q., Et al., Real-time earthquake detection using convolutional neural network and social data, IEEE Third Int. Conf. Multimedia Big Data, pp. 154-157, (2017); Maksymiv O., Rak T., Peleshko D., Real-time fire detection method combining AdaBoost, LBP and convolutional neural network in video sequence, 14th Int. Conf. Experience Des. And Appl. CAD Syst. Microelectron., pp. 351-353, (2017); Dunnings A.J., Breckon T.P., Experimentally defined convolutional neural network architecture variants for non-temporal real-time fire detection, 25th IEEE Int. Conf. Image Process, pp. 1558-1562, (2018); Nugraha B.T., Su S.F., Fahmizal, Towards self-driving car using convolutional neural network and road lane detector, 2nd Int. Conf. Autom., Cognit. Sci., Opt., Micro Electro-Mech. Syst., and Inf. Technol., pp. 65-69, (2017); Bejiga M.B., Zeggada A., Melgani F., Convolutional neural networks for near realtime object detection from UAV imagery in avalanche search and rescue operations, IEEE Int. Geosci. And Remote Sens. Symp., pp. 693-696, (2016); Zhan C., Et al., Deep learning approach in automatic iceberg-ship detection with SAR remote sensing data, Int. Geophys. Conf., pp. 1782-1785, (2018); Chen Y., Et al., A UAV-based forest fire detection algorithm using convolutional neural network, 37th Chin. Control Conf., pp. 10305-10310, (2018); Chen Y., Et al., A visual attention based convolutional neural network for image classification, 12th World Cong. Intell. Control and Autom., pp. 764-769, (2016); Zhu T., Et al., Integrating saliency and ResNet for airport detection in large-size remote sensing images, 2nd Int. Conf. Image, Vision and Comput., pp. 20-25, (2017); Zhao R., Et al., Optimizing CNN-based object detection algorithms on embedded FPGA platforms, Lect. Notes Comput. Sci., pp. 255-267, (2017); Qiu J., Et al., Going deeper with embedded FPGA platform for convolutional neural network, Proc. ACM/SIGDA Int. Symp. Field-Programmable Gate Arrays, pp. 26-35, (2016); Subrahmanyam V., Kumar C., Jannesari A., Efficient Object Detection Model for Realtime UAV Applications; Zhao H., Et al., Embedded deep learning for ship detection and recognition, Future Internet, 11, (2019); Nils T., Et al., Embedded real-time object detection for a UAV warning system, IEEE Int. Conf. Comput. Vision Workshops, pp. 2110-2118, (2017); Alexander W., Et al., YOLO Nano: A Highly Compact You only Look Once Convolutional Neural Network for Object Detection; Mao H., Et al., Towards real-time object detection on embedded systems, IEEE Trans. Emerging Top. Comput., 6, 3, pp. 417-431, (2018); Hossain S., Lee D.-J., Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices, Sensors, 19, 15, (2019); Christos K., Et al., DroNet: Efficient Convolutional Neural Network Detector for Real-time UAV Applications; Iandola F.N., Et al., SqueezeNet: AlexNet-level Accuracy with 50× Fewer Parameters and <0.5 Mb Model Size; Sandler M., Et al., MobileNetV2: Inverted residuals and linear bottlenecks, IEEE/CVF Conf. Comput. Vision and Pattern Recognit, pp. 4510-4520, (2018); Liu W., Et al., SSD: Single shot multibox detector, Lect. Notes Comput. Sci., 9905, pp. 21-37, (2015); Li L., Zhang S., Wu J., Efficient object detection framework and hardware architecture for remote sensing images, Remote Sens., 11, 20, (2019); Krizhevsky A., Sutskever I., Hinton G., ImageNet classification with deep convolutional neural networks, Adv. Neural Inf. Process. Syst., 25, 2, pp. 1097-1105, (2012); Christian S., Et al., Going deeper with convolutions, IEEE Conf. Comput. Vision and Pattern Recognit, pp. 1-9, (2015); Zhang K., Et al., Joint face detection and alignment using multitask cascaded convolutional networks, IEEE Signal Process Lett., 23, 10, pp. 1499-1503, (2016); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition; He K., Et al., Deep residual learning for image recognition, IEEE Conf. Comput. Vision and Pattern Recognit, pp. 770-778, (2016); Shaoqing R., Et al., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2015); Joseph R., Farhadi A., YOLOv3: An Incremental Improvement; Howard A.G., Et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications; Gong C., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens., 54, 12, pp. 7405-7415, (2016); Joseph R., Et al., You only look once: Unified, real-time object detection, IEEE Conf. Comput. Vision and Pattern Recognit, pp. 779-788, (2016); Jia Y., Et al., Caffe: Convolutional architecture for fast feature embedding, Proc. ACM Int. Conf. Multimedia, pp. 675-678, (2014)","H. Bai; Nanjing University of Science and Technology, School of Energy and Power Engineering, Nanjing, China; email: hongyang@njust.edu.cn","","SPIE","","","","","","19313195","","JARSC","","English","J. Appl. Remote Sens.","Article","Final","","Scopus","2-s2.0-85083031521"
"Prabowo Y.; Candra D.S.; Maulana R.","Prabowo, Yudhi (57406702100); Candra, Danang Surya (56237082500); Maulana, Rachmat (57209317918)","57406702100; 56237082500; 57209317918","Cloud Detection for Pleiades and SPOT 6/7 Imageries Using Modified K-means and Deep Learning","2021","International Journal on Advanced Science, Engineering and Information Technology","11","6","","2459","2469","10","1","10.18517/ijaseit.11.6.14312","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122592647&doi=10.18517%2fijaseit.11.6.14312&partnerID=40&md5=3ae8a297b519de4adee14c2eb0a3a5cf","Remote Sensing Technology and Data Center, National Institute of Aeronautics and Space of Indonesia (LAPAN), Jakarta, 13710, Indonesia","Prabowo Y., Remote Sensing Technology and Data Center, National Institute of Aeronautics and Space of Indonesia (LAPAN), Jakarta, 13710, Indonesia; Candra D.S., Remote Sensing Technology and Data Center, National Institute of Aeronautics and Space of Indonesia (LAPAN), Jakarta, 13710, Indonesia; Maulana R., Remote Sensing Technology and Data Center, National Institute of Aeronautics and Space of Indonesia (LAPAN), Jakarta, 13710, Indonesia","Cloud detection is one of the important stages in optical remote sensing activities as the cloud's existence interferes with the works. Many methods have been developed to detect the cloud, but it is still a few methods for high-resolution images, which mostly have limited multispectral bands. In this paper, a novel method of cloud detection for the images is proposed by integrating an unsupervised algorithm and deep learning. This method has three main steps: (1) pre-processing; (2) segmentation using modified K-means; and (3) cloud detection using CNN. In the segmentation step, an unsupervised algorithm, K-means is modified and used to divide pixels values into k clusters. Our modified K-means method can separate thin clouds from relative bright objects in gray clusters that will be grouped into potential cloud pixels. Afterward, a design of convolutional neural network (CNN) is used to extract the multiscale features from each cluster and classify them into two classes: (1) cloud, which consists of thin cloud and thick cloud, and (2) noncloud. The potential cloud area from the first step is used for guiding the result of CNN to provide accurate cloud areas. Several Pleiades and SPOT 6/7 images were used to test the reliability of the proposed method. As a result, our modified K-means has an improvement to increase the accuracy of the results. The results showed that the proposed method could detect cloud and non-cloud accurately and has the highest accuracy of the results compared to the other methods. © 2021. All Rights Reserved.","Cloud detection; convolutional neural network; deep learning; high resolution; modified K-means; Pleiades; SPOT 6/7","","","","","","LAPAN; National Institute of Aeronautics and Space of Indonesian; Remote Sensing Technology and Data Center","The authors would like to thank and appreciate the anonymous reviewers. The authors also would like to thank Remote Sensing Technology and Data Center, National Institute of Aeronautics and Space of Indonesian (LAPAN) for providing Pleiades and SPOT 6/7 images.","Zhang W., Tang P., Zhao L., Remote Sensing Image Scene Classification Using CNN-CapsNet, Remote Sens, 11, 5, (2019); You Y., Et al., Building Detection from VHR Remote Sensing Imagery Based on the Morphological Building Index, Remote Sens, 10, 8, pp. 1-22, (2018); Piermattei L., Et al., Impact of the Acquisition Geometry of Very High-Resolution Pl e iades Imagery on the Accuracy of Canopy Height Models over Forested Alpine Regions, Remote Sens, 10, 10, (2018); Marmorino G., Chen W., Use of WorldView-2 Along-Track Stereo Imagery to Probe a Baltic Sea Algal Spiral, Remote Sens, 11, 7, pp. 1-9, (2019); Marcello J., Eugenio F., Martin J., Marques F., Seabed Mapping in Coastal Shallow Waters Using High Resolution Multispectral and Hyperspectral Imagery, Remote Sens, 10, 8, (2018); Amitrano D., Et al., Long-Term Satellite Monitoring of the Slumgullion Landslide Using Space-Borne Synthetic Aperture Radar Sub-Pixel Offset Tracking, Remote Sens, 11, 3, pp. 1-13, (2019); Candra D. S., Phinn S., Scarth P., Automated cloud and cloudshadow masking for Landsat 8 using multitemporal images in a variety of environments, Remote Sens, 11, 17, (2019); Candra D. S., Phinn S., Scarth P., Cloud and cloud shadow masking for Sentinel-2 using multitemporal images in global area, Int. J. Remote Sens, 41, 8, (2020); Shi Q., Binbin H., Zhe Z., Zhanmang L., Xingwen Q., Improving Fmask cloud and cloud shadow detection in mountainous area for Landsats 4-8 images, Remote Sens. Environ, 199, 2017, pp. 107-119, (2017); Frantz D., Hass E., Uhl A., Stoffels J., Hill J., Improvement of the Fmask algorithm for Sentinel-2 images: Separating clouds from bright surfaces based on parallax effects, Remote Sens. Environ, 215, 2018, pp. 471-481, (2017); Nafiseh G., Mehdi A., Introducing two Random Forest based methods for cloud detection in remote sensing images, Adv. Sp. Res, 62, pp. 288-303, (2018); Chen Y., Fan R., Bilal M., Yang X., Wang J., Li W., Multilevel cloud detection for high-resolution remote sensing imagery using multiple convolutional neural networks, ISPRS Int. J. GeoInformation, 7, 5, (2018); Fengyin X., Mengyun S., Zhenwei S., Jihao Y., Danpei Z., Multilevel Cloud Detection in Remote Sensing Images Based on Deep Learning, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 10, 8, pp. 1-10, (2017); Sun L., Et al., A cloud detection algorithm-generating method for remote sensing data at visible to short-wave infrared wavelengths, ISPRS J. Photogramm. Remote Sens, 124, pp. 70-88, (2017); Cihlar J., Howarth J., Detection and removal of cloud contamination from AVHRR images, IEEE Trans. Geosci. Remote Sens, 32, 3, pp. 583-589, (1994); Lee B., Di Girolamo L., Zhao G., Zhan Y., Three-dimensional cloud volume reconstruction from the Multi-Angle Imaging Spectro Radiometer, Remote Sens, 10, 11, (2018); Gary J., Stephanie H., Frank L., Spatial and Temporal Varying Thresholds for Cloud Detection in GOES Imagery, IEEE Trans. Geosci. Remote Sens, 46, 6, pp. 1705-1717, (2008); Haruma I., Yu O., Keitaro M., Keigo M., Takashi Y. N., Development of a support vector machine based cloud detection method for MODIS with the adjustability to various conditions, Remote Sens. Environ, 205, pp. 390-407, (2018); Pengfei L., Limin D., Huachao X., Mingliang X., A cloud image detection method based on SVM vector machine, Neurocomputing, 169, pp. 34-42, (2015); Bai T., Li D., Sun K., Chen Y., Li W., Cloud detection for highresolution satellite imagery using machine learning and multi-feature fusion, Remote Sens, 8, 9, pp. 1-21, (2016); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks, Remote Sens, 11, 1, (2019); Paoletti M. E., Haut J. M., Plaza J., Plaza A., Deep & Dense convolutional neural network for hyperspectral image classification, Remote Sens, 10, 9, pp. 1-28, (2018); Korznikov K. A., Kislov D. E., Altman J., Dolezal J., Vozmishcheva A. S., Krestov P. V., Using u-net-like deep convolutional neural networks for precise tree recognition in very high resolution rgb (Red, green, blue) satellite images, Forests, 12, 1, pp. 1-17, (2021); Segal-Rozenhaimer M., Li A., Das K., Chirayath V., Cloud detection algorithm for multi-modal satellite imagery using convolutional neural-networks (CNN), Remote Sens. Environ, 237, (2020); Almabdy S., Elrefaei L., Deep convolutional neural networkbased approaches for face recognition, Appl. Sci, 9, 20, (2019); Kamycki K., Kapuscinski T., Oszust M., Data augmentation with suboptimal warping for time-series classification, Sensors, 20, 1, (2020); Qi J., Yu Y., Wang L., Liu J., Wang Y., An effective and efficient hierarchical K-means clustering algorithm, Int. J. Distrib. Sens. Networks, 13, 8, pp. 1-17, (2017); Mehta S., Shen X., Gou J., Niu D., A new nearest centroid neighbor classifier based on k local means using harmonic mean distance, Information, 9, 9, (2018); Hu L., Qin M., Zhang F., Du Z., Liu R., RSCNN: A cnn-based method to enhance low-light remote-sensing images, Remote Sens, 13, 1, pp. 1-13, (2021); Ba R., Song W., Li X., Xie Z., Lo S., Integration of multiple spectral indices and a neural network for burned area mapping based on MODIS data, Remote Sens, 11, 3, (2019); Hwang J. I., Jung H. S., Automatic ship detection using the artificial neural network and support vector machine from X-Band Sar satellite images, Remote Sens, 10, 11, (2018); Waseem R., Zenghui W., Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review, Neural Comput, 29, 9, pp. 1-98, (2017); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens, 9, 1, (2017); Yu C., Duo W., Pan Z., Tao Z., Model compression and acceleration for deep neural networks: The principles, progress, and challenges, IEEE Signal Process. Mag, 35, 1, pp. 126-136, (2018); Wang Y., Li Y., Song Y., Rong X., The influence of the activation function in a convolution neural network model of facial expression recognition, Appl. Sci, 10, 5, (2020); Hu Y., Zhang Q., Zhang Y., Yan H., A deep convolution neural network method for land cover mapping: A case study of Qinhuangdao, China, Remote Sens, 10, 12, (2018); Elmes A., Et al., Accounting for training data error in machine learning applied to earth observations, Remote Sens, 12, 6, pp. 86-88, (2020); Mohammadimanesh F., Salehi B., Mahdianpari M., Gill E., Molinier M., A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem, ISPRS J. Photogramm. Remote Sens, 151, pp. 223-236, (2019)","","","Insight Society","","","","","","20885334","","","","English","Int. J. Adv. Sci. Eng. Inf. Technol.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85122592647"
"Napiorkowska M.; Petit D.; Martí P.","Napiorkowska, Milena (56049862900); Petit, David (57191270639); Martí, Paula (57191267927)","56049862900; 57191270639; 57191267927","Three applications of deep learning algorithms for object detection in satellite imagery","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518102","4839","4842","3","12","10.1109/IGARSS.2018.8518102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063147895&doi=10.1109%2fIGARSS.2018.8518102&partnerID=40&md5=2df071654b11fb1c4c0e278ecfe7b424","Deimos Space UK Ltd., United Kingdom; Deimos Engenharia, United Kingdom","Napiorkowska M., Deimos Space UK Ltd., United Kingdom; Petit D., Deimos Space UK Ltd., United Kingdom; Martí P., Deimos Engenharia, United Kingdom","Detection of objects in images has been long used in computer vision applications (image and video analysis) in fields such as surveillance or robotics. The last decade saw a break-through in this area when deep convolutional neural networks were introduced, in addition of the GPU computing capacity. In remote sensing, satellite images are also used for feature extraction and often classic machine learning techniques are used for the classification of the pixels in the image. This paper shows how one of the networks developed for the ImageNet challenge can be applied to satellite imagery for object detection using three examples: roads, palm trees and cars. © 2018 IEEE","Classification; Convolutional networks; Deep learning; Machine learning; Object detection; Remote sensing; VGG","","","","","","","","ImageNet; Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105; Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Proceedings of ICLR, (2015); Shelhamer E., Long J., Darrell T., Fully convolutional networks for semantic segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence Archive, 39, 4, pp. 640-651, (2017)","","","Institute of Electrical and Electronics Engineers Inc.","Geoscience and Remote Sensing Society (GRSS); The Institute of Electrical and Electronics Engineers (IEEE)","38th Annual IEEE International Geoscience and Remote Sensing Symposium, IGARSS 2018","22 July 2018 through 27 July 2018","Valencia","141934","","978-153867150-4","IGRSE","","English","Dig Int Geosci Remote Sens Symp (IGARSS)","Conference paper","Final","","Scopus","2-s2.0-85063147895"
"Mohan A.; Singh A.K.; Kumar B.; Dwivedi R.","Mohan, Amrita (57196078933); Singh, Amit Kumar (55726466900); Kumar, Basant (57220803153); Dwivedi, Ramji (57198818996)","57196078933; 55726466900; 57220803153; 57198818996","Review on remote sensing methods for landslide detection using machine and deep learning","2021","Transactions on Emerging Telecommunications Technologies","32","7","e3998","","","","82","10.1002/ett.3998","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087214140&doi=10.1002%2fett.3998&partnerID=40&md5=64d5da30511c28f0ab77c8f2a6bc3fed","GIS Cell, MNNIT Allahabad, Allahabad, India; CSE Department, NIT Patna, Patna, India; ECE Department, MNNIT Allahabad, Allahabad, India","Mohan A., GIS Cell, MNNIT Allahabad, Allahabad, India; Singh A.K., CSE Department, NIT Patna, Patna, India; Kumar B., ECE Department, MNNIT Allahabad, Allahabad, India; Dwivedi R., GIS Cell, MNNIT Allahabad, Allahabad, India","Landslide, one of the most critical natural hazards, is caused due to specific compositional slope movement. In the past decades, due to inflation of urbanized area and climate change, a compelling expansion in landslide prevalence took place which is also termed as mass/slope movement and mass wasting, causing extensive collapse around the world. The principal reason for its pursuance is a reduction in the internal resistance of soil and rocks, classified as a slide, topple, fall, and flow. Slopes can be differentiated based on earth material and the nature of its movements. The downward flow of landslides occurs due to excessive rainfall, snowmelt, earthquake, volcanic eruption, and so on. This review article revisits the conventional approaches for identification of landslides, predicting future risk, associated with slope failures, followed by emphasizing the advantages of modern geospatial techniques such as aerial photogrammetry, satellite remote sensing images (ie, panchromatic, multispectral, radar images), Terrestrial laser scanning, and High-Resolution Digital Elevation Model (HR-DEM) in updating landslide inventory maps. Machine learning techniques like Support Vector Machine, Artificial neural network, deep learning has been extensively used with geographical data producing effective results for assessment of natural hazard/resources and environmental research. Based on recent studies, deep learning is a reliable tool addressing remote sensing challenges such as trade-off in imaging system producing poor quality investigation, in addition, to expedite consequent task such as image recognition, object detection, classification, and so on. Conventional methods, like pixel and object-based machine learning methods, have been broadly explored. Advanced development in deep learning technique like CNN (Convolutional neural network) has been extensively successful in information extraction from an image and has exceeded other traditional approaches. Over the past few years, minor attempts have been made for landslide susceptibility mapping using CNN. In addition, small sample sizes for training purpose will be major drawback and notably remarkable while using deep learning techniques. Also, assessment of the model's performance with diverse training and testing proportion other than commonly utilized ratio, that is, 70/30 needs to be explored further. The review article briefly highlights the remote sensing methods for landslide detection using machine learning and deep learning. © 2020 John Wiley & Sons, Ltd.","","Antennas; Climate change; Convolutional neural networks; Economic and social effects; Hazards; Image recognition; Landslides; Learning algorithms; Learning systems; Object detection; Photogrammetry; Remote sensing; Support vector machines; Surveying; Volcanoes; Digital elevation model; Environmental researches; Landslide susceptibility mapping; Machine learning methods; Machine learning techniques; Satellite remote sensing; Terrestrial laser scanning; Traditional approaches; Deep learning","","","","","","","Madaan S., Causes, effect and types of landslides, Bull Seismolog Soc Am, pp. 9-10, (2017); Martha T.R., Kamala P., Jose J., Vinod Kumar K., Jai Sankar G., Identification of new landslides from high resolution satellite data covering a large area using object-based change detection methods, J Indian Soc Remote Sens, 44, 4, pp. 515-524, (2016); Gorum T., Fan X., van Westen C.J., Et al., Distribution pattern of earthquake-induced landslides triggered by the May 12, 2008 Wenchuan earthquake, Geomorphology, 133, 3-4, pp. 152-167, (2011); Xu C., Xu X., Yao X., Dai F., Three (nearly) complete inventories of landslides triggered by the may 12, 2008 Wenchuan mw 7.9 earthquake of China and their spatial distribution statistical analysis, Landslides, 11, 3, pp. 441-461, (2014); Martha T.R., Babu Govindharaj K., Vinod Kumar K., Damage and geological assessment of the September 18, 2011 Mw6.9 earthquake in Sikkim, India using very high resolution satellite data, Geosci Front, 6, 6, pp. 793-805, (2015); Martha T.R., Kerle N., Van Westen C.J., Jetten V., Kumar K.V., Object-oriented analysis of multi-temporal panchromatic images for creation of historical landslide inventories, ISPRS J Photogramm Remote Sens, 67, pp. 105-119, (2012); Arbanas S.M., Arbanas Z., Landslide mapping and monitoring: Review of conventional and advanced techniques, (2014); Guzzetti F., (2005); Guzzetti F., Cardinali M., Reichenbach P., Carrara A., Comparing landslide maps: a case study in the upper Tiber river basin, Central Italy, Environ Manag, 25, 3, pp. 247-263, (2000); Guzzetti F., Mondini A.C., Cardinali M., Fiorucci F., Santangelo M., Chang K.T., Landslide inventory maps: new tools for an old problem, Earth-Sci Rev, 112, 1-2, pp. 42-66, (2012); Galli M., Ardizzone F., Cardinali M., Guzzetti F., Reichenbach P., Comparing landslide inventory maps, Geomorphology, 94, 3-4, pp. 268-289, (2008); Parker R.N., Densmore A.L., Rosser N.J., Et al., Mass wasting triggered by the 2008 Wenchuan earthquake is greater than orogenic growth, Nat Geosci, 4, 7, pp. 449-452, (2011); Tsai F., Hwang J.H., Chen L.C., Lin T.H., Post-disaster assessment of landslides in southern Taiwan after 2009 typhoon Morakot using remote sensing and spatial analysis, Nat Hazards Earth Syst Sci, 10, 10, pp. 2179-2190, (2010); Fiorucci F., Cardinali M., Carla R., Et al., Geomorphology seasonal landslide mapping and estimation of landslide mobilization rates using aerial and satellite images, Geomorphology, 129, 1-2, pp. 59-70, (2011); Goals M.P., Goals M.P., Table of Contents目次, Nippon Ronen Igakkai Zasshi Japanese J Geriatr, 56, 1, pp. 1-9, (2019); Living with landslide risk in Europe: assessment, effects of global change, and risk management strategies, Work, 3, (2012); van Westen C., Kappes M., Luna B.Q., Frigerio S., Glade T., Malet J.P., Mountain Risks: From Prediction to Management and Governance (Full Book), 34, (2014); Zhu M., He Y., He Q., A review of researches on deep learning in remote sensing application, Int J Geosci, 10, 1, pp. 1-11, (2019); Li D., Tong Q., Li R., Gong J., Zhang L., Current issues in high-resolution earth observation technology, Sci China Earth Sci, 55, 7, pp. 1043-1051, (2012); Kavzoglu T., Colkesen I., Sahin E.K., Sahin EK. Machine learning techniques in landslide susceptibility mapping: A survey and a case study, Landslides: Theory, Practice and Modelling. Advances in Natural and Technological Hazards Research, (2019); Brenning A., Spatial prediction models for landslide hazards: review, comparison and evaluation, Natural Hazards and Earth System Science, 5, 6, pp. 853-862, (2005); Bai S.B., Wang J., Thiebes B., Cheng C., Chang Z.Y., Susceptibility assessments of the Wenchuan earthquake-triggered landslides in Longnan using logistic regression, Environ Earth Sci, 71, 2, pp. 731-743, (2014); Marjanovic M., Kovacevic M., Bajat B., Mihalic Arbanas S., Abolmasov B., Landslide assessment of the Strača basin (Croatia) using machine learning algorithms, Acta Geotech Slovenica, 8, 2, pp. 45-55, (2011); Costanzo D., Rotigliano E., Irigaray C., Jimenez-Peralvarez J.D., Chacon J., Factors selection in landslide susceptibility modelling on large scale following the gis matrix method: application to the river Beiro basin (Spain), Natural Hazards Earth Syst Sci, 12, 2, pp. 327-340, (2012); Bui D.T., Ho T.C., Revhaug I., Pradhan B., Nguyen D.B., Landslide susceptibility mapping along the national road 32 of Vietnam using GIS-based J48 decision tree classifier and its ensembles, Cartography from Pole to Pole, pp. 303-317, (2014); Goetz J.N., Brenning A., Petschko H., Leopold P., Computers & geosciences evaluating machine learning and statistical prediction techniques for landslide susceptibility modeling, Comput Geosci, 81, pp. 1-11, (2015); Pham B.T., Bui D.T., Prakash I., Dholakia M.B., Rotation forest fuzzy rule-based classifier ensemble for spatial prediction of landslides using GIS, Natural Hazards, 83, 1, pp. 97-127, (2016); Pagot E., Pesaresi M., Systematic study of the urban postconflict change classification performance using spectral and structural features in a support vector machine, IEEE J Selected Topics Appl Earth Observ Remote Sens, 1, 2, pp. 120-128, (2008); Rajmohan G., Chinnappan C.V., John-William A.D., Chandrakrishan-Balakrishnan S., Anand-Muthu B., Manogaran G., Revamping land coverage analysis using aerial satellite image mapping, Trans Emerg Telecommun Technol, February, pp. 1-14, (2020); Lecun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Zhong C., Liu Y., Gao P., Et al., Landslide mapping with remote sensing: challenges and opportunities, Int J Remote Sens, 41, 4, pp. 1555-1581, (2020); Jung H.S., Lee S., Special issue on machine learning techniques applied to geoscience information system and remote sensing, Appl Sci, 9, 12, pp. 9-13, (2019); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: a meta-analysis and review, ISPRS J Photogramm Remote Sens, 152, April, pp. 166-177, (2019); Tavakkoli Piralilou S., Shahabi H., Jarihani B., Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S., Aryal J., Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas, Remote Sens, 11, 21, pp. 1-26, (2019); Lai J.S., Tsai F., Improving GIS-based landslide susceptibility assessments with multi-temporal remote sensing and machine learning, Sensors, 19, 17, pp. 1-26, (2019); Sameen M.I., Pradhan B., Landslide detection using residual networks and the fusion of spectral and topographic information, IEEE Access, 7, pp. 114363-114373, (2019); Nhu V.H., Hoang N.D., Nguyen H., Et al., Effectiveness assessment of Keras based deep learning with different robust optimization algorithms for shallow landslide susceptibility mapping at tropical area, Catena, 188, November 2019, (2020); Alhaidari M.A.A., Atta-Ur-Rahman F.A.A., Telecommunication networks in disaster management: a review, J Commun, 14, 6, pp. 432-447, (2019); Butt T.A., Context-aware cognitive disaster management using fog-based internet of things, Trans Emerg Telecommun Tech, February, (2019); Phengsuwan J., Shah T., Sun R., James P., Thakker D., Ranjan R., An ontology-based system for discovering landslide-induced emergencies in electrical grid, Tran. Emerg Telecommun Technol, October 2019, pp. 1-16, (2020); Prasad N.B., Technical Report Centre for Water Resources Development and Management, Landslides-Causes & Mitigation, (1995); Highland L.M., Bobrowsky P., The Landslide Handbook—A Guide to Understanding Landslides, 1325, (2008); Pradhan S.P., Siddique T., Mass wasting: an overview, Adv Nat Technol Hazards Res, 50, pp. 3-20, (2019); Hungr O., Leroueil S., Picarelli L., The Varnes classification of landslide types, an update, Landslides, 11, 2, pp. 167-194, (2014); Marcelino E.V., Formaggio A.R., Maeda E.E., Landslide inventory using image fusion techniques in Brazil, Int J Appl Earth Obs Geoinf, 11, 3, pp. 181-191, (2009); Sassa K., Canuti P., Yin Y., Landslide Science for a Safer Geoenvironment: Volume 2: Methods of Landslide Studies, 2, pp. 1-851, (2014); Othman A.A., Gloaguen R., River courses affected by landslides and implications for hazard assessment: a high resolution remote sensing case study in ne Iraq-w Iran, Remote Sens, 5, 3, pp. 1024-1044, (2013); Agliardi F., Crosta G.B., Zanchi A., Ravazzi C., Onset and timing of deep-seated gravitational slope deformations in the eastern Alps, Italy, Geomorphology, 103, 1, pp. 113-129, (2009); Eisenbeiss H., The autonomous mini helicopter: a powerful platform for mobile mapping, Int Arch Photogramm Remote Sens Spat Inf Sci, 37, pp. 977-983, (2008); Cheng K.S., Wei C., Chang S.C., Locating landslides using multi-temporal satellite images, Adv Sp Res, 33, 3, pp. 296-301, (2004); Barlow J., Franklin S., Martin Y., High spatial resolution satellite imagery, DEM derivatives, and image segmentation for the detection of mass wasting processes, Photogramm Eng Remote Sens, 72, 6, pp. 687-692, (2006); Chigira M., Duan F., Yagi H., Furuya T., Using an airborne laser scanner for the identification of shallow landslides and susceptibility assessment in an area of ignimbrite overlain by permeable pyroclastics, Landslides, 1, 3, pp. 203-209, (2004); Tarolli P., Sofia G., Dalla Fontana G., Geomorphic features extraction from high-resolution topography: landslide crowns and bank erosion, Nat Hazards, 61, 1, pp. 65-83, (2012); Lim M., Petley D.N., Rosser N.J., Allison R.J., Long A.J., Pybus D., Combined digital photogrammetry and time-of-flight laser scanning for monitoring cliff evolution, Photogramm Rec, 20, 110, pp. 109-129, (2005); Oppikofer T., Jaboyedoff M., Blikra L., Derron M.H., Metzger R., Characterization and monitoring of the Âknes rockslide using terrestrial laser scanning, Nat Hazards Earth Syst Sci, 9, 3, pp. 1003-1019, (2009); Haeberlin Y., Turberg P., Retiere A., Senegas O., Parriaux A., Validation of Spot-5 satellite imagery for geological hazard identification and risk assessment for landslides, mud and debris flows in Matagalpa, Nicaragua, Int Arch Photogramm Remote Sens Spat Inf Sci, 35, 2004, pp. 273-278, (2002); Alkevli T., Ercanoglu M., Assessment of ASTER satellite images in landslide inventory mapping: Yenice-Gökçebey (Western Black Sea region, Turkey), Bull Eng Geol Environ, 70, 4, pp. 607-617, (2011); Ferretti A., Prati C., Rocca F., Permanent scatterers in SAR interferometry, IEEE Trans Geosci Remote Sens, 39, 1, pp. 8-20, (2001); Farina P., Colombo D., Fumagalli A., Marks F., Moretti S., Permanent Scatterers for landslide investigations: outcomes from the ESA-SLAM project, Eng Geol, 88, 3-4, pp. 200-217, (2006); Lauknes T.R., Piyush Shanker A., Dehls J.F., Zebker H.A., Henderson I.H.C., Larsen Y., Detailed rockslide mapping in northern Norway with small baseline and persistent scatterer interferometric SAR time series methods, Remote Sens Environ, 114, 9, pp. 2097-2109, (2010); Brunsden D., Mass movement; the research frontier and beyond: a geomorphological approach, Geomorphology, 7, 1-3, pp. 85-128, (1993); Razak K.A., Straatsma M.W., van Westen C.J., Malet J.P., de Jong S.M., Airborne laser scanning of forested landslides characterization: terrain model quality and visualization, Geomorphology, 126, 1-2, pp. 186-200, (2011); Toth C.K., Raton B., Hetherington D., Topographic Laser Ranging and Scanning : Principles and Processing, 1161, pp. 3333-3334, (2010); Soeters R., van Westen C.J., Slope instability recognition, analysis and zonation landslides investigation and mitigation, Landslides Investig. Mitigation; Transp Res Board Spec Rep., 247, December, pp. 129-177, (1996); Pohl C., Van Genderen J.L., Review article multisensor image fusion in remote sensing: Concepts, methods and applications, Int J Remote Sens, 19, 5, (1998); Bajracharya B., Bajracharya S.R., (2008); Mikos M., Vilimek V., Yin Y., Sassa K., Landslide Risk Assessment for the Built Environment in Sub-Saharan Africa, Adv Cult Liv Landslides, 5, pp. 1-557, (2017); Cascini L., Fornaro G., Peduto D., Advanced low- and full-resolution DInSAR map generation for slow-moving landslide analysis at different scales, Eng Geol, 112, 1-4, pp. 29-42, (2010); Cigna F., Del Ventisette C., Liguori V., Casagli N., Advanced radar-interpretation of InSAR time series for mapping and characterization of geological processes, Natural Hazards Earth Syst Sci, 11, 3, pp. 865-881, (2011); Li Y., Zhang H., Xue X., Jiang Y., Shen Q., Deep learning for remote sensing image classification: a survey, Wiley Interdiscip Rev Data Mining Knowl Discov, 8, 6, pp. 1-17, (2018); Li C., Wang Y., Zhang X., Gao H., Yang Y., Wang J., Deep belief network for spectral-spatial classification of hyperspectral remote sensor data, Sensors, 19, 1, (2019); Chen J., Jin Q., Chao J., Design of Deep Belief Networks for Short-Term Prediction of Drought Index Using Data in the Huaihe River Basin, Math ProblEng, 2012, pp. 1-16, (2012); Zhang L., Wei W., Shen C., van den Hengel A., Shi Q., Cluster sparsity field: An internal hyperspectral imagery prior for reconstruction, Int J Comput Vis, 126, 8, pp. 797-821, (2018); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of different machine learning methods and deep-learning convolutional neural networks for landslide detection, Remote Sens, 11, 2, pp. 1-21, (2019); Prakash N., Manconi A., Loew S., Mapping landslides on EO data: performance of deep learning models vs. traditional machine learning models, Remote Sens, 12, 3, (2020); Ghorbanzadeh O., Meena S.R., Blaschke T., Aryal J., UAV-based slope failure detection using deep-learning convolutional neural networks, Remote Sens, 11, 17, pp. 1-24, (2019); Wang Y., Wang X., Jian J., Remote sensing landslide recognition based on convolutional neural network, Math Probl Eng, 2019, pp. 1-12, (2019); Bui D.T., Tsangaratos P., Nguyen V.T., Van Liem N., Trinh P.T., Comparing the prediction performance of a deep learning neural network model with conventional machine learning models in landslide susceptibility assessment, Catena, 188, December 2019, (2020); Ye C., Li Y., Cui P., Et al., Landslide detection of hyperspectral remote sensing data based on deep learning with constrains, IEEE J Sel Top Appl Earth Obs Remote Sens, 12, 12, pp. 5047-5060, (2019); Lu H., Ma L., Fu X., Liu C., Wang Z., Tang M., Li N., Landslides information extraction using object-oriented image analysis paradigm based on deep learning and transfer learning, Remote Sens, 12, 5, pp. 1-22, (2020); Pelletier C., Webb G.I., Petitjean F., Temporal convolutional neural network for the classification of satellite image time series, Remote Sens, 11, 5, pp. 1-25, (2019); Zhao H., Liu F., Zhang H., Liang Z., Convolutional neural network based heterogeneous transfer learning for remote-sensing scene classification, Int J Remote Sens, 40, 22, pp. 8506-8527, (2019); Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community, J Appl Remote Sens, 11, 4, (2017); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Cheng G., Ma C., Zhou P., Yao X., Han J., Scene classification of high resolution remote sensing images using convolutional neural networks, IEEE Int Geosci Remote Sens Symp, pp. 767-770, (2016); Mboga N., Georganos S., Grippa T., Lennert M., Vanhuysse S., Wolff E., Fully convolutional networks and geographic object-based image analysis for the classification of VHR imagery, Remote Sens, 11, 5, (2019); Badrinarayanan V., Kendall A., Cipolla R., Segnet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Trans Pattern Anal Mach Intell, 39, 12, pp. 2481-2495, (2017); Chen L., Papandreou G., Member S., Kokkinos I., Murphy K., Yuille A.L., DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, IEEE Trans Pattern Anal Mach Intell, 40, 4, pp. 834-848, (2018); Long J., Shelhamer E., Darrell T., Fully Convolutional Networks for Semantic Segmentation, IEEE Conf Comput Vis Pattern Recognit, pp. 3431-3440, (2015); Chen L.C., Papandreou G., Schroff F., Adam H., Rethinking atrous convolution for semantic image segmentation, (2017); Chen L.C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-decoder with Atrous separable convolution for semantic image segmentation, Proceedings of the European conference on computer vision (ECCV), (2018); Pak M., Kim S., A review of deep learning in image recognition. In: 2017 4th International Conference on computer applications and information processing technology (CAIPT), August 1-3, (2017); Gu Y., Wang Y., Li Y., A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and scene-guided object detection, Appl Sci, 9, 10, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans Pattern Anal Mach Intell, 39, 6, pp. 1137-1149, (2017); Huang Z., Huang L., Gong Y., Huang C., Wang X.; Uijlings J.R.R., Van De Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int J Comput Vis, 104, pp. 154-171, (2013); Paola J.D., Schowengerdt R.A., A review and analysis of backpropagation neural networks for classification of remotely-sensed multi-spectral imagery, Int. J. Remote Sens., 16, 16, pp. 3033-3058, (1995); Girshick R., Donahue J., Darrell T., Malik J., Berkeley U.C., Rich feature hierarchies for accurate object detection and semantic segmentation, IEEE Conf Comput Vis Pattern Recognit, 2014, pp. 580-587, (2014); Ding A., Zhang Q., Zhou X., Dai B., Automatic recognition of landslide based on CNN and texture change detection, 31st Youth Academic Annual Conference of Chinese Association of Automation (YAC), pp. 444-448, (2016); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans Pattern Anal Mach Intell, 37, 9, pp. 1904-1916, (2015); Girshick R., (2015); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Et al., (2016); Redmon J., Divvala S., Girshick R., Farhadi A., (2016); Manconi A., Casu F., Ardizzone F., Et al., Brief communication: rapid mapping of landslide events: the December 3, 2013 Montescaglioso landslide, Italy, Nat. Hazards Earth Syst. Sci., 14, 7, pp. 1835-1841, (2014); Duro D.C., Franklin S.E., Dube M.G., A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery, Remote Sens Environ, 118, pp. 259-272, (2012); Roodposhti M.S., Aryal J., Bryan B.A., A novel algorithm for calculating transition potential in cellular automata models of land-use/cover change, Environ. Model. Softw., 112, pp. 70-81, (2019); Mezaal M.R., Pradhan B., Rizeei H.M., Improving landslide detection from airborne laser scanning data using optimized Dempster-Shafer, Remote Sens, 10, 7, pp. 1-26, (2018); Tien Bui D., Tuan T.A., Klempe H., Pradhan B., Revhaug I., Spatial prediction models for shallow landslide hazards: a comparative assessment of the efficacy of support vector machines, artificial neural networks, kernel logistic regression, and logistic model tree, Landslides, 13, 2, pp. 361-378, (2016); Pham B.T., Pradhan B., Tien Bui D., Prakash I., Dholakia M.B., A comparative study of different machine learning methods for landslide susceptibility assessment: a case study of Uttarakhand area (India), Environ Model Software, 84, pp. 240-250, (2016); Pham B.T., Bui D., Prakash I., Dholakia M.B., Evaluation of predictive ability of support vector machines and naive Bayes trees methods for spatial prediction of landslides in Uttarakhand state (India) using GIS, J Geomatics, 10, 1, pp. 71-79, (2016); Micheletti N., Foresti L., Robert S., Leuenberger M., Pedrazzini A., Jaboyedoff M., Kanevski M., Machine Learning Feature Selection Methods for Landslide Susceptibility Mapping, Mathematical Geosciences, 46, 1, pp. 33-57, (2014); Bialas J., Oommen T., Rebbapragada U., Levin E., Object-based classification of earthquake damage from high-resolution optical imagery using machine learning, J Appl Remote Sens, 10, 3, (2016); Huang Y., Zhao L., Review on landslide susceptibility mapping using support vector machines, Catena, 165, January, pp. 520-529, (2018); Ghorbanzadeh O., Blaschke T., Optimizing sample patches selection of CNN to improve the mIOU on landslide detection, 5th International Conference on Geographical Information Systems Theory, Applications and Management, 1, pp. 33-40, (2019); Tien Bui D., Ho T.C., Pradhan B., Pham B.T., Nhu V.H., Revhaug I., GIS-based modeling of rainfall-induced landslides using data mining-based functional trees classifier with AdaBoost, Bagging, and MultiBoost ensemble frameworks, Environ Earth Sci, 75, 14, pp. 1-22, (2016); Wang L.J., Guo M., Sawada K., Lin J., Zhang J., A comparative study of landslide susceptibility maps using logistic regression, frequency ratio, decision tree, weights of evidence and artificial neural network, Geosci J, 20, 1, pp. 117-136, (2016); Breiman L., Bagging predictors, Mach Learn, 24, 2, pp. 123-140, (1996); Yu X., Wang Y., Niu R., Hu Y., A Combination of Geographically Weighted Regression, Particle Swarm Optimization and Support Vector Machine for Landslide Susceptibility Mapping: A Case Study at Wanzhou in the Three Gorges Area, China, International Journal of Environmental Research and Public Health, 13, 5, pp. 1-35, (2016); Ahmadyfard A., Modares H., Combining PSO and k-means to enhance data clustering, Int Symp Telecommun IST, pp. 688-691, (2008); Wan S., Entropy-based particle swarm optimization with clustering analysis on landslide susceptibility mapping, Environ Earth Sci, 68, 5, pp. 1349-1366, (2013); Alimohammadlou Y., Najafi A., Gokceoglu C., Estimation of rainfall-induced landslides using ANN and fuzzy clustering methods: a case study in Saeen slope, Azerbaijan province, Iran, Catena, 120, pp. 149-162, (2014); Walia N.K., Singh H., Sharma A., ANFIS: Adaptive Neuro-Fuzzy Inference System- A Survey, International Journal of Computer Applications, 123, 13, pp. 32-38, (2015); Lee S., Ryu J.H., Lee M.J., Won J.S., Use of an artificial neural network for analysis of the susceptibility to landslides at Boun, Korea, Environ Geol, 44, 7, pp. 820-833, (2003); Vasu N.N., Lee S.-R., A hybrid feature selection algorithm integrating an extreme learning machine for landslide susceptibility modeling of Mt. Woomyeon, South Korea, Geomorphology, 263, pp. 50-70, (2016); Kavzoglu T., Kutlug Sahin E., Colkesen I., An assessment of multivariate and bivariate approaches in landslide susceptibility mapping: a case study of Duzkoy district, Nat Hazards, 76, 1, pp. 471-496, (2015); Nguyen P.T., Tuyen T.T., Shirzadi A., Pham B.T., Shahabi H., Omidvar E., Amini A., Entezami H., Prakash I., Phong T.V., Vu T.B., Thanh T., Saro L., Bui D.T., Development of a Novel Hybrid Intelligence Approach for Landslide Spatial Prediction, Applied Sciences, 9, 14, pp. 1-26, (2019); Sevgen E., Kocaman S., Nefeslioglu H.A., Gokceoglu C., A novel performance assessment approach using photogrammetric techniques for landslide susceptibility mapping with logistic regression, ANN Random Forest Sens, 19, 18, (2019); Roodposhti M.S., Aryal J., Pradhan B., A novel rule-based approach in mapping landslide susceptibility, Sensors, 19, 10, pp. 1-20, (2019); Dou J., Yunus A.P., Bui D.T., Merghadi A., Sahana M., Zhu Z., Chen C., Han Z., Pham B., Improved landslide assessment using support vector machine with bagging, boosting, and stacking ensemble machine learning framework in a mountainous watershed, Japan, Landslides, 17, 3, pp. 641-658, (2020); Breiman L., Random forests, Mach Learn, 45, 1, pp. 5-32, (2001); Melville B., Lucieer A., Aryal J., Object-based random forest classification of Landsat ETM+ and WorldView-2 satellite imagery for mapping lowland native grassland communities in Tasmania, Australia, Int J Appl Earth Obs Geoinf, 66, pp. 46-55, (2018); Wang Y., Wu X., Chen Z., Ren F., Feng L., Du Q., Optimizing the predictive ability of machine learning methods for landslide susceptibility mapping using smote for Lishui city in Zhejiang province, China, Int J Environ Res Publ Health, 16, 3, pp. 1-37, (2019); Sun X., Chen J., Bao Y., Han X., Zhan J., Peng W., Landslide susceptibility mapping using logistic regression analysis along the Jinsha river and its tributaries close to Derong and Deqin County, Southwestern China, ISPRS Int J Geo-Information, 7, 11, pp. 1-29, (2018); Ian H.W., Eibe F., Mark A.H., Christopher J.P., Data Mining: Practical Machine Learning Tools and Techniques, (2017); Lee S., Ryu J.H., Lee M.J., Won J.S., The application of artificial neural networks to landslide susceptibility mapping at Janghung, Korea, Math Geol, 38, 2, pp. 199-220, (2006); Kohavi R., Scaling up the accuracy of NB classifier: a DT hybrid, Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pp. 202-207, (1996); Golovko D., Roessner S., Behling R., Wetzel H.U., Kleinschmit B., Evaluation of remote-sensing-based landslide inventories for hazard assessment in southern Kyrgyzstan, Remote Sensing (Special Issue: Remote Sensing of Landslides), 9, 9, pp. 1-22, (2017); Zhao Y., Zhang Y., Comparison of decision tree methods for finding active objects, Adv Sp Res, 41, 12, pp. 1955-1959, (2008); Brodley C.E., Utgoff P.E., Multivariate Versus Univariate Decision Trees, (1992); Kadavi P.R., Lee C.W., Lee S., Application of ensemble-based machine learning models to landslide susceptibility mapping, Remote Sens, 10, 8, pp. 1-18, (2018); Harvey P.K., Brewer T.S., On the neutron absorption properties of basic and ultrabasic rocks: The significance of minor and trace elements, Geol Soc Spec Publ, 240, pp. 207-217, (2005); Pourghasemi H.R., Gayen A., Park S., Lee C.W., Lee S., Assessment of landslide-prone areas and their zonation using logistic regression, logitBoost, and NaïveBayes machine-learning algorithms, Sustainability, 10, 10, pp. 2-23, (2018); Friedman J., Hastie T., Tibshirani R., Additive logistic regression: a statistical view of boosting, Ann Stat, 28, 2, pp. 337-407, (2000); Shirzadi A., Soliamani K., Habibnejhad M., Kavian A., Chapi K., Shahabi H., Chen W., Khosravi K., Pham B.T., Pradhan B., Ahmad A., Ahmad B.B., Bui D.T., Novel GIS Based Machine Learning Algorithms for Shallow Landslide Susceptibility Mapping, Sensors (Special Issue: Remote Sensing of Earth Observation: Special Focus on Natural Resources and the Environment), 18, 11, pp. 2-28, (2018)","A. Mohan; GIS Cell, MNNIT Allahabad, Allahabad, India; email: er.amritacs@gmail.com","","John Wiley and Sons Inc","","","","","","21615748","","","","English","Trans. emerg. telecommun. technol.","Article","Final","","Scopus","2-s2.0-85087214140"
"Bhuiyan M.A.E.; Witharana C.; Liljedahl A.K.; Jones B.M.; Daanen R.; Epstein H.E.; Kent K.; Griffin C.G.; Agnew A.","Bhuiyan, Md Abul Ehsan (57213791557); Witharana, Chandi (55453025500); Liljedahl, Anna K. (23091638200); Jones, Benjamin M. (16636910800); Daanen, Ronald (22949833100); Epstein, Howard E. (7102707599); Kent, Kelcy (57216670596); Griffin, Claire G. (56924571500); Agnew, Amber (57219716595)","57213791557; 55453025500; 23091638200; 16636910800; 22949833100; 7102707599; 57216670596; 56924571500; 57219716595","Understanding the effects of optimal combination of spectral bands on deep learning model predictions: A case study based on permafrost tundra landform mapping using high resolution multispectral satellite imagery","2020","Journal of Imaging","6","9","97","","","","20","10.3390/JIMAGING6090097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094897531&doi=10.3390%2fJIMAGING6090097&partnerID=40&md5=010722f70159a73836a8a1c1e6fa7bee","Department of Natural Resources and the Environment, University of Connecticut, Storrs, 06269, CT, United States; Woodwell Climate Research Center, Falmouth, 02540, MA, United States; Institute of Northern Engineering, University of Alaska Fairbanks, Fairbanks, 99775, AK, United States; Alaska Department of Natural Resources, Division of Geological & Geophysical Surveys, Fairbanks, 99775, AK, United States; Department of Environmental Sciences, University of Virginia, Charlottesville, 22904, VA, United States","Bhuiyan M.A.E., Department of Natural Resources and the Environment, University of Connecticut, Storrs, 06269, CT, United States; Witharana C., Department of Natural Resources and the Environment, University of Connecticut, Storrs, 06269, CT, United States; Liljedahl A.K., Woodwell Climate Research Center, Falmouth, 02540, MA, United States, Institute of Northern Engineering, University of Alaska Fairbanks, Fairbanks, 99775, AK, United States; Jones B.M., Institute of Northern Engineering, University of Alaska Fairbanks, Fairbanks, 99775, AK, United States; Daanen R., Alaska Department of Natural Resources, Division of Geological & Geophysical Surveys, Fairbanks, 99775, AK, United States; Epstein H.E., Department of Environmental Sciences, University of Virginia, Charlottesville, 22904, VA, United States; Kent K., Department of Environmental Sciences, University of Virginia, Charlottesville, 22904, VA, United States; Griffin C.G., Department of Environmental Sciences, University of Virginia, Charlottesville, 22904, VA, United States; Agnew A., Department of Natural Resources and the Environment, University of Connecticut, Storrs, 06269, CT, United States","Deep learning (DL) convolutional neural networks (CNNs) have been rapidly adapted in very high spatial resolution (VHSR) satellite image analysis. DLCNN-based computer visions (CV) applications primarily aim for everyday object detection from standard red, green, blue (RGB) imagery, while earth science remote sensing applications focus on geo object detection and classification from multispectral (MS) imagery. MS imagery includes RGB and narrow spectral channels from near- and/or middle-infrared regions of reflectance spectra. The central objective of this exploratory study is to understand to what degree MS band statistics govern DLCNN model predictions. We scaffold our analysis on a case study that uses Arctic tundra permafrost landform features called ice-wedge polygons (IWPs) as candidate geo objects. We choose Mask RCNN as the DLCNN architecture to detect IWPs from eight-band Worldview-02 VHSR satellite imagery. A systematic experiment was designed to understand the impact on choosing the optimal three-band combination in model prediction. We tasked five cohorts of three-band combinations coupled with statistical measures to gauge the spectral variability of input MS bands. The candidate scenes produced high model detection accuracies for the F1 score, ranging between 0.89 to 0.95, for two different band combinations (coastal blue, blue, green (1,2,3) and green, yellow, red (3,4,5)). The mapping workflow discerned the IWPs by exhibiting low random and systematic error in the order of 0.17-0.19 and 0.20-0.21, respectively, for band combinations (1,2,3). Results suggest that the prediction accuracy of the Mask-RCNN model is significantly influenced by the input MS bands. Overall, our findings accentuate the importance of considering the image statistics of input MS bands and careful selection of optimal bands for DLCNN predictions when DLCNN architectures are restricted to three spectral channels. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).","Arctic; Deep learning; Ice-wedge polygons; Mask R-CNN; Permafrost; Satellite imagery; Tundra","","","","","","NSF-OPP, (1043681, 1559691); Polar Geospatial Center; U.S. National Science Foundation, (1720875, 1722572); eXtreme Science and Engineering Discovery Environment, (DPP190001); National Science Foundation, NSF, (1721030, 1929170, OIA-1929170)","Funding text 1: This research was supported by the U.S. National Science Foundation Grant Nos.: 1720875, 1722572, and 1721030. Supercomputing resources were provided by the eXtreme Science and Engineering Discovery Environment (Award No. DPP190001). The authors would like to thank Polar Geospatial Center, University of Minnesota for imagery support. BMJ was supported by the U.S National Science Foundation Grant No. OIA-1929170. We would like to thank Torre Jorgenson and Mikhail Kanevskiy for their domain expertise on permafrost feature classification. Geospatial support for this work provided by the Polar Geospatial Center under NSF-OPP awards 1043681 and 1559691.; Funding text 2: Funding: This research was supported by the U.S. National Science Foundation Grant Nos.: 1720875, 1722572, and 1721030. Supercomputing resources were provided by the eXtreme Science and Engineering Discovery Environment (Award No. DPP190001). The authors would like to thank Polar Geospatial Center, University of Minnesota for imagery support. BMJ was supported by the U.S National Science Foundation Grant No. OIA-1929170.","LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Blaschke T., Object based image analysis for remote sensing, ISPRS J. Photogramm. Remote Sens, 65, pp. 2-16, (2010); Lang S., Baraldi A., Tiede D., Hay G., Blaschke T., Towards a (GE) OBIA 2.0 manifesto-Achievements and open challenges in information & knowledge extraction from big Earth data, Proceedings of the GEOBIA, (2018); Witharana C., Lynch H., An object-based image analysis approach for detecting penguin guano in very high spatial resolution satellite images, Remote Sens, 8, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci. Remote Sens. Lett, 14, pp. 778-782, (2017); Li Y., Qi H., Dai J., Ji X., Wei Y., Fully convolutional instance-aware semantic segmentation, (2016); Yang B., Kim M., Madden M., Assessing optimal image fusion methods for very high spatial resolution satellite images to support coastal monitoring, Giscience Remote Sens, 49, pp. 687-710, (2012); Wei X., Fu K., Gao X., Yan M., Sun X., Chen K., Sun H., Semantic pixel labelling in remote sensing images using a deep convolutional encoder-decoder model, Remote Sens. Lett, 9, pp. 199-208, (2018); Abdulla W., Mask R-Cnn for Object Detection and Instance Segmentation on Keras and Tensorflow; Dai J., He K., Sun J., Instance-aware semantic segmentation via multi-task network cascades, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, (2016); Ren Z., Sudderth E.B., Three-dimensional object detection and layout prediction using clouds of oriented gradients, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1525-1533, (2016); Vuola A.O., Akram S.U., Kannala J., Mask-RCNN and U-net ensembled for nuclei segmentation, Proceedings of the 2019 IEEE 16th International Symposium on Biomedical Imaging, pp. 208-212, (2019); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proceedings of the MICCAI 2015, 9351, pp. 234-241, (2015); Woodcock C.E., Strahler A.H., The factor of scale in remote sensing, Remote Sens. Environ, 21, pp. 311-332, (1987); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., Van der Meer F., Van der Werff H., Van Coillie F., Et al., Geographic object-based image analysis-Towards a new paradigm, ISPRS J. Photogramm. Remote Sens, 87, pp. 180-191, (2014); Hay G.J., Visualizing ScaleDomain Manifolds: A multiscale geoobjectbased approach, Scale Issues Remote Sens, pp. 139-169, (2014); Bhuiyan M.A.E., Witharana C., Liljedahl A.K., Big Imagery as a Resource to Understand Patterns, Dynamics, and Vulnerability of Arctic Polygonal Tundra, Proceedings of the AGUFM 2019, pp. C13E-1374, (2019); Witharana C., Bhuiyan M.A.E., Liljedahl A.K., Towards First pan-Arctic Ice-wedge Polygon Map: Understanding the Synergies of Data Fusion and Deep Learning in Automated Ice-wedge Polygon Detection from High Resolution Commercial Satellite Imagery, Proceedings of the AGUFM 2019, pp. C22C-07, (2019); Zhang W., Witharana C., Liljedahl A., Kanevskiy M., Deep convolutional neural networks for automated characterization of arctic ice-wedge polygons in very high spatial resolution aerial imagery, Remote Sens, 10, (2018); Zhang W., Liljedahl A.K., Kanevskiy M., Epstein H.E., Jones B.M., Jorgenson M.T., Kent K., Transferability of the deep learning mask R-CNN model for automated mapping of ice-wedge polygons in high-resolution satellite and UAV images, Remote Sens, 12, (2020); Liljedahl A.K., Boike J., Daanen R.P., Fedorov A.N., Frost G.V., Grosse G., Hinzman L.D., Iijma Y., Jorgenson J.C., Matveyeva N., Et al., Pan-Arctic ice-wedge degradation in warming permafrost and its influence on tundra hydrology, Nat. Geosci, 9, pp. 312-318, (2016); Turetsky M.R., Abbott B.W., Jones M.C., Anthony K.W., Olefeldt D., Schuur E.A.G., Koven C., McGuire A.D., Grosse G., Kuhry P., Permafrost Collapse is Accelerating Carbon Release, (2019); Jones B.M., Grosse G., Arp C.D., Miller E., Liu L., Hayes D.J., Larsen C.F., Recent Arctic tundra fire initiates widespread thermokarst development, Sci. Rep, 5, (2015); Ulrich M., Grosse G., Strauss J., Schirrmeister L., Quantifying Wedge-Ice Volumes in Yedoma and Thermokarst Basin Deposits, Permafr. Periglac. Process, 25, pp. 151-161, (2014); Muster S., Heim B., Abnizova A., Boike J., Water body distributions across scales: A remote sensing based comparison of three arctic tundra wetlands, Remote Sens, 5, pp. 1498-1523, (2013); Lousada M., Pina P., Vieira G., Bandeira L., Mora C., Evaluation of the use of very high resolution aerial imagery for accurate ice-wedge polygon mapping (Adventdalen, Svalbard), Sci. Total Environ, 615, pp. 1574-1583, (2018); Skurikhin A.N., Wilson C.J., Liljedahl A., Rowland J.C., Recursive active contours for hierarchical segmentation of wetlands in high-resolution satellite imagery of arctic landscapes, Proceedings of the Southwest Symposium on Image Analysis and Interpretation 2014, pp. 137-140, (2014); Abolt C.J., Young M.H., Atchley A.L., Wilson C.J., Brief communication: Rapid machine-learning-based extraction and measurement of ice wedge polygons in high-resolution digital elevation models, Cryosphere, 13, pp. 237-245, (2019); Huang F., Zhang J., Zhou C., Wang Y., Huang J., Zhu L., A deep learning algorithm using a fully connected sparse autoencoder neural network for landslide susceptibility prediction, Landslides, 17, pp. 217-229, (2020); Jiang P., Chen Y., Liu B., He D., Liang C., Real-time detection of apple leaf diseases using deep learning approach based on improved convolutional neural networks, IEEE Access, 7, pp. 59069-59080, (2019); Samuel D., Karam L., Understanding how image quality affects deep neural networks, Proceedings of the 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), (2016); Dodge S., Karam L., A study and comparison of human and deep learning recognition performance under visual distortions, Proceedings of the 2017 26th International Conference on Computer Communication and Networks (ICCCN), pp. 1-7, (2017); Vasiljevic I., Chakrabarti A., Shakhnarovich G., Examining the impact of blur on recognition by convolutional networks, (2016); Karahan S., Yildirum M.K., Kirtac K., Rende F.S., Butun G., Ekenel H.K., How image degradations affect deep cnn-based face recognition?, Proceedings of the 2016 International Conference of the Biometrics Special Interest Group (BIOSIG), pp. 1-5, (2016); Gu Y., Wang Y., Li Y., A Survey on Deep Learning-Driven Remote Sensing Image Scene Understanding: Scene Classification, Scene Retrieval and Scene-Guided Object Detection, Appl. Sci, 9, (2019); Russakovsky O.J., Deng H., Su J., Krause S., Satheesh S., Ma Z., Huang A., Karpathy A., Khosla M., Bernstein A.C., Et al., ImageNet Large Scale Visual Recognition Challenge, Int. J. Comput. Vis, 115, pp. 211-252, (2015); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proceedings of the 2012 Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Raynolds M.K., Walker D.A., Balser A., Bay C., Campbell M., Cherosov M.M., Daniels F.J., Eidesen P.B., Ermokhina K.A., Frost G.V., Et al., A raster version of the Circumpolar Arctic Vegetation Map (CAVM), Remote Sens. Environ, 232, (2019); Walker D.A.F.J.A., Daniels N.V., Matveyeva J., Sibik M.D., Walker A.L., Breen L.A., Druckenmiller M.K., Raynolds H., Bultmann S., Hennekens M., Et al., Wirth Circumpolar arctic vegetation classification Phytocoenologia, Phytocoenologia, 48, pp. 181-201, (2018); He K., Gkioxari G., Dollar P., Girshick R., Mask RCNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); Rumelhart D.E., Hinton G.E., Williams R.J., Learning Internal Representations by Error Propagation; Ehlers M., Klonus S., Astrand P.J., Rosso P., Multi-sensor image fusion for pansharpening in remote sensing, Int. J. Image Data Fusion, 1, pp. 25-45, (2010); Wang S., Quan D., Liang X., Ning M., Guo Y., Jiao L., A deep learning framework for remote sensing image registration, ISPRS J. Photogramm. Remote Sens, 145, pp. 48-164, (2018); Arnall D.B., Relationship between Coefficient of Variation Measured by Spectral Reflectance and Plant Density at Early Growth Stages, (2004); Chuvieco E., Fundamentals of Satellite Remote Sensing: An Environmental Approach, (2016); Bovolo F., Bruzzone L., A detail-preserving scale-driven approach to change detection in multitemporal SAR images, IEEE Trans. Geosci. Remote Sens, 43, pp. 2963-2972, (2005); Jhan J.P., Rau J.Y., A normalized surf for multispectral image matching and band Co-Registration, International Archives of the Photogrammetry. Remote Sens. Spat. Inf. Sci, (2019); Inamdar S., Bovolo F., Bruzzone L., Chaudhuri S., Multidimensional probability density function matching for preprocessing of multitemporal remote sensing images, IEEE Trans. Geosci. Remote Sens, 46, pp. 1243-1252, (2008); Pitie F., Kokaram A., Dahyot R., N-dimensional probability function transfer and its application to color transfer, Proceedings of the IEEE International Conference Comput Vision, 2, pp. 1434-1439, (2005); Pitie F., Kokaram A., Dahyot R., Automated colour grading using colour distribution transfer, Comput. Vis. Image Underst, 107, pp. 123-137, (2007); Liang Y., Sun K., Zeng Y., Li G., Xing M., An Adaptive hierarchical detection method for ship targets in high-resolution SAR images, Remote Sens, 12, (2020); McKight P.E., Najab J., Kruskal-wallis test, The Corsini Encyclopedia of Psychology, (2010); Fagerland M.W., Sandvik L., The Wilcoxon-Mann-Whitney test under scrutiny, Stat. Med, 28, pp. 1487-1497, (2009); Bhuiyan M.A., Nikolopoulos E.I., Anagnostou E.N., Machine learning-based blending of satellite and reanalysis precipitation datasets: A multiregional tropical complex terrain evaluation, J. Hydrometeor, 20, pp. 2147-2161, (2019); Bhuiyan M.A., Begum F., Ilham S.J., Khan R.S., Advanced wind speed prediction using convective weather variables through machine learning application, Appl. Comput. Geosci, 1, (2019); Bhuiyan M.A.E., Nikolopoulos E.I., Anagnostou E.N., Quintana-Segui P., Barella-Ortiz A., A nonparametric statistical technique for combining global precipitation datasets: Development and hydrological evaluation over the Iberian Peninsula, Hydrol. Earth Syst. Sci, 22, pp. 1371-1389, (2018); Bhuiyan M.A.E., Yang F., Biswas N.K., Rahat S.H., Neelam T.J., Machine learning-based error modeling to improve GPM IMERG precipitation product over the brahmaputra river basin, Forecasting, 2, (2020); Powers D., Evaluation: From precision, recall and f-measure to ROC, informedness, markedness & correlation, J. Mach. Learn. Technol, 2, pp. 37-63, (2011)","M.A.E. Bhuiyan; Department of Natural Resources and the Environment, University of Connecticut, Storrs, 06269, United States; email: md.bhuiyan@uconn.edu","","MDPI AG","","","","","","2313433X","","","","English","J. Imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85094897531"
"Maxwell A.E.; Warner T.A.; Guillén L.A.","Maxwell, Aaron E. (55183805600); Warner, Timothy A. (57204253092); Guillén, Luis Andrés (57221113552)","55183805600; 57204253092; 57221113552","Accuracy assessment in convolutional neural network-based deep learning remote sensing studies—part 2: Recommendations and best practices","2021","Remote Sensing","13","13","2591","","","","25","10.3390/rs13132591","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110226604&doi=10.3390%2frs13132591&partnerID=40&md5=feb385a3bd37480e2f9555e3680e83fa","Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States","Maxwell A.E., Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States; Warner T.A., Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States; Guillén L.A., Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States","Convolutional neural network (CNN)-based deep learning (DL) has a wide variety of applications in the geospatial and remote sensing (RS) sciences, and consequently has been a focus of many recent studies. However, a review of accuracy assessment methods used in recently published RS DL studies, focusing on scene classification, object detection, semantic segmentation, and instance segmentation, indicates that RS DL papers appear to follow an accuracy assessment approach that diverges from that of traditional RS studies. Papers reporting on RS DL studies have largely abandoned traditional RS accuracy assessment terminology; they rarely reported a complete confusion matrix; and sampling designs and analysis protocols generally did not provide a population-based confusion matrix, in which the table entries are estimates of the probabilities of occurrence of the mapped landscape. These issues indicate the need for the RS community to develop guidance on best practices for accuracy assessment for CNN-based DL thematic mapping and object detection. As a first step in that process, we explore key issues, including the observation that accuracy assessments should not be biased by the CNN-based training and inference processes that rely on image chips. Furthermore, accuracy assessments should be consistent with prior recommendations and standards in the field, should support the estimation of a population confusion matrix, and should allow for assessment of model generalization. This paper draws from our review of the RS DL literature and the rich record of traditional remote sensing accuracy assessment research while considering the unique nature of CNN-based deep learning to propose accuracy assessment best practices that use appropriate sampling methods, training and validation data partitioning, assessment metrics, and reporting standards. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Accuracy assessment; Deep learning; Feature extraction; Instance segmentation; Object detection; Semantic segmentation; Thematic mapping","Convolution; Convolutional neural networks; Image resolution; Object detection; Object recognition; Remote sensing; Semantics; Accuracy assessment; Assessment metrics; Confusion matrices; Inference process; Model generalization; Reporting standards; Scene classification; Semantic segmentation; Deep learning","","","","","National Science Foundation, NSF, (2046059)","This work was funded by the National Science Foundation (NSF) (Federal Award ID Number 2046059: “CAREER: Mapping Anthropocene Geomorphology with Deep Learning, Big Data Spatial Analytics, and LiDAR”).","Maxwell A., Warner T., Guillen L., Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies—Part 1: Literature Review, Remote Sens, 13, (2021); Foody G.M., Status of land cover classification accuracy assessment, Remote Sens. Environ, 80, pp. 185-201, (2002); Stehman S.V., Foody G.M., Key issues in rigorous accuracy assessment of land cover products, Remote Sens. Environ, 231, (2019); Zhu X.X., Tuia D., Mou L., Xia G.-S., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote Sens. Mag, 5, pp. 8-36, (2017); Zhang L., Zhang L., Du B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art, IEEE Geosci. Remote Sens. Mag, 4, pp. 22-40, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Hoeser T., Bachofer F., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review—Part II: Applications, Remote Sens, 12, (2020); Hoeser T., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part I: Evolution and Recent Trends, Remote Sens, 12, (2020); Basu S., Ganguly S., Mukhopadhyay S., DiBiano R., Karki M., Nemani R., DeepSat: A Learning Framework for Satellite Imagery, Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 1-10, (2015); Ren Z., Sudderth E.B., Three-Dimensional Object Detection and Layout Prediction Using Clouds of Oriented Gradients, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1525-1533; Wei X., Fu K., Gao X., Yan M., Sun X., Chen K., Sun H., Semantic pixel labelling in remote sensing images using a deep convolutional encoder-decoder model, Remote Sens. Lett, 9, pp. 199-208, (2017); Dai J., He K., Sun J., Instance-Aware Semantic Segmentation via Multi-task Network Cascades, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3150-3158; Li Y., Qi H., Dai J., Ji X., Wei Y., Fully Convolutional Instance-Aware Semantic Segmentation, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4438-4446; Boguszewski A., Batorski D., Ziemba-Jankowska N., Zambrzycka A., Dziedzic T., LandCover. ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Commun. ACM, 60, pp. 84-90, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Chollet F., Xception: Deep Learning with Depthwise Separable Convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1251-1258; Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., SSD: Single Shot MultiBox Detector, Proceedings of the Transactions on Petri Nets and Other Models of Concurrency XV, pp. 21-37; Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 2481-2495, (2017); Badrinarayanan V., Handa A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling, (2015); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional Networks for Biomedical Image Segmentation, (2015); Zhou Z., Siddiquee M.R., Tajbakhsh N., Liang J., UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation, IEEE Trans. Med. Imaging, 39, pp. 1856-1867, (2020); Zhou Z., Rahman Siddiquee M.M., Tajbakhsh N., Liang J., UNet++: A Nested U-Net Architecture for Medical Image Segmenta-tion, Proceedings of the Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11, (2018); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell, 40, pp. 834-848, (2018); Chen L.-C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation, Proceedings of the European conference on computer vision (ECCV), (2018); Pinheiro P.O., Collobert R., Dollar P., Learning to Segment Object Candidates, (2015); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2961-2969; Matterport/Mask_RCNN, (2021); Cheng T., Wang X., Huang L., Liu W., Boundary-Preserving Mask R-CNN, Transactions on Petri Nets and Other Models of Concurrency XV, pp. 660-676, (2020); Foody G., Thematic Map Comparison, Photogramm. Eng. Remote Sens, 70, pp. 627-633, (2004); Foody G.M., Harshness in image classification accuracy assessment, Int. J. Remote Sens, 29, pp. 3137-3158, (2008); Stehman S.V., Thematic map accuracy assessment from the perspective of finite population sampling, Int. J. Remote Sens, 16, pp. 589-593, (1995); Stehman S., Statistical Rigor and Practical Utility in Thematic Map Accuracy Assessment, Photogramm. Eng. Remote Sens, 67, pp. 727-734, (2001); Stehman S.V., Comparison of Systematic and Random Sampling for Estimating the Accuracy of Maps Generated from Remotely Sensed Data, PE & RS-Photogramm. Eng. Remote Sens, 58, pp. 1343-1350, (1992); Stehman S.V., Foody G.M., Others Accuracy assessment, The SAGE Handbook of Remote Sensing, pp. 297-309, (2009); Stehman S.V., Selecting and interpreting measures of thematic classification accuracy, Remote Sens. Environ, 62, pp. 77-89, (1997); Stehman S.V., Basic probability sampling designs for thematic map accuracy assessment, Int. J. Remote Sens, 20, pp. 2423-2441, (1999); Stehman S.V., Practical Implications of Design-Based Sampling Inference for Thematic Map Accuracy Assessment, Remote Sens. Environ, 72, pp. 35-45, (2000); Stehman S.V., A Critical Evaluation of the Normalized Error Matrix in Map Accuracy Assessment, Photogramm. Eng. Remote Sens, 70, pp. 743-751, (2004); Stehman S.V., Sampling designs for accuracy assessment of land cover, Int. J. Remote Sens, 30, pp. 5243-5272, (2009); Stehman S.V., Estimating area and map accuracy for stratified random sampling when the strata are different from the map classes, Int. J. Remote Sens, 35, pp. 4923-4939, (2014); Stehman S.V., Czaplewski R.L., Design and Analysis for Thematic Map Accuracy Assessment: Fundamental Principles, Remote Sens. Environ, 64, pp. 331-344, (1998); Stehman S.V., Wickham J.D., Pixels, blocks of pixels, and polygons: Choosing a spatial unit for thematic accuracy assessment, Remote Sens. Environ, 115, pp. 3044-3055, (2011); Congalton R., Accuracy Assessment of Remotely Sensed Data: Future Needs and Directions, Proceedings of the Pecora, 12, pp. 383-388, (1994); Congalton R.G., A review of assessing the accuracy of classifications of remotely sensed data, Remote Sens. Environ, 37, pp. 35-46, (1991); Congalton R.G., Green K., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices, (2019); Congalton R.G., Oderwald R.G., Mead R.A., Assessing Landsat Classification Accuracy Using Discrete Multivariate Analysis Statistical Techniques, Photogramm. Eng. Remote Sens, 49, pp. 1671-1678, (1983); Stehman S.V., Estimating area from an accuracy assessment error matrix, Remote Sens. Environ, 132, pp. 202-211, (2013); Stehman S.V., Impact of sample size allocation when using stratified random sampling to estimate accuracy and area of land-cover change, Remote Sens. Lett, 3, pp. 111-120, (2012); Clinton N., Holt A., Scarborough J., Yan L., Gong P., Accuracy Assessment Measures for Object-based Image Segmentation Goodness, Photogramm. Eng. Remote Sens, 76, pp. 289-299, (2010); Kucharczyk M., Hay G., Ghaffarian S., Hugenholtz C., Geographic Object-Based Image Analysis: A Primer and Future Directions, Remote Sens, 12, (2020); Lizarazo I., Accuracy assessment of object-based image classification: Another STEP, Int. J. Remote Sens, 35, pp. 6135-6156, (2014); Radoux J., Bogaert P., Fasbender D., Defourny P., Thematic accuracy assessment of geographic object-based image classification, Int. J. Geogr. Inf. Sci, 25, pp. 895-911, (2011); Radoux J., Bogaert P., Good Practices for Object-Based Accuracy Assessment, Remote Sens, 9, (2017); Maxwell A.E., Warner T.A., Thematic Classification Accuracy Assessment with Inherently Uncertain Boundaries: An Argument for Center-Weighted Accuracy Assessment Metrics, Remote Sens, 12, (2020); Li W., Wang Z., Wang Y., Wu J., Wang J., Jia Y., Gui G., Classification of High-Spatial-Resolution Remote Sensing Scenes Method Using Transfer Learning and Deep Convolutional Neural Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 1986-1995, (2020); Pontius R.G., Millones M., Death to Kappa: Birth of quantity disagreement and allocation disagreement for accuracy assessment, Int. J. Remote Sens, 32, pp. 4407-4429, (2011); Foody G.M., Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification, Remote Sens. Environ, 239, (2020); Howard J., Gugger S., Deep Learning for Coders with Fastai and PyTorch, (2020); Subramanian V., Deep Learning with PyTorch: A Practical Approach to Building Neural Network Models Using PyTorch, (2018); Howard J., Gugger S., Fastai: A Layered API for Deep Learning, Information, 11, (2020); Graf L., Bach H., Tiede D., Semantic Segmentation of Sentinel-2 Imagery for Mapping Irrigation Center Pivots, Remote Sens, 12, (2020); Tharwat A., Classification assessment methods, Appl. Comput. Inform, 17, pp. 168-192, (2021); Singh A., Kalke H., Loewen M., Ray N., River Ice Segmentation with Deep Learning, IEEE Trans. Geosci. Remote Sens, 58, pp. 7570-7579, (2020); Zhang W., Liljedahl A.K., Kanevskiy M., Epstein H.E., Jones B.M., Jorgenson M.T., Kent K., Transferability of the Deep Learning Mask R-CNN Model for Automated Mapping of Ice-Wedge Polygons in High-Resolution Satellite and UAV Images, Remote Sens, 12, (2020); Maxwell A.E., Bester M.S., Guillen L.A., Ramezan C.A., Carpinello D.J., Fan Y., Hartley F.M., Maynard S.M., Pyron J.L., Semantic Segmentation Deep Learning for Extracting Surface Mine Extents from Historic Topographic Maps, Remote Sens, 12, (2020); Maxwell A.E., Pourmohammadi P., Poyner J.D., Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data, Remote Sens, 12, (2020); Zhang W., Witharana C., Liljedahl A.K., Kanevskiy M., Deep Convolutional Neural Networks for Automated Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery, Remote Sens, 10, (2018); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark, Proceedings of the 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229; Robinson C., Hou L., Malkin K., Soobitsky R., Czawlytko J., Dilkina B., Jojic N., Large Scale High-Resolution Land Cover Mapping with Multi-Resolution Data, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12726-12735; Qi K., Yang C., Hu C., Guan Q., Tian W., Shen S., Peng F., Polycentric Circle Pooling in Deep Convolutional Networks for High-Resolution Remote Sensing Image Recognition, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 632-641, (2020); Prakash N., Manconi A., Loew S., Mapping Landslides on EO Data: Performance of Deep Learning Models vs. Traditional Machine Learning Models, Remote Sens, 12, (2020); Ii D.J.G., Haupt S.E., Nychka D.W., Thompson G., Interpretable Deep Learning for Spatial Analysis of Severe Hailstorms, Mon. Weather Rev, 147, pp. 2827-2845, (2019); Ngo P.T.T., Panahi M., Khosravi K., Ghorbanzadeh O., Kariminejad N., Cerda A., Lee S., Evaluation of deep learning algorithms for national scale landslide susceptibility mapping of Iran, Geosci. Front, 12, pp. 505-519, (2021); Lobo J.M., Jimenez-Valverde A., Real R., AUC: A misleading measure of the performance of predictive distribution models, Glob. Ecol. Biogeogr, 17, pp. 145-151, (2008); Saito T., Rehmsmeier M., The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, PLoS ONE, 10, (2015); Pham M.-T., Courtrai L., Friguet C., Lefevre S., Baussard A., YOLO-Fine: One-Stage Detector of Small Objects under Various Backgrounds in Remote Sensing Images, Remote Sens, 12, (2020); Chen J., Wan L., Zhu J., Xu G., Deng M., Multi-Scale Spatial and Channel-wise Attention for Improving Object Detection in Remote Sensing Imagery, IEEE Geosci. Remote Sens. Lett, 17, pp. 681-685, (2020); Oh S., Chang A., Ashapure A., Jung J., Dube N., Maeda M., Gonzalez D., Landivar J., Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework, Remote Sens, 12, (2020); Henderson P., Ferrari V., End-to-End Training of Object Class Detectors for Mean Average Precision, Proceedings of the Asian Conference on Computer Vision, pp. 198-213; Zheng Z., Zhong Y., Ma A., Han X., Zhao J., Liu Y., Zhang L., HyNet: Hyper-scale object detection network framework for multiple spatial resolution remote sensing imagery, ISPRS J. Photogramm. Remote Sens, 166, pp. 1-14, (2020); COCO-Common Objects in Context; Wu T., Hu Y., Peng L., Chen R., Improved Anchor-Free Instance Segmentation for Building Extraction from High-Resolution Remote Sensing Images, Remote Sens, 12, (2020); Cheng G., Xie X., Han J., Guo L., Xia G.-S., Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 3735-3756, (2020); Maxwell A.E., Warner T.A., Fang F., Implementation of machine-learning classification in remote sensing: An applied review, Int. J. Remote Sens, 39, pp. 2784-2817, (2018); Koutsoukas A., Monaghan K.J., Li X., Huan J., Deep-learning: Investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data, J. Cheminform, 9, pp. 1-13, (2017); Musgrave K., Belongie S., Lim S.-N., A Metric Learning Reality Check, Proceedings of the European Conference on Computer Vision; Sejnowski T.J., The unreasonable effectiveness of deep learning in artificial intelligence, Proc. Natl. Acad. Sci. USA, 117, pp. 30033-30038, (2020); Haralick R.M., Statistical and structural approaches to texture, Proc. IEEE, 67, pp. 786-804, (1979); Warner T., Kernel-Based Texture in Remote Sensing Image Classification, Geogr. Compass, 5, pp. 781-798, (2011); Kim M., Madden M., Warner T.A., Forest Type Mapping using Object-specific Texture Measures from Multispectral Ikonos Imagery, Photogramm. Eng. Remote Sens, 75, pp. 819-829, (2009); Kim M., Warner T.A., Madden M., Atkinson D.S., Multi-scale GEOBIA with very high spatial resolution digital aerial imagery: Scale, texture and image objects, Int. J. Remote Sens, 32, pp. 2825-2850, (2011); Fern C., Warner T.A., Scale and Texture in Digital Image Classification, Photogramm. Eng. Remote Sens, 68, pp. 51-63, (2002); Foody G.M., Sample size determination for image classification accuracy assessment and comparison, Int. J. Remote Sens, 30, pp. 5273-5291, (2009); Cortes C., Mohri M., Confidence Intervals for the Area Under the ROC Curve, Adv. Neural Inf. Process. Syst, 17, pp. 305-312, (2005); Demler O.V., Pencina M.J., Misuse of DeLong test to compare AUCs for nested models, Stat. Med, 31, pp. 2577-2587, (2012); Maxwell A.E., Warner T.A., Is high spatial resolution DEM data necessary for mapping palustrine wetlands?, Int. J. Remote Sens, 40, pp. 118-137, (2018); Maxwell A.E., Warner T.A., Strager M.P., Predicting Palustrine Wetland Probability Using Random Forest Machine Learning and Digital Elevation Data-Derived Terrain Variables, Photogramm. Eng. Remote Sens, 82, pp. 437-447, (2016); Wright C., Gallant A., Improved wetland remote sensing in Yellowstone National Park using classification trees to combine TM imagery and ancillary environmental data, Remote Sens. Environ, 107, pp. 582-605, (2007); Fuller R., Groom G., Jones A., The Land-Cover Map of Great Britain: An Automated Classification of Landsat Thematic Mapper Data, Photogramm. Eng. Remote Sens, 60, pp. 553-562, (1994); Foody G., Approaches for the production and evaluation of fuzzy land cover classifications from remotely-sensed data, Int. J. Remote Sens, 17, pp. 1317-1340, (1996); Foody G.M., Local characterization of thematic classification accuracy through spatially constrained confusion matrices, Int. J. Remote Sens, 26, pp. 1217-1228, (2005); Berberoglu S., Lloyd C., Atkinson P., Curran P., The integration of spectral and textural information using neural networks for land cover mapping in the Mediterranean, Comput. Geosci, 26, pp. 385-396, (2000); Rodriguez-Galiano V.F., Olmo M.C., Abarca-Hernandez F., Atkinson P., Jeganathan C., Random Forest classification of Mediterranean land cover using multi-seasonal imagery and multi-seasonal texture, Remote Sens. Environ, 121, pp. 93-107, (2012); Rodriguez-Galiano V.F., Chica-Rivas M., Evaluation of different machine learning methods for land cover mapping of a Mediterranean area using multi-seasonal Landsat images and Digital Terrain Models, Int. J. Digit. Earth, 7, pp. 492-509, (2014); Senf C., Leitao P.J., Pflugmacher D., van der Linden S., Hostert P., Mapping land cover in complex Mediterranean landscapes using Landsat: Improved classification accuracies from integrating multi-seasonal and synthetic imagery, Remote Sens. Environ, 156, pp. 527-536, (2015); Stow D.A., Hope A., McGuire D., Verbyla D., Gamon J., Huemmrich F., Houston S., Racine C., Sturm M., Tape K., Et al., Remote sensing of vegetation and land-cover change in Arctic Tundra Ecosystems, Remote Sens. Environ, 89, pp. 281-308, (2004); Rees W., Williams M., Vitebsky P., Mapping land cover change in a reindeer herding area of the Russian Arctic using Landsat TM and ETM+ imagery and indigenous knowledge, Remote Sens. Environ, 85, pp. 441-452, (2003); Bartsch A., Hofler A., Kroisleitner C., Trofaier A.M., Land Cover Mapping in Northern High Latitude Permafrost Regions with Satellite Data: Achievements and Remaining Challenges, Remote Sens, 8, (2016); Cingolani A.M., Renison D., Zak M.R., Cabido M.R., Mapping Vegetation in a Heterogeneous Mountain Rangeland Using Landsat Data: An Alternative Method to Define and Classify Land-Cover Units, Remote Sens. Environ, 92, pp. 84-97, (2004); Rigge M., Homer C., Shi H., Wylie B., Departures of Rangeland Fractional Component Cover and Land Cover from Landsat-Based Ecological Potential in Wyoming, USA, Rangel. Ecol. Manag, 73, pp. 856-870, (2020); Herold M., Scepan J., Clarke K., The Use of Remote Sensing and Landscape Metrics to Describe Structures and Changes in Urban Land Uses, Environ. Plan. A Econ. Space, 34, pp. 1443-1458, (2002); Huang X., Wang Y., Li J., Chang X., Cao Y., Xie J., Gong J., High-resolution urban land-cover mapping and landscape analysis of the 42 major cities in China using ZY-3 satellite images, Sci. Bull, 65, pp. 1039-1048, (2020); Dennis M., Barlow D., Cavan G., Cook P.A., Gilchrist A., Handley J., James P., Thompson J., Tzoulas K., Wheater C.P., Et al., Mapping Urban Green Infrastructure: A Novel Landscape-Based Approach to Incorporating Land Use and Land Cover in the Mapping of Human-Dominated Systems, Land, 7, (2018); Li X., Shao G., Object-Based Land-Cover Mapping with High Resolution Aerial Photography at a County Scale in Midwestern USA, Remote Sens, 6, pp. 11372-11390, (2014); Witharana C., Bhuiyan A.E., Liljedahl A.K., Kanevskiy M., Epstein H.E., Jones B.M., Daanen R., Griffin C.G., Kent K., Jones M.K.W., Understanding the synergies of deep learning and data fusion of multispectral and panchromatic high resolution commercial satellite imagery for automated ice-wedge polygon detection, ISPRS J. Photogramm. Remote Sens, 170, pp. 174-191, (2020); Mou L., Hua Y., Zhu X.X., Relation Matters: Relational Context-Aware Fully Convolutional Network for Semantic Segmentation of High-Resolution Aerial Images, IEEE Trans. Geosci. Remote Sens, 58, pp. 7557-7569, (2020); Luo S., Li H., Shen H., Deeply supervised convolutional neural network for shadow detection based on a novel aerial shadow imagery dataset, ISPRS J. Photogramm. Remote Sens, 167, pp. 443-457, (2020)","A.E. Maxwell; Department of Geology and Geography, West Virginia University, Morgantown, 26505, United States; email: Aaron.Maxwell@mail.wvu.edu","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110226604"
"Stasenko N.; Chernova E.; Shadrin D.; Ovchinnikov G.; Krivolapov I.; Pukalchik M.","Stasenko, Nikita (57226395212); Chernova, Elizaveta (57222625110); Shadrin, Dmitrii (57203155501); Ovchinnikov, George (8729918600); Krivolapov, Ivan (57214139426); Pukalchik, Mariia (56368460100)","57226395212; 57222625110; 57203155501; 8729918600; 57214139426; 56368460100","Deep Learning for improving the storage process: Accurate and automatic segmentation of spoiled areas on apples","2021","Conference Record - IEEE Instrumentation and Measurement Technology Conference","2021-May","","9460071","","","","7","10.1109/I2MTC50364.2021.9460071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113710909&doi=10.1109%2fI2MTC50364.2021.9460071&partnerID=40&md5=cd58c718b6d9746ed7bda6b15dfa1b6a","Skolkovo Institute of Science and Technology, Cdise, Moscow, Russian Federation; Skolkovo Institute of Science and Technology, Cdise, Digital Agriculture Lab, Moscow, Russian Federation; Michurinsk State Agrarian University, Department of Technological Processes and Technosphere Safety, Michurinsk, Russian Federation","Stasenko N., Skolkovo Institute of Science and Technology, Cdise, Moscow, Russian Federation; Chernova E., Skolkovo Institute of Science and Technology, Cdise, Digital Agriculture Lab, Moscow, Russian Federation; Shadrin D., Skolkovo Institute of Science and Technology, Cdise, Digital Agriculture Lab, Moscow, Russian Federation; Ovchinnikov G., Skolkovo Institute of Science and Technology, Cdise, Moscow, Russian Federation; Krivolapov I., Michurinsk State Agrarian University, Department of Technological Processes and Technosphere Safety, Michurinsk, Russian Federation; Pukalchik M., Skolkovo Institute of Science and Technology, Cdise, Digital Agriculture Lab, Moscow, Russian Federation","Artificial Intelligence (AI) methods and technologies have been successfully applied for recognizing objects, detecting and segmenting RGB images. Today, such technologies are widely used in precision agriculture to estimate food quality, especially when assessing plants and fruits at various harvest stages. There are also several processes taking place in food during the postharvest stages, such as decay and moldy. However, the number of AI approaches allowing for assessing the postharvest food conditions is limited. In this work, we trained U-Net and Deeplab models based on Convolutional Neural Networks (CNNs) to detect and predict decay areas in postharvest apples stored at room temperatures. The models were trained on a dataset that includes 4440 images of apples with segmented decay areas. Images were captured by a digital camera mounted on a custom-made testbed. We achieved 99.71% of the mean Intersection over Union (mIoU) at the testing stage for the U-Net model and 99.99% of the mIoU at the testing stage for the Deeplab model trained on 651 images. We also presented the first masks for decay areas in apples predicted by U-Net. Our approach seems to be promising for improving the food storage process in precision agriculture by enabling the automatic detection and quantification of the decayed areas. © 2021 IEEE.","ANN; Apple fruits; Deep learning; Postharvest decay; RGB","Agricultural robots; Convolutional neural networks; Decay (organic); Food storage; Fruits; Object detection; Precision agriculture; Automatic Detection; Automatic segmentations; Food quality; Net model; Postharvest; RGB images; Deep learning","","","","","Russian Foundation for Basic Research, РФФИ, (19-29-09085)","V. ACKNOWLEDGMENT The reported study was funded by RFBR, project number 19-29-09085 MK.","Bruinsma J., Et al., The resource outlook to 2050: By how much do land, water and crop yields need to increase by 2050, Expert Meeting on How to Feed the World, 2050, pp. 24-26, (2009); Godfray H.C.J., Beddington J.R., Crute I.R., Haddad L., Lawrence D., Muir J.F., Pretty J., Robinson S., Thomas S.M., Toulmin C., Food security: The challenge of feeding 9 billion people, Science, 327, 5967, pp. 812-818, (2010); Sterne J., Artificial Intelligence for Marketing: Practical Applications, (2017); Sodhro A.H., Pirbhulal S., De Albuquerque V.H.C., Artificial intelligence-driven mechanism for edge computing-based industrial applications, Ieee Transactions on Industrial Informatics, 15, 7, pp. 4235-4243, (2019); Purandare H., Ketkar N., Pansare S., Padhye P., Ghotkar A., Analysis of post-harvest losses: An internet of things and machine learning approach, 2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT), pp. 222-226, (2016); Kulbacki M., Segen J., Kniec W., Klempous R., Kluwak K., Nikodem J., Kulbacka J., Serester A., Survey of drones for agriculture automation from planting to harvest, 2018 Ieee 22nd International Conference on Intelligent Engineering Systems (INES). Ieee, pp. 000353-000358, (2018); Williams H.A., Jones M.H., Nejati M., Seabright M.J., Bell J., Penhall N.D., Barnett J.J., Duke M.D., Scarfe A.J., Ahn H.S., Et al., Robotic kiwifruit harvesting using machine vision, convolutional neural networks, and robotic arms, Biosystems Engineering, 181, pp. 140-156, (2019); Puttemans S., Vanbrabant Y., Tits L., Goedeme T., Automated visual fruit detection for harvest estimation and robotic harvesting, 2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA), pp. 1-6, (2016); Jana S., Basak S., Parekh R., Automatic fruit recognition from natural images using color and texture features, 2017 Devices for Integrated Circuit (DevIC), pp. 620-624, (2017); Bhargava A., Bansal A., Automatic detection and grading of multiple fruits by machine learning, Food Analytical Methods, 13, 3, pp. 751-761, (2020); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Communications of the Acm, 60, 6, pp. 84-90, (2017); Naranjo-Torres J., Mora M., Hernandez-Garcia R., Barrientos R.J., Fredes C., Valenzuela A., A review of convolutional neural network applied to fruit image processing, Applied Sciences, 10, 10, (2020); Sharma P., Berwal Y.P.S., Ghai W., Performance analysis of deep learning cnn models for disease detection in plants using image segmentation, Information Processing in Agriculture, 7, 4, pp. 566-574, (2020); Al-Shawwa M.O., Classification of Apple Fruits by Deep Learning, (2020); Bargoti S., Underwood J.P., Image segmentation for fruit detection and yield estimation in apple orchards, Journal of Field Robotics, 34, 6, pp. 1039-1060, (2017); Dias P.A., Tabb A., Medeiros H., Apple flower detection using deep convolutional networks, Computers in Industry, 99, pp. 17-28, (2018); Khan M.A., Lali M.I.U., Sharif M., Javed K., Aurangzeb K., Haider S.I., Altamrah A.S., Akram T., An optimized method for segmentation and classification of apple diseases based on strong correlation and genetic algorithm based feature selection, Ieee Access, 7, 261, (2019); Ranjit K., Raghunandan K., Naveen C., Chethan H., Sunil C., Deep features based approach for fruit disease detection and classification, Int. J. Comput. Sci. Eng, 7, 4, pp. 2347-2693, (2019); Alharbi A.G., Arif M., Detection and classification of apple diseases using convolutional neural networks, 2020 2nd International Conference on Computer and Information Sciences (ICCIS), pp. 1-6, (2020); Tian Y., Yang G., Wang Z., Li E., Liang Z., Detection of apple lesions in orchards based on deep learning methods of cyclegan and yolov3-dense, Journal of Sensors, 2019, (2019); Valdez P., Apple Defect Detection Using Deep Learning Based Object Detection for Better Post Harvest Handling, (2020); Chu P., Li Z., Lammers K., Lu R., Liu X., Deepapple: Deep Learning-based Apple Detection Using a Suppression Mask R-cnn, (2020); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-assisted Intervention, pp. 234-241, (2015); Yurtkulu S.C., Sahin Y.H., Unal G., Semantic segmentation with extended deeplabv3 architecture, 2019 27th Signal Processing and Communications Applications Conference (SIU), pp. 1-4, (2019); Zhao T., Yang Y., Niu H., Wang D., Chen Y., Comparing unet convolutional network with mask r-cnn in the performances of pomegranate tree canopy segmentation, Multispectral, Hyperspectral, and Ultraspectral Remote Sensing Technology, Techniques and Applications Vii, (2018); Wei S., Zhang H., Wang C., Wang Y., Xu L., Multi-temporal sar data large-scale crop mapping based on u-net model, Remote Sensing, 11, 1, (2019); Apaza M.A.C., Monzon H.M.B., Garrido R.P.A., Semantic segmentation of weeds and crops in multispectral images by using a convolutional neural networks based on u-net, International Conference on Applied Technologies, pp. 473-485, (2019); Shadrin D., Chashchin A., Ovchinnikov G., Somov A., System identification-soilless growth of tomatoes, 2019 Ieee International Instrumentation and Measurement Technology Conference (I2MTC). Ieee, pp. 1-6, (2019); Stasenko N., Spoiled Apples, (2020); Chen L.-C., Papandreou G., Schroff F., Adam H., Rethinking Atrous Convolution for Semantic Image Segmentation, (2017); Jiang B., He J., Yang S., Fu H., Li T., Song H., He D., Fusion of machine vision technology and alexnet-cnns deep learning network for the detection of postharvest apple pesticide residues, Artificial Intelligence in Agriculture, 1, pp. 1-8, (2019)","","","Institute of Electrical and Electronics Engineers Inc.","IEEE; IEEE Instrumentation and Measurement Society","2021 IEEE International Instrumentation and Measurement Technology Conference, I2MTC 2021","17 May 2021 through 20 May 2021","Virtual, Glasgow","171021","10915281","978-172819539-1","CRIIE","","English","Conf. Rec. IEEE Instrum. Meas. Technol. Conf.","Conference paper","Final","","Scopus","2-s2.0-85113710909"
"Zhan T.; Gong M.; Jiang X.; Zhang M.","Zhan, Tao (57052462500); Gong, Maoguo (8933846400); Jiang, Xiangming (57191078133); Zhang, Mingyang (56549116200)","57052462500; 8933846400; 57191078133; 56549116200","Unsupervised Scale-Driven Change Detection with Deep Spatial-Spectral Features for VHR Images","2020","IEEE Transactions on Geoscience and Remote Sensing","58","8","8985538","5653","5665","12","26","10.1109/TGRS.2020.2968098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089209815&doi=10.1109%2fTGRS.2020.2968098&partnerID=40&md5=a8817d1819e43c60f9954ea8991bd0d0","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Computer Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi'an, China","Zhan T., Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Computer Science and Technology, Xidian University, Xi'an, China; Gong M., Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi'an, China; Jiang X., Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi'an, China; Zhang M., Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi'an, China","The rapid development of remote sensing technology has enabled the acquisition of very high spatial resolution (VHR) multitemporal images in Earth observation. However, how to effectively exploit these existing data to accurately monitor land surface changes is still a challenging task. In this article, we propose an unsupervised scale-driven change detection (CD) framework for VHR images by jointly analyzing the spatial-spectral change information, which combines the advantages of deep feature learning and multiscale decision fusion. First, a well pretrained deep fully convolutional network (FCN) is used to automatically extract the deep spatial context information from the acquired images. Then, the uncertainty analysis incorporating the deep spatial feature and the image spectral feature is implemented to generate a pseudobinary change map. On this basis, it is easy to choose suitable samples to train an excellent support vector machine (SVM) classifier, thus detecting changes occurred on the ground. In addition, the multiscale superpixel segmentation technique is introduced to make full use of the spatial structural information, which takes an image-object as the basic analysis unit. Finally, a robust binary change map with high detection precision can be achieved by merging the CD results obtained at different scales. The impressive experimental results on four real data sets demonstrate the effectiveness and flexibility of the proposed framework.  © 1980-2012 IEEE.","Change detection (CD); feature extraction; fully convolutional network (FCN); multitemporal images; remote sensing; support vector machine (SVM)","Convolutional neural networks; Deep learning; Image analysis; Image segmentation; Remote sensing; Support vector machines; Uncertainty analysis; Convolutional networks; Deep feature learning; Land surface change; Multi-temporal image; Remote sensing technology; Spatial structural information; Superpixel segmentations; Very-high spatial resolutions; AVHRR; detection method; satellite imagery; spatial analysis; spectral analysis; unsupervised classification; Feature extraction","","","","","National Natural Science Foundation of China, NSFC, (61772393, 61906147); China Postdoctoral Science Foundation, (2019M663931XB); Shanxi Provincial Key Research and Development Project, (2018ZDXM-GY-045)","Manuscript received July 15, 2019; revised November 5, 2019; accepted January 15, 2020. Date of publication February 6, 2020; date of current version July 22, 2020. This work was supported in part by the National Natural Science Foundation of China under Grant 61772393 and Grant 61906147, in part by the Key Research and Development Program of Shaanxi Province under Grant 2018ZDXM-GY-045, and in part by the China Post-Doctoral Science Foundation under Grant 2019M663931XB. (Corresponding author: Maoguo Gong.) Tao Zhan is with the Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Computer Science and Technology, Xidian University, Xi’an 710071, China (e-mail: omegazhant@gmail.com).","Bruzzone L., Bovolo F., A novel framework for the design of change-detection systems for very-high-resolution remote sensing images, Proc. Ieee, 101, 3, pp. 609-630, (2013); Chi M., Plaza A., Benediktsson J.A., Sun Z., Shen J., Zhu Y., Big data for remote sensing: Challenges and opportunities, Proc. Ieee, 104, 11, pp. 2207-2219, (2016); Liu S., Bruzzone L., Bovolo F., Du P., Hierarchical unsupervised change detection in multitemporal hyperspectral images, Ieee Trans. Geosci. Remote Sens., 53, 1, pp. 244-260, (2015); Singh A., Review article digital change detection techniques using remotely-sensed data, Int. J. Remote Sens., 10, 6, pp. 989-1003, (1989); Coppin P., Jonckheere I., Nackaerts K., Muys B., Digital change detection methods in ecosystem monitoring: A review, Int. J. Remote Sens., 25, 9, pp. 1565-1596, (2004); Brunner D., Lemoine G., Bruzzone L., Earthquake damage assessment of buildings using VHR optical and SAR imagery, Ieee Trans. Geosci. Remote Sens., 48, 5, pp. 2403-2420, (2010); Zhu Z., Woodcock C.E., Continuous change detection and classification of land cover using all available Landsat data, Remote Sens. Environment, 144, pp. 152-171, (2014); Serra P., Pons X., Sauri D., Post-classification change detection with data from different sensors: Some accuracy considerations, Int. J. Remote Sens., 24, 23, pp. 4975-4976, (2003); Meddens A.J., Hicke J.A., Vierling L.A., Hudak A.T., Evaluating methods to detect bark beetle-caused tree mortality using single-date and multi-date Landsat imagery, Remote Sens. Environ., 132, pp. 49-58, (2013); Hussain M., Chen D., Cheng A., Wei H., Stanley D., Change detection from remotely sensed images: From pixel-based to objectbased approaches, Isprs J. Photogramm. Remote Sens., 80, pp. 91-106, (2013); Bovolo F., Bruzzone L., A theoretical framework for unsupervised change detection based on change vector analysis in the polar domain, Ieee Trans. Geosci. Remote Sens., 45, 1, pp. 218-236, (2007); Thonfeld F., Feilhauer H., Braun M., Menz G., Robust change vector analysis (RCVA) for multi-sensor very high resolution optical satellite data, Int. J. Appl. Earth Observ. Geoinf., 50, pp. 131-140, (2016); Ortiz-Rivera V., Velez-Reyes M., Roysam B., Change detection in hyperspectral imagery using temporal principal components, Proc. Spie, 6233, (2006); Falco N., Cavallaro G., Marpu P.R., Benediktsson J.A., Unsupervised change detection analysis to multi-channel scenario based on morphological contextual analysis, Proc. Ieee Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 3374-3377, (2016); Nielsen A.A., The regularized iteratively reweighted mad method for change detection in multi-and hyperspectral data, Ieee Trans. Image Process., 16, 2, pp. 463-478, (2007); Bovolo F., Bruzzone L., A detail-preserving scale-driven approach to change detection in multitemporal SAR images, Ieee Trans. Geosci. Remote Sens., 43, 12, pp. 2963-2972, (2005); Inglada J., Mercier G., A new statistical similarity measure for change detection in multitemporal SAR images and its extension to multiscale change analysis, Ieee Trans. Geosci. Remote Sens., 45, 5, pp. 1432-1445, (2007); Bazi Y., Melgani F., Al-Sharari H.D., Unsupervised change detection in multispectral remotely sensed imagery with level set methods, Ieee Trans. Geosci. Remote Sens., 48, 8, pp. 3178-3187, (2010); Hao M., Shi W., Zhang H., Li C., Unsupervised change detection with expectation-maximization-based level set, Ieee Geosci. Remote Sens. Lett., 11, 1, pp. 210-214, (2014); Celik T., Change detection in satellite images using a genetic algorithm approach, Ieee Geosci. Remote Sens. Lett., 7, 2, pp. 386-390, (2010); Zhan T., Tang Z., Gong M., Jiang X., Shi J., Decompositionbased multiobjective particle swarm optimization for change detection in SAR images, Proc. Genetic Evol. Comput. Conf. Companion, pp. 1729-1736, (2018); Chen Y., Cao Z., An improved MRF-based change detection approach for multitemporal remote sensing imagery, Signal Process., 93, 1, pp. 163-175, (2013); Benedek C., Shadaydeh M., Kato Z., Sziranyi T., Zerubia J., Multilayer Markov random field models for change detection in optical remote sensing images, Isprs J. Photogramm. Remote Sens., 107, pp. 22-37, (2015); Blaschke T., Object based image analysis for remote sensing, Isprs J. Photogramm. Remote Sens., 65, 1, pp. 2-16, (2010); Im J., Jensen J.R., Tullis J.A., Object-based change detection using correlation image analysis and image segmentation, Int. J. Remote Sens., 29, 2, pp. 399-423, (2008); Yousif O., Ban Y., A novel approach for object-based change image generation using multitemporal high-resolution SAR images, Int. J. Remote Sens., 38, 7, pp. 1765-1787, (2017); Lopez-Fandino J., Heras D.B., Arguello F., Mura M.D., GPU framework for change detection in multitemporal hyperspectral images, Int. J. Parallel Program., 47, 2, pp. 272-292, (2019); Chen Q., Chen Y., Multi-feature object-based change detection using self-adaptive weight change vector analysis, Remote Sens., 8, 7, (2016); Wang X., Liu S., Du P., Liang H., Xia J., Li Y., Object-based change detection in urban areas from high spatial resolution images based on multiple features and ensemble learning, Remote Sens., 10, 2, (2018); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, (2015); Chen X., Xiang S., Liu C.-L., Pan C.-H., Vehicle detection in satellite images by hybrid deep convolutional neural networks, Ieee Geosci. Remote Sens. Lett., 11, 10, pp. 1797-1801, (2014); Dahl G.E., Yu D., Deng L., Acero A., Context-dependent pretrained deep neural networks for large-vocabulary speech recognition, Ieee Trans. Audio, Speech, Language Process., 20, 1, pp. 30-42, (2012); Young T., Hazarika D., Poria S., Cambria E., Recent trends in deep learning based natural language processing, Ieee Comput. Intell. Mag., 13, 3, pp. 55-75, (2018); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, Ieee Geosci. Remote Sens. Mag., 4, 2, pp. 22-40, (2016); Zheng X., Yuan Y., Lu X., Dimensionality reduction by spatial-spectral preservation in selected bands, Ieee Trans. Geosci. Remote Sens., 55, 9, pp. 5185-5197, (2017); Fang L., He N., Li S., Plaza A.J., Plaza J., A new spatial-spectral feature extraction method for hyperspectral images using local covariance matrix representation, Ieee Trans. Geosci. Remote Sens., 56, 6, pp. 3534-3546, (2018); Yuan Q., Zhang Q., Li J., Shen H., Zhang L., Hyperspectral image denoising employing a spatial-spectral deep residual convolutional neural network, Ieee Trans. Geosci. Remote Sens., 57, 2, pp. 1205-1218, (2019); Zhan Y., Fu K., Yan M., Sun X., Wang H., Qiu X., Change detection based on deep siamese convolutional network for optical aerial images, Ieee Geosci. Remote Sens. Lett., 14, 10, pp. 1845-1849, (2017); Liu J., Gong M., Qin K., Zhang P., A deep convolutional coupling network for change detection based on heterogeneous optical and radar images, Ieee Trans. Neural Netw. Learn. Syst., 29, 3, pp. 545-559, (2018); Pomente A., Picchiani M., Frate F.D., Sentinel-2 change detection based on deep features, Proc. Ieee Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 6859-6862, (2018); Saha S., Bovolo F., Bruzzone L., Unsupervised deep change vector analysis for multiple-change detection in VHR images, Ieee Trans. Geosci. Remote Sens., 57, 6, pp. 3677-3693, (2019); Gong M., Zhao J., Liu J., Miao Q., Jiao L., Change detection in synthetic aperture radar images based on deep neural networks, Ieee Trans. Neural Netw. Learn. Syst., 27, 1, pp. 125-138, (2016); Gong M., Zhan T., Zhang P., Miao Q., Superpixel-based difference representation learning for change detection in multispectral remote sensing images, Ieee Trans. Geosci. Remote Sens., 55, 5, pp. 2658-2673, (2017); Lopez-Fandi No J., Garea A.S., Heras D.B., Arguello F., Stacked autoencoders for multiclass change detection in hyperspectral images, Proc. Ieee Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 1906-1909, (2018); Li X., Yuan Z., Wang Q., Unsupervised deep noise modeling for hyperspectral image change detection, Remote Sens., 11, 3, (2019); Pacifici F., Longbotham N., Emery W.J., The importance of physical quantities for the analysis of multitemporal and multiangular optical very high spatial resolution images, Ieee Trans. Geosci. Remote Sens., 52, 10, pp. 6241-6256, (2014); Han Y., Bovolo F., Bruzzone L., An approach to fine coregistration between very high resolution multispectral images based on registration noise distribution, Ieee Trans. Geosci. Remote Sens., 53, 12, pp. 6650-6662, (2015); Pan S.J., Yang Q., A survey on transfer learning, Ieee Trans. Knowl. Data Eng., 22, 10, pp. 1345-1359, (2010); Yosinski J., Clune J., Bengio Y., Lipson H., How transferable are features in deep neural networks?, Proc. Adv. Neural Inf. Process. Syst., pp. 3320-3328, (2014); Jiao L., Liang M., Chen H., Yang S., Liu H., Cao X., Deep fully convolutional network-based spatial distribution prediction for hyperspectral image classification, Ieee Trans. Geosci. Remote Sens., 55, 10, pp. 5585-5599, (2017); Zhan T., Gong M., Jiang X., Li S., Log-based transformation feature learning for change detection in heterogeneous images, Ieee Geosci. Remote Sens. Lett., 15, 9, pp. 1352-1356, (2018); Saha S., Bovolo F., Bruzzone L., Destroyed-buildings detection from VHR SAR images using deep features, Proc. SPIE, Image Signal Process. Remote Sens., 10789, (2018); Bruzzone L., Prieto D., Automatic analysis of the difference image for unsupervised change detection, Ieee Trans. Geosci. Remote Sens., 38, 3, pp. 1171-1182, (2000); Zanetti M., Bruzzone L., A theoretical framework for change detection based on a compound multiclass statistical model of the difference image, Ieee Trans. Geosci. Remote Sens., 56, 2, pp. 1129-1143, (2018); Achanta R., Shaji A., Smith K., Lucchi A., Fua P., Susstrunk S., SLIC superpixels compared to state-of-The-art superpixel methods, Ieee Trans. Pattern Anal. Mach. Intell., 34, 11, pp. 2274-2282, (2012); Daudt R.C., Le Saux B., Boulch A., Gousseau Y., Urban change detection for multispectral Earth observation using convolutional neural networks, Proc. Ieee Int. Geosci. Remote Sens. Symp. (IGARSS), pp. 2115-2118, (2018); Rosenfield G.H., Fitzpatrick-Lins K., A coefficient of agreement as a measure of thematic classification accuracy, Photogramm. Eng. Remote Sens., 52, 2, pp. 223-227, (1986); Csillik O., Fast segmentation and classification of very high resolution remote sensing data using SLIC superpixels, Remote Sens., 9, 3, (2017); Bazi Y., Bruzzone L., Melgani F., An unsupervised approach based on the generalized Gaussian model to automatic change detection in multitemporal SAR images, Ieee Trans. Geosci. Remote Sens., 43, 4, pp. 874-887, (2005); Chen G., Hay G.J., Carvalho L.M.T., Wulder M.A., Object-based change detection, Int. J. Remote Sens., 33, 14, pp. 4434-4457, (2012); Gao F., Dong J., Li B., Xu Q., Automatic change detection in synthetic aperture radar images based on PCANet, Ieee Geosci. Remote Sens. Lett., 13, 12, pp. 1792-1796, (2016)","M. Gong; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi'an, China; email: gong@ieee.org","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85089209815"
"He Z.; Cao W.; Yuan J.; He Z.; Zhang Z.","He, Zhiquan (35848578000); Cao, Wenming (7402082924); Yuan, Jianhe (57200789526); He, Zhihai (7403885484); Zhang, Zhi (57188625787)","35848578000; 7402082924; 57200789526; 7403885484; 57188625787","Fast Deep Neural Networks with Knowledge Guided Training and Predicted Regions of Interests for Real-Time Video Object Detection","2018","IEEE Access","6","","","8990","8999","9","45","10.1109/ACCESS.2018.2795798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041006339&doi=10.1109%2fACCESS.2018.2795798&partnerID=40&md5=d7ac20885cfa9534d877969fb1f2d5e4","Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, 518060, China; Department of Electrical and Computer Engineering, Video Processing and Communication Lab, University of Missouri, Columbia, 65211, MO, United States","He Z., Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, 518060, China; Cao W., Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, 518060, China, Department of Electrical and Computer Engineering, Video Processing and Communication Lab, University of Missouri, Columbia, 65211, MO, United States; Yuan J., Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, 518060, China; He Z., Department of Electrical and Computer Engineering, Video Processing and Communication Lab, University of Missouri, Columbia, 65211, MO, United States; Zhang Z., Department of Electrical and Computer Engineering, Video Processing and Communication Lab, University of Missouri, Columbia, 65211, MO, United States","It has been recognized that deeper and wider neural networks are continuously advancing the state-of-The-Art performance of various computer vision and machine learning tasks. However, they often require large sets of labeled data for effective training and suffer from extremely high computational complexity, preventing them from being deployed in real-Time systems, for example vehicle object detection from vehicle cameras for assisted driving. In this paper, we aim to develop a fast deep neural network for real-Time video object detection by exploring the ideas of knowledge-guided training and predicted regions of interest. Specifically, we will develop a new framework for training deep neural networks on datasets with limited labeled samples using cross-network knowledge projection which is able to improve the network performance while reducing the overall computational complexity significantly. A large pre-Trained teacher network is used to observe samples from the training data. A projection matrix is learned to project this teacher-level knowledge and its visual representations from an intermediate layer of the teacher network to an intermediate layer of a thinner and faster student network to guide and regulate the training process. To further speed up the network, we propose to train a low-complexity object detection using traditional machine learning methods, such as support vector machine. Using this low-complexity object detector, we identify the regions of interest that contain the target objects with high confidence. We obtain a mathematical formula to estimate the regions of interest to save the computation for each convolution layer. Our experimental results on vehicle detection from videos demonstrated that the proposed method is able to speed up the network by up to 16 times while maintaining the object detection performance. © 2013 IEEE.","Assisted driving; deep neural networks; knowledge projection; speed optimization; vehicle detection","Artificial intelligence; Complex networks; Computational complexity; Deep learning; Deep neural networks; Education; Interactive computer systems; Learning systems; Motion compensation; Object recognition; Personnel training; Real time systems; Support vector machines; Teaching; Vehicles; Assisted drivings; Detection performance; knowledge projection; Machine learning methods; Speed optimization; State-of-the-art performance; Vehicle detection; Visual representations; Object detection","","","","","National Science Foundation, NSF, (1539389, 1647213); National Natural Science Foundation of China, NSFC, (61375015, 61771322)","This work was supported in part by the National Natural Science Foundation of China under Grant 61771322 and Grant 61375015 and in part by the National Science Foundation through US Ignite under Grant 1647213 and through CyberSEES under Grant 1539389.","Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Process. Syst, pp. 1097-1105, (2012); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-Time object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst, pp. 91-99, (2015); Liu W., Et al., SSD: Single shot multibox detector, Proc. Eur. Conf. Comput. Vis, pp. 21-37, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-Time object detection, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 779-788, (2016); Russakovsky O., Et al., ImageNet large scale visual recognition challenge, Int. J. Comput. Vis, 115, 3, pp. 211-252, (2015); Lin T.-Y., Et al., Microsoft COCO: Common Objects in Context, (2014); Hassibi B., Stork D.G., Second order derivatives for network pruning: Optimal brain surgeon, Proc. Adv. Neural Inf. Process. Syst, pp. 164-171, (1993); Jaderberg M., Vedaldi A., Zisserman A., Speeding Up Convolutional Neural Networks with Low Rank Expansions, (2014); Hinton G., Vinyals O., Dean J., Distilling the Knowledge in A Neural Network, (2015); Zhang K., Scholkopf B., Muandet K., Wang Z., Domain adaptation under target and conditional shift, Proc ICML, pp. 819-827, (2013); Wang X., Schneider J., Flexible transfer learning under support and model shift, Proc. Adv. Neural Inf. Process. Syst, pp. 1898-1906, (2014); Tzeng E., Hoffman J., Darrell T., Saenko K., Simultaneous deep transfer across domains and tasks, Proc IEEE Int. Conf. Comput. Vis, pp. 4068-4076, (2015); Cortes C., Vapnik V., Support vector machine, Mach. Learn, 20, 3, pp. 273-297, (1995); Sun Z., Miller R., Bebis G., DiMeo D., A real-Time precrash vehicle detection system, Proc. 6th IEEE Workshop Appl. Comput. Vis. (WACV, pp. 171-176, (2002); Jones W.D., Building safer cars, IEEE Spectr, 39, 1, pp. 82-85, (2002); El Jaafari I., El Ansari M., Koutti L., Ellahyani A., Char S., A novel approach for on-road vehicle detection and tracking, Int. J. Adv. Comput. Sci. Appl, 7, 1, pp. 594-601, (2016); Suykens J.A.K., Vandewalle J., Least squares support vector machine classifiers, Neural Process. Lett, 9, 3, pp. 293-300, (1999); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proc IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit, 1, pp. 886-893, (2005); Liu W., Wen X., Duan B., Yuan H., Wang N., Rear vehicle detection and tracking for lane change assist, Proc IEEE Intell. Vehicles Symp, pp. 252-257, (2007); Cheon M., Lee W., Yoon C., Park M., Vision-based vehicle detection system with consideration of the detecting location, IEEE Trans. Intell. Transp. Syst, 13, 3, pp. 1243-1252, (2012); Sun Z., Bebis G., Miller R., Monocular precrash vehicle detection: Features and classifiers, IEEE Trans. Image Process, 15, 7, pp. 2019-2034, (2006); Cui J., Liu F., Li Z., Jia Z., Vehicle localisation using a single camera, Proc IEEE Intell. Vehicles Symp. (IV, pp. 871-876, (2010); Sivaraman S., Trivedi M.M., Active learning based robust monocular vehicle detection for on-road safety systems, Proc IEEE Intell. Vehicles Symp, pp. 399-404, (2009); Lowe D.G., Object recognition from local scale-invariant features, Proc IEEE Int. Conf. Comput. Vis, 2, pp. 1150-1157, (1999); Zhang X., Zheng N., He Y., Wang F., Vehicle detection using an extended hidden random field model, Proc. 14th Int. IEEE Conf. Intell. Transp. Syst. (ITSC), pp. 1555-1559, (2011); Li B., Zhang T., Xia T., Vehicle Detection from 3D LiDAR Using Fully Convolutional Network, (2016); Dong Z., Wu Y., Pei M., Jia Y., Vehicle type classification using a semisupervised convolutional neural network, IEEE Trans. Intell. Transp. Syst, 16, 4, pp. 2247-2256, (2015); Chen X., Xiang S., Liu C.-L., Pan C.-H., Vehicle detection in satellite images by hybrid deep convolutional neural networks, IEEE Geosci. Remote Sens. Lett, 11, 10, pp. 1797-1801, (2014); Chen X., Xiang S., Liu C.-L., Pan C.-H., Vehicle detection in satellite images by parallel deep convolutional neural networks, Proc IEEE 2nd IAPR Asian Conf. Pattern Recognit. (ACPR, pp. 181-185, (2013); Park Y.-K., Park J.-K., On H.-I., Kang D.-J., Convolutional neural network-based system for vehicle front-side detection, J. Inst. Control, 21, 11, pp. 1008-1016, (2015); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, (2016); Wang H., Cai Y., Chen L., A vehicle detection algorithm based on deep belief network, Sci. World J, 2014, (2014); Kim K., Lee S., Kim J.-Y., Kim M., Yoo H.-J., A configurable heterogeneous multicore architecture with cellular neural network for real-Time object recognition, IEEE Trans. Circuits Syst. Video Technol, 19, 11, pp. 1612-1622, (2009); Sudha N., Mohan A.R., Meher P.K., A self-configurable systolic architecture for face recognition system based on principal component neural network, IEEE Trans. Circuits Syst. Video Technol, 21, 8, pp. 1071-1084, (2011); Pan S.J., Tsang I.W., Kwok J.T., Yang Q., Domain adaptation via transfer component analysis, IEEE Trans. Neural Netw, 22, 2, pp. 199-210, (2011); Pan S.J., Yang Q., Asurvey on transfer learning, IEEE Trans. Knowl. Data Eng, 22, 10, pp. 1345-1359, (2010); Long M., Cao Y., Wang J., Jordan M.I., Learning transferable features with deep adaptation networks, Proc ICML, pp. 97-105, (2015); Ganin Y., Lempitsky V., Unsupervised Domain Adaptation by Backpropagation, (2014); Ajakan H., Germain P., Larochelle H., Laviolette F., Marchand M., Domain-Adversarial Neural Networks, (2014); Ghifary M., Kleijn W.B., Zhang M., Domain adaptive neural networks for object recognition, Proc. Pacific Rim Int. Conf. Artif. Intell, pp. 898-904, (2014); Zeiler M.D., Fergus R., Visualizing and understanding convolutional networks, Proc. Eur. Conf. Comput. Vis, pp. 818-833, (2014); Oquab M., Bottou L., Laptev I., Sivic J., Learning and transferring mid-level image representations using convolutional neural networks, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1717-1724, (2014); Yosinski J., Clune J., Bengio Y., Lipson H., How transferable are features in deep neural networks?, Proc. Adv. Neural Inf. Process. Syst, pp. 3320-3328, (2014); LeCun Y., Denker J.S., Solla S.A., Optimal brain damage, Proc. Adv. Neural Inf. Process. Syst, pp. 598-605, (1990); Han S., Pool J., Tran J., Dally W., Learning both weights and connections for efficient neural network, Proc. Adv. Neural Inf. Process. Syst, pp. 1135-1143, (2015); Han S., Mao H., Dally W.J., Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, (2015); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014); Denton E.L., Zaremba W., Bruna J., LeCun Y., Fergus R., Exploiting linear structure within convolutional networks for efficient evaluation, Proc. Adv. Neural Inf. Process. Syst, pp. 1269-1277, (2014); Zhang X., Zou J., He K., Sun J., Accelerating very deep convolutional networks for classification and detection, IEEE Trans. Pattern Anal. Mach. Intell, 38, 10, pp. 1943-1955, (2016); Gong Y., Liu L., Yang M., Bourdev L., Compressing Deep Convolutional Networks Using Vector Quantization, (2014); Cirean D.C., Meier U., Masci J., Gambardella L.M., Schmidhuber J., High-performance Neural Networks for Visual Object Classification, (2011); Bucilua C., Caruana R., Niculescu-Mizil A., Model compression, Proc. 12th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pp. 535-541, (2006); Romero A., Ballas N., Kahou S.E., Chassang A., Gatta C., Bengio Y., FitNets: Hints for Thin Deep Nets, (2014); Erhan D., Manzagol P.-A., Bengio Y., Bengio S., Vincent P., The difficulty of training deep architectures and the effect of unsupervised pre-Training, Proc. AISTATS, 5, pp. 153-160, (2009); Lee C.-Y., Xie S., Gallagher P.W., Zhang Z., Tu Z., Deeplysupervised nets, Proc. AISTATS, 2, 3, pp. 562-570, (2015); Szegedy C., Et al., Going deeper with convolutions, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1-9, (2015); Ba J., Caruana R., Do deep nets really need to be deep?, Proc. Adv. Neural Inf. Process. Syst, pp. 2654-2662, (2014); Xu C., Et al., Multi-loss regularized deep neural network, IEEE Trans. Circuits Syst. Video Technol, 26, 12, pp. 2273-2283, (2016); Chen T., Et al., MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems, (2015); Krizhevsky A., Hinton G., Learning Multiple Layers of Features from Tiny Images, (2009); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 770-778, (2016); Lee C.-Y., Gallagher P.W., Tu Z., Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree, Proc. Int. Conf. Artif. Intell. Stat, pp. 464-472, (2016); Mishkin D., Matas J., All You Need Is A Good Init, (2015); Goodfellow I.J., Warde-Farley D., Mirza M., Courville A.C., Bengio Y., Maxout networks, Proc. ICML, 28, pp. 1319-1327, (2013); Springenberg J.T., Dosovitskiy A., Brox T., Riedmiller M., Striving for Simplicity: The All Convolutional Net, (2014)","Z. He; Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, 518060, China; email: zhiquan@szu.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85041006339"
"Huang X.; Xia K.; Feng H.; Yang Y.; Du X.","Huang, Xinxi (57218348585); Xia, Kai (57206181913); Feng, Hailin (23392323100); Yang, Yinhui (56431764400); Du, Xiaochen (55352281700)","57218348585; 57206181913; 23392323100; 56431764400; 55352281700","Research on individual tree crown detection and segmentation using UAV imaging and Mask R-CNN","2021","Journal of Forestry Engineering","6","2","","133","140","7","2","10.13360/j.issn.2096-1359.202009004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113616683&doi=10.13360%2fj.issn.2096-1359.202009004&partnerID=40&md5=3fe0c59ae7517a373b2d0f4ffe190747","Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China; Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology, State Forestry Administration, Hangzhou, 311300, China; Key Laboratory of Forestry Perception Technology and Intelligent Equipment, State Forestry Administration, Hangzhou, 311300, China","Huang X., Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China, Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology, State Forestry Administration, Hangzhou, 311300, China, Key Laboratory of Forestry Perception Technology and Intelligent Equipment, State Forestry Administration, Hangzhou, 311300, China; Xia K., Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China, Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology, State Forestry Administration, Hangzhou, 311300, China, Key Laboratory of Forestry Perception Technology and Intelligent Equipment, State Forestry Administration, Hangzhou, 311300, China; Feng H., Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China, Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology, State Forestry Administration, Hangzhou, 311300, China, Key Laboratory of Forestry Perception Technology and Intelligent Equipment, State Forestry Administration, Hangzhou, 311300, China; Yang Y., Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China, Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology, State Forestry Administration, Hangzhou, 311300, China, Key Laboratory of Forestry Perception Technology and Intelligent Equipment, State Forestry Administration, Hangzhou, 311300, China; Du X., Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China, Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology, State Forestry Administration, Hangzhou, 311300, China, Key Laboratory of Forestry Perception Technology and Intelligent Equipment, State Forestry Administration, Hangzhou, 311300, China","Detecting and segmenting the single tree crown using the unmanned aerial vehicle ( UAV) remote sensing ima ges and obtaining parameters such as tree crown width (CW) and tree crown area (CA) can provide an efficient and fast way for forestry resources investigation in different urban scenes. At the same time, it can generate reference value for the estimation of urban tree health and growth status. The Ginkgo biloba tree in the city was selected as the research object, in which, the single G. biloba tree crown data set was obtained from UAV remote sensing images. The convolu- tional neural network Mask R-CNN algorithm and digital orthophoto map were used to detect the tree crown and draw the tree crown boundary to obtain the relevant tree crown parameters of different scenes in the city. The experimental re sults showed that the Mask R-CNN network model trained with the UAV G. biloba tree crown image data set can be well applied to the detection and segmentation of single G. biloba tree crown in different scenes in the city. In the four test scenes, the overall precision rate of the 86 single G. biloba tree crown targets reached 93.90%, and the recall rate reached 89.53%, F1-score was 91.66%, and mean average precision was 90.86%. In addition, the tree crown width and tree crown area of the single G. biloba tree crown can be accurately extracted. The average relative error (ARE) and root mean square error (RMSE) were 7.50% and 0.55 for the predicted tree crown width, and 11.15% and 2.48 for the predicted tree crown area, respectively. It was shown that the application of UAV images and appropriate deep learning algorithm in the urban forestry resource investigation can obtain accurate tree crown detection and contour segmentation, and effectively improve the efficiency of urban forestry resource investigation. The method of this study can also provide technical support for the extraction of relevant tree parameters in the city. © 2021 Nanjing Forestry University. All rights reserved","Crown area; Crown width; Mask r-cnn; Tree crown detection; Tree crown segmentation; Unmanned aerial vehicle (uav)","","","","","","","","YIN D M, WANG L., How to assess the accuracy of the individual tree?based forest inventory derived from remotely sensed data: a review[J], International Journal of Remote Sensing, 37, 19, pp. 4521-4553, (2016); SUN Z F, ZHANG X L, LI N W., Comparison of individual tree crown extraction method and suitability of airborne and spaceborne high?resolution remote sensing images, Journal of Beijing Forestry University, 41, 11, pp. 66-75, (2019); LEE J H, KO Y, MCPHERSON E G., The feasibility of remotely sensed data to estimate urban tree dimensions and biomass, Urban Forestry & Urban Greening, 16, pp. 208-220, (2016); BIRDAL A C, AVDAN U, TURK T., Estimating tree heights with images from an unmanned aerial vehicle, Geomatics, Natural Hazards and Risk, 8, 2, pp. 1144-1156, (2017); WANG P, XING Y Q, WANG C, Et al., A graph cut?based ap? proach for individual tree detection using airborne LiDAR data, Journal of University of Chinese Academy of Sciences, 36, 3, pp. 385-391, (2019); NAVARRO A, YOUNG M, ALLAN B, Et al., The application of Unmanned Aerial Vehicles (UAVs) to estimate above?ground bi? omass of mangrove ecosystems, Remote Sensing of Environ? ment, 242, (2020); LOBO TORRES D, QUEIROZ FEITOSA R, NIGRI HAPP P, Et al., Applying fully convolutional architectures for semantic segmen? tation of a single tree species in urban environment on high reso? lution UAV optical imagery, Sensors, 20, 2, (2020); XU D P, REN Y, YAN Z, Et al., The CNN driven quality evalua? tion for UAV remote sensing images, Journal of Forestry Engi? neering, 3, 5, pp. 121-127, (2018); LIU W P, ZHONG T Y, SONG Y N., Prediction of trees diameter at breast height based on unmanned aerial vehicle image analysis, Transactions of the Chinese Society of Agricultural Engineer? ing, 33, 21, pp. 99-104, (2017); YU D H, FENG Z K., Tree crown volume measurement method based on oblique aerial images of UAV, Transactions of the Chinese Society of Agricultural Engineering, 35, 1, pp. 90-97, (2019); CHEN C C, LI X, HUANG H Y., 3D segmentation of individual tree canopy in forest nursery based on drone image?matching point cloud, Transactions of the Chinese Society for Agricultural Machinery, 49, 2, pp. 149-155, (2018); LI L, DONG X B, ZHAO Q H., Application research of improved mask R?CNN in aerial disaster detection, Computer Engineering and Applications, 55, 21, pp. 166-175, (2019); LI S S, WU Q., Multi?target detection and segmentation of remote sensing images based on improved mask R?CNN, Computer Engineering and Applications, 56, 14, pp. 183-190, (2020); WEINSTEIN B G, MARCONI S, BOHLMAN S, Et al., Individual tree?crown detection in RGB imagery using semi?supervised deep learning neural networks [ J], bioRxiv, (2019); HE K M, GKIOXARI G, DOLLAR P, Et al., Mask R?CNN, Proceedings of the IEEE international conference on computer vi? sion, pp. 2961-1969, (2017); LARSEN M, ERIKSSON M, DESCOMBES X, Et al., Comparison of six individual tree crown detection algorithms evaluated under varying forest conditions [ J], International Journal of Remote Sensing, 32, 20, pp. 5827-5852, (2011); GU Z H, JIANG W G., Improved remote sensing image ship de? tection based on mask R?CNN, Computer Engineering and Applications, 56, 8, pp. 171-176, (2020); YANG C H, WANG Z, XIONG L Y, Et al., Identification and re? construction of Citrus branches under complex background based on mask R?CNN[J], Transactions of the Chinese Society for Agri? cultural Machinery, 50, 8, pp. 22-30, (2019); XI R, JIANG K, ZHANG W Z, Et al., Recognition method for po? tato buds based on improved faster R?CNN, Transactions of the Chinese Society for Agricultural Machinery, 51, 4, pp. 216-223, (2020); WEN Y L, LI L Y, SHANG X R, Et al., An instance segmentation method based on improved mask rcnn feature fusion [J], Computer Applications and Software, 36, 10, pp. 130-133, (2019); BRAGA J R G, PERIPATO V, DALAGNOL R, Et al., Tree crown delineation algorithm based on a convolutional neural net? work[ J], Remote Sensing, 12, 8, (2020); ZIMMERMANN R S, SIEMS J N., Faster training of Mask R?CNN by focusing on instance boundaries, Computer Vision and Im? age Understanding, 188, (2019); PIIROINEN R, FASSNACHT F E, HEISKANEN J, Et al., Inva? sive tree species detection in the Eastern Arc Mountains biodiver? sity hotspot using one class classification, Remote Sensing of Environment, 218, pp. 119-131, (2018); XIAO C, QIN R, HUANG X, Et al., A study of using fully convo? lutional network for treetop detection on remote sensing data, ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 163-169, (2018); GOMES M F, MAILLARD P, DENG H W., Individual tree crown detection in sub?meter satellite imagery using Marked Point Processes and a geometrical?optical model, Remote Sensing of Environment, 211, pp. 184-195, (2018); SAFONOVA A, TABIK S, ALCARAZ?SEGURA D, Et al., Detec? tion of fir trees (Abies sibirica) damaged by the bark beetle in un? manned aerial vehicle images with deep learning [ J], Remote Sensing, 11, 6, (2019)","K. Xia; Zhejiang Agriculture and Forestry University, College of Information Engineering, Hangzhou, 311300, China; email: xiakai@zafu.edu.cn","","Nanjing Forestry University","","","","","","20961359","","","","Chinese","J. For. Eng.","Article","Final","","Scopus","2-s2.0-85113616683"
"Kavitha T.; Lakshmi K.","Kavitha, T. (57202227891); Lakshmi, K. (57211327585)","57202227891; 57211327585","A drone detection system for preventing security threats using YOLO deep learning network","2020","International Journal of Advanced Science and Technology","29","5 Special Issue","","1366","1376","10","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083963617&partnerID=40&md5=d0d66ad26522be4084a5745f16b1331c","Department of Computer Science and Engineering, Periyar Maniammai Institute of Science & Technology, Thanjavur, Vallam, Tamil Nadu, India","Kavitha T., Department of Computer Science and Engineering, Periyar Maniammai Institute of Science & Technology, Thanjavur, Vallam, Tamil Nadu, India; Lakshmi K., Department of Computer Science and Engineering, Periyar Maniammai Institute of Science & Technology, Thanjavur, Vallam, Tamil Nadu, India","There has been a tremendous increase of use of UAVs or drones in the recent years. It is used in many fields such as aerial photography, shipping and delivery, crowd monitoring, crop monitoring in agriculture, law enforcement and surveillance etc. The cost of these devices become very low and makes it highly accessible to common public. It also increases the work efficiency and accuracy. Sometimes these drones are used for capturing information from secured areas by trespassing. This has raised several security concerns. In order to protect privacy and security at important locations, several solutions have been proposed in recent years. The drones are detected by using radio frequencies, acoustic waves, optic sensors and radars but they have certain limitations. Hence, this paper focuses on deep learning-based computer vision technique for detecting drones. To eradicate such security threats, the existing techniques for drone detection have been analyzed and designed a robust system based on You Only Look Once (YOLO) object detection algorithm. The YOLO based drone detection system performed well and produced results better than the other compared system. © 2020 SERSC.","Computer vision; Convolutional neural networks; Deep learning; Object detection; Unmanned aerial vehicles; YOLO","","","","","","","","Schumann A., Sommer L., Klatte J., Schuchert T., Beyerer J., Deep Cross-Domain Flying Object Classification for Robust UAV Detection; Gao Q., Parslow A., Tan M., Object Motion Detection Based on Perceptual Edge Tracking, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., Unified, Real-Time Object Detection, (2016); Aker C., Kalkan S., Using Deep Networks for Drone Detection, (2017); Dosovitskiy A., Fischer P., Ilg E., Hausser P., Hazirbas C., Golkov V., FlowNet: Learning Optical Flow with Convolutional Networks, (2015); Li X., Chen Q., Chen H., Detection and Tracking of Moving Object Based on PTZ Camera, (2012); Moving Object Detection and Locating Based on Region Shrinking Algorithm, (2012); Devi R.B., Manglem K., A Survey on Different Background Subtraction Method for Moving Object Detection, (2016); Saqib M., Sharma A., Khan S.D., Blumenstein M., A Study on Detecting Drones Using Deep Convolutional Neural Networks, (2017); Yang M., A Moving Objects Detection Algorithm in Video Sequence, (2014); Reiser C., Bounding box detection of drones (small scale quadcopters) with CNTK Fast R-CNN; Lin C., Drone-net; Alexey, Windows and Linux version of Darknet Yolo v3 & v2 Neural Networks for object detection; Redmon J., Darknet: Open Source Neural Networks in C; Miasnikov E., Threat of Terrorism Using Unmanned Aerial Vehicles: Technical Aspects. Center for Arms Control, (2015); Humpreys T., Statement on the Security Threat Posed by Unmanned Aerial Systems and Possible Countermeasures; Sathyamoorthy D., A Review of Security Threats of Unmanned Aerial Vehicles and Mitigation Steps, (2015); Guvenc I., Koohifar F., Singh S., Sichitiu M.L., Matolak D., Detection, Tracking, and Interdiction for Amateur Drones, (2018); Kavitha T., Lakshmi K., Training a Deep Learning Network with an Insignificantly Small Dataset, IJITEE, (2020)","","","Science and Engineering Research Support Society","","","","","","20054238","","","","English","Int. J. Adv. Sci. Technol.","Article","Final","","Scopus","2-s2.0-85083963617"
"Hartling S.; Sagan V.; Sidike P.; Maimaitijiang M.; Carron J.","Hartling, Sean (55961174300); Sagan, Vasit (57200370110); Sidike, Paheding (56585990900); Maimaitijiang, Maitiniyazi (56524539600); Carron, Joshua (57207833661)","55961174300; 57200370110; 56585990900; 56524539600; 57207833661","Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning","2019","Sensors (Switzerland)","19","6","1284","","","","131","10.3390/s19061284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062980001&doi=10.3390%2fs19061284&partnerID=40&md5=8505bde4326b9662e87b501e42d3766c","Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, MO, United States","Hartling S., Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, MO, United States; Sagan V., Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, MO, United States; Sidike P., Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, MO, United States; Maimaitijiang M., Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, MO, United States; Carron J., Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, MO, United States","Urban areas feature complex and heterogeneous land covers which create challenging issues for tree species classification. The increased availability of high spatial resolution multispectral satellite imagery and LiDAR datasets combined with the recent evolution of deep learning within remote sensing for object detection and scene classification, provide promising opportunities to map individual tree species with greater accuracy and resolution. However, there are knowledge gaps that are related to the contribution of Worldview-3 SWIR bands, very high resolution PAN band and LiDAR data in detailed tree species mapping. Additionally, contemporary deep learning methods are hampered by lack of training samples and difficulties of preparing training data. The objective of this study was to examine the potential of a novel deep learning method, Dense Convolutional Network (DenseNet), to identify dominant individual tree species in a complex urban environment within a fused image of WorldView-2 VNIR, Worldview-3 SWIR and LiDAR datasets. DenseNet results were compared against two popular machine classifiers in remote sensing image analysis, Random Forest (RF) and Support Vector Machine (SVM). Our results demonstrated that: (1) utilizing a data fusion approach beginning with VNIR and adding SWIR, LiDAR, and panchromatic (PAN) bands increased the overall accuracy of the DenseNet classifier from 75.9% to 76.8%, 81.1% and 82.6%, respectively. (2) DenseNet significantly outperformed RF and SVM for the classification of eight dominant tree species with an overall accuracy of 82.6%, compared to 51.8% and 52% for SVM and RF classifiers, respectively. (3) DenseNet maintained superior performance over RF and SVM classifiers under restricted training sample quantities which is a major limiting factor for deep learning techniques. Overall, the study reveals that DenseNet is more effective for urban tree species classification as it outperforms the popular RF and SVM techniques when working with highly complex image scenes regardless of training sample size. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network (CNN); Data fusion; Deep learning; Dense convolutional network (DenseNet); Random forest (RF); Support vector machine (SVM); Tree species classification","Deep Learning; Humans; Neural Networks (Computer); Support Vector Machine; Classification (of information); Complex networks; Convolution; Data fusion; Decision trees; Forestry; Neural networks; Object detection; Optical radar; Remote sensing; Sampling; Satellite imagery; Support vector machines; Complex urban environments; Convolutional networks; Convolutional neural network; High spatial resolution; Multispectral satellite imagery; Random forests; Remote sensing images; Tree species; artificial neural network; human; support vector machine; Deep learning","","","","","Digital Globe Foundation; National Science Foundation, NSF, (IIA-1355406, IIA-1430427); National Aeronautics and Space Administration, NASA, (NNX15AK03H); Office of the Director, OD, (1355406, 1430427)","Funding text 1: This work was supported in part by the National Science Foundation (IIA-1355406 and IIA-1430427) and in part by the National Aeronautics and Space Administration (NNX15AK03H).; Funding text 2: Acknowledgments: Worldview multispectral imagery was obtained through a grant from the Digital Globe Foundation.","Anderson L.M., Cordell H.K., Residential property values improved by landscaping with trees, South. J. Appl. For., 9, pp. 162-166, (1985); Raupp M.J., Cumming A.B., Raupp E.C., Street tree diversity in eastern north america and its potential for tree loss to exotic borers, Aboriculture Urban For, 32, pp. 297-304, (2006); Alonzo M., Bookhagen B., Roberts D.A., Urban tree species mapping using hyperspectral and lidar data fusion, Remote Sens. Environ., 148, pp. 70-83, (2014); Pu R., Landry S., A comparative analysis of high spatial resolution ikonos and worldview-2 imagery for mapping urban tree species, Remote Sens. Environ., 124, pp. 516-533, (2012); Li D., Ke Y., Gong H., Li X., Object-based urban tree species classification using bi-temporal worldview-2 and worldview-3 images, Remote Sens, 7, pp. 16917-16937, (2015); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens, 9, pp. 22-34, (2016); Clark M., Roberts D., Clark D., Hyperspectral discrimination of tropical rain forest tree species at leaf to crown scales, Remote Sens. Environ., 96, pp. 375-398, (2005); Immitzer M., Atzberger C., Koukal T., Tree species classification with random forest using very high spatial resolution 8-band worldview-2 satellite data, Remote Sens, 4, pp. 2661-2693, (2012); Cho M.A., Malahlela O., Ramoelo A., Assessing the utility worldview-2 imagery for tree species mapping in south african subtropical humid forest and the conservation implications: Dukuduku forest patch as case study, Int. J. Appl. Earth Obs. Geoinf., 38, pp. 349-357, (2015); Waser L., Kuchler M., Jutte K., Stampfer T., Evaluating the potential of worldview-2 data to classify tree species and different levels of ash mortality, Remote Sens, 6, pp. 4515-4545, (2014); Liu L., Coops N.C., Aven N.W., Pang Y., Mapping urban tree species using integrated airborne hyperspectral and lidar remote sensing data, Remote Sens. Environ., 200, pp. 170-182, (2017); Lu D., Weng Q., A survey of image classification methods and techniques for improving classification performance, Int. J. Remote Sens., 28, pp. 823-870, (2007); Benediktsson J.A., Swain P.H., Ersoy O.K., Neural network approaches versus statistical methods in classification of multisource remote sensing data, IEEE Trans. Geosci. Remote Sens., 8, pp. 540-552, (1990); Foody G.M., Land cover classification by an artificial neural network with ancillary information, Int. J. Geogr. Inf. Syst, 9, pp. 527-542, (1995); Friedl M.A., Brodley C.E., Decision tree classification of land cover from remotely sensed data, Remote Sens. Environ., 61, pp. 399-409, (1997); Dalponte M., Bruzzone L., Gianelle D., Tree species classification in the southern alps based on the fusion of very high geometrical resolution multispectral/hyperspectral images and lidar data, Remote Sens. Environ., 123, pp. 258-270, (2012); Jones T.G., Coops N.C., Sharma T., Assessing the utility of airborne hyperspectral and LiDAR data for species distribution mapping in the coastal Pacific Northwest, Canada, Remote Sens. Environ., 114, pp. 2841-2852, (2010); Koetz B., Morsdorf F., van der Linden S., Curt T., Allgower B., Multi-source land cover classification for forest fire management based on imaging spectrometry and lidar data, For. Ecol. Manag., 256, pp. 263-271, (2008); Sommer C., Holzwarth S., Heiden U., Heurich M., Muller J., Mauser W., Feature-Based Tree Species Classification Using Hyperspectral and Lidar Data in the Bavarian Forest National Park. Earsel Eproc, 14, pp. 49-70, (2016); Deng L., Deep learning: Methods and applications, Found. Trends Signal Process, 7, pp. 197-387, (2014); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40, (2016); Zou Q., Ni L., Zhang T., Wang Q., Deep learning based feature selection for remote sensing scene classification, IEEE Geosci. Remote Sens. Lett., 12, pp. 2321-2325, (2015); Basu S., Ganguly S., Mukhopadhyay S., Dibiano R., Karki M., Nemani R., Deepsat: A learning framework for satellite imagery, Proceedings of the 23Rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, (2015); Zou X., Cheng M., Wang C., Xia Y., Li J., Tree classification in complex forest point clouds based on deep learning, IEEE Geosci. Remote Sens. Lett., 14, pp. 2360-2364, (2017); Huang G., Liu Z., Weinberger K.Q., van der Maaten L., Densely connected convolutional networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Too E.C., Yujian L., Njuki S., Yingchun L., A comparative study of fine-tuning deep learning models for plant disease identification, Comput. Electron. Agric., (2018); Oyama T., Yamanaka T., Fully convolutional densenet for saliency-map prediction, Proceedings of the 2017 4Th IAPR Asian Conference on Pattern Recognition (ACPR), pp. 334-339; Sidike P., Sagan V., Maimaitijiang M., Maimaitiyiming M., Shakoor N., Burken J., Mockler T., Fritschi F.B., DPEN: Deep progressively expanded network for mapping heterogeneous agricultural landscape using worldview-3 satellite imagery, Remote Sens. Environ., 221, pp. 756-772, (2019); Sidike P., Asari V.K., Sagan V., Progressively expanded neural network (Pen net) for hyperspectral image classification: A new neural network paradigm for remote sensing image analysis, ISPRS J. Photogramm. Remote Sens., 146, pp. 161-181, (2018); Cho M.A., Mathieu R., Asner G.P., Naidoo L., van Aardt J., Ramoelo A., Debba P., Wessels K., Main R., Smit I.P.J., Et al., Mapping tree species composition in south african savannas using an integrated airborne spectral and lidar system, Remote Sens. Environ., 125, pp. 214-226, (2012); Sagan V., Maimaitijiang M., Sidike P., Eblimit K., Peterson K.T., Hartling S., Esposito F., Khanal K., Newcomb M., Pauli D., Uav-based high resolution thermal imaging for vegetation monitoring, and plant phenotyping using ici 8640 p, flir vue pro r 640, and thermomap cameras, Remote Sens, 11, (2019); Coble D., Walsh M.S., Louis Urban Tree Canopy Assessment, Forest Releaf of Missouri, pp. 1-20, (2012); Kuester M., Radiometric Use of Worldview-3 Imagery, (2016); Matthew M.W., Adler-Golden S.M., Berk A., Richtsmeier S.C., Levine R.Y., Bernstein L.S., Acharya P.K., Anderson G.P., Felde G.W., Hoke M.L., Status of atmospheric correction using a modtran4-based algorithm, Proceedings of the Algorithms for Multispectral, Hyperspectral, and Ultraspectral Imagery International Society for Optics and Photonics VI, pp. 199-208; Abreu L., Anderson G., The Modtran 2/3 Report and Lowtran 7 Model, (1996); Kaufman Y., Wald A., Remer L., Gao B., Li R., Flynn L., The modis 2.1 µm channel-correlation with visible reflectance for use in remote sensing of aerosol, IEEE Trans. Geosci. Remote, 35, pp. 1286-1298, (1997); Kwak D.-A., Lee W.-K., Lee J.-H., Biging G.S., Gong P., Detection of individual trees and estimation of tree height using lidar data, J. For. Res., 12, pp. 425-434, (2017); Wasser L., Day R., Chasmer L., Taylor A., Influence of vegetation structure on lidar-derived canopy height and fractional cover in forested riparian buffers during leaf-off and leaf-on conditions, Plos ONE, 8, (2013); Jakubowski M., Li W., Guo Q., Kelly M., Delineating individual trees from lidar data: A comparison of vector-and raster-based segmentation approaches, Remote Sens, 5, pp. 4163-4186, (2013); Hopkinson C., Chasmer L.E., Sass G., Creed I.F., Sitar M., Kalbfleisch W., Treitz P., Vegetation class dependent errors in lidar ground elevation and canopy height estimates in a boreal wetland environment, Can. J. Remote Sens., 31, pp. 191-206, (2005); Sadeghi Y., St-Onge B., Leblon B., Simard M., Canopy height model (CHM) derived from a tandem-x insar dsm and an airborne lidar dtm in boreal forest, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 381-397, (2016); Sreedhar M., Muralikrishnan S., Dadhwal V.K., Automatic conversion of DSM to DTM by classification techniques using multi-date stereo data from cartosat-1, J. Indian Soc. Remote Sens., 43, pp. 513-520, (2015); Jin X., Schafer R., Method and System for Automatic Registration of Images, (2016); Laben C.A., Brower B.V., Process for Enhancing the Spatial Resolution of Multispectral Imagery Using Pan-Sharpening, (2000); Padwick C., Deskevich M., Pacifici F., Smallwood S., Worldview-2 pan-sharpening, Proceedings of the ASPRS 2010 Annual Conference, San Diego, CA, USA, 26–30 April 2010; Kosaka N., Akiyama T., Tsai B., Kojima T., Forest type classification using data fusion of multispectral and panchromatic high-resolution satellite imageries, Proceedings of the Geoscience and Remote Sens, pp. 2980-2983; Jin X., Segmentation-Based Image Processing System, (2012); Kaufman Y.J., Tanre D., Atmospherically resistant vegetation index (Arvi) for eos-modis, IEEE Trans. Geosci. Remote Sens., 30, pp. 261-270, (1992); Fitzgerald G., Rodriguez D., O'Leary G., Measuring and predicting canopy nitrogen nutrition in wheat using a spectral index—the canopy chlorophyll content index (CCCI), Field Crops Res, 116, pp. 318-324, (2010); Felderhof L., Gillieson D., Near-infrared imagery from unmanned aerial systems and satellites can be used to specify fertilizer application rates in tree crops, Can. J. Remote Sens., 37, pp. 376-386, (2012); Lymburner L., Beggs P.J., Jacobson C.R., Estimation of canopy-average surface-specific leaf area using landsat tm data, Photogramm. Eng. Remote Sens., 66, pp. 183-192, (2000); Ismail R., Mutanga O., Bob U., Forest health and vitality: The detection and monitoring of pinus patula trees infected by sirex noctilio using digital multispectral imagery, South. Hemisph. For. J., 69, pp. 39-47, (2007); Gitelson A.A., Merzlyak M.N., Lichtenthaler H.K., Detection of red edge position and chlorophyll content by reflectance measurements near 700 nm, J. Plant Physiol., 148, pp. 501-508, (1996); Eitel J.U., Vierling L.A., Litvak M.E., Long D.S., Schulthess U., Ager A.A., Krofcheck D.J., Stoscheck L., Broadband, red-edge information from satellites improves early stress detection in a new mexico conifer woodland, Remote Sens. Environ., 115, pp. 3640-3646, (2011); Ustuner M., Sanli F., Abdikan S., Esetlili M., Kurucu Y., Crop type classification using vegetation indices of rapideye imagery, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 40, (2014); Gamon J.A., Field C.B., Goulden M.L., Griffin K.L., Hartley A.E., Joel G., Penuelas J., Valentini R., Relationships between ndvi, canopy structure, and photosynthesis in three californian vegetation types, Ecol. Appl., 5, pp. 28-41, (1995); Gould W., Remote sensing of vegetation, plant species richness, and regional biodiversity hotspots, Ecol. Appl., 10, pp. 1861-1870, (2000); Zimmermann N., Edwards T., Moisen G.G., Frescino T., Blackard J., Remote sensing-based predictors improve distribution models of rare, early successional and broadleaf tree species in utah, J. Appl. Ecol., 44, pp. 1057-1067, (2007); Gitelson A.A., Kaufman Y.J., Merzlyak M.N., Use of a green channel in remote sensing of global vegetation from eos-modis, Remote Sens. Environ., 58, pp. 289-298, (1996); Nouri H., Beecham S., Anderson S., Nagler P., High spatial resolution worldview-2 imagery for mapping ndvi and its relationship to temporal urban landscape evapotranspiration factors, Remote Sens, 6, pp. 580-602, (2014); Wolf A.F., Using worldview-2 vis-NIR multispectral imagery to support land mapping and feature extraction using normalized difference index ratios, In Proceedings of the Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XVIII, Nternational Society for Optics and Photonics, (2012); Maglione P., Parente C., Vallario A., Coastline extraction using high resolution worldview-2 satellite imagery, Eur. J. Remote Sens., 47, pp. 685-699, (2014); Gao B.-C., Ndwi—A normalized difference water index for remote sensing of vegetation liquid water from space, Remote Sens. Environ., 58, pp. 257-266, (1996); Pu R., Gong P., Yu Q., Comparative analysis of eo-1 ali and hyperion, and landsat etm+ data for mapping forest crown closure and leaf area index, Sensors, 8, pp. 3744-3766, (2008); Sims D.A., Gamon J.A., Relationships between leaf pigment content and spectral reflectance across a wide range of species, leaf structures and developmental stages, Remote Sens. Environ., 81, pp. 337-354, (2002); Huete A.R., A soil-adjusted vegetation index (Savi), Remote Sens. Environ., 25, pp. 295-309, (1988); Gitelson A.A., Kaufman Y.J., Stark R., Rundquist D., Novel algorithms for remote estimation of vegetation fraction, Remote Sens. Environ., 80, pp. 76-87, (2002); Raju P.D.R., Neelima G., Image segmentation by using histogram thresholding, Int. J. Comput. Sci. Eng. Technol., 2, pp. 776-779, (2012); Song H., Huang B., Zhang K., Shadow detection and reconstruction in high-resolution satellite images via morphological filtering and example-based learning, IEEE Trans. Geosci. Remote Sens., 52, pp. 2545-2554, (2014); Breiman L., Random forests, Mach. Learn., 45, pp. 5-32, (2001); Cortes C., Vapnik V., Support-vector networks, Mach. Learn., 20, pp. 273-297, (1995); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proceedings of the Advances in Neural Information Processing Systems, pp. 1097-1105; Landis J.R., Koch G.G., The measurement of observer agreement for categorical data, Biometrics, 33, pp. 159-174, (1977)","V. Sagan; Department of Earth and Atmospheric Sciences, Saint Louis University, St. Louis, 63108, United States; email: vasit.sagan@slu.edu","","MDPI AG","","","","","","14248220","","","30875732","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85062980001"
"Zhang N.; Wei X.; Chen H.; Liu W.","Zhang, Ning (57216812839); Wei, Xin (57203166961); Chen, He (57218309708); Liu, Wenchao (58308938600)","57216812839; 57203166961; 57218309708; 58308938600","FPGA implementation for CNN-based optical remote sensing object detection","2021","Electronics (Switzerland)","10","3","282","1","24","23","33","10.3390/electronics10030282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099921170&doi=10.3390%2felectronics10030282&partnerID=40&md5=3339768a6cc0b35c30afb3de43d873fd","Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing Institute of Technology, Beijing, 100081, China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China","Zhang N., Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing Institute of Technology, Beijing, 100081, China; Wei X., Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing Institute of Technology, Beijing, 100081, China; Chen H., Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing Institute of Technology, Beijing, 100081, China; Liu W., Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China","In recent years, convolutional neural network (CNN)-based methods have been widely used for optical remote sensing object detection and have shown excellent performance. Some aerospace systems, such as satellites or aircrafts, need to adopt these methods to observe objects on the ground. Due to the limited budget of the logical resources and power consumption in these systems, an embedded device is a good choice to implement the CNN-based methods. However, it is still a challenge to strike a balance between performance and power consumption. In this paper, we propose an efficient hardware-implementation method for optical remote sensing object detection. Firstly, we optimize the CNN-based model for hardware implementation, which establishes a foundation for efficiently mapping the network on a field-programmable gate array (FPGA). In addition, we propose a hardware architecture for the CNN-based remote sensing object detection model. In this architecture, a general processing engine (PE) is proposed to implement multiple types of convolutions in the network using the uniform module. An efficient data storage and access scheme is also proposed, and it achieves low-latency calculations and a high memory bandwidth utilization rate. Finally, we deployed the improved YOLOv2 network on a Xilinx ZYNQ xc7z035 FPGA to evaluate the performance of our design. The experimental results show that the performance of our implementation on an FPGA is only 0.18% lower than that on a graphics processing unit (GPU) in mean average precision (mAP). Under a 200 MHz working frequency, our design achieves a throughput of 111.5 giga-operations per second (GOP/s) with a 5.96 W on-chip power consumption. Comparison with the related works demonstrates that the proposed design has obvious advantages in terms of energy efficiency and that it is suitable for deployment on embedded devices. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","CNN; Deep learning; FPGA; Hardware implementation; Object detection; Remote sensing; You-only-look-once (YOLO)","","","","","","Chang Jiang Scholars Program; Hundred Leading Talent Project of Beijing Science and Technology, (Z141101001514005); MYHT Program of China, (B0201); National Key R & D Program of China; Changjiang Scholar Program of Chinese Ministry of Education, (T2012122); National Key Research and Development Program of China, NKRDPC, (2017YFB0502800, 2017YFB0502804)","Funding text 1: This research was funded by the National Key R & D Program of China under Grant No. 2017YFB0502800 and Grant No. 2017YFB0502804, the MYHT Program of China under Grant No. B0201. This work was supported by the Chang Jiang Scholars Program under Grant T2012122 and the Hundred Leading Talent Project of Beijing Science and Technology under Grant Z141101001514005.; Funding text 2: Acknowledgments: This work was supported by the Chang Jiang Scholars Program under Grant T2012122 and the Hundred Leading Talent Project of Beijing Science and Technology under Grant Z141101001514005.; Funding text 3: Funding: This research was funded by the National Key R & D Program of China under Grant No. 2017YFB0502800 and Grant No. 2017YFB0502804, the MYHT Program of China under Grant No. B0201.","Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens, 117, pp. 11-28, (2016); Zeng D., Zhang S., Chen F., Wang Y., Multi-scale CNN based garbage detection of airborne hyperspectral data, IEEE Access, 7, pp. 104514-104527, (2019); He H., Yang D., Wang S.C., Wang S.Y., Li Y., Road extraction by using atrous spatial pyramid pooling integrated encoderdecoder network and structural similarity loss, Remote Sens, 11, (2019); Liu W., Ma L., Wang J., Chen H., Detection of Multiclass Objects in Optical Remote Sensing Images, IEEE Geosci. Remote S, pp. 791-795, (2018); Zhu M., Xu Y., Ma S., Li S., Ma H., Han Y., Effective Airplane Detection in Remote Sensing Images Based on Multilayer Feature Fusion and Improved Nonmaximal Suppression Algorithm, Remote Sens, 11, (2019); Liu W., Long M., He C., Arbitrary-Oriented Ship Detection Framework in Optical Remote-Sensing Images, IEEE Geosci. Remote S, 15, pp. 937-941, (2018); Gong Z., Zhong P., Hu W., Hua Y., Joint learning of the center points and deep metrics for land-use classification in remote sensing, Remote S, 11, (2019); Qi B., Shi H., Zhuang Y., Chen H., Chen L., On-Board, Real-Time Preprocessing System for Optical Remote-Sensing Imagery, Sensors, 18, (2018); Joyce K.E., Belliss S.E., Samsonov S.V., McNeill S.J., Glassey P.J., A Review of the Status of Satellite Remote Sensing and Image Processing Techniques for Mapping Natural Hazards and Disasters, Prog. Phys. Geogr, 33, pp. 183-207, (2009); Zhou G., Zhang R., Liu N., Huang J., Zhou X., On-Board Ortho-Rectification for Images Based on an FPGA, Remote Sens, 9, (2017); Du Q., Nekovei R., Fast real-time onboard processing of hyperspectral imagery for detection and classification, J. Real-Time Image Process, 4, pp. 273-286, (2009); Li L., Zhang S., Wu J., Efficient Object Detection Framework and Hardware Architecture for Remote Sensing Images, Remote Sens, 11, (2019); Liu Z., Chow P., Xu J., Jiang J., Dou Y., Zhou J., A Uniform Architecture Design for Accelerating 2D and 3D CNNs on FPGAs, Electronics, 8, (2019); Wei X., Liu W., Chen L., Ma L., Chen H., Zhuang Y., FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications, Sensors, 19, (2019); PD S.M., Lin J., Zhu S., Yin Y., Liu X., Huang X., Song C., Zhang W., Yan M., Yu H., Et al., A scalable network-on-chip microprocessor with 2.5 D integrated memory and accelerator, IEEE Trans. Circuits Syst. Regul. Pap, 64, pp. 1432-1443, (2017); Li W., He C., Fu H., Zheng J., Dong R., Xia M., Yu L., Luk W., A Real-Time Tree Crown Detection Approach for Large-Scale Remote Sensing Images on FPGAs, Remote Sens, 11, (2019); Lei C., Xin W., Wenchao L., He C., Liang C., Hardware Implementation of Convolutional Neural Network Based Remote Sensing Image Classification Method, Proceedings of the 9th International Conference on Communications, Signal Processing, and Systems, pp. 140-148; Mohammadnia M.R., Shannon L., A multi-beam Scan Mode Synthetic Aperture Radar processor suitable for satellite operation, Proceedings of the 2016 IEEE 27th International Conference on Application-Specific Systems, Architectures and Processors (ASAP), pp. 83-90, (2016); Gonzalez C., Bernabe S., Mozos D., Plaza A., FPGA implementation of an algorithm for automatically detecting targets in remotely sensed hyperspectral images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 9, pp. 4334-4343, (2016); Yap J. W., Yussof Z. M., Salim S. I. M., A scalable FPGA based accelerator for Tiny-YOLO-v2 using OpenCL, Int. J. Embed. Syst, 8, pp. 206-214, (2019); Zhang C., Li P., Sun G., Guan Y., Xiao B., Cong J., Optimizing fpga-based accelerator design for deep convolutional neural networks, Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pp. 161-170, (2015); Peemen M., Setio A.A., Mesman B., Corporaal H., Memory-centric accelerator design for convolutional neural networks, Proceedings of the 2013 IEEE 31st International Conference (ICCD), pp. 13-19, (2013); Sun F., Wang C., Gong L., Xu C., Xu C.C., Zhang Y.W., Lu Y.T., Li X., Zhou X., A High-Performance Accelerator for Large-Scale Convolutional Neural Networks, Proceedings of the 2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC), pp. 1-9, (2018); Li H., Fan X., Li J., Wei C., Zhou X., Wang L., A high performance FPGA-based accelerator for large-scale Convolutional Neural Networks, Proceedings of the 26th International Conference on Field Programmable Logic and Applications (FPL), pp. 34-42, (2016); Zhou Y., Jiang J., An FPGA-based accelerator implementation for deep convolutional neural networks, Proceedings of the 2015 4th International Conference on Computer Science and Network Technology, ICCSNT 2015, 1, pp. 829-832, (2015); Li Z., Wang L., Guo S., Deng Y., Dou Q., Zhou H., Lu W., Laius: an 8-bit fixed-point CNN hardware inference engine, Proceedings of the 2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC), (2017); Fan H., Liu S.L., Ferianc M., Ng H.C., Que Z.Q., Liu S., Niu X.Y., Luk W., A real-time object detection accelerator with compressed SSDLite on FPGA, Proceedings of the 2018 International Conference on Field-Programmable Technology (FPT), (2018); Liang S., Yin S., Liu L., Luk W., Wei S., FP-BNN: Binarized neural network on FPGA, Neurocomputing, 275, pp. 1072-1086, (2017); Nguyen D.T., Nguyen T.N., Kim H., Lee H., A High-Throughput and Power-Efficient FPGA Implementation of YOLO CNN for Object Detection, IEEE Trans. Very Large Scale Integr. (VLSI) Syst, 27, pp. 1861-1873, (2019); Jiao L., Luo C., Cao W., Zhou X., Wang L., Accelerating low bit-width convolutional neural networks with embedded FPGA, Proceedings of the 27th International Conference on Field Programmable Logic and Applications (FPL), pp. 1-4, (2017); Xia G.S., Bai X., Ding J., Zhu Z., Belongie S., Luo J., Datcu M., Pelillo M., Zhang L., DOTA: A large-scale dataset for object detection in aerial images, (2017); Girshick R., Donahue J., Darrell T., Malik J., Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014), pp. 580-587, (2014); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single shot multibox detector, Proceedings of the Computer Vision-European conference on computer vision 2016, pp. 21-37, (2016); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, (2016); Nakahara H., Yonekawa H., Fujii T., Sato S., A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA, Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pp. 31-40, (2018); Li F., Chen H., Liu Z., Zhang X. D., Jiang M. S., Wu Z. Z., Zhou K. Q., Deep learning-based automated detection of retinal diseases using optical coherence tomography images, Biomed. Opt. Express, 10, pp. 6204-6226, (2019); Kheradpisheh S.R., Ghodrati M., Ganjtabesh M., Masquelier T., Deep Networks Can Resemble Human Feed-Forward Vision in Invariant Object Recognition, Sci. Rep, 6, (2016); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, (2015); Abdelouahab K., Pelcat M., Serot J., Berry F., Accelerating CNN inference on FPGAs: A survey, (2018); Zhang N., Wei X., Chen L., Chen H., Three-level Memory Access Architecture for FPGA-based Real-time Remote Sensing Image Processing System, processing of the 2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP), pp. 1-6, (2019); Tao Y., Ma R., Shyu M. L., Chen S. C., Challenges in Energy-Efficient Deep Neural Network Training With FPGA, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, (2020); Zhao R., Niu X., Wu Y., Luk W., Liu Q., Optimizing CNN-based object detection algorithms on embedded FPGA platforms, Proceedings of the International Symposium on Applied Reconfigurable Computing, pp. 255-267, (2017); Wai Y.J., bin Mohd Yussof Z., bin Salim S.I., Chuan L.K., Fixed Point Implementation of Tiny-Yolo-v2 using OpenCL on FPGA, Int. J. Adv. Comput. Sci. Appl, 9, pp. 506-512, (2018); Lv H., Zhang S., Liu X., Liu S., Liu Y., Han W., Xu S., Research on Dynamic Reconfiguration Technology of Neural Network Accelerator Based on Zynq, J. Phys. Conf. Ser, 1650, (2020); Zhang S., Cao J., Zhang Q., Zhang Q., Zhang Y., Wang Y., An FPGA-Based Reconfigurable CNN Accelerator for YOLO, Proceedings of the 2020 IEEE 3rd International Conference on Electronics Technology (ICET), pp. 74-78, (2020); Xu K., Wang X., Liu X., Cao C., Li H., Peng H., Wang D., A dedicated hardware accelerator for real-time acceleration of YOLOv2, J. Real Time Image Process, pp. 412-423, (2020)","W. Liu; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China; email: liuwenchao@mail.tsinghua.edu.cn","","MDPI AG","","","","","","20799292","","","","English","Electronics (Switzerland)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099921170"
"Saha S.; Mou L.; Qiu C.; Zhu X.X.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Mou, Lichao (55953611600); Qiu, Chunping (57194601941); Zhu, Xiao Xiang (55696622200); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 55953611600; 57194601941; 55696622200; 9943212600; 7006892410","Unsupervised Deep Joint Segmentation of Multitemporal High-Resolution Images","2020","IEEE Transactions on Geoscience and Remote Sensing","58","12","9091105","8780","8792","12","40","10.1109/TGRS.2020.2990640","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097341452&doi=10.1109%2fTGRS.2020.2990640&partnerID=40&md5=c3b7ec475cf26c0830f1ba76b27f2067","Fondazione Bruno Kessler, Trento, 38123, Italy; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, 82234, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, 80333, Germany; Fondazione Bruno Kessler, Trento, 38123, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, 38123, Italy","Saha S., Fondazione Bruno Kessler, Trento, 38123, Italy; Mou L., Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, 82234, Germany; Qiu C., Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, 80333, Germany; Zhu X.X., Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, 82234, Germany; Bovolo F., Fondazione Bruno Kessler, Trento, 38123, Italy; Bruzzone L., Department of Information Engineering and Computer Science, University of Trento, Trento, 38123, Italy","High/very-high-resolution (HR/VHR) multitemporal images are important in remote sensing to monitor the dynamics of the Earth's surface. Unsupervised object-based image analysis provides an effective solution to analyze such images. Image semantic segmentation assigns pixel labels from meaningful object groups and has been extensively studied in the context of single-image analysis, however not explored for multitemporal one. In this article, we propose to extend supervised semantic segmentation to the unsupervised joint semantic segmentation of multitemporal images. We propose a novel method that processes multitemporal images by separately feeding to a deep network comprising of trainable convolutional layers. The training process does not involve any external label, and segmentation labels are obtained from the argmax classification of the final layer. A novel loss function is used to detect object segments from individual images as well as establish a correspondence between distinct multitemporal segments. Multitemporal semantic labels and weights of the trainable layers are jointly optimized in iterations. We tested the method on three different HR/VHR data sets from Munich, Paris, and Trento, which shows the method to be effective. We further extended the proposed joint segmentation method for change detection (CD) and tested on a VHR multisensor data set from Trento. © 1980-2012 IEEE.","Deep learning; high resolution (HR); multitemporal image; segmentation","Bavaria; France; Germany; Ile de France; Italy; Munich; Paris; Trentino-Alto Adige; Trento; Ville de Paris; Varanidae; Object detection; Remote sensing; Semantics; Effective solution; High resolution image; Joint segmentation; Multi-sensor data; Multi-temporal image; Object based image analysis; Semantic segmentation; Training process; artificial neural network; image analysis; image resolution; optimization; pixel; segmentation; sensor; unsupervised classification; Image segmentation","","","","","European Union’s Horizon 2020 Research and Innovation Program, (ERC-2016-StG-714087); Helmholtz Artificial Intelligence Cooperation Unit; European Research Council, ERC","Manuscript received December 27, 2019; revised March 14, 2020; accepted April 20, 2020. Date of publication May 11, 2020; date of current version November 24, 2020. The work of X. X. Zhu was supported in part by the European Research Council (ERC) through the European Union’s Horizon 2020 Research and Innovation Program (So2Sat) under Grant ERC-2016-StG-714087, in part by Helmholtz Artificial Intelligence Cooperation Unit (HAICU)—Local Unit “Munich Unit @ Aeronautics, Space and Transport (MASTr),” and in part by Helmholtz Excellent Professorship “Data Science in Earth Observation—Big Data Fusion for Urban Research.” (Corresponding author: Francesca Bovolo.) Sudipan Saha is with Fondazione Bruno Kessler, 38123 Trento, Italy, and also with the Department of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy (e-mail: saha@fbk.eu).","Bovolo F., Bruzzone L., The time variable in data fusion: A change detection perspective, Ieee Geosci. Remote Sens. Mag., 3, 3, pp. 8-26, (2015); Volpi M., Tuia D., Bovolo F., Kanevski M., Bruzzone L., Supervised change detection in VHR images using contextual information and support vector machines, Int. J. Appl. Earth Observ. Geoinf., 20, pp. 77-85, (2013); Bruzzone L., Bovolo F., A novel framework for the design of change-detection systems for very-high-resolution remote sensing images, Proc. Ieee, 101, 3, pp. 609-630, (2013); Bovolo F., A multilevel parcel-based approach to change detection in very high resolution multitemporal images, Ieee Geosci. Remote Sens. Lett., 6, 1, pp. 33-37, (2009); Saha S., Bovolo F., Bruzzone L., Unsupervised deep change vector analysis for multiple-change detection in VHR images, Ieee Trans. Geosci. Remote Sens., 57, 6, pp. 3677-3693, (2019); Solano-Correa Y.T., Bovolo F., Bruzzone L., Fernandez-Prieto D., Spatio-temporal evolution of crop fields in Sentinel-2 satellite image time series, Proc. 9th Int. Workshop Anal. Multitemporal Remote Sens. Images (MultiTemp), pp. 1-4, (2017); Wu L., Zhang Z., Wang Y., Liu Q., A segmentation based change detection method for high resolution remote sensing image, Proc. Chin. Conf. Pattern Recognit, pp. 314-324, (2014); Tasar O., Tarabalka Y., Alliez P., Incremental learning for semantic segmentation of large-scale remote sensing data, Ieee J. Sel. Topics Appl. Earth Observ. Remote Sens., 12, 9, pp. 3524-3537, (2019); Volpi M., Tuia D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks, Ieee Trans. Geosci. Remote Sens., 55, 2, pp. 881-893, (2017); Garcia-Garcia A., Orts-Escolano S., Oprea S., Villena-Martinez V., Martinez-Gonzalez P., Garcia-Rodriguez J., A survey on deep learning techniques for image and video semantic segmentation, Appl. Soft Comput., 70, pp. 41-65, (2018); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., Jun., pp. 3431-3440, (2015); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent, pp. 234-241, (2015); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation, Ieee Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Xia X., Kulis B., W-Net: A Deep Model for Fully Unsupervised Image Segmentation, (2017); Kanezaki A., Unsupervised image segmentation by backpropagation, Proc. Ieee Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr., pp. 1543-1547, (2018); Saha S., Sudhakaran S., Banerjee B., Pendurkar S., Semantic guided deep unsupervised image segmentation, Proc. Int. Conf. Image Anal. Process, pp. 499-510, (2019); Xie J., Girshick R., Farhadi A., Unsupervised deep embedding for clustering analysis, Proc. Int. Conf. Mach. Learn., pp. 478-487, (2016); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., Jun., pp. 580-587, (2014); Paszke A., Chaurasia A., Kim S., Culurciello E., ENet: A Deep Neural Network Architecture for Real-time Semantic Segmentation, (2016); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, Ieee Trans. Pattern Anal. Mach. Intell., 40, 4, pp. 834-848, (2018); Razavian A.S., Azizpour H., Sullivan J., Carlsson S., CNN features Off-the-shelf: An astounding baseline for recognition, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. Workshops, Jun., pp. 806-813, (2014); Shrivastava A., Pfister T., Tuzel O., Susskind J., Wang W., Webb R., Learning from simulated and unsupervised images through adversarial training, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul., pp. 2107-2116, (2017); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes Challenge 2012 (VOC) Results, (2012); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards realtime object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); Zhou B., Lapedriza A., Xiao J., Torralba A., Oliva A., Learning deep features for scene recognition using places database, Proc. Adv. Neural Inf. Process. Syst., pp. 487-495, (2014); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, Ieee Geosci. Remote Sens. Mag., 4, 2, pp. 22-40, (2016); Ma X., Geng J., Wang H., Hyperspectral image classification via contextual deep learning, Eurasip J. Image Video Process., 2015, 1, pp. 1-12, (2015); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Highresolution Semantic Labeling with Convolutional Neural Networks, (2016); Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community, J. Appl. Remote Sens., 11, 4, (2017); Zhan Y., Fu K., Yan M., Sun X., Wang H., Qiu X., Change detection based on deep siamese convolutional network for optical aerial images, Ieee Geosci. Remote Sens. Lett., 14, 10, pp. 1845-1849, (2017); Zhang P., Gong M., Su L., Liu J., Li Z., Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images, Isprs J. Photogramm. Remote Sens., 116, pp. 24-41, (2016); Gao F., Dong J., Li B., Xu Q., Automatic change detection in synthetic aperture radar images based on PCANet, Ieee Geosci. Remote Sens. Lett., 13, 12, pp. 1792-1796, (2016); Lyu H., Lu H., Mou L., Learning a transferable change rule from a recurrent neural network for land cover change detection, Remote Sens., 8, 6, (2016); Mou L., Bruzzone L., Zhu X.X., Learning Spectral-Spatial-Temporal features via a recurrent convolutional neural network for change detection in multispectral imagery, Ieee Trans. Geosci. Remote Sens., 57, 2, pp. 924-935, (2019); Geng J., Wang H., Fan J., Ma X., Change detection of SAR images based on supervised contractive autoencoders and fuzzy clustering, Proc. Int. Workshop Remote Sens. Intell. Process. (RSIP), pp. 1-3, (2017); Xu Y., Xiang S., Huo C., Pan C., Change detection based on autoencoder model for VHR images, Proc. Spie, 8919, (2013); Saha S., Bovolo F., Bruzzone L., Destroyed-buildings detection from VHR SAR images using deep features, Proc. Spie, 10789, (2018); Zhang X., Xiao P., Feng X., Yuan M., Separate segmentation of multi-temporal high-resolution remote sensing images for object-based change detection in urban area, Remote Sens. Environ., 201, pp. 243-255, (2017); Peng D., Zhang Y., Guan H., End-to-end change detection for high resolution satellite images using improved UNet++, Remote Sens., 11, 11, (2019); Correa Y.T.S., Bovolo F., Bruzzone L., Change detection in very high resolution multisensor optical images, Proc. Spie, 9244, (2014); Goodfellow I., Bengio Y., Courville A., Deep Learning, (2016); Ulyanov D., Vedaldi A., Lempitsky V., Instance Normalization: The Missing Ingredient for Fast Stylization, (2016); Rusiecki A., Trimmed categorical cross-entropy for deep learning with label noise, Electron. Lett., 55, 6, pp. 319-320, (2019); Glorot X., Bengio Y., Understanding the difficulty of training deep feedforward neural networks, Proc. 13th Int. Conf. Artif. Intell. Statist., pp. 249-256, (2010); Saha S., Bovolo F., Brurzone L., Unsupervised multiple-change detection in VHR optical images using deep features, Proc. Ieee Int. Geosci. Remote Sens. Symp. IGARSS, Jul., pp. 1902-1905, (2018); Schmitt M., Hughes L.H., Qiu C., Zhu X.X., Aggregating cloud-free Sentinel-2 images with Google Earth engine, Isprs Ann. Photogramm., Remote Sens. Spatial Inf. Sci., 4, pp. 145-152, (2019); Saha S., Bovolo F., Bruzzone L., Unsupervised multiple-change detection in VHR multisensor images via deep-learning based adaptation, Proc. Ieee Int. Geosci. Remote Sens. Symp. IGARSS, Jul., pp. 5033-5036, (2019)","F. Bovolo; Fondazione Bruno Kessler, Trento, 38123, Italy; email: bovolo@fbk.eu","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097341452"
"Li Y.; Melgani F.; He B.","Li, Youyou (57190756646); Melgani, Farid (35613488300); He, Binbin (12753475500)","57190756646; 35613488300; 12753475500","CSVM Architectures for Pixel-Wise Object Detection in High-Resolution Remote Sensing Images","2020","IEEE Transactions on Geoscience and Remote Sensing","58","9","9020017","6059","6070","11","6","10.1109/TGRS.2020.2972289","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094129323&doi=10.1109%2fTGRS.2020.2972289&partnerID=40&md5=9003af5a7418543ad133a6c8364e2f23","School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","Li Y., School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; Melgani F., Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; He B., School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China","Detecting objects becomes an increasingly important task in very high resolution (VHR) remote sensing imagery analysis. With the development of GPU-computing capability, a growing number of deep convolutional neural networks (CNNs) have been designed to address the object detection challenge. However, compared with CPU, GPU is much more costly. Therefore, GPU-based methods are less attractive in practical applications. In this article, we propose a CPU-based method that is based on convolutional support vector machines (CSVMs) to address the object detection challenge in VHR images. Experiments are conducted on three VHR and two unmanned aerial vehicle (UAV) data sets with very limited training data. Results show that the proposed CSVM achieves competitive performance compared to U-Net which is an efficient CNN-based model designed for small training data sets. © 1980-2012 IEEE.","Convolutional neural network (CNN); convolutional support vector machine (CSVM); object detection; remote sensing; very high resolution (VHR)","Antennas; Convolution; Convolutional neural networks; Deep neural networks; Object recognition; Remote sensing; Support vector machines; Unmanned aerial vehicles (UAV); Competitive performance; Detecting objects; GPU computing; High resolution remote sensing images; Limited training data; Remote sensing imagery; Small training; Very high resolution; Object detection","","","","","National Natural Science Foundation of China, NSFC, (41601373, 41671361); China Scholarship Council, CSC","Funding text 1: Manuscript received November 1, 2019; revised January 18, 2020; accepted January 27, 2020. Date of publication March 2, 2020; date of current version August 28, 2020. This work was supported by the National Natural Science Foundation of China under Grant 41671361 and Grant 41601373. (Corresponding authors: Binbin He; Farid Melgani.) Youyou Li and Binbin He are with the School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu 611731, China (e-mail: liyouyou@std.uestc.edu.cn; binbinhe@uestc. edu.cn).; Funding text 2: ACKNOWLEDGMENT Y. Li thanks the China Scholarship Council (CSC) for a scholarship that allowed her to study in Italy.","Kumar A., Chen F., Barlage M., Ek M.B., Niyogi D., Assessing impacts of integrating modis vegetation data in the weather research and forecasting (WRF) model coupled to two different canopy-resistance approaches, J. Appl. Meteor. Climatol, 53, 6, pp. 1362-1380, (2014); Rosel A., Kaleschke L., Exceptional melt pond occurrence in the years 2007 and 2011 on the Arctic sea ice revealed from MODIS satellite data, J. Geophys. Res., Oceans, 117, 5, (2012); Friedl M.A., Et al., Global land cover mapping from MODIS: Algorithms and early results, Remote Sens. Environ, 83, 1-2, pp. 287-302, (2002); Margono B.A., Et al., Mapping and monitoring deforestation and forest degradation in Sumatra (Indonesia) using Landsat time series data sets from 1990 to 2010, Environ. Res. Lett, 7, 3, (2012); Pickell P.D., Hermosilla T., Frazier R.J., Coops N.C., Wulder M.A., Forest recovery trends derived from Landsat time series for North American boreal forests, Int. J. Remote Sens, 37, 1, pp. 138-149, (2016); Schroeder W., Oliva P., Giglio L., Quayle B., Lorenz E., Morelli F., Active fire detection using Landsat-8/OLI data, Remote Sens. Environ, 185, pp. 210-220, (2016); Blaes X., Chome G., Lambert M.-J., Traore P., Schut A., Defourny P., Quantifying fertilizer application response variability with VHR satellite NDVI time series in a rainfed smallholder cropping system of Mali, Remote Sens, 8, 6, (2016); Ammour N., Alhichri H., Bazi Y., Benjdira B., Alajlan N., Zuair M., Deep learning approach for car detection in UAV imagery, Remote Sens, 9, 4, (2017); Li Q., Mou L., Liu Q., Wang Y., Zhu X.X., HSF-Net: Multiscale deep feature embedding for ship detection in optical remote sensing imagery, IEEE Trans. Geosci. Remote Sens, 56, 12, pp. 7147-7161, (2018); Xu S., Et al., Automatic building rooftop extraction from aerial images via hierarchical RGB-D priors, IEEE Trans. Geosci. Remote Sens, 56, 12, pp. 7369-7387, (2018); Wang Q., Liu S., Chanussot J., Li X., Scene classification with recurrent attention of VHR remote sensing images, IEEE Trans. Geosci. Remote Sens, 57, 2, pp. 1155-1167, (2019); Bergado J.R., Persello C., Stein A., Recurrent multiresolution convolutional networks for VHR image classification, IEEE Trans. Geosci. Remote Sens, 56, 11, pp. 6361-6374, (2018); Carleer A.P., Debeir O., Wolff E., Assessment of very high spatial resolution satellite image segmentations, Photogramm. Eng. Remote Sens, 71, 11, pp. 1285-1294, (2005); Cushnie J.L., The interactive effect of spatial resolution and degree of internal variability within land-cover types on classification accuracies, Int. J. Remote Sens, 8, 1, pp. 15-29, (1987); Key T., A comparison of multispectral and multitemporal information in high spatial resolution imagery for classification of individual tree species in a temperate hardwood forest, Remote Sens. Environ, 75, 1, pp. 100-112, (2001); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 580-587, (2014); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, pp. 3431-3440, (2015); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 39, 12, pp. 2481-2495, (2017); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell, 40, 4, pp. 834-848, (2018); Persello C., Stein A., Deep fully convolutional networks for the detection of informal settlements in VHR images, IEEE Geosci. Remote Sens. Lett, 14, 12, pp. 2325-2329, (2017); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional neural networks for large-scale remote-sensing image classification, IEEE Trans. Geosci. Remote Sens, 55, 2, pp. 645-657, (2017); Volpi M., Tuia D., Dense semantic labeling of subdecimeter resolution images with convolutional neural networks, IEEE Trans. Geosci. Remote Sens, 55, 2, pp. 881-893, (2017); Hu G., Peng X., Yang Y., Hospedales T.M., Verbeek J., Frankenstein: Learning deep face representations using small data, IEEE Trans. Image Process, 27, 1, pp. 293-303, (2018); Taigman Y., Yang M., Ranzato M., Wolf L., DeepFace: Closing the gap to human-level performance in face verification, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1701-1708, (2014); Schroff F., Kalenichenko D., Philbin J., FaceNet: A unified embedding for face recognition and clustering, Proc IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, pp. 815-823, (2015); Yao X., Han J., Cheng G., Qian X., Guo L., Semantic annotation of high-resolution satellite images via weakly supervised learning, IEEE Trans. Geosci. Remote Sens, 54, 6, pp. 3660-3671, (2016); Erhan D., Bengio Y., Courville A., Manzagol P.-A., Vincent P., Bengio S., Why does unsupervised pre-Training help deep learning?, J. Mach. Learn. Res, 11, pp. 625-660, (2010); Yosinski J., Clune J., Bengio Y., Lipson H., How transferable are features in deep neural networks?, Advances in Neural Information Processing Systems, pp. 3320-3328, (2014); Tajbakhsh N., Et al., Convolutional neural networks for medical image analysis: Full training or fine tuning?, IEEE Trans. Med. Imag, 35, 5, pp. 1299-1312, (2016); Kornblith S., Shlens J., Le Q.V., Do better imagenet models transfer better?, Proc IEEE/CVF Conf. Comput. Vis. Pattern Recognit. CVPR, pp. 2661-2671, (2019); Marmanis D., Datcu M., Esch T., Stilla U., Deep learning earth observation classification using imagenet pretrained networks, IEEE Geosci. Remote Sens. Lett, 13, 1, pp. 105-109, (2016); Muhammad U., Wang W., Chattha S.P., Ali S., Pre-Trained VGGNet architecture for remote-sensing image scene classification, Proc. 24th Int. Conf. Pattern Recognit. ICPR, pp. 1622-1627, (2018); Nogueira K., Penatti O.A.B., Dos Santos J.A., Towards better exploiting convolutional neural networks for remote sensing scene classification, Pattern Recognit, 61, pp. 539-556, (2017); Bazi Y., Melgani F., Convolutional SVM networks for object detection in UAV imagery, IEEE Trans. Geosci. Remote Sens, 56, 6, pp. 3107-3118, (2018); Mittal S., Vetter J.S., A survey of CPU-GPU heterogeneous computing techniques, ACM Comput. Surv, 47, 4, pp. 1-35, (2015); Canziani A., Paszke A., Culurciello E., An analysis of deep neural network models for practical applications, arXiv:1605 07678, (2016); Gregg C., Hazelwood K., Where is the data? Why you cannot debate CPU vs. GPU performance without the answer, Proc IEEE ISPASS) IEEE Int. Symp. Perform. Anal. Syst. Softw, pp. 134-144, (2011); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent, pp. 234-241, (2015); Luo W., Li Y., Urtasun R., Zemel R., Understanding the effective receptive field in deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 4898-4906, (2016); Burges C.J.C., A tutorial on support vector machines for pattern recognition, Data Mining Knowl. Discovery, 2, 2, pp. 121-167, (1998); Melgani F., Bruzzone L., Classification of hyperspectral remote sensing images with support vector machines, IEEE Trans. Geosci. Remote Sens, 42, 8, pp. 1778-1790, (2004); Chen S., Fern A., Todorovic S., Multi-object tracking via constrained sequential labeling, Proc IEEE Conf. Comput. Vis. Pattern Recognit, pp. 1130-1137, (2014); Shao Z., Yang K., Zhou W., Performance evaluation of single-label and multi-label remote sensing image retrieval using a dense labeling dataset, Remote Sens, 10, 6, (2018); Chaudhuri B., Demir B., Chaudhuri S., Bruzzone L., Multilabel remote sensing image retrieval using a semisupervised graphtheoretic method, IEEE Trans. Geosci. Remote Sens, 56, 2, pp. 1144-1158, (2018); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Proc. 18th SIGSPATIAL Int. Conf. Adv. Geographic Inf. Syst. (GIS, pp. 270-279, (2010)","B. He; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; email: binbinhe@uestc.edu.cn","","Institute of Electrical and Electronics Engineers Inc.","","","","","","01962892","","IGRSD","","English","IEEE Trans Geosci Remote Sens","Article","Final","","Scopus","2-s2.0-85094129323"
"","","","Autonomous Vehicles and Machines 2020","2020","IS and T International Symposium on Electronic Imaging Science and Technology","2020","16","","","","2244","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094886601&partnerID=40&md5=ac1d02ed9bc3be2c40c35120ff4270ef","","","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.","","","","","","","","","","","Denny P.; Robin J.; van Beek P.","Society for Imaging Science and Technology","AutoSens","2020 Autonomous Vehicles and Machines Conference, AVM 2020","26 January 2020 through 30 January 2020","Burlingame","163996","24701173","","","","English","IS T Intl. Symposium Electronic Imaging Science Technol.","Conference review","Final","","Scopus","2-s2.0-85094886601"
"Alotaibi B.; Alotaibi M.","Alotaibi, Bandar (56823476600); Alotaibi, Munif (56275433000)","56823476600; 56275433000","A Hybrid Deep ResNet and Inception Model for Hyperspectral Image Classification","2020","PFG - Journal of Photogrammetry, Remote Sensing and Geoinformation Science","88","6","","463","476","13","28","10.1007/s41064-020-00124-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090208705&doi=10.1007%2fs41064-020-00124-x&partnerID=40&md5=c69a60aace4802e7550c626a94fceba6","Department of Computer Science and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; College of Computing and Information Technology, Shaqra University, Shaqra, Saudi Arabia","Alotaibi B., Department of Computer Science and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; Alotaibi M., College of Computing and Information Technology, Shaqra University, Shaqra, Saudi Arabia","Over the past few decades, hyperspectral image (HSI) classification has garnered increasing attention from the remote sensing research community. The largest challenge faced by HSI classification is the high feature dimensions represented by the different HSI bands given the limited number of labeled samples. Deep learning and convolutional neural networks (CNNs), in particular, have been shown to be highly effective in several computer vision problems such as object detection and image classification. In terms of accuracy and computational cost, one of the best CNN architectures is the Inception model i.e., the winner of the ImageNet Large Scale Visual Recognition Competition (ILSVRC) 2014 challenge. Another architecture that has significantly improved image recognition performance is the Residual Network (ResNet) architecture i.e., the winner of the ILSVRC 2015 challenge. Inspired by the incredible performance introduced by the Inception and ResNet architectures, we investigate the possibility of combining the core ideas of these two models into a hybrid architecture to improve the HSI classification performance. We tested this combined model on four standard HSI datasets, and it shows competitive results compared with other existing HSI classification methods. Our hybrid deep ResNet-Inception architecture obtained accuracies of 95.31% on the Pavia University dataset, 99.02% on the Pavia Centre scenes dataset, 95.33% on the Salinas dataset and 90.57% on the Indian Pines dataset. © 2020, Deutsche Gesellschaft für Photogrammetrie, Fernerkundung und Geoinformation (DGPF) e.V.","Convolutional neural network (CNN); Data mining; Deep learning; Hyperspectral image (HSI) classification; Machine learning; Remote sensing; Residual neural network (ResNet)","accuracy assessment; artificial neural network; data mining; image classification; remote sensing; spectral analysis","","","","","Deanship of Scientific Research, University of Tabuk, (S-0181-1439); Deanship of Scientific Research, King Faisal University, DSR, KFU","This research was financially supported by the Deanship of Scientific Research, University of Tabuk, Tabuk, Saudi Arabia under grant number S-0181-1439.","Abdel-Hamid O., Ar M., Jiang H., Deng L., Penn G., Yu D., Convolutional neural networks for speech recognition, IEEE/ACM Trans Audio Speech Language Process, 22, 10, pp. 1533-1545, (2014); Buitinck L., Louppe G., Blondel M., Pedregosa F., Mueller A., Grisel O., Niculae V., Prettenhofer P., Gramfort A., Grobler J., Layton R., Vanderplas J., Joly A., Holt B., Varoquaux G., API design for machine learning software: Experiences from the scikit-learn project. In: ECML PKDD workshop: languages for data mining and machine learning, pp 108–122, (2013); Carreiras J.M., Jones J., Lucas R.M., Shimabukuro Y.E., Mapping major land cover types and retrieving the age of secondary forests in the Brazilian Amazon by combining single-date optical and radar remote sensing data, Remote Sens Environ, 194, pp. 16-32, (2017); Chen Y., Lin Z., Zhao X., Wang G., Gu Y., Deep learning-based classification of hyperspectral data, IEEE J Select Top Appl Earth Obser Remote Sens, 7, 6, pp. 2094-2107, (2014); Chen Y., Jiang H., Li C., Jia X., Ghamisi P., Deep feature extraction and classification of hyperspectral images based on convolutional neural networks, IEEE Trans Geosci Remote Sens, 54, 10, pp. 6232-6251, (2016); Deep autoencoder neural networks for gene ontology annotation predictions. In: Proceedings of the 5th ACM conference on bioinformatics, computational biology, and health informatics, ACM, Pp 533–540, (2014); Choi E., Schuetz A., Stewart W.F., Sun J., Using recurrent neural network models for early detection of heart failure onset, J Am Med Inform Assoc, 24, 2, pp. 361-370, (2016); Chollet F., Et al., Keras, (2015); Ciregan D., Meier U., Schmidhuber J., Multi-column deep neural networks for image classification, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference On, IEEE, pp. 3642-3649, (2012); Deng L., Yu D., Et al., Deep learning: methods and applications, Found Trends Signal Process, 7, 3-4, pp. 197-387, (2014); Erturk A., Iordache M.D., Plaza A., Sparse unmixing-based change detection for multitemporal hyperspectral images, IEEE J Select Topics Appl Earth Obser Remote Sens, 9, 2, pp. 708-719, (2016); Garcia-Garcia A., Orts-Escolano S., Oprea S., Villena-Martinez V., Garcia-Rodriguez J., A review on deep learning techniques applied to semantic segmentation. ArXiv:170406857, (2017); Garg V., Kumar A.S., Aggarwal S., Kumar V., Dhote P., Thakur P.K., Nikam B.R., Sambare R., Siddiqui A., Muduli P.R., Et al., Spectral similarity approach for mapping turbidity of an inland waterbody, J Hydrol, (2017); Glorot X., Bengio Y., Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the 13th international conference on artificial intelligence and statistics, Pp 249–256, (2010); Graves A., Jaitlyar N., Hybrid speech recognition with deep bidirectional lstm, Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop On, IEEE, pp. 273-278, (2013); Graves A., Mohamed A., Hinton G., Speech recognition with deep recurrent neural networks, Acoustics, Speech and Signal Processing (Icassp), 2013, pp. 6645-6649, (2013); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, pp. 770-778, (2016); Hu W., Huang Y., Wei L., Zhang F., Li H., Deep Convolutional Neural Networks for Hyperspectral Image Classification, (2015); Jakob S., Zimmermann R., Gloaguen R., The need for accurate geometric and radiometric corrections of drone-borne hyperspectral data for mineral exploration: Mephysto–a toolbox for pre-processing drone-borne hyperspectral data, Remote Sens, 9, 1, (2017); Kingma D., Ba J., Adam: A method for stochastic optimization, Arxiv:14126980, (2014); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet Classification with Deep Convolutional Neural Networks, pp. 1097-1105, (2012); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci Remote Sens Lett, 14, 5, pp. 778-782, (2017); LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc IEEE, 86, 11, pp. 2278-2324, (1998); Li Y., Hu J., Zhao X., Xie W., Li J., Hyperspectral image super-resolution using deep convolutional neural network, Neurocomputing, (2017); Li Y., Zhang H., Shen Q., Spectral-spatial classification of hyperspectral imagery with 3d convolutional neural network, Remote Sens, 9, 1, (2017); Mesnil G., Dauphin Y., Yao K., Bengio Y., Deng L., Hakkani-Tur D., He X., Heck L., Tur G., Yu D., Et al., Using recurrent neural networks for slot filling in spoken language understanding, IEEE/ACM Trans Audio Speech Language Process (TASLP), 23, 3, pp. 530-539, (2015); Mou L., Ghamisi P., Zhu X.X., Deep recurrent neural networks for hyperspectral image classification, IEEE Trans Geosci Remote Sens, (2017); Olmanson L.G., Brezonik P.L., Bauer M.E., Airborne hyperspectral remote sensing to assess spatial distribution of water quality characteristics in large rivers: The mississippi river and its tributaries in minnesota, Remote Sens Environ, 130, pp. 254-265, (2013); Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay E., Scikit-learn: machine learning in Python, J Mach Learn Res, 12, pp. 2825-2830, (2011); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition. ArXiv:14091556, (2014); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going Deeper with Convolutions, pp. 1-9, (2015); Tao C., Pan H., Li Y., Zou Z., Unsupervised spectral-spatial feature learning with stacked sparse autoencoder for hyperspectral imagery classification, IEEE Geosci Remote Sens Lett, 12, 12, pp. 2438-2442, (2015); Wang L., Zhang J., Liu P., Choo K.K.R., Huang F., Spectral-spatial multi-feature-based deep learning for hyperspectral remote sensing image classification, Soft Comput, 21, 1, pp. 213-221, (2017); Yu S., Jia S., Xu C., Convolutional neural networks for hyperspectral image classification, Neurocomputing, 219, pp. 88-98, (2017); Zhang L., Zhang L., Du B., Deep learning for remote sensing data: a technical tutorial on the state of the art, IEEE Geosci Remote Sens Magn, 4, 2, pp. 22-40, (2016); Zhao W., Du S., Spectral-spatial feature extraction for hyperspectral image classification: A dimension reduction and deep learning approach, IEEE Trans Geosci Remote Sens, 54, 8, pp. 4544-4554, (2016); Zhong S., Liu Y., Liu Y., Bilinear deep learning for image classification. In: Proceedings of the 19th ACM international conference on Multimedia, ACM, Pp 343–352, (2011); Zhong Z., Li J., Luo Z., Chapman M., Spectral-spatial residual network for hyperspectral image classification: A 3-d deep learning framework, IEEE Trans Geosci Remote Sens, (2017); Zhong Z., Li J., Ma L., Jiang H., Zhao H., Deep Residual Networks for Hyperspectral Image Classification., (2017)","M. Alotaibi; College of Computing and Information Technology, Shaqra University, Shaqra, Saudi Arabia; email: munif@su.edu.sa","","Springer Science and Business Media Deutschland GmbH","","","","","","25122789","","","","English","PFG - J. Photogrammetry Remote Sensing Geoinformation Sci.","Article","Final","","Scopus","2-s2.0-85090208705"
"Tayara H.; Chong K.T.","Tayara, Hilal (57192558128); Chong, Kil To (7102553910)","57192558128; 7102553910","Object detection in very high-resolution aerial images using one-stage densely connected feature pyramid network","2018","Sensors (Switzerland)","18","10","3341","","","","88","10.3390/s18103341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054774125&doi=10.3390%2fs18103341&partnerID=40&md5=41fd7e9df2a4fd36cd41da569fffe040","Department of Electronics and Information Engineering, Chonbuk National University, Jeonju, 54896, South Korea; Advanced Electronics and Information Research Center, Chonbuk National University, Jeonju, 54896, South Korea","Tayara H., Department of Electronics and Information Engineering, Chonbuk National University, Jeonju, 54896, South Korea; Chong K.T., Advanced Electronics and Information Research Center, Chonbuk National University, Jeonju, 54896, South Korea","Object detection in very high-resolution (VHR) aerial images is an essential step for a wide range of applications such as military applications, urban planning, and environmental management. Still, it is a challenging task due to the different scales and appearances of the objects. On the other hand, object detection task in VHR aerial images has improved remarkably in recent years due to the achieved advances in convolution neural networks (CNN). Most of the proposed methods depend on a two-stage approach, namely: a region proposal stage and a classification stage such as Faster R-CNN. Even though two-stage approaches outperform the traditional methods, their optimization is not easy and they are not suitable for real-time applications. In this paper, a uniform one-stage model for object detection in VHR aerial images has been proposed. In order to tackle the challenge of different scales, a densely connected feature pyramid network has been proposed by which high-level multi-scale semantic feature maps with high-quality information are prepared for object detection. This work has been evaluated on two publicly available datasets and outperformed the current state-of-the-art results on both in terms of mean average precision (mAP) and computation time. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Aerial images; Convolution neural network (cnn); Deep learning; Feature pyramid network; Focal loss; Object detection","Antennas; Convolution; Deep learning; Environmental management; Feature extraction; Image enhancement; Military applications; Military photography; Object recognition; Semantics; Aerial images; Convolution neural network; Feature pyramid; High quality information; Real-time application; Semantic features; Two stage approach; Very high resolution aerial images; Object detection","","","","","National Research Foundation of Korea, NRF; Ministry of Science and ICT, South Korea, MSIT, (NRF-2017M3C7A1044815)","Funding: This research was supported by the Brain Research Program of the National Research Foundation (NRF) funded by the Korean government (MSIT) (No. NRF-2017M3C7A1044815).","Colomina I., Molina P., Unmanned aerial systems for photogrammetry and remote sensing: A review, ISPRS J. Photogramm. Remote Sens., 92, pp. 79-97, (2014); Zhang F., Du B., Zhang L., Xu M., Weakly supervised learning based on coupled convolutional neural networks for aircraft detection, IEEE Trans. Geosci. Remote Sens., 54, pp. 5553-5563, (2016); Kamusoko C., Importance of Remote Sensing and Land Change Modeling for Urbanization Studies, (2017); Barrett E., Introduction to Environmental Remote Sensing, (2003); Cheng G., Han J., Zhou P., Guo L., Scalable multi-class geospatial object detection in high-spatial-resolution remote sensing images, Proceedings of the 2014 IEEE Geoscience and Remote Sensing Symposium, pp. 2479-2482; Tayara H., Soo K.G., Chong K.T., Vehicle detection and counting in high-resolution aerial images using convolutional regression neural network, IEEE Access, 6, pp. 2220-2230, (2018); Moranduzzo T., Melgani F., Automatic car counting method for unmanned aerial vehicle images, IEEE Trans. Geosci. Remote Sens., 52, pp. 1635-1647, (2014); Moranduzzo T., Melgani F., Detecting cars in uav images with a catalog-based approach, IEEE Trans. Geosci. Remote Sens., 52, pp. 6356-6367, (2014); Wen X., Shao L., Fang W., Xue Y., Efficient feature selection and classification for vehicle detection, IEEE Trans. Circuits Syst. Video Technol., 25, pp. 508-517, (2015); Yu X., Shi Z., Vehicle detection in remote sensing imagery based on salient information and local shape feature, Optik-Int. J. Light Electron Opt., 126, pp. 2485-2490, (2015); Cai H., Su Y., Airplane detection in remote sensing image with a circle-frequency filter, Proceedings of the 2005 International Conference on Space Information Technology, (2005); An Z., Shi Z., Teng X., Yu X., Tang W., An automated airplane detection system for large panchromatic image with high spatial resolution, Optik-Int. J. Light Electron Opt., 125, pp. 2768-2775, (2014); Bo S., Jing Y., Region-based airplane detection in remotely sensed imagery, Proceedings of the 2010 3Rd International Congress on Image and Signal Processing; Sirmacek B., Unsalan C., A probabilistic framework to detect buildings in aerial and satellite images, IEEE Trans. Geosci. Remote Sens., 49, pp. 211-221, (2011); Stankov K., He D.C., Detection of buildings in multispectral very high spatial resolution images using the percentage occupancy hit-or-miss transform, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7, pp. 4069-4080, (2014); Zhang L., Shi Z., Wu J., A hierarchical oil tank detector with deep surrounding features for high-resolution optical satellite imagery, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8, pp. 4895-4909, (2015); Ok A.O., Baseski E., Circular oil tank detection from panchromatic satellite images: A new automated approach, IEEE Geosci. Remote Sens. Lett., 12, pp. 1347-1351, (2015); Dai D., Yang W., Satellite image classification via two-layer sparse coding with biased image representation, IEEE Geosci. Remote Sens. Lett., 8, pp. 173-176, (2011); Zhang D., Meng D., Han J., Co-saliency detection via a self-paced multiple-instance learning framework, IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 865-878, (2017); Tian Y., Cehn C., Shah M., Cross-view image matching for geo-localization in urban environments, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Girshick R., Fast R-CNN; Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 1137-1149, (2017); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet Classification with Deep Convolutional Neural Networks; Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., Ssd: Single Shot Multibox Detector; Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Everingham M., Ali Eslami S.M., Gool L.V., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes Challenge: A Retrospective; Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft Coco: Common Objects in Context. Computer Vision—ECCV 2014, (2014); Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28, (2016); Cheng G., Zhou P., Han J., Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images, IEEE Trans. Geosci. Remote Sens., 54, pp. 7405-7415, (2016); Qu T., Zhang Q., Sun S., Vehicle detection from high-resolution aerial images using spatial pyramid pooling-based deep convolutional neural networks, Multimedia Tools Appl, 76, pp. 21651-21663, (2017); Tang T., Zhou S., Deng Z., Zou H., Lei L., Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining, Sensors, 17, (2017); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition; He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (2016); Han J., Zhou P., Zhang D., Cheng G., Guo L., Liu Z., Bu S., Wu J., Efficient, simultaneous detection of multi-class geospatial targets based on visual saliency modeling and discriminative learning of sparse coding, ISPRS J. Photogramm. Remote Sens., 89, pp. 37-48, (2014); Sun H., Sun X., Wang H., Li Y., Li X., Automatic target detection in high-resolution remote sensing images using spatial sparse coding bag-of-words model, IEEE Geosci. Remote Sens. Lett., 9, pp. 109-113, (2012); Xu S., Fang T., Li D., Wang S., Object classification of aerial images with bag-of-visual words, IEEE Geosci. Remote Sens. Lett., 7, pp. 366-370, (2010); Cheng G., Han J., Zhou P., Guo L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors, ISPRS J. Photogramm. Remote Sens., 98, pp. 119-132, (2014); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), (2017); Yang Y., Zhuang Y., Bi F., Shi H., Xie Y., M-fcn: Effective fully convolutional network-based airplane detection framework, IEEE Geosci. Remote Sens. Lett., 14, pp. 1293-1297, (2017); Han J., Zhang D., Cheng G., Guo L., Ren J., Object detection in optical remote sensing images based on weakly supervised learning and high-level feature learning, IEEE Trans. Geosci. Remote Sens., 53, pp. 3325-3337, (2015); Jun G., Ghosh J., Semisupervised learning of hyperspectral data with unknown land-cover classes, IEEE Trans. Geosci. Remote Sens., 51, pp. 273-282, (2013); Chen C., Gong W., Hu Y.F., Chen Y., Ding Y.S., Learning Oriented Region-Based Convolutional Neural Networks for Building Detection in Satellite Remote Sensing Images; Kampffmeyer M., Salberg A.B., Jenssen R., Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), (2016); Wegner J.D., Branson S., Hall D., Schindler K., Perona P., Cataloging public objects using aerial and street-level images x2014; urban trees, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Xu Z., Xu X., Wang L., Yang R., Pu F., Deformable convnet with aspect ratio constrained nms for object detection in remote sensing imagery, Remote Sens, 9, 12, (2017); Guo W., Yang W., Zhang H., Hua G., Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network, Remote Sens, 10, 1, (2018); Li K., Cheng G., Bu S., You X., Rotation-insensitive and context-augmented object detection in remote sensing images, IEEE Trans. Geosci. Remote Sens., 56, pp. 2337-2348, (2018); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Et al., Imagenet large scale visual recognition challenge, Int. J. Comput. Vis., 115, 3, (2015); Long Y., Gong Y., Xiao Z., Liu Q., Accurate object localization in remote sensing images based on convolutional neural networks, IEEE Trans. Geosci. Remote Sens., 55, pp. 2486-2498, (2017); Han X., Zhong Y., Zhang L., An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery, Remote Sens, 9, 7, (2017)","K.T. Chong; Advanced Electronics and Information Research Center, Chonbuk National University, Jeonju, 54896, South Korea; email: kitchong@jbnu.ac.kr","","MDPI AG","","","","","","14248220","","","30301221","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85054774125"
"Alburshaid E.A.; Mangoud M.A.","Alburshaid, E.A. (57306196600); Mangoud, M.A. (6602536151)","57306196600; 6602536151","Palm Trees Detection Using the Integration between GIS and Deep Learning","2021","2021 International Symposium on Networks, Computers and Communications, ISNCC 2021","","","","","","","3","10.1109/ISNCC52172.2021.9615721","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123429807&doi=10.1109%2fISNCC52172.2021.9615721&partnerID=40&md5=bbaacd95679139195a067b1a8cb28da0","Natioanl Space Sicnec Agency; University Of Bahrain, Bahamas","Alburshaid E.A., Natioanl Space Sicnec Agency; Mangoud M.A., University Of Bahrain, Bahamas","In this paper, the ability to utilize deep learning in developing a geodatabase in the GIS environment for the kingdom of Bahrain is assessed. Two convolution neural network models have been presented and evaluated, the first model applies RetinaNet, which belongs to the single-stage detector, and the second model utilizes MRCNN, which belongs to the two-stage detector. The tested area was including all the land territory of the kingdom of Bahrain that covered around 790 sq.km. Very high-resolution (VHR) satellite imagery has been used as a raw material to extract palm tree distributions and specify the exact locations of the inventory that can contribute to the overall food security in Bahrain. This paper has proved the advantage of integrating AI within the GIS environment that helps in generating agricultural statistics from high-resolution satellite imagery efficiently, accurately, and timely. © 2021 IEEE.","CNN; Deep learning; GIS; High-resolution satellite imagery; Mask R-CNN; Object detection; Palm trees; Remote Sensing; RetinaNet; Segmentation","Deep learning; Food supply; Forestry; Geographic information systems; Object detection; Satellite imagery; CNN; Deep learning; Geodatabase; High resolution satellite imagery; Kingdom of Bahrain; PaLM-tree; Remote-sensing; Retinanet; Segmentation; Tree detections; Remote sensing","","","","","","","Ibrahim K.M., The role of date palm tree in improvement of the environment, Acta Hortic., 882, 882, pp. 777-778, (2010); Almansoori T.A., Al-Khalifa M.A., Mohamed A.M.A., Date palm status and perspective in bahrain, Date Palm Genetic Resources and Utilization: Volume 2: Asia and Europe, 2, pp. 353-386, (2015); Al-Ruzouq R., Shanableh A., Gibril M.B.A., Al-Mansoori S., Image segmentation parameter selection and ant colony optimization for date palm tree detection and mapping from very-high-spatialresolution aerial imagery, Remote Sens., 10, 9, (2018); Chong K.L., Kanniah K.D., Pohl C., Tan K.P., A review of remote sensing applications for oil palm studies, Geo-Spatial Inf. Sci., 20, 2, pp. 184-200, (2017); Mubin N.A., Nadarajoo E., Shafri H.Z.M., Hamedianfar A., Young and mature oil palm tree detection and counting using convolutional neural network deep learning method, Int. J. Remote Sens., 40, 19, pp. 7500-7515, (2019); Cheng G., Han J., A survey on object detection in optical remote sensing images, ISPRS J. Photogramm. Remote Sens., 117, pp. 11-28, (2016); Culman M.M.M.M., Delalieux S., Van Tricht K., Van Tricht S.D.K., Palm tree inventory from aerial images using retinanet, 2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS), pp. 314-317, (2020); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for highresolution remote sensing images, Remote Sens., 9, 1, (2017); Hass F., Classification of Ocean Objects Detected in Sentinel-1 SAR, Iceberg-Ship Discrimination Master, (2020); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, IEEE Trans. Pattern Anal. Mach. Intell., 42, 2, pp. 318-327, (2018); Lu X., Li Q., Li B., Yan J., MimicDet: Bridging the Gap between One-Stage and Two-Stage Object Detection, (2020); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual tree-crown detection in rgb imagery using semi-supervised deep learning neural networks, Remote Sens., 11, 11, pp. 1-13, (2019); Temitope Yekeen S., Balogun A.L., Wan Yusof K.B., A novel deep learning instance segmentation model for automated marine oil spill detection, ISPRS J. Photogramm. Remote Sens., 167, pp. 190-200, (2020); Czum J.M., Dive into deep learning, J. Am. Coll. Radiol., 17, 5, pp. 637-638, (2020); Maxwell A.E., Pourmohammadi P., Poyner J.D., Mapping the topographic features of mining-related valley fills using mask R-CNN deep learning and digital elevation data, Remote Sens., 12, 3, pp. 1-23, (2020)","","","Institute of Electrical and Electronics Engineers Inc.","","2021 International Symposium on Networks, Computers and Communications, ISNCC 2021","31 October 2021 through 2 November 2021","Dubai","174926","","978-073811316-6","","","English","Int. Symp. Networks, Comput. Commun., ISNCC","Conference paper","Final","","Scopus","2-s2.0-85123429807"
"Unal Z.","Unal, Zeynep (57444928100)","57444928100","Smart Farming Becomes even Smarter with Deep Learning - A Bibliographical Analysis","2020","IEEE Access","8","","9108212","105587","105609","22","57","10.1109/ACCESS.2020.3000175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086712805&doi=10.1109%2fACCESS.2020.3000175&partnerID=40&md5=d75f1a2310ade71c24e980bfa651ec1c","Niǧtaş Company, Niǧde, Turkey","Unal Z., Niǧtaş Company, Niǧde, Turkey","Smart farming is a new concept that makes agriculture more efficient and effective by using advanced information technologies. The latest advancements in connectivity, automation, and artificial intelligence enable farmers better to monitor all procedures and apply precise treatments determined by machines with superhuman accuracy. Farmers, data scientists and, engineers continue to work on techniques that allow optimizing the human labor required in farming. With valuable information resources improving day by day, smart farming turns into a learning system and becomes even smarter. Deep learning is a type of machine learning method, using artificial neural network principles. The main feature by which deep learning networks are distinguished from neural networks is their depth and that feature makes them capable of discovering latent structures within unlabeled, unstructured data. Deep learning networks that do not need human intervention while performing automatic feature extraction have a significant advantage over previous algorithms. The focus of this study is to explore the advantages of using deep learning in agricultural applications. This bibliography reviews the potential of using deep learning techniques in agricultural industries. The bibliography contains 120 papers from the database of the Science Citation Index on the subject that were published between 2016 and 2019. These studies have been retrieved from 39 scientific journals. The papers are classified into the following categories as disease detection, plant classification, land cover identification, precision livestock farming, pest recognition, object recognition, smart irrigation, phenotyping, and weed detection. © 2013 IEEE.","artificial neural networks; internet of things; Machine learning; precision agriculture","Agricultural robots; Agriculture; Bibliographies; Learning systems; Neural networks; Object recognition; Weed control; Agricultural industries; Automatic feature extraction; Information resource; Land cover identifications; Machine learning methods; Plant classification; Precision livestock farming; Science citation index; Deep learning","","","","","","","Ayaz M., Ammad-Uddin M., Sharif Z., Mansour A., Aggoune E.-H.-M., Internet-of-Things (IoT)-based smart agriculture: Toward making the fields talk, IEEE Access, 7, pp. 129551-129583, (2019); Smart Farming: The Future of Agriculture, (2019); Smart Farming is Key for the Future of Agriculture, (2019); Varghese R., Sharma S., Affordable smart farming using IoT and machine learning, Proc. 2nd Int. Conf. Intell. Comput. Control Syst. (ICICCS), pp. 645-650, (2018); Goodfellow I., Bengio Y., Courville A., Deep Learning, pp. 1-15, (2016); Raschka S., Mirjalili V., Machine Learning and Deep Learning with Python, Scikit-Learn and TensorFlow, pp. 2-6, (2017); Fyfe C., Artificial neural networks and information theory, Ph.D. Dissertation, Dept. Comput. Inf. Syst., Univ. Paisley, Paisley, U.K., (2000); Deng L., Yu D., Deep learning: Methods and applications, Found. Trends Signal Process., 7, 3-4, pp. 197-387, (2014); Heaton J., Artificial intelligence for humans, Neural Networks and Deep Learning, 3, pp. 165-180, (2015); Graupe D., Principles of Artificial Neural Networks, pp. 1-12, (2007); Krose B., Smagt P., An Introduction to Neural Networks, pp. 15-20, (1996); Ofigucu M.O., Yapay sinir afiglari ile sistem tanfima, M.S. Thesis, Inst. Sci. Technol., Istanbul Teknik Univ., Istanbul, Turkey, (2006); Freeman J.A., Skapura D.M., Neural Networks Algorithms, Appli-cations,and Programming Techniques, pp. 18-50, (1991); Emir S., Classification performance comparison of artificial neural networks andsupport vector machines methods: An empirical study on predicting stockmarket index movement direction, Ph.D. Dissertation, Inst. Social Sci., Istanbul Univ., Istanbul, Turkey, (2013); Sharma A., Understanding Activation Functions in Deep Learning, (2019); Bengio Y., Learning Deep Architectures for AI, 2, 1, pp. 1-127, (2009); Gulli A., Pal S., Deep LearningWith Keras, Birmingham, U, pp. 68-90, (2017); Toussaint M., Introduction to Optimization, (2019); Patterson J., Gibson A., Deep Learning: A Practitioner's Approach, pp. 102-121, (2017); Despois J., Memorizing is Not Learning!? 6 Tricks to Prevent Overfitting in Machine Learning, (2019); Ganguly K., Learning Generative Adversarial Networks, pp. 25-41, (2017); Lu J., Hu J., Zhao G., Mei F., Zhang C., An in-field automatic wheat disease diagnosis system, Comput. Electron. Agricult., 142, pp. 369-379, (2017); Fuentes A., Yoon S., Kim S., Park D., A robust deep-learningbased detector for real-time tomato plant diseases and pests recognition, Sensors, 17, 9, (2017); Kerkech M., Haflane A., Canals R., Deep leaning approach with colorimetric spaces and vegetation indices for vine diseases detection in UAV images, Comput. Electron. Agricult., 155, pp. 237-243, (2018); Hu G., Wu H., Zhang Y., Wan M., A low shot learning method for tea leaf's disease identification, Comput. Electron. Agricult., 163, (2019); Coulibaly S., Kamsu-Foguem B., Kamissoko D., Traore D., Deep neural networks with transfer learning in millet crop images, Comput. Ind., 108, pp. 115-120, (2019); Cruz A., Ampatzidis Y., Pierro R., Materazzi A., Panattoni A., De Bellis L., Luvisi A., Detection of grapevine yellows symptoms in vitis vinifera L. With artificial intelligence, Comput. Electron. Agricult., 157, pp. 63-76, (2019); Picon A., Alvarez-Gila A., Seitz M., Ortiz-Barredo A., Echazarra J., Johannes A., Deep convolutional neural networks for mobile capture device-based crop disease classification in the wild, Comput. Electron. Agricult., 161, pp. 280-290, (2019); Grinblat G.L., Uzal L.C., Larese M.G., Granitto P.M., Deep learning for plant identification using vein morphological patterns, Comput. Electron. Agricult., 127, pp. 418-424, (2016); Rahnemoonfar M., Sheppard C., Deep count: Fruit counting based on deep simulated learning, Sensors, 17, 4, (2017); Veeramani B., Raymond J.W., Chanda P., DeepSort: Deep convolutional networks for sorting haploid maize seeds, BMC Bioinf., 19, S9, pp. 85-93, (2018); Altuntas Y., Comert Z., Kocamaz A.F., Identification of haploid and diploid maize seeds using convolutional neural networks and a transfer learning approach, Comput. Electron. Agricult., 163, (2019); Knoll F.J., Czymmek V., Poczihoski S., Holtorf T., Hussmann S., Improving efficiency of organic farming by using a deep learning classification approach, Comput. Electron. Agricult., 153, pp. 347-356, (2018); Hani N., Roy P., Isler V., A comparative study of fruit detection and counting methods for yield mapping in apple orchards, J. Field Robot., 37, pp. 1-20, (2019); Tian Y., Yang G., Wang Z., Wang H., Li E., Liang Z., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agricult., 157, pp. 417-426, (2019); Gene-Mola J., Vilaplana V., Rosell-Polo J.R., Morros J.-R., Ruiz-Hidalgo J., Gregorio E., Multi-modal deep learning for fuji apple detection using RGB-D cameras and their radiometric capabilities, Comput. Electron. Agricult., 162, pp. 689-698, (2019); Kang H., Chen C., Fruit detection and segmentation for apple harvesting using visual sensor in orchards, Sensors, 19, 20, (2019); Yu Y., Zhang K., Yang L., Zhang D., Fruit detection for strawberry harvesting robot in non-structural environment based on mask-RCNN, Comput. Electron. Agricult., 163, (2019); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning for real-time fruit detection and orchard fruit load estimation: Benchmarking of 'MangoYOLO, ' Precis. Agricult., 20, 6, pp. 1107-1135, (2019); Arad B., Kurtser P., Barnea E., Harel B., Edan Y., Ben-Shahar O., Controlled lighting and illumination-independent target detection for real-time cost-efficient Applications. The case study of sweet pepper robotic harvesting, Sensors, 19, 6, (2019); Kussul N., Lavreniuk M., Kolotii A., Skakun S., Rakoid O., Shumilo L., A workflow for sustainable development goals indicators assessment based on high-resolution satellite data, Int. J. Digit. Earth, 13, 2, pp. 309-321, (2019); Persello C., Tolpekin V.A., Bergado J.R., By R.A., Delineation of agricultural fields in smallholder farms from satellite images using fully convolutional networks and combinatorial grouping, Remote Sens. Environ., 231, (2019); Zhou Y., Luo J., Feng L., Yang Y., Chen Y., Wu W., Longshort-term-memory-based crop classification using high-resolution optical images and multi-temporal SAR data, GISci. Remote Sens., 56, 8, pp. 1170-1191, (2019); Zhao S., Liu X., Ding C., Liu S., Wu C., Wu L., Mapping rice paddies in complex landscapes with convolutional neural networks and phenological metrics, GISci. Remote Sens., 57, 1, pp. 37-48, (2019); Yang Q., Shi L., Han J., Zha Y., Zhu P., Deep convolutional neural networks for Rice grain yield estimation at the ripening stage using UAVbased remotely sensed images, Field Crops Res., 235, pp. 142-153, (2019); Dyson J., Mancini A., Frontoni E., Zingaretti P., Deep learning for soil and crop segmentation from remotely sensed data, Remote Sens., 11, 16, (2019); Nevavuori P., Narra N., Lipping T., Crop yield prediction with deep convolutional neural networks, Comput. Electron. Agricult., 163, (2019); Gorczyca M.T., Milan H.F.M., Maia A.S.C., Gebremedhin K.G., Machine learning algorithms to predict core, skin, and hair-coat temperatures of piglets, Comput. Electron. Agricult., 151, pp. 286-294, (2018); Kvam J., Kongsro J., In vivo prediction of intramuscular fat using ultrasound and deep learning, Comput. Electron. Agricult., 142, pp. 521-523, (2017); Huang X., Hu Z., Wang X., Yang X., Zhang J., Shi D., An improved single shot multibox detector method applied in body condition score for dairy cows, Animals, 9, 7, (2019); Yukun S., Pengju H., Yujie W., Ziqi C., Yang L., Baisheng D., Runze L., Yonggen Z., Automatic monitoring system for individual dairy cows based on a deep learning framework that provides identification via body parts and estimation of body condition score, J. Dairy Sci., 102, 11, pp. 10140-10151, (2019); Zhang Y., Cai J., Xiao D., Li Z., Xiong B., Real-time sow behavior detection based on deep learning, Comput. Electron. Agricult., 163, (2019); Li X., Cai C., Zhang R., Ju L., He J., Deep cascaded convolutional models for cattle pose estimation, Comput. Electron. Agricult., 164, (2019); Christiansen P., Nielsen L.N., Steen K.A., Jorgensen R.N., Karstoft H., DeepAnomaly: Combining background subtraction and deep learning for detecting obstacles and anomalies in an agricultural field, Sensors, 16, 11, pp. 2-21, (2016); Ma N., Peng Y., Wang S., Leong P., An unsupervised deep hyperspectral anomaly detector, Sensors, 18, 3, (2018); Rong D., Xie L., Ying Y., Computer vision detection of foreign objects in walnuts using deep learning, Comput. Electron. Agricult., 162, pp. 1001-1010, (2019); Rasmussen C.B., Moeslund T.B., Maize silage kernel fragment estimation using deep learning-based object recognition in non-separated Kernel/StoverRGBimages, Sensors, 19, 16, (2019); Zhang Z., Liu H., Meng Z., Chen J., Deep learning-based automatic recognition network of agricultural machinery images, Comput. Elec-tron. Agricult., 166, (2019); Cheng X., Zhang Y., Chen Y., Wu Y., Yue Y., Pest identification via deep residual learning in complex background, Comput. Electron. Agricult., 141, pp. 351-356, (2017); Ding W., Taylor G., Automatic moth detection from trap images for pest management, Comput. Electron. Agricult., 123, pp. 17-28, (2016); Zhu L.-Q., Ma M.-Y., Zhang Z., Zhang P.-Y., Wu W., Wang D.-D., Zhang D.-X., Wang X., Wang H.-Y., Hybrid deep learning for automated lepidopteran insect image classification, Oriental Insects, 51, 2, pp. 79-91, (2017); Shen Y., Zhou H., Li J., Jian F., Jayas D.S., Detection of storedgrain insects using deep learning, Comput. Electron. Agricult., 145, pp. 319-325, (2018); Partel V., Nunes L., Stansly P., Ampatzidis Y., Automated visionbased system for monitoring asian citrus psyllid in orchards utilizing arti-ficial intelligence, Comput. Electron. Agricult., 162, pp. 328-336, (2019); Thenmozhi K., Srinivasulu Reddy U., Crop pest classification based on deep convolutional neural network and transfer learning, Comput. Electron. Agricult., 164, (2019); Dawei W., Limiao D., Jiangong N., Jiyue G., Hongfei Z., Zhongzhi H., Recognition pest by image-based transfer learning, J. Sci. Food Agricult., 99, 10, pp. 4524-4531, (2019); Li R., Jia X., Hu M., Zhou M., Li D., Liu W., Wang R., Zhang J., Xie C., Liu L., Wang F., Chen H., Chen T., Hu H., An effective data augmentation strategy for CNN-based pest localization and recognition in the field, IEEE Access, 7, pp. 160274-160283, (2019); AlZu'Bi S., Hawashin B., Mujahed M., Jararweh Y., Gupta B.B., An efficient employment of Internet of multimedia things in smart and future agriculture, Multimedia Tools Appl., 78, 20, pp. 29581-29605, (2019); Song X., Zhang G., Liu F., Li D., Zhao Y., Yang J., Modeling spatio-temporal distribution of soil moisture by deep learning-based cellular automata model, J. Arid Land, 8, 5, pp. 734-748, (2016); Sirsat M.S., Cernadas E., Fernandez-Delgado M., Barro S., Automatic prediction of village-wise soil fertility for several nutrients in India using a wide range of regression methods, Comput. Electron. Agricult., 154, pp. 120-133, (2018); Zambrano F., Vrieling A., Nelson A., Meroni M., Tadesse T., Prediction of drought-induced reduction of agricultural productivity in Chile from MODIS, rainfall estimates, and climate oscillation indices, Remote Sens. Environ., 219, pp. 15-30, (2018); Uzal L.C., Grinblat G.L., Namias R., Larese M.G., Bianchi J.S., Morandi E.N., Granitto P.M., Seed-per-pod estimation for plant breeding using deep learning, Comput. Electron. Agricult., 150, pp. 196-204, (2018); Ampatzidis Y., Partel V., Meyering B., Albrecht U., Citrus rootstock evaluation utilizing UAV-based remote sensing and artifi-cial intelligence, Comput. Electron. Agricult., 164, (2019); Yang H.W., Hsu H.C., Yang C.K., Tsai M.J., Kuo Y.F., Differentiating between morphologically similar species in genus Cinnamomum (Lauraceae) using deep convolutional neural networks, Comput. Elec-tron. Agricult., 162, pp. 739-774, (2019); Milella A., Marani R., Petitti A., Reina G., In-field high throughput grapevine phenotyping with a consumer-grade depth camera, Comput. Electron. Agricult., 156, pp. 293-306, (2019); Feng X., Zhan Y., Wang Q., Yang X., Yu C., Wang H., Tang Z., Jiang D., Peng C., He Y., Hyperspectral imaging combined with machine learning as a tool to obtain high-throughput plant salt-stress phenotyping, Plant J., 101, 6, pp. 1448-1461, (2019); Dos Santos Ferreira A., Freitas D.M., Da Silva G.G., Pistori H., Folhes M.T., Weed detection in soybean crops using ConvNets, Com-put. Electron. Agricult., 143, pp. 314-324, (2017); Moshia M.E., Newete S.W., Mexican poppy (Argemone mexicana) control in cornfield using deep learning neural networks: A perspective, Acta Agriculturae Scandinavica, Sect. B, Soil Plant Sci., 69, 3, pp. 228-234, (2019); Bah M., Haflane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens., 10, 11, (2018); Kounalakis T., Triantafyllidis G.A., Nalpantidis L., Deep learningbased visual recognition of rumex for robotic precision farming, Com-put. Electron. Agricult., 165, (2019); Partel V., Charan Kakarla S., Ampatzidis Y., Development and evaluation of a low-cost and smart technology for precision weed management utilizing artificial intelligence, Comput. Electron. Agricult., 157, pp. 339-350, (2019); Manzini N., Single Hidden Layer Neural Network; Bazzi H., Baghdadi N., Ienco D., El Hajj M.Z.M., Belhouchette H., Escorihuela M.J., Demarez V., Mapping irrigated areas using sentinel-1 time series in catalonia, Spain, Remote Sens., 11, 15, pp. 1-25, (2019); Wang G., Sun Y., Wang J., Automatic image-based plant disease severity estimation using deep learning, Comput. Intell. Neurosci., 2017, pp. 1-8, (2017); Yamamoto K., Togami T., Yamaguchi N., Super-resolution of plant disease images for the acceleration of image-based phenotyping and vigor diagnosis in agriculture, Sensors, 17, 11, (2017); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep learning classification of land cover and crop types using remote sensing data, IEEE Geosci. Remote Sens. Lett., 14, 5, pp. 778-782, (2017); Skovsen S., Dyrmann M., Mortensen A.K., Steen K.A., Green O., Eriksen J., Gislum R., Jorgensen R.N., Karstoft H., Estimation of the botanical composition of clover-grass leys from RGB images using data simulation and fully convolutional neural networks, Sensor, 17, 13, (2017); Liu Z., Zhang W., Lin S., Quek T.Q.S., Heterogeneous sensor data fusion by deep multimodal encoding, IEEE J. Sel. Topics Signal Process., 11, 3, pp. 479-491, (2017); Gadhiya T., Roy A.K., Optimized wishart network for an efficient classification of multifrequency PolSAR data, IEEE Geosci. Remote Sens. Lett., 15, 11, pp. 1720-1724, (2018); Ferentinos K.P., Deep learning models for plant disease detection and diagnosis, Comput. Electron. Agricult., 145, pp. 311-318, (2018); Barbedo J.G.A., Impact of dataset size and variety on the effectiveness of deep learning and transfer learning for plant disease classification, Comput. Electron. Agricult., 153, pp. 46-53, (2018); Waldmann P., Approximate Bayesian neural networks in genomic prediction, Genet. Selection Evol., 50, 1, (2018); Khan Z., Rahimi-Eichi V., Haefele S., Garnett T., Miklavcic S.J., Estimation of vegetation indices for high-throughput phenotyping of wheat using aerial imaging, Plant Methods, 14, 1, (2018); Ghosal S., Blystone D., Singh A.K., Ganapathysubramanian B., Singh A., Sarkar S., An explainable deep machine vision framework for plant stress phenotyping, Proc. Nat. Acad. Sci. USA, 115, 18, pp. 4613-4618, (2018); Gaetano R., Ienco D., Ose K., Cresson R., A two-branch CNN architecture for land cover classification of PAN and MS imagery, Remote Sens., 10, 11, (2018); D'Andrimont R., Lemoine G., Velde Der M.Van, Targeted grassland monitoring at parcel level using sentinels, street-level images and field observations, Remote Sens., 10, 8, pp. 1-27, (2018); Dominguez C., Heras J., Mata E., Pascual V., DecoFungi: A Web application for automatic characterisation of dye decolorisation in fungal strains, BMC Bioinf., 19, 1, (2018); Yuksel M.E., Basturk N.S., Badem H., Caliskan A., Basturk A., Classification of high resolution hyperspectral remote sensing data using deep neural networks, J. Intell. Fuzzy Syst., 34, 4, pp. 2273-2285, (2018); Dias P.A., Tabb A., Medeiros H., Apple flower detection using deep convolutional networks, Comput. Ind., 99, pp. 17-28, (2018); Li W., Dong R., Fu H., Yu A.L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks, Remote Sens., 11, 1, (2018); Sulistyo S.B., Woo W.L., Dlay S.S., Gao B., Building a globally optimized computational intelligent image processing algorithm for onsite inference of nitrogen in plants, IEEE Intell. Syst., 33, 3, pp. 15-26, (2018); Pereira D.R., Papa J.P., Saraiva G.F.R., Souza G.M., Automatic classification of plant electrophysiological responses to environmental stimuli using machine learning and interval arithmetic, Comput. Elec-tron. Agricult., 145, pp. 35-42, (2018); Wang Z., Hu M., Zhai G., Application of deep learning architectures for accurate and rapid detection of internal mechanical damage of blueberry using hyperspectral transmittance data, Sensors, 18, 4, (2018); Zheng C., Zhu X., Yang X., Wang L., Tu S., Xue Y., Automatic recognition of lactating sow postures from depth images by deep learning detector, Comput. Electron. Agricult., 147, pp. 51-63, (2018); Fabijanska A., Danek M., DeepDendro-A tree rings detector based on a deep convolutional neural network, Comput. Electron. Agricult., 150, pp. 353-363, (2018); Too E.C., Yujian L., Njuki S., Yingchun L., A comparative study of fine-tuning deep learning models for plant disease identification, Comput. Electron. Agricult., 161, pp. 272-279, (2019); Liang Q., Xiang S., Hu Y., Coppola G., Zhang D., Sun W., PD2SENet: Computer-assisted plant disease diagnosis and severity estimation network, Comput. Electron. Agricult., 157, pp. 518-529, (2019); Han Z., Gao J., Pixel-level aflatoxin detecting based on deep learning and hyperspectral imaging, Comput. Electron. Agricult., 164, (2019); Singh U.P., Chouhan S.S., Jain S., Jain S., Multilayer convolution neural network for the classification of mango leaves infected by anthracnose disease, IEEE Access, 7, pp. 43721-43729, (2019); Nagasubramanian K., Jones S., Singh A.K., Sarkar S., Singh A., Ganapathysubramanian B., Plant disease identification using explainable 3D deep learning on hyperspectral images, Plant Methods, 15, 1, (2019); Stewart E.L., Wiesner-Hanks T., Kaczmar N., DeChant C., Wu H., Lipson H., Nelson R.J., Gore M.A., Quantitative phenotyping of northern leaf blight in UAV images using deep learning, Remote Sens., 11, 19, (2019); Arsenovic M., Karanovic M., Sladojevic S., Anderla A., Stefanovic D., Solving current limitations of deep learning based approaches for plant disease detection, Symmetry, 11, 7, pp. 1-21, (2019); Turkoglu M., Hanbay D., Plant disease and pest detection using deep learning-based features, Turkish J. Electr. Eng. Comput. Sci., 27, 3, pp. 1636-1651, (2019); Espejo-Garcia B., Lopez-Pellicer F.J., Lacasta J., Moreno R.P., Zarazaga-Soria F.J., End-to-end sequence labeling via deep learning for automatic extraction of agricultural regulations, Comput. Electron. Agricult., 162, pp. 106-111, (2019); Moon T., Hong S., Choi H.Y., Jung D.H., Chang S.H., Son J.E., Interpolation of greenhouse environment data using multilayer perceptron, Comput. Electron. Agricult., 166, (2019); Ringland J., Bohm M., Baek S.-R., Characterization of food cultivation along roadside transects with Google street view imagery and deep learning, Comput. Electron. Agricult., 158, pp. 36-50, (2019); Zhong L., Hu L., Zhou H., Deep learning based multi-temporal crop classification, Remote Sens. Environ., 221, pp. 430-443, (2019); Garcia-Pedrero A., Lillo-Saavedra M., Rodriguez-Esparragon D., Gonzalo-Martin C., Deep learning for automatic outlining agricultural parcels: Exploiting the land parcel identification system, IEEE Access, 7, pp. 158223-158236, (2019); Zheng Y.-Y., Kong J.-L., Jin X.-B., Wang X.-Y., Zuo M., CropDeep: The crop vision dataset for deep-learning-based classification and detection in precision agriculture, Sensors, 19, 5, (2019); Du Z., Yang J., Ou C., Zhang T., Smallholder crop area mapped with a semantic segmentation deep learning method, Remote Sens., 11, 7, (2019); Jiao W., Li J., A microbial image recognition method based on convolutional neural networks, Acta Microscopica, 28, 6, pp. 1450-1458, (2019); Arredondo-Santoyo M., Dominguez C., Heras J., Mata E., Pascual V., Vazquez-Garciduenas M.S., Vazquez-Marrufo G., Automatic characterisation of dye decolourisation in fungal strains using expert, traditional, and deep features, Soft Comput., 23, 23, pp. 12799-12812, (2019); Remya S., Sasikala R., Classification of rubberized coir fibres using deep learning-based neural fuzzy decision tree approach, Soft Comput., 23, 18, pp. 8471-8485, (2019); Liu J., Wang X., Wang T., Classification of tree species and stock volume estimation in ground forest images using deep learning, Comput. Electron. Agricult., 166, (2019); Barth R., IJsselmuiden J., Hemming J., Van Henten E.J., Synthetic bootstrapping of convolutional neural networks for semantic plant part segmentation, Comput. Electron. Agricult., 161, pp. 291-304, (2019); Zhao Y., Yuan Y., Wang Q., Fast spectral clustering for unsupervised hyperspectral image classification, Remote Sens., 11, 4, (2019); Zhang W., Gai J., Zhang Z., Tang L., Liao Q., Ding Y., Double-DQN based path smoothing and tracking control method for robotic vehicle navigation, Comput. Electron. Agricult., 166, (2019); Wu L., Liu Z., Bera T., Ding H., Langley D.A., Jenkins-Barnes A., Furlanello C., Maggio V., Tong W., Xu J., A deep learning model to recognize food contaminating beetle species based on elytra fragments, Comput. Electron. Agricult., 166, (2019); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sens., 11, 4, (2019); Kaya A., Keceli A.S., Catal C., Yalic H.Y., Temucin H., Tekinerdogan B., Analysis of transfer learning for deep neural network based plant classification models, Comput. Electron. Agricult., 158, pp. 20-29, (2019); Gao L., Lin X., Fully automatic segmentation method for medicinal plant leaf images in complex background, Comput. Electron. Agricult., 164, (2019); Fernandes A.M., Utkin A.B., Eiras-Dias J., Cunha J., Silvestre J., Melo-Pinto P., Grapevine variety identification using 'big data' collected with miniaturized spectrometer combined with support vector machines and convolutional neural networks, Comput. Electron. Agricult., 163, (2019); Przybylo J., Jablonski M., Using deep convolutional neural network for oak acorn viability recognition based on color images of their sections, Comput. Electron. Agricult., 156, pp. 490-499, (2019); Ozkan K., Isk S, Yavuz B.T., Identification of wheat kernels by fusion of RGB, SWIR, and VNIR samples, J. Sci. Food Agricult., 99, 11, pp. 4977-4984, (2019); Bjerge K., Frigaard C.E., Mikkelsen P.H., Nielsen T.H., Misbih M., Kryger P., A computer vision system to monitor the infestation level of Varroa destructor in a honeybee colony, Comput. Electron. Agricult., 164, pp. 1001-1010, (2019); Tian M., Guo H., Chen H., Wang Q., Long C., Ma Y., Automated pig counting using deep learning, Comput. Electron. Agricult., 163, pp. 1-10, (2019); Jiang B., Wu Q., Yin X., Wu D., Song H., He D., FLYOLOv3 deep learning for key parts of dairy cow body detection, Comput. Electron. Agricult., 166, (2019); Shahinfar S., Kelman K., Kahn L., Prediction of sheep carcass traits from early-life records using machine learning, Comput. Electron. Agricult., 156, pp. 159-177, (2019); Huang L., He A., Zhai M., Wang Y., Bai R., Nie X., A multi-feature fusion based on transfer learning for chicken embryo eggs classification, Symmetry, 11, 5, (2019); Sa J., Choi Y., Lee H., Chung Y., Park D., Cho J., Fast pig detection with a top-view camera under various illumination conditions, Symme-try, 11, 2, (2019); Dijkstra K., Loosdrecht J., Schomaker L.R.B., Wiering M.A., Hyperspectral demosaicking and crosstalk correction using deep learning, Mach. Vis. Appl., 30, 1, pp. 1-21, (2019); Dong W., Wu T., Luo J., Sun Y., Xia L., Land parcel-based digital soil mapping of soil nutrient properties in an alluvial-diluvia plain agricultural area in China, Geoderma, 340, pp. 234-248, (2019); Saggi M.K., Jain S., Reference evapotranspiration estimation and modeling of the punjab northern India using deep learning, Comput. Electron. Agricult., 156, pp. 387-398, (2019)","Z. Unal; Niǧtaş Company, Niǧde, Turkey; email: zeynepunal1010@hotmail.com","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086712805"
"Li M.; Zhang Z.; Lei L.; Wang X.; Guo X.","Li, Min (58361218800); Zhang, Zhijie (57217729916); Lei, Liping (57203377707); Wang, Xiaofan (57208274062); Guo, Xudong (9336669100)","58361218800; 57217729916; 57203377707; 57208274062; 9336669100","Agricultural greenhouses detection in high‐resolution satellite images based on convolutional neural networks: Comparison of faster R‐CNN, YOLO v3 and SSD","2020","Sensors (Switzerland)","20","17","4938","1","14","13","63","10.3390/s20174938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090049390&doi=10.3390%2fs20174938&partnerID=40&md5=f2bce24e71989eb9b071eb0a4ac26161","Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; Key Laboratory of Land Use, Ministry of Natural Resources, China Land Surveying and Planning Institute, Beijing, 100035, China","Li M., Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; Zhang Z., Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; Lei L., Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Wang X., Key Laboratory of Land Use, Ministry of Natural Resources, China Land Surveying and Planning Institute, Beijing, 100035, China; Guo X., Key Laboratory of Land Use, Ministry of Natural Resources, China Land Surveying and Planning Institute, Beijing, 100035, China","Agricultural greenhouses (AGs) are an important facility for the development of modern agriculture. Accurately and effectively detecting AGs is a necessity for the strategic planning of modern agriculture. With the advent of deep learning algorithms, various convolutional neural network (CNN)‐based models have been proposed for object detection with high spatial resolution images. In this paper, we conducted a comparative assessment of the three well‐established CNN‐based models, which are Faster R‐CNN, You Look Only Once‐v3 (YOLO v3), and Single Shot Multi‐Box Detector (SSD) for detecting AGs. The transfer learning and fine‐tuning approaches were implemented to train models. Accuracy and efficiency evaluation results show that YOLO v3 achieved the best performance according to the average precision (mAP), frames per second (FPS) metrics and visual inspection. The SSD demonstrated an advantage in detection speed with an FPS twice higher than Faster R‐CNN, although their mAP is close on the test set. The trained models were also applied to two independent test sets, which proved that these models have a certain transability and the higher resolution images are significant for accuracy improvement. Our study suggests YOLO v3 with superiorities in both accuracy and computational efficiency can be applied to detect AGs using high‐resolution satellite images operationally. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Agricultural greenhouse detection; Convolutional neural network; Faster R‐CNN; SSD; YOLO v3","Agricultural robots; Agriculture; Computational efficiency; Convolution; Deep learning; Efficiency; Greenhouses; Image enhancement; Learning algorithms; Learning systems; Object detection; Transfer learning; Accuracy Improvement; Agricultural greenhouse; Comparative assessment; Efficiency evaluation; Frames per seconds; High spatial resolution images; Higher resolution images; Modern agricultures; article; convolutional neural network; greenhouse; satellite imagery; transfer of learning; velocity; Convolutional neural networks","","","","","National Key Research and Development Program of China, (2016YFB0501505)","Funding: This research was financially supported by the National Key Research and Development Program of China (2016YFB0501505).","Cantliffe D.J., Protected agriculture—A regional solution for water scarcity and production of high‐value crops in the Jordan Valley, Proceedings of the Water in the Jordan Valley: Technical Solutions and Regional Cooperation Conference, (2001); Levin N., Lugassi R., Ramon U., Braun O., Ben-Dor E., Remote sensing as a tool for monitoring plasticulture in agricultural landscapes, Int. J. Remote Sens, 28, pp. 183-202, (2007); Picuno P., Innovative material and improved technical design for a sustainable exploitation of agricultural plastic film, Polym. Plast. Technol. Eng, 53, pp. 1000-1011, (2014); Picuno P., Tortora A., Capobianco R.L., Analysis of plasticulture landscapes in Southern Italy through remote sensing and solid modelling techniques, Landsc. Urban Plan, 100, pp. 45-56, (2011); Chaofan W., Jinsong D., Ke W., Ligang M., Tahmassebi A.R.S., Object‐based classification approach for greenhouse mapping using Landsat‐8 imagery, Int. J. Agric. Biol. Eng, 9, pp. 79-88, (2016); Knickel K., Changes in Farming Systems, Landscape, and Nature: Key Success Factors of Agri‐Environmental Schemes (AES), proceedings of the EUROMAB Symposium, (1999); Du X.M., Wu Z.H., Zhang Y.Q., PEI X.X., Study on changes of soil salt and nutrient in greenhouse of different planting years, J. Soil Water Conserv, 2, pp. 78-80, (2007); Hanan J.J., Holley W.D., Goldsberry K.L., Greenhouse Management, (2012); Arel I., Rose D.C., Karnowski T.P., Deep machine learning‐a new frontier in artificial intelligence research [research frontier], IEEE Comput. Intel. Mag, 5, pp. 13-18, (2010); Bishop C.M., Pattern Recognition and Machine Learning, (2007); Ma Y., Wu H., Wang L., Huang B., Ranjan R., Zomaya A., Jie W., Remote sensing big data computing: Challenges and opportunities, Future Gener. Comput. Sys, 51, pp. 47-60, (2015); Benediktsson J.A., Chanussot J., Moon W.M., Very High‐Resolution Remote Sensing: Challenges and Opportunities, Proc. IEEE, 100, pp. 1907-1910, (2012); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Li H., Deep learning for natural language processing: advantages and challenges, Natl. Sci. Rev, 5, pp. 24-26, (2017); Otter D.W., Medina J.R., Kalita J.K., A survey of the usages of deep learning for natural language processing, IEEE Trans. Neural Netw. Learn. Syst, pp. 1-21, (2020); Brunetti A., Buongiorno D., Trotta G.F., Bevilacqua V., Computer vision and deep learning techniques for pedestrian detection and tracking: A survey, Neurocomputing, 300, pp. 17-33, (2018); Silver D., Huang A., Maddison C.J., Guez A., Sifre L., Van Den Driessche G., Dieleman S., Mastering the game of Go with deep neural networks and tree search, Nature, 529, pp. 484-489, (2016); Chen Y., Fan R., Yang X., Wang J., Latif A., Extraction of urban water bodies from high‐resolution remote‐sensing imagery using deep learning, Water, 10, (2018); Gao L., Song W., Dai J., Chen Y., Road extraction from high‐resolution remote sensing imagery using refined deep residual convolutional neural network, Remote Sens, 11, (2019); Alshehhi R., Marpu P.R., Woon W.L., Dalla Mura M., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks, ISPRS J. Photogramm. Remote Sens, 130, pp. 139-149, (2017); Kamel A., Sheng B., Yang P., Li P., Shen R., Feng D.D., Deep convolutional neural networks for human action recognition using depth maps and postures, IEEE Trans. Sys. Man Cybern. Syst, 49, pp. 1806-1819, (2018); Xu Y., Xie Z., Feng Y., Chen Z., Road extraction from high‐resolution remote sensing imagery using deep learning, Remote Sens, 10, (2018); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional neural networks for large‐scale remote‐sensing image classification, IEEE Trans. Geosci. Remote Sens, 55, pp. 645-657, (2016); Pan X., Zhao J., High‐resolution remote sensing image classification method based on convolutional neural network and restricted conditional random field, Remote Sens, 10, (2018); Wang Y., Wang C., Zhang H., Dong Y., Wei S., Automatic Ship Detection Based on RetinaNet Using Multi‐Resolution Gaofen‐3 Imagery, Remote Sens, 11, (2019); Chen C., Gong W., Chen Y., Li W., Learning a two‐stage CNN model for multi‐sized building detection in remote sensing images, Remote Sens. Lett, 10, pp. 103-110, (2019); Koga Y., Miyazaki H., Shibasaki R., A Method for Vehicle Detection in High‐Resolution Satellite Images that Uses a Region‐Based Object Detector and Unsupervised Domain Adaptation, Remote Sens, 12, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2014); Girshick R., Fast r‐cnn, Proceedings of the IEEE International Conference on Computer Vision, (2015); Ren S., He K., Girshick R., Sun J., Faster r‐cnn: Towards real‐time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Proceedings of the European Conference on Computer Vision—ECCV2016, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real‐time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement; Everingham M., Van Gool L., Williams C.K., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, Int. J. Comput. Vis, 88, pp. 303-338, (2010); Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L., ImageNet: A large‐scale hierarchical image database, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Zitnick C.L., Microsoft COCO: Common Objects in Context, Lecture Notes in Computer Science, Proceedings of the European Conference on Computer Vision—ECCV2014, 8693, pp. 740-755, (2014); Cheng G., Zhou P., Han J., Learning rotation‐invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens, 54, pp. 7405-7415, (2016); Guo W., Yang W., Zhang H., Hua G., Geospatial object detection in high resolution satellite images based on multi‐scale convolutional neural network, Remote Sens, 10, (2018); Zhang S., Wu R., Xu K., Wang J., Sun W., R‐CNN‐Based Ship Detection from High Resolution Remote Sensing Imagery, Remote Sens, 11, (2019); Chen Z., Zhang T., Ouyang C., End‐to‐end airplane detection using transfer learning in remote sensing images, Remote Sens, 10, (2018); Ma H., Liu Y., Ren Y., Yu J., Detection of Collapsed Buildings in Post‐Earthquake Remote Sensing Images Based on the Improved YOLOv3, Remote Sens, 12, (2019); Paisitkriangkrai S., Sherrah J., Janney P., Hengel V.D., Effective semantic pixel labelling with convolutional networks and Conditional Random Fields, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 36-43; Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large‐Scale Image Recognition, (2014); West J., Ventura D., Warnick S., Spring Research Presentation: A Theoretical Foundation for Inductive Transfer, 1, (2007); Bochkovskiy A., Wang C.Y., Liao H.Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020)","Z. Zhang; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; email: zhangzj2018@radi.ac.cn; Z. Zhang; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; email: zhangzj2018@radi.ac.cn","","MDPI AG","","","","","","14248220","","","32878345","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090049390"
"Jiang S.; Zhao H.; Wu W.; Tan Q.","Jiang, Shulong (57200322299); Zhao, Hongrui (7404779392); Wu, Wenjia (56510518100); Tan, Qifan (57202044626)","57200322299; 7404779392; 56510518100; 57202044626","A novel framework for remote sensing image scene classification","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","3","","657","663","6","15","10.5194/isprs-archives-XLII-3-657-2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046977060&doi=10.5194%2fisprs-archives-XLII-3-657-2018&partnerID=40&md5=15e1f0727871716242e7e199c7c8c0e4","Department of Civil Engineering, Tsinghua University, Beijing, 10084, China; 3S Center, Tsinghua University, Beijing, 10084, China","Jiang S., Department of Civil Engineering, Tsinghua University, Beijing, 10084, China, 3S Center, Tsinghua University, Beijing, 10084, China; Zhao H., Department of Civil Engineering, Tsinghua University, Beijing, 10084, China, 3S Center, Tsinghua University, Beijing, 10084, China; Wu W., Department of Civil Engineering, Tsinghua University, Beijing, 10084, China, 3S Center, Tsinghua University, Beijing, 10084, China; Tan Q., Department of Civil Engineering, Tsinghua University, Beijing, 10084, China, 3S Center, Tsinghua University, Beijing, 10084, China","High resolution remote sensing (HRRS) images scene classification aims to label an image with a specific semantic category. HRRS images contain more details of the ground objects and their spatial distribution patterns than low spatial resolution images. Scene classification can bridge the gap between low-level features and high-level semantics. It can be applied in urban planning, target detection and other fields. This paper proposes a novel framework for HRRS images scene classification. This framework combines the convolutional neural network (CNN) and XGBoost, which utilizes CNN as feature extractor and XGBoost as a classifier. Then, this framework is evaluated on two different HRRS images datasets: UC-Merced dataset and NWPU-RESISC45 dataset. Our framework achieved satisfying accuracies on two datasets, which is 95.57% and 83.35% respectively. From the experiments result, our framework has been proven to be effective for remote sensing images classification. Furthermore, we believe this framework will be more practical for further HRRS scene classification, since it costs less time on training stage. © Authors 2018.","Convolutional neural network; Deep learning; Fully-connected layer; Scene classification; XGBoost","Convolution; Deep learning; Neural networks; Remote sensing; Semantics; Uranium compounds; Convolutional neural network; Convolutional Neural Networks (CNN); Fully-connected layers; High resolution remote sensing; Remote sensing images classification; Scene classification; Spatial distribution patterns; XGBoost; Image classification","","","","","National Natural Science Foundation of China, (41571414)","This study was supported by the National Natural Science Foundation of China “Eco-environment assessment of Yanhe watershed based on temporal-spatial entropy” [Grant number 41571414].","Zhang L., Zhang L., Du B., Deep learning for remote sensing data: A technical tutorial on the state of the art, IEEE Geoscience and Remote Sensing Magazine, 4, pp. 22-40, (2016); Fu G., Liu C., Zhou R., Sun T., Zhang Q., Classification for high resolution remote sensing imagery using a fully convolutional network, Remote Sensing, 9, pp. 498-519, (2017); Bratasanu D., Nedelcu I., Datcu M., Bridging the semantic gap for satellite image annotation and automatic mapping applications, IEEE Journal of Selected Topics in Applied Earth Observations & Remote Sensing, 4, pp. 193-204, (2011); Lienou M., Maitre H., Datcu M., Semantic annotation of satellite images using latent dirichlet allocation, IEEE Geoscience & Remote Sensing Letters, 7, pp. 28-32, (2010); Lecun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Castelluccio M., Poggi G., Sansone C., Et al., Land use classification in remote sensing images by convolutional neural networks, Acta Ecologica Sinica, 28, pp. 627-635, (2015); Penatti O.A.B., Nogueira K., Santos J.A.D., Do deep features generalize from everyday objects to remote sensing and aerial scenes domains, Computer Vision and Pattern Recognition Workshops IEEE, pp. 44-51, (2015); Hu F., Xia G., Hu J., Et al., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Remote Sensing, 7, pp. 14680-14707, (2015); Zou Q., Ni L., Zhang T., Et al., Deep learning based feature selection for remote sensing scene classification, IEEE Geoscience and Remote Sensing Letters, 12, pp. 2321-2325, (2015); Chen T., Guestrin C., XGBoost: A Scalable Tree Boosting System, pp. 785-794, (2016); Fei-Fei L., Deng J., Li K., ImageNet: Constructing a large-scale image database, Journal of Vision, 9, (2009); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Sigspatial International Conference on Advances in Geographic Information Systems, pp. 270-279, (2010); Zhong Y., Fei F., Zhang L., Large patch convolutional neural networks for the scene classification of high spatial resolution imagery, Journal of Applied Remote Sensing, 10, (2016); Marmanis D., Datcu M., Esch T., Et al., Deep learning earth observation classification using ImageNet pretrained networks, IEEE Geoscience and Remote Sensing Letters, 13, pp. 105-109, (2016); Cheng G., Han J., Lu X., Remote sensing image scene classification: Benchmark and state of the art, Proc. IEEE; Weng Q., Mao Z., Lin J., Et al., Land-use classification via extreme learning classifier based on deep convolutional features, IEEE Geoscience & Remote Sensing Letters, PP, 99, pp. 1-5, (2017); Blei D.M., Ng A.Y., Jordan M.I., Latent dirichlet allocation, Journal of Machine Learning Research, 3, pp. 993-1022, (2015); Bosch A., Zisserman A., Munoz X., Scene classification via pLSA, Computer Vision - Eccv, pp. 517-530, (2006); Yang Y., Newsam S., Bag-of-visual-words and spatial extensions for land-use classification, Sigspatial International Conference on Advances in Geographic Information Systems. ACM, pp. 270-279, (2010)","H. Zhao; Department of Civil Engineering, Tsinghua University, Beijing, 10084, China; email: zhr@tsinghua.edu.cn","Liang X.; Osmanoglu B.; Soergel U.; Honkavaara E.; Scaioni M.; Peled A.; Shaker A.; Wu L.; Abdulmuttalib H.M.; Zhang H.; Di K.; Tanzi J.J.; Komp K.; Li R.; Stilla U.; Jiang J.; Faruque F.S.; Zhang J.; Yoshimura M.","International Society for Photogrammetry and Remote Sensing","","2018 ISPRS TC III Mid-Term Symposium on Developments, Technologies and Applications in Remote Sensing","7 May 2018 through 10 May 2018","Beijing","136130","16821750","","","","English","Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85046977060"
"Abdel-Khalek S.; Algarni M.; Mansour R.F.; Gupta D.; Ilayaraja M.","Abdel-Khalek, Sayed (6506630609); Algarni, Mariam (57205136996); Mansour, Romany F. (36960713000); Gupta, Deepak (56985108600); Ilayaraja, M. (55662288300)","6506630609; 57205136996; 36960713000; 56985108600; 55662288300","Quantum neural network-based multilabel image classification in high-resolution unmanned aerial vehicle imagery","2021","Soft Computing","","","","","","","11","10.1007/s00500-021-06460-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120383192&doi=10.1007%2fs00500-021-06460-3&partnerID=40&md5=9da9c7ff3a067ceea69f86820296fa73","Department of Mathematics and Statistics, College of Science, Taif University, P.O. Box 11099, Taif, 21944, Saudi Arabia; Mathematical Sciences Department, College of Science, Princess Nourah Bint Abdulrahman University, Riyadh, 11564, Saudi Arabia; Department of Mathematics, Faculty of Science, New Valley University, El-Kharga, 72511, Egypt; Department of Computer Science and Engineering, Maharaja Agrasen Institute of Technology, Delhi, India; Department of Computer Science and Information Technology, Kalasalingam Academy of Research and Education, Krishnankoil, India","Abdel-Khalek S., Department of Mathematics and Statistics, College of Science, Taif University, P.O. Box 11099, Taif, 21944, Saudi Arabia; Algarni M., Mathematical Sciences Department, College of Science, Princess Nourah Bint Abdulrahman University, Riyadh, 11564, Saudi Arabia; Mansour R.F., Department of Mathematics, Faculty of Science, New Valley University, El-Kharga, 72511, Egypt; Gupta D., Department of Computer Science and Engineering, Maharaja Agrasen Institute of Technology, Delhi, India; Ilayaraja M., Department of Computer Science and Information Technology, Kalasalingam Academy of Research and Education, Krishnankoil, India","Latest advancements in real-time remote sensing sensors and platform lead to the development of unmanned aerial vehicles (UAV) which enable the accessibility of high-resolution imaging data. Since image classification appears like a basic interconnection among aerial images and the corresponding applications, it intends to categorize images into semantic classes. Several earlier works have concentrated on the classification of an image into a single semantic label, whereas in the real world, an aerial image is commonly interrelated to many class labels, for instance, multiple object-level labels. Moreover, a broad image of present objects in provided high-resolution aerial image offers an extensive interpretation of the investigated area. To attain this, this paper presents a new quantum neural network based multilabeled aerial image classification (QNN-MLAIC) model. The proposed QNN-MLAIC model involves different processes, namely image acquisition, preprocessing, object detection, feature extraction, and classification. Initially, the aerial images are acquired by wireless UAVs and are preprocessed. Then, the Faster RCNN technique with Inception with Residual Network-v2 model as the baseline model is applied as an object detector, which detects the existence of multiple objects in the aerial image and generates a helpful set of feature vectors. Finally, QNN is employed as classifier, which categorizes the aerial images into multiple class labels. In order to tune the parameters involved in QNN model, the beetle antenna search algorithm is employed. Detailed performance analysis of the proposed QNN-MLAIC model takes place using the UC Merced dataset (UCM) aerial dataset, and the outcomes are investigated under many dimensions. The experimental outcome ensured the goodness of the presented QNN-MLAIC method on the applied UCM aerial dataset over the compared methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Multilabel image classification; Quantum computing; Quantum neural network; UAV networks","Antennas; Deep learning; Feature extraction; Image acquisition; Image classification; Object detection; Object recognition; Quantum computers; Remote sensing; Semantics; Aerial images; Class labels; Classification models; Deep learning; Images classification; Network-based; Quantum Computing; Quantum neural networks; Unmanned aerial vehicle network; Vehicle network; Unmanned aerial vehicles (UAV)","","","","","","","Alshehri A., Bazi Y., Ammour N., Almubarak H., Alajlan N., Deep attention neural network for multi-label classification in unmanned aerial vehicle imagery, IEEE Access, 7, pp. 119873-119880, (2019); Baranwal E., Raghvendra S., Tiwari P.S., Pande H., Health monitoring and assessment of the cultural monument through unmanned aerial vehicle (UAV) image processing, Advances in Systems Engineering, pp. 145-160, (2021); Bashmal L., Bazi Y., Al Rahhal M.M., Alhichri H., Al Ajlan N., UAV image multi-labeling with data-efficient transformers, Appl Sci, 11, 9, (2021); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., van der Meer F., van der Werff H., van Coillie F., Et al., Geographic object-based image analysis—towards a new paradigm, ISPRS J Photogramm Remote Sens, 87, pp. 180-191, (2014); Boonpook W., Tan Y., Ye Y., Torteeka P., Torsri K., Dong S., A deep learning approach on building detection from unmanned aerial vehicle-based images in riverbank monitoring, Sensors, 18, 11, (2018); Borlea I.D., Precup R.E., Borlea A.B., Iercan D., A unified form of fuzzy C-means and K-means algorithms and its partitional implementation, Knowl Based Syst, 214, (2021); Cong M., Wang Z., Tao Y., Xi J., Ren C., Xu M., Unsupervised self-adaptive deep learning classification network based on the optic nerve microsaccade mechanism for unmanned aerial vehicle remote sensing image classification, Geocarto Int, 89, pp. 1-20, (2019); Cong M., Xi J., Han L., Gu J., Yang L., Tao Y., Xu M., Multi-resolution classification network for high-resolution UAV remote sensing images, Geocarto Int, 56, pp. 1-25, (2020); Guan W., Zhou H., Su Z., Zhang X., Zhao C., Ship steering control based on quantum neural network, Complexity, 2019, pp. 1-10, (2019); Helge A., Eija H., Arko L., Pablo J.Z.T., Quantitative remote sensing at ultra-high resolution with UAV spectroscopy: a review of sensor technology, measurement procedures, and data correction workflows, Remote Sens, 10, 7, (2018); Hua Y., Mou L., Zhu X.X., Recurrently exploring class-wise attention in a hybrid convolutional and bidirectional LSTM network for multi-label aerial image classification, ISPRS J Photogramm Remote Sens, 149, pp. 188-199, (2019); Huang Q., Xie L., Yin G., Ran M., Liu X., Zheng J., Acoustic signal analysis for detecting defects inside an arc magnet using a combination of variational mode decomposition and beetle antennae search, ISA Trans, 102, pp. 347-364, (2020); Jiao W., Wang L., McCabe M.F., Multi-sensor remote sensing for drought characterization: current status, opportunities and a roadmap for the future, Remote Sens Environ, 256, (2021); Kamble R.M., Chan G.C., Perdomo O., Kokare M., Gonzalez F.A., Muller H., Meriaudeau F., Automated diabetic macular edema (DME) analysis using fine tuning with inception-resnet-v2 on OCT images, 2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES), pp. 442-446, (2018); Lei X., Sui Z., Intelligent fault detection of high voltage line based on the Faster R-CNN, Measurement, 138, pp. 379-385, (2019); Li Y., Chen R., Zhang Y., Zhang M., Chen L., Multi-label remote sensing image scene classification by combining a convolutional neural network and a graph neural network, Remote Sens, 12, 23, (2020); Lin D., Lin J., Zhao L., Wang Z.J., Chen Z., Multilabel aerial image classification with a concept attention graph neural network, IEEE Trans Geosci Remote Sens, 23, pp. 1-12, (2021); Moser G., Serpico S.B., Benediktsson J.A., Land-cover mapping by Markov modeling of spatial-contextual information in very-high-resolution remote sensing images, Proc IEEE, 101, pp. 631-651, (2013); Munawar H.S., Ullah F., Qayyum S., Heravi A., Application of deep learning on UAV-based aerial images for flood detection, Smart Cities, 4, 3, pp. 1220-1242, (2021); Najafi P., Feizizadeh B., Navid H., A comparative approach of fuzzy object based image analysis and machine learning techniques which are applied to crop residue cover mapping by using Sentinel-2 satellite and UAV imagery, Remote Sens, 13, 5, (2021); Pustokhina I.V., Pustokhin D.A., Kumar Pareek P., Gupta D., Khanna A., Shankar K., Energy-efficient cluster-based unmanned aerial vehicle networks with deep learning-based scene classification model, Int J Commun Syst, 34, 8, (2021); Saeed S., Latif M.A., Rajput M.A., Fuzzy-based multi-crop classification using high resolution UAV imagery, Quaid-E-Awam Univ Res J Eng Sci Technol Nawabshah, 19, 1, pp. 1-8, (2021); Sobrino J.A., Frate F.D., Drusch M., Jimenez-Munoz J.C., Manunta P., Regan A., Review of thermal infrared applications and requirements for future high-resolution sensors, IEEE Trans Geosci Remote Sens, 54, 5, pp. 2963-2972, (2016); Sumbul G., DemIr B., A deep multi-attention driven approach for multi-label remote sensing image classification, IEEE Access, 8, pp. 95934-95946, (2020); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., Inception-v4, inception-resnet and the impact of residual connections on learning, AAAI, (2017); Tan Q., Liu Y., Chen X., Yu G., Multi-label classification based on low rank representation for image annotation, Remote Sens, 9, 2, (2017); Xie H., Chen Y., Ghamisi P., Remote sensing image scene classification via label augmentation and intra-class constraint, Remote Sens, 13, 13, (2021); Zeggada A., Benbraika S., Melgani F., Mokhtari Z., Multilabel conditional random field classification for UAV images, IEEE Geosci Remote Sens Lett, 15, 3, pp. 399-403, (2018); Zhang X., Han L., Dong Y., Shi Y., Huang W., Han L., Gonzalez-Moreno P., Ma H., Ye H., Sobeih T., A deep learning-based approach for automated yellow rust disease detection from high-resolution hyperspectral UAV images, Remote Sens, 11, 13, (2019); Zhou Y., Zhang R., Wang S., Wang F., Feature selection method based on high-resolution remote sensing images and the effect of sensitive features on classification accuracy, Sensors, 18, 7, (2018)","R.F. Mansour; Department of Mathematics, Faculty of Science, New Valley University, El-Kharga, 72511, Egypt; email: romanyf@sci.nvu.edu.eg","","Springer Science and Business Media Deutschland GmbH","","","","","","14327643","","","","English","Soft Comput.","Article","Article in press","","Scopus","2-s2.0-85120383192"
"Xu X.; Tao H.; Li C.; Cheng C.; Guo H.; Zhou J.","Xu, Xinluo (57218365058); Tao, Huan (55660960500); Li, Cunjun (36910680800); Cheng, Cheng (57218227175); Guo, Hang (55358147500); Zhou, Jingping (55928189900)","57218365058; 55660960500; 36910680800; 57218227175; 55358147500; 55928189900","Detection and Location of Pine Wilt Disease Induced Dead Pine Trees Based on Faster R-CNN; [基于Faster R-CNN的松材线虫病受害木识别与定位]","2020","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","51","7","","228","236","8","13","10.6041/j.issn.1000-1298.2020.07.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088919683&doi=10.6041%2fj.issn.1000-1298.2020.07.026&partnerID=40&md5=6fb91621aa084ea9c032512764efe148","School of Information Engineering, Nanchang University, Nanchang, 330031, China; Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China","Xu X., School of Information Engineering, Nanchang University, Nanchang, 330031, China; Tao H., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Li C., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Cheng C., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; Guo H., School of Information Engineering, Nanchang University, Nanchang, 330031, China; Zhou J., Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China","Pine wilt disease (PWD) is a devastating infectious disease for the rapid spread, short disease period, and strong pathogenic ability. At present, detecting the PWD induced dead pine trees (DPT) timely and then taking corresponding measures are vital to control the spread of PWD. An unmanned aerial vehicle (UAV) platform equipped with the Vis-RGB digital camera was used to obtain the ultra-high spatial resolution images. Deep learning object detection of Faster R-CNN was adopted to detect the DPT automatically. Different from the previous research on the DPT identification, the influences of other dead trees and red broad-leaved trees on DPT identification were considered. The results showed that Faster R-CNN can effectively identify the DPT. The 6.78 percentage points detection accuracy of the DPT would be improved when taking the anchor size, other dead trees and red broad-leaved trees into consideration. The overall accuracy of DPT detection can reach 82.42%, which can meet the protector for felling of the DPT. Finally, the position of predicted DPT was calculated accurately using coordinate transformation. Combined with the point combination process, 494 DPT were correctly located. This research had the advantages of low cost, high efficiency and automatic identification, and can provide technical support for the prevention and control of PWD. The combination of UAV remote sensing and object detection algorithms was a promising method to monitor the occurrence of PWD and the distribution of the DPT, which provided important basis for the consequence harmless treatment of PWD induced DPT. © 2020, Chinese Society of Agricultural Machinery. All right reserved.","Faster R-CNN; Location; Object detection; Pine wilt disease; Unmanned aerial vehicle image","Aircraft detection; Antennas; Automation; Convolutional neural networks; Deep learning; Forestry; Object detection; Object recognition; Remote sensing; Unmanned aerial vehicles (UAV); Co-ordinate transformation; Combination process; Corresponding measures; Detection accuracy; Harmless treatments; Infectious disease; Object detection algorithms; Prevention and controls; Titration","","","","","","","(2003); ZHAO B G., Pine wilt disease in China, pp. 18-25, (2008); KIYOHARA T, TOKUSHIGE Y., Inoculation experiments of a nematode, Bursaphelenchus sp., Onto Pine trees, Journal of the Japanese Forestry Society, 53, 7, pp. 210-218, (1971); JONES J T, HAEGEMAN A, DANCHIN E G, Et al., Top 10 plant-parasitic nematodes in molecular plant pathology, Molecular Plant Pathology, 14, 9, pp. 946-961, (2013); 3, pp. 46-51; YE Jianren, Epidemicstatues of pile wilt disease in China and its prevention and control techniques and counter measures, Scientia Silvae Sinicae, 55, 9, pp. 1-9, (2019); 4; XU Jiang, Study on the treatment technology of fumigation and detoxification of pine wood nematode disease, Modern Agricultural Science and Technology, 12, pp. 150-151, (2017); 12; FOIT J, CERMAK V, GAAR V, Et al., New insights into the life history of Monochamus galloprovincialis can enhance surveillance strategies for the pinewood nematode, Journal of Pest Science, 92, 3, pp. 1203-1215, (2019); WULDER M A, DYMOND C C, WHITE J C, Et al., Detection, mapping, and monitoring of the mountain pine beetle, pp. 123-154, (2006); SEUNG-HO L, HYUN-KOOK C, WOO-KYUN L, Et al., Detection of the pine trees damaged by pine wilt disease using high resolution satellite and airborne optical imagery, Korean Journal of Remote Sensing, 23, 5, pp. 409-420, (2007); KIM S R, LEE W K, LIM C H, Et al., Hyperspectral analysis of pine wilt disease to determine an optimal detection index, Forests, 9, 3, pp. 115-127, (2018); LI Weizheng, SHEN Shiguang, HE Peng, Et al., A precisely positioning technique by remote sensing the dead trees in stands with inexpensive small UAV, Journal of Forestry Engineering, 28, 6, pp. 102-106, (2014); LU Xiaojun, WANG Jun, YU Weiguo, Et al., Study on monitoring forest pests and diseases by unmanned aerial vehicle, Hubei Forestry Science and Technology, 45, 4, pp. 30-33, (2016); TAO Huan, LI Cunjun, XIE Chunchun, Et al., Recognition of red-attack pine trees from UAV imagery based on the HSV threshold method, Journal of Nanjing Forestry University (Natural Sciences), 43, 3, pp. 99-106, (2019); LIU Xialing, CHENG Duoxiang, LI Tao, Et al., Preliminary study on automatic monitoring trees infected by pine wood nematode with high resolution images from unmanned aerial vehicle, Forest Pest and Disease, 37, 5, pp. 16-21, (2018); HU Gensheng, ZHANG Xuemin, LIANG Dong, Et al., Infected pine recognition in remote sensing images based on weighted support vector data description, Transactions of the Chinese Society for Agricultural Machinery, 44, 5, pp. 258-263, (2013); SONG Yining, LIU Wenping, LUO Youqing, Et al., Monitoring of dead trees in forest images based on linear spectral clustering, Scientia Silvae Sinicae, 55, 4, pp. 187-195, (2019); ZHANG Yannan, Application of UAV remote sensing in forestry, Inner Mongolia Forestry Investigation and Design, 40, 6, pp. 77-78, (2017); BANU T P, BORLEA G F, BANU C., The use of drones in forestry, Journal of Environmental Science and Engineering, B5, pp. 557-562, (2016); TANG L, SHAO G., Drone remote sensing for forestry research and practices, Journal of Forestry Research, 26, 4, pp. 791-797, (2015); REN S, HE K, GIRSHICK R, Et al., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 6, pp. 1137-1149, (2017); SUN J, HE X F, GE X, Et al., Detection of key organs in tomato based on deep migration learning in a complex background, Agriculture, 8, 12, pp. 196-211, (2018); ZHANG S, WU R, XU K, Et al., R-CNN-based ship detection from high resolution remote sensing imagery, Remote Sensing, 11, 6, pp. 631-646, (2019); HAN X, ZHONG Y, ZHANG L., An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery, Remote Sensing, 9, 7, pp. 666-688, (2017); HONG S J, HAN Y, KIM S Y, Et al., Application of deep-learning methods to bird detection using unmanned aerial vehicle imagery, Sensors, 19, 7, pp. 1651-1667, (2019); REN Y, ZHU C, XIAO S., Small object detection in optical remote sensing images via modified Faster R-CNN, Applied Science, 8, 5, pp. 813-824, (2018); GIRSHICK R., Fast R-CNN, The IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); GIRSHICK R, DONAHUE J, DARRELL T, Et al., Region-based convolutional networks for accurate object detection and segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 1, pp. 142-158, (2016); LIU W, ANGUELOV D, ERHAN D, Et al., SSD: single shot multiBox detector, European Conference on Computer Vision, pp. 21-37, (2016); REDMON J, DIVVALA S, GIRSHICK R, Et al., You only look once: unified, real-time object detection, The IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); SUN Hong, LI Song, LI Minzan, Et al., Research progress of image sensing and deep learning in agriculture, Transactions of the Chinese Society for Agricultural Machinery, 51, 5, pp. 1-17, (2020); SIMONYAN K, ZISSERMAN, A very deep convolutional networks for large-scale image recognition, International Conference on Learning Representations, pp. 1-14, (2015); KOIRALA A, WALSH K B, WANG Z, Et al., Deep learning-method overview and review of use for fruit detection and yield estimation, Computers and Electronics in Agriculture, 162, pp. 219-234, (2019); WANG Zhen, ZHANG Xiaoli, AN Shujie, Spectral characteristics analysis of Pinus massoniana suffered by Bursaphelenchus xylophilus, Remote Sensing Technology and Application, 22, 3, pp. 367-370, (2007); XU Huachao, LUO Youqing, ZHANG Tingting, Et al., Changes of reflectance spectra of pine needles in different stage after being infected by pine wood nematode, Spectroscopy and Spectral Analysis, 31, 5, pp. 1352-1356, (2011)","H. Tao; Beijing Research Center for Information Technology in Agriculture, Beijing, 100097, China; email: taoh.11s@igsnrr.ac.cn","","Chinese Society of Agricultural Machinery","","","","","","10001298","","NUYCA","","Chinese","Nongye Jixie Xuebao","Article","Final","","Scopus","2-s2.0-85088919683"
"Song S.; Liu J.; Liu Y.; Feng G.; Han H.; Yao Y.; Du M.","Song, Shiran (57203151101); Liu, Jianhua (55660498400); Liu, Yuan (57211088198); Feng, Guoqiang (57213605172); Han, Hui (57213592632); Yao, Yuan (57213607062); Du, Mingyi (34881413000)","57203151101; 55660498400; 57211088198; 57213605172; 57213592632; 57213607062; 34881413000","Intelligent object recognition of urban water bodies based on deep learning for multi-source and multi-temporal high spatial resolution remote sensing imagery","2020","Sensors (Switzerland)","20","2","397","","","","26","10.3390/s20020397","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077838041&doi=10.3390%2fs20020397&partnerID=40&md5=280a81cf53c34d893e5abe215338b10b","School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; Key Laboratory for Urban Geomatics of National Administration of Surveying, Mapping and Geoinformation, Beijing, 100044, China","Song S., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; Liu J., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China, Key Laboratory for Urban Geomatics of National Administration of Surveying, Mapping and Geoinformation, Beijing, 100044, China; Liu Y., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; Feng G., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; Han H., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; Yao Y., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; Du M., School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China, Key Laboratory for Urban Geomatics of National Administration of Surveying, Mapping and Geoinformation, Beijing, 100044, China","High spatial resolution remote sensing image (HSRRSI) data provide rich texture, geometric structure, and spatial distribution information for surface water bodies. The rich detail information provides better representation of the internal components of each object category and better reflects the relationships between adjacent objects. In this context, recognition methods such as geographic object-based image analysis (GEOBIA) have improved significantly. However, these methods focus mainly on bottom-up classifications from visual features to semantic categories, but ignore top-down feedback which can optimize recognition results. In recent years, deep learning has been applied in the field of remote sensing measurements because of its powerful feature extraction ability. A special convolutional neural network (CNN) based region proposal generation and object detection integrated framework has greatly improved the performance of object detection for HSRRSI, which provides a new method for water body recognition based on remote sensing data. This study uses the excellent “self-learning ability” of deep learning to construct a modified structure of the Mask R-CNN method which integrates bottom-up and top-down processes for water recognition. Compared with traditional methods, our method is completely data-driven without prior knowledge, and it can be regarded as a novel technical procedure for water body recognition in practical engineering application. Experimental results indicate that the method produces accurate recognition results for multi-source and multi-temporal water bodies, and can effectively avoid confusion with shadows and other ground features. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; High spatial resolution remotely sensed imagery; Multi-source and multi-temporal; Object recognition; Water body","Image enhancement; Image resolution; Neural networks; Object detection; Object recognition; Remote sensing; Semantics; Surface waters; Textures; Bottom-up and top-down; Convolutional neural network; Geographic object-based image analysis; High spatial resolution; Multi-temporal; Practical engineering applications; Remotely sensed imagery; Waterbodies; Deep learning","","","","","Fundamental Research Funds for Beijing Universities of Civil Engineering and Architecture, (ZC01-42030); National Natural Science Foundation of China, NSFC, (41301489); Beijing Municipal Commission of Education, (21147518608, YETP1647); Beijing Municipal Education Commission; Natural Science Foundation of Beijing Municipality, (4142013, 4192018); Beijing University of Civil Engineering and Architecture, BUCEA, (21082716012)","Funding text 1: Funding: This research was funded by the National Natural Science Foundation of China (No. 41301489), Beijing Natural Science Foundation (No. 4192018, No. 4142013), and Outstanding Youth Teacher Program of Beijing Municipal Education Commission (No. YETP1647, No. 21147518608), Outstanding Youth Researcher Program of Beijing University of Civil Engineering and Architecture (No. 21082716012) and the Fundamental Research Funds for Beijing Universities of Civil Engineering and Architecture (No. ZC01-42030).; Funding text 2: This research was funded by the National Natural Science Foundation of China (No. 41301489), Beijing Natural Science Foundation (No. 4192018, No. 4142013), and Outstanding Youth Teacher Program of Beijing Municipal Education Commission (No. YETP1647, No. 21147518608), Outstanding Youth Researcher Program of Beijing University of Civil Engineering and Architecture (No. 21082716012) and the Fundamental Research Funds for Beijing Universities of Civil Engineering and Architecture (No. ZC01-42030).","Zhu C.M., Luo J.C., Shen Z.F., River Linear Water Adaptive Auto-extraction on Remote Sensing Image Aided by DEM, Acta Geod. Cartogr. Sin., 42, pp. 277-283, (2013); Sheng Y.W., Xiao Q.G., Water-body identification in cloud contaminated NOAA/AVHRR image, Environ. Remote Sens., 9, pp. 247-255, (1994); Zhou C.H., Du Y.Y., Luo J.C., A description model based on knowledge for automatically recognizing water from NOAA/AVHRR, J. Nat. Disasters, 5, pp. 100-108, (1996); Du Y.Y., Zhou C.H., Automatically extracting remote sensing information for water bodies, J. Remote Sens., 2, pp. 264-269, (1998); Yang C.J., Xu M., Study on the water-body extraction methods of remote sensing information mechanism, Geogr. Res., 17, pp. 86-89, (1998); Du J.K., Huang Y.S., Feng X.Z., Wang Z.L., Study on water bodies extraction and classification from SPOT Image, J. Remote Sens., 5, pp. 214-219, (2001); He Z.Y., Zhang X.C., Huang Z.C., Jiang H.X., A water extraction technique based on high-spatial remote sensing images, J. Zhejiang Univ., 31, pp. 701-707, (2004); Liu H., Jezek K.C., Automated extraction of coastline from satellite imagery by integrating canny edge detection and locally adaptive thresholding methods, Int. J. Remote Sens., 25, pp. 937-958, (2004); Zhong C., Zeng C., Liu Z., Study on Terrestrial Water Information Identified Based on the Analysis of Spectral Signature and Ratio Index, Geo-Inf. Sci., 10, pp. 663-669, (2008); Luo J.C., Sheng Y.W., Shen Z.F., Li J.L., Gao L., Automatic and high-precise extraction for water information from multispectral images with the step-by-step iterative transformation mechanism, J. Remote Sens., 13, pp. 604-615, (2009); Chen J.-B., Liu S.-X., Wang C.-Y., You S.-C., Wang Z.-W., Research on Urban Water Body Extraction Using Knowledge-based Decision Tree, Remote Sens. Inf., 28, pp. 29-33, (2013); McFeeters S.K., The use of normalized difference water index (NDWI) in the delineation of open water features, Int. J. Remote Sens., 17, pp. 1425-1432, (1996); Gao B.C., NDWI-a normalized difference water index for remote sensing of vegetation liquid water from space, Remote Sens. Environ., 58, pp. 257-266, (1996); Xu H.Q., A study on information extraction of water body with the modified normalized difference water index (MNDWI), J. Remote Sens., 9, pp. 589-595, (2005); Feyisa G.L., Meilby H., Fensholt R., Proud S.R., Automated Water Extraction Index: A New Technique for Surface Water Mapping Using Landsat Imagery, Remote Sens. Environ., 140, pp. 23-35, (2014); Ji L., Zhang L., Wylie B., Analysis of Dynamic Thresholds for the Normalized Difference Water Index, Photogramm. Eng. Remote Sens., 75, pp. 1307-1317, (2009); Blaschke T., Strobl J., What’s wrong with pixels? Some recent developments interfacing remote sensing and GIS, Geobit/Gis, 14, pp. 12-17, (2001); Blaschke T., Burnett C., Pekkarinen A., New Contextual Approaches Using Image Segmentation for Object-based Classification, Remote Sensing Image Analysis: Including the Spatial Domain, pp. 211-236, (2004); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., van der Meer F., van der Werff H., Coillie V., Et al., Geographic objectbased image analysis-towards a new paradigm, ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191, (2014); Feng W., Zhang Y.J., Object-oriented Change Detection for Remote Sensing Images Based on Multi-scale fusion, Acta Geod. Cartogr. Sin., 44, pp. 1142-1151, (2015); Li H., Tang Y.W., Liu Q.J., An Improved Algorithm Based on Minimum Spanning Tree for Multi-scale Segmentation of Remote Sensing Imagery, Acta Geod. Cartogr. Sin., 44, pp. 791-796, (2015); Shen L., Tang H., Wang S., River Extraction from the High-Resolution Remote Sensing Image Based on Spatially Correlated Pixels Template and Adboost Algorithm, Acta Geod. Cartogr. Sin., 42, pp. 344-350, (2013); Liu J., Du M., Mao Z., Scale Computation on High Spatial Resolution Remotely Sensed Imagery Multi-Scale Segmentation, Int. J. Remote Sens., 38, pp. 5186-5214, (2017); Li X., Zhang S., Pang Z., The Use of Vector Analysis Theory on Landscape Pattern in Remote Sensing Information Extraction: A Case Study on Qian-an Group Lakes, J. Remote Sens., 12, pp. 291-296, (2008); Tansey K., Chambers I., Anstee A., Denniss A., Lamb A., Object-oriented classification of very high resolution airborne imagery for the extraction of hedgerows and field margin cover in agricultural areas, Appl. Geogr., 29, pp. 145-157, (2009); Zhou C.-Y., Ping W., Zhang Z.-Y., Qi C.-T., Classification of Urban Land Based on Object-oriented Information Extraction Technology, Remote Sens. Technol. Appl., 23, pp. 31-35, (2008); Wang J., Li Z., Wu F., Advances in Digital Map Generalization, pp. 75-82, (2011); Belgiu M., Dragu L., Comparing supervised and unsupervised multiresolution segmentation approaches for extracting buildings from very high resolution imagery, ISPRS J. Photogramm. Remote Sens., 96, pp. 67-75, (2014); Castilla G., Hernando A., Zhang C., McDermid G.J., The impact of object size on the thematic accuracy of landcover maps, Int. J. Remote Sens., 35, pp. 1029-1037, (2014); Huang H.P., Wu B.F., Li M.M., Zhou W.F., Wang Z.W., Detecting Urban Vegetation Efficiently with High Resolution Remote Sensing Data, J. Remote. Sens., 8, pp. 69-75, (2004); Ming D., Li J., Wang J., Zhang M., Scale Parameter Selection by Spatial Statistics for GEOBIA: Using Mean-shift Based Multi-scale Segmentation as an Example, ISPRS J. Photogramm. Remote. Sens., 106, pp. 28-41, (2015); Ming D., Ci T., Cai H., Li L., Qiao C., Du J., Semivariogram-based Spatial Bandwidth Selection for Remote Sensing Image Segmentation with Mean-shift Algorithm, IEEE Geosci. Remote Sens. Lett., 9, pp. 813-817, (2012); Min H., Wenjun Z., Weihong W., Optimal Segmentation Scale Model Based on Object-Oriented and Analysis Method, J. Geod. Geodyn., 1, pp. 110-113, (2009); Liu J., Zhang J., Xu F., Huang Z., Li Y., Adaptive Algorithm for Automated Polygonal Approximation of High Spatial Resolution Remote Sensing Imagery Segmentation Contours, IEEE Trans. Geosci. Remote Sens., 52, pp. 1099-1106, (2014); Girshick R., Donahue J., Darrell T., Malik J., Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, Commun. ACM, 2012; Long J., Shelhamer E., Darrell T., Fully Convolutional Networks for Semantic Segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, pp. 640-651, (2014); Lecun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, (2015); Zhou P., Han J., Cheng G., Zhang B., Learning Compact and Discriminative Stacked Autoencoder for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens., 57, pp. 4823-4833, (2019); Cheng G., Zhou P., Han J., Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images, IEEE Trans. Geosci. Remote Sens., 54, pp. 7405-7415, (2016); Zhong Y., Han X., Zhang L., Multi-class geospatial object detection based on a position-sensitive balancing framework for high spatial resolution remote sensing imagery, ISPRS J. Photogramm. Remote Sens., 138, pp. 281-294, (2018); Cheng G., Han J., Zhou P., Guo L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors, ISPRS J. Photogramm. Remote Sens., 98, pp. 119-132, (2014); He L., Li J., Liu C., Li S., Recent Advances on Spectral-Spatial Hyperspectral Image Classification: An Overview and New Guidelines, IEEE Trans. Geosci. Remote Sens., 56, pp. 1579-1597, (2017); Zhao J., Zhong Y., Jia T., Wang X., Xu Y., Shu H., Zhang L., Spectral-spatial classification of hyperspectral imagery with cooperative game, ISPRS J. Photogramm. Remote Sens., 135, pp. 31-42, (2018); Peng J., Du Q., Robust Joint Sparse Representation Based on Maximum Correntropy Criterion for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens., 55, pp. 7152-7164, (2017); Dong Y., Du B., Zhang L., Zhang L., Dimensionality Reduction and Classification of Hyperspectral Images Using Ensemble Discriminative Local Metric Learning, IEEE Trans. Geosci. Remote Sens., 55, pp. 2509-2524, (2017); Liu T., Gu Y., Jia X., Benediktsson J.A., Chanussot J., Class-Specific Sparse Multiple Kernel Learning for Spectral–Spatial Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens., 54, pp. 7351-7365, (2016); Xu X., Li J., Huang X., Dalla Mura M., Plaza A., Multiple Morphological Component Analysis Based Decomposition for Remote Sensing Image Classification, IEEE Trans. Geosci. Remote. Sens., 54, pp. 3083-3102, (2016); Bian X., Chen C., Xu Y., Du Q., Robust Hyperspectral Image Classification by Multi-Layer Spatial-Spectral Sparse Representations, Remote Sens, 8, (2016); Li J., Huang X., Gamba P., Bioucas-Dias J.M., Zhang L., Benediktsson J.A., Plaza A., Multiple Feature Learning for Hyperspectral Image Classification, IEEE Trans. Geosci. Remote Sens., 53, pp. 1592-1606, (2015); Chen C., Li W., Su H., Liu K., Spectral-Spatial Classification of Hyperspectral Image Based on Kernel Extreme Learning Machine, Remote Sens, 6, pp. 5795-5814, (2014); Ji S., Wei S., Lu M., A scale robust convolutional neural network for automatic building extraction from aerial and satellite imagery, Int. J. Remote Sens., 40, pp. 3308-3322, (2018); Song S.R., Liu J.H., Pu H., Liu Y., Luo J.Y., The Comparison of Fusion Methods for HSRRSI Considering the Effectiveness of Land Cover (Features) Object Recognition Based on Deep Learning, Remote. Sens., 11, (2019); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 27-30, (2016); Bishop C.M., Neural Networks for Pattern Recognition, (1995); Ripley B.D., Pattern Recognition and Neural Networks, (1996); Venables W., Ripley B., Modern Applied Statistics with S-Plus, (1999); Simonyan K.A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (1556); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 21-26, (2017); Mask R-CNN for Object Detection and Instance Segmentation on Keras and Tensorflow; He K., Gkioxari G., Dollar P., Girshick R., Mask R.-C., In Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22–29 October 2017, (2017)","J. Liu; School of Geomatics and Urban Spatial Information, Beijing University of Civil, Engineering and Architecture, Beijing, 100044, China; email: liujianhua@bucea.edu.cn","","MDPI AG","","","","","","14248220","","","31936791","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85077838041"
"Pathak K.; Arya A.; Hatti P.; Handragal V.; Lee K.","Pathak, Kanaad (57216273042); Arya, Arti (26653367800); Hatti, Prakash (57222320740); Handragal, Vidyadhar (57222328415); Lee, Kristopher (57222317299)","57216273042; 26653367800; 57222320740; 57222328415; 57222317299","A study of different disease detection and classification techniques using deep learning for cannabis plant","2021","International Journal of Computing and Digital Systems","10","1","","53","62","9","1","10.12785/ijcds/100106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102296170&doi=10.12785%2fijcds%2f100106&partnerID=40&md5=a3b77cbf4d2e51d09b6e7b402134f57d","PES University, Bangalore, India; MCA department, PES University, Bangalore, India; Goalsr India Pvt Ltd, Bangalore, India; Goalsr Inc, San Jose, United States","Pathak K., PES University, Bangalore, India; Arya A., MCA department, PES University, Bangalore, India; Hatti P., Goalsr India Pvt Ltd, Bangalore, India; Handragal V., Goalsr Inc, San Jose, United States; Lee K., Goalsr Inc, San Jose, United States","In this paper, different models for disease detection and classification are studied for cannabis plants. Cannabis plants are used for medical and recreational purposes with its recent legalization in some places. Cannabis farmers face problems in cultivation of the crop since it’s susceptible to multiple disorders. With early detection of the disease in the crop it is possible to prevent large waste of yield in the crop. A real dataset is considered for disease detection and classification purposes which is a combination of text and image data and that has been collected over a period of one and a half years (Feb 2018-August 2019). The models used in this study are Fast Region Convolutional Neural Network(F-RCNN), MobileNet Single Shot Multibox Detector(MobileNet-SSD), You Only Look Once(YOLO) and Residual Network-50 Layers (ResNet50). It is found that the MobileNet-SSD provided the best accuracy amongst all the object detection models that are studied and has a lesser training time as well. ResNet 50 is used for identifying the number of images required for a good fit without having to label first and then studied for the object detection models. © 2021 University of Bahrain. All rights reserved.","Cannabis Plant; Classification; Disease Detection; F-RCNN; MobileNet-SSD; RCNN; ResNet50; YOLO","","","","","","","","Gould Julie, The cannabis crop, Nature, 525, 7570, pp. S2-S2, (2015); Bifulco Maurizio, Pisanti Simona, Medicinal use of cannabis in Europe, EMBO reports, 16, 2, pp. 130-132, (2015); Madras Bertha K., Update of cannabis and its medical use, (2015); Volkow Nora D., Baler Ruben D., Compton Wilson M., Weiss Susan RB, Adverse health effects of marijuana use, New England Journal of Medicine, 370, 23, pp. 2219-2227, (2014); Borji Ali, Cheng Ming-Ming, Hou Qibin, Jiang Huaizu, Li Jia, Salient object detection: A survey, Computational Visual Media, pp. 1-34, (2014); Krizhevsky Alex, Sutskever Ilya, Hinton Geoffrey E., Imagenet classification with deep convolutional neural networks, In Advances in neural information processing systems, pp. 1097-1105, (2012); Girshick Ross, Fast r-cnn, Proceedings of the IEEE international conference on computer vision, pp. 1440-1448, (2015); Tang Tianyu, Zhou Shilin, Deng Zhipeng, Zou Huanxin, Lei Lin, Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining, Sensors, 17, 2, (2017); Wang Xiaolong, Shrivastava Abhinav, Gupta Abhinav, A-fast-rcnn: Hard positive generation via adversary for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); Redmon Joseph, Divvala Santosh, Girshick Ross, Farhadi Ali, You only look once: Unified, real-time object detection, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779-788, (2016); Li Yiting, Huang Haisong, Xie Qingsheng, Yao Liguo, Chen Qipeng, Research on a surface defect detection algorithm based on MobileNet-SSD, Applied Sciences, 8, 9, (2018); Biswas Debojit, Su Hongbo, Wang Chengyi, Stevanovic Aleksandar, Wang Weimin, An automatic traffic density estimation using Single Shot Detection (SSD) and MobileNetSSD, Physics and Chemistry of the Earth, Parts A/B/C, 110, pp. 176-184, (2019); Girshick Ross, Fast R-CNN, Github, (2018); Model Zoo TensorFlow, GitHub, (2019); Loss Function PyTorch, Torch.nn - PyTorch Master Documentation; Gao Hao, Understand Single Shot MultiBox Detector (SSD) and Implement It in Pytorch, Medium, Medium, (2018); He Kaiming, Zhang Xiangyu, Ren Shaoqing, Sun Jian, Deep residual learning for image recognition, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, (2016); Jay Prakash, Understanding And Implementing Architectures Of Resnet And Resnext For State-Of-The-Art Image Classification: From Microsoft To Facebook [Part 1], Medium, (2018); Albawi Saad, Mohammed Tareq Abed, Al-Zawi Saad, Understanding of a convolutional neural network, 2017 International Conference on Engineering and Technology (ICET), pp. 1-6, (2017); Tzutalin/Labelimg, (2015); Budiharto W., Gunawan A. A. S., Suroso J. S., Chowanda A., Patrik A., Utama G., Fast Object Detection for Quadcopter Drone Using Deep Learning, 2018 3rd International Conference on Computer and Communication Systems (ICCCS), pp. 192-195, (2018); iu Wei, Anguelov Dragomir, Erhan Dumitru, Szegedy Christian, Reed Scott, Fu Cheng-Yang, Berg Alexander C., Ssd: Single shot multibox detector, European conference on computer vision, pp. 21-37, (2016); Zhou Xinyi, Gong Wei, Fu WenLong, Du Fengtong, Application of deep learning in object detection, 2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS), pp. 631-634, (2017); Everingham Mark, Van Gool Luc, Williams Christopher KI, Winn John, Zisserman Andrew, The pascal visual object classes (voc) challenge, International journal of computer vision, 88, 2, pp. 303-338, (2010); Lin Tsung-Yi, Maire Michael, Belongie Serge, Hays James, Perona Pietro, Ramanan Deva, Dollar Piotr, Lawrence Zitnick C., Microsoft ´ coco: Common objects in context, European conference on computer vision, pp. 740-755, (2014)","","","University of Bahrain","","","","","","2210142X","","","","English","Int. J. Comput. Digit. Syst.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102296170"
"Ozkan C.; Ozturk C.; Sunar F.; Karaboga D.","Ozkan, Coskun (56265996700); Ozturk, Celal (23091274400); Sunar, Filiz (6603011829); Karaboga, Dervis (6701575189)","56265996700; 23091274400; 6603011829; 6701575189","The artificial bee colony algorithm in training artificial neural network for oil spill detection","2011","Neural Network World","21","6","","473","492","19","20","10.14311/NNW.2011.21.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856487667&doi=10.14311%2fNNW.2011.21.028&partnerID=40&md5=01c8d1775b4d11d74297ea2662b7bcdc","Geomatics Engineering, Erciyes University, Kayseri, Turkey; Computer Engineering, Erciyes University, Kayseri, Turkey; Geomatics Engineering, Istanbul Technical University, Turkey","Ozkan C., Geomatics Engineering, Erciyes University, Kayseri, Turkey; Ozturk C., Computer Engineering, Erciyes University, Kayseri, Turkey; Sunar F., Geomatics Engineering, Istanbul Technical University, Turkey; Karaboga D., Computer Engineering, Erciyes University, Kayseri, Turkey","Nowadays, remote sensing technology is being used as an essential tool for monitoring and detecting oil spills to take precautions and to prevent the damages to the marine environment. As an important branch of remote sensing, satellite based synthetic aperture radar imagery (SAR) is the most effective way to accomplish these tasks. Since a marine surface with oil spill seems as a dark object because of much lower backscattered energy, the main problem is to recognize and differentiate the dark objects of oil spills from others to be formed by oceanographic and atmospheric conditions. In this study, Radarsat-1 images covering Lebanese coasts were employed for oil spill detection. For this purpose, a powerful classifier, Artificial Neural Network Multilayer Perceptron (ANN MLP) was used. As the original contribution of the paper, the network was trained by a novel heuristic optimization algorithm known as Artificial Bee Colony (ABC) method besides the conventional Backpropagation (BP) and Levenberg-Marquardt (LM) learning algorithms. A comparison and evaluation of different network training algorithms regarding reliability of detection and robustness show that for this problem best result is achieved with the Artificial Bee Colony algorithm (ABC). © ICS AS CR 2011.","Artificial bee colony (ADC); Artificial neural network (ANN); Oil spill","Backpropagation; Backpropagation algorithms; Damage detection; Deep neural networks; Evolutionary algorithms; Heuristic algorithms; Heuristic methods; Marine pollution; Neural networks; Oil spills; Radar imaging; Remote sensing; Satellite imagery; Synthetic aperture radar; Tracking radar; Artificial bee colonies; Artificial bee colonies (ABC); Artificial bee colony algorithms; Artificial bee colony algorithms (ABC); Heuristic optimization algorithms; Levenberg-Marquardt learning algorithms; Remote sensing technology; Synthetic Aperture Radar Imagery; Optimization","","","","","","","Alpers W., Wismann V., Theis H., Huhnerfuss H., Bartsch N., Moreira J., Lyden J., The damping of ocean surface waves by monomolecular sea slicks measured by airborne multi-frequency radars during the SAXON-FPN experiment, Proceedings of the International Geoscience and Remote Sensing Symposium, pp. 1987-1990, (1991); Assilzadeh H., Mansor S.B., Early warning system for oil spill using SAR images, Proceedins of ACRS 2001-22nd Asian Conference on Remote Sensing, 2001, pp. 460-465; Bava J., Tristan O., Yasnikouski J., Earth observation applications through systems and instruments of high performance computer, ASI/CONEA Training Course, (2002); Bishop C., Neural Networks for Pattern Recognition, (1995); Brekke C., Solberg A.H.S., Feature extraction for oil spill detection based on SAR images, Lecture Notes in Computer Science, 3540, pp. 75-84, (2005); Brekke C., Solberg A.H.S., Oil spill detection by satellite remote sensing, Remote Sensing of Environment, 95, 1, pp. 1-13, (2005); Congalton R., Green K., Assessing the accuracy of remotely sensed data, Principles and Practices (CRC/Lewis Piess, Boca Raton, FL.), (1999); Dayhof J., Neural-Network Architectures, (1990); Del Frate F., Petrocchi A., Lichtenegger J., Calabresi G., Neural networks for oil spill detection using ERS-SAR data, IEEE Transactions on Geoscience and Remote Sensing, 38, pp. 2282-2287, (2000); Fiscella B., Giancaspro A., Nirchio F., Pavese P., Trivero P., Oil spill detection using marine SAR images, International Journal of Remote Sensing, 21, 18, pp. 3561-3566, (2000); Foody G.M., Thematic mapping from remotely sensed data with neural networks: MLP, RBF and PNN based approaches, Journal of Geographical Systems, 3, 3, pp. 217-232, (2001); Hagan M.T., Menhaj M.B., Training feedforward networks with the marquardt algorithm, IEEE Transactions on Neural Networks, 6, pp. 989-993, (1994); Harahsheh H., Essa S., Shiobarac M., Nishidaid T., Onumad T., Operational satellite monitoring and detection for oil spill in offshore of United Arab Emirates, Proceedings of Commission VII, (2004); Haralick R.M., Shanmugan K., Dinstein I., Textural features for image classification, IEEE Transactions on Systems, Man, and Cybernetics, 3, pp. 610-621, (1973); Haralick R., Statistical and structural approaches to texture, Proceedings of IEEE, 5, pp. 786-803, (1979); Haykin S., Neural Networks, (1999); Hovland H.A., Johannessen J.A., Digranes G., Slick detection in SAR images, Proceedings of the International Geoscience and Remote Sensing Symposium, pp. 2038-2040, (1994); Johnson R.A., Bhattacharyya G.K., Statistics, (2000); Karaboga D., An idea based on honey bee swarm for numerical optimization, Technical Report-TR06, (2005); Karaboga D., Akay B., Comparative study of artificial bee colony algorithm, Applied Mathematics and Computation, 214, pp. 108-132, (2009); Karaboga D., Basturk B.A., Powerful and efficient algorithm for numerical function optimization: Artificial Bee Colony (ABC) algorithm, Journal of Global Optimization, 3, pp. 459-171, (2007); Karaboga D., Basturk B., Artificial Bee Colony (ABC) optimization algorithm for solving constrained optimization problems, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 4529, pp. 789-798, (2007); Karaboga D., Ozturk C., Neural networks training by Artificial Bee Colony algorithm on pattern classification, Neural Networks World, 3, pp. 279-292, (2009); Karaboga D., Ozturk CA novel clustering approach: Artificial Bee Colony (ABC) algorithm, Applied Soft Computing, 1, 2011, pp. 652-657; Karathanassi V., Topouzelis K., Pavlakis P., Rokos D., An object-oriented methodology to detect oil spills, International Journal of Remote Sensing, 27, 23, pp. 5235-5251, (2006); Kavzoglu T., Mather P.M., The use of backpropagating artificial neural networks in land cover classification, International Journal of Remote Sensing, 24, (2003); Keramitsoglou I., Cartalis C., Kiranoudis OAutomatic identification of oil spills on satellite images, Environmental Modeling and Software, 21, (2005); Kermani B.G., Schiffman S.S., Nagle H.T., Performance of the Levenberg-Marquardt neural network training method in electronic nose applications, Sensors and Actuators, B: Chemical, 110, 1, pp. 13-22, (2005); Kurbaii T., Besdok E.A., Comparison of RBF neural network training algorithms for inertial sensor based terrain classification, Sensors, 9, pp. 6312-6329, (2009); Lawrence J., Introduction to Neural Networks, (1993); Mas J.F., Flores J.J., The application of artificial neural networks to the analysis of remotely sensed data, International Journal of Remote Sensing, 3, pp. 617-663, (2008); (2010); Paola J.D., Schowengerdt R.A., A detailed comparison of backpropagation neural network and maximum-likelihood classifiers for urban land use classification, IEEE Transactions on Geoscience and Remote Sensing, 33, pp. 981-996, (1995); Pavlakis P., Tarchi D., Sieber A., On the monitoring of illicit vessel discharges, A Reconnaissance Study in the Mediterranean Sea, (2001); Rumelhart D.E., Hinton G.E., Williams R.J., Learning representations by backpropagation errors, Nature, 323, pp. 533-536, (1986); Sabins F.F., Remote Sensing Principles and Interpretation, (1997); Shi L., Ivanov A., Yu II M., Zhao C., Oil spill mapping in the western part of the East China Sea using synthetic aperture radar imagery, International Journal of Remote Sensing, 21, pp. 6315-6329, (2008); Solberg A., Brekke C., Husoy P.O., Oil spill detection in Radarsat and Envisat SAR images, IEEE Transactions on Geoscience and Remote Sensing, 45, pp. 746-755, (2007); Solbeig A., Storvik G., Solberg R., Volden E., Automatic detection of oil spills in ERS SAR images, IEEE Transactions on Geoscience and Remote Sensing, 37, pp. 1916-1924, (1999); Solberg R., Theophilopoulos N.A., Envisys - A solution for automatic oil spill detection in the Mediterranean, Proceedings of 4th Thematic Conference on Remote Sensing for Marine and Coastal Environments, pp. 3-12, (1997); Sunar F., Ozkan C., Forest fire analysis with remote sensing data, International Journal of Remote Sensing, 22, 12, pp. 2265-2277, (2001); Topouzelis K.N., Oil spill detection by SAR images: Dark formation detection feature extraction and classification algorithms, Sensors, 10, pp. 6642-6659, (2008); Topouzelis K., Stathakis D., Karathanassi V., Investigation of genetic algorithms contribution to feature selection for oil spill detection, International Journal of Remote Sensing, 3, pp. 611-625, (2009); Toruiii M., Luninie J., Pyysalo U., Patrikainen N., Luojus K., Tree species classification using ERS SAR and MODIS NDVI images, Proceedings of Commission, 7, pp. 927-932, (2004); Weigend A., On Overfitting and the efective number of hidden units, Proceedings of the Connectionist Models Summer School, pp. 335-342, (2004); Xiaoying J., Segmentation-based Image Processing System, (2009)","C. Ozkan; Geomatics Engineering, Erciyes University, Kayseri, Turkey; email: cozkan@erciyes.edu.tr","","Institute of Computer Science","","","","","","12100552","","NNWOF","","English","Neural Network World","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-84856487667"
"Qiao R.; Ghodsi A.; Wu H.; Chang Y.; Wang C.","Qiao, Rui (57202280436); Ghodsi, Ali (55522191698); Wu, Honggan (26656379400); Chang, Yuanfei (57217028395); Wang, Chengbo (57188984801)","57202280436; 55522191698; 26656379400; 57217028395; 57188984801","Simple weakly supervised deep learning pipeline for detecting individual red-attacked trees in VHR remote sensing images","2020","Remote Sensing Letters","11","7","","650","658","8","18","10.1080/2150704X.2020.1752410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085860557&doi=10.1080%2f2150704X.2020.1752410&partnerID=40&md5=b629ab675c82149d49c5a9300d267ede","Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, ON, Canada; Research Institute of Forest Resource Information Techniques, Chinese Academy of Forestry, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","Qiao R., Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, ON, Canada; Ghodsi A., Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, ON, Canada; Wu H., Research Institute of Forest Resource Information Techniques, Chinese Academy of Forestry, Beijing, China; Chang Y., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Wang C., Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","After an attack the by pine wood nematode, pine tree needles turn red. Using convolutional neural networks (CNNs) based object detection methods, machines can detect red-attacked trees. However, most deep learning object detection algorithms (such as Faster R-CNN and YOLO among others) often require a large number of labelled training datasets, where in each image every object must be given a bounding box label. To increase the cost-effectiveness of this process, we propose a simple yet efficient weakly supervised processing pipeline, based on class activation maps to locate the target. Unlike object detection methods that require bounding-box-labelled data for training, the proposed pipeline only needs image-level-labelled data. Using the proposed pipeline, we could achieve an average precision (AP) of 91.82% on test dataset. Comparing with sliding window-based method which achieves an average precision (AP) of 89.95%, our method not only gets a better AP but also runs faster than sliding window-based pipeline. This result not only indicates that the pipeline is a highly effective one but also demonstrates that image-level-labelled aerial images can be used for the detection of red-attacked tree. The proposed method should also find use in other object detection applications in the field of remote sensing. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","","Bursaphelenchus; Antennas; Convolutional neural networks; Cost effectiveness; Forestry; Large dataset; Object detection; Object recognition; Pipelines; Remote sensing; Statistical tests; Activation maps; Aerial images; Learning objects; Object detection method; Pine wood nematodes; Remote sensing images; Sliding window-based; Training data sets; algorithm; artificial neural network; data set; detection method; image analysis; numerical method; precision; remote sensing; satellite imagery; Deep learning","","","","","Institute of Forest Resource Information Techniques, (CAFYBB2017ZC001)","This work was supported by the Program of Institute of Forest Resource Information Techniques under CAFYBB2017ZC001. The authors also thank the reviewers for their constructive criticism and feedback.","Chen C., Gong W., Chen Y., Li W., Learning a Two-stage CNN Model for Multi-sized Building Detection in Remote Sensing Images, Remote Sensing Letters, 10, 2, pp. 103-110, (2019); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, International Journal of Computer Vision, 88, 2, pp. 303-338, (2010); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, IEEE Transactions on Pattern Analysis and Machine Intelligence, 42, 2, pp. 386-397, (2018); Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., Fischer I., Et al., Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3296-3297, (2017); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, International Conference on Neural Information Processing Systems (NIPS), pp. 1097-1105, (2012); Lecun Y., Bottou L., Bengio Y., Haffner P., Gradient-based Learning Applied to Document Recognition, Proceedings of the IEEE, 86, 11, pp. 2278-2324, (1998); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-resolution Remote Sensing Images, Remote Sensing, 9, 1, (2017); Lin T., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Lawrence Zitnick C., Microsoft COCO: Common Objects in Context, (2014); Meddens A.J.H., Hicke J.A., Vierling L.A., Hudak A.T., Evaluating Methods to Detect Bark Beetle-caused Tree Mortality Using Single-date and Multi-date Landsat Imagery, Remote Sensing of Environment, 132, pp. 49-58, (2013); Announcement of National Forestry Administration (No. 4), (2019); Oumar Z., Mutanga O., Using WorldView-2 Bands and Indices to Predict Bronze Bug (Thaumastocoris Peregrinus) Damage in Plantation Forests, International Journal of Remote Sensing, 34, 6, pp. 2236-2249, (2013); Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779-788, (2016); Redmon J., Farhadi A., YOLO9000: Better, Faster, Stronger, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6517-6525, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, International Conference on Neural Information Processing Systems (NIPS), pp. 91-99, (2015); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Sean M., Huang Z., Et al., ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); Stone C., Mohammed C., Application of Remote Sensing Technologies for Assessing Planted Forests Damaged by Insect Pests and Fungal Pathogens: A Review, Current Forestry Reports, 3, 2, pp. 75-92, (2017); Yang Y., Newsam S., Bag-of-visual-words and Spatial Extensions for Land-use Classification, Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems (GIS ‘10), (2010); Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A., Learning Deep Features for Discriminative Localization, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2921-2929, (2016)","C. Wang; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 20 Datun Rd, 100101, China; email: wangcb@aircas.ac.cn","","Taylor and Francis Ltd.","","","","","","2150704X","","","","English","Remote Sens. Lett.","Article","Final","","Scopus","2-s2.0-85085860557"
"Nithya P.; Arulselvi G.","Nithya, P. (55605112900); Arulselvi, G. (57210428876)","55605112900; 57210428876","Vegetation analysis and land cover and crop types classification of granite quarry area of Dharmapuri and Krishna Giri districts of Tamil Nadu","2019","International Journal of Engineering and Advanced Technology","8","5","","2670","2678","8","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070714846&partnerID=40&md5=2d955d3efe98c83a56f1b300e1d8195a","Department of Computer Science and Engineering, Annamalai University, Annamalai Nagar, 608 002, India","Nithya P., Department of Computer Science and Engineering, Annamalai University, Annamalai Nagar, 608 002, India; Arulselvi G., Department of Computer Science and Engineering, Annamalai University, Annamalai Nagar, 608 002, India","Deep Learning (DL) constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture and their allied services. The study mentioned that the aspect and altitude influenced the forest types and vegetation pattern. Deep learning (DL) is a powerful state-of-the-art technique for image processing including Remote Sensing (RS)images. This letter describes a multilevel deep learning (DL) architecture that targets land cover and crop type classification and detection from multitemporal multisource satellite imagery. The ubiquitous and wide applications like scene image understanding, video surveillance, robotics, and self-driving systems triggered vast research in the domain of computer vision in the most recent decade. Being the core of all these applications, visual recognition systems which encompasses image classification, localization and detection have achieved great research now. Due to significant development in neural networks especially deep learning, these visual recognition systems have reached remarkable performance. Object detection is one of these domains witnessing great success in computer vision. This research paper demystifies the role of deep learning techniques based on Convolutional Neural Network(CNN) for object detection. Deep learning frameworks and services available for object detection are also enunciated. Deep learning techniques for state-of-the-art object detection systems are assessed in this research paper. Experiments are carried out for the joint experiment of crop assessment and monitoring test site in Ukraine for classification of crops in a heterogeneous environment using nineteen multitemporal scenes acquired by LANDSAT-8 and SENTINEL-1A RS satellites. The architecture with an ensemble of CNNs outperforms the one with MLPs allowing us to better discriminate certain summer crop types, in particular Teak and Sugarcane, and yielding the target accuracies more than 85% for all major crops in Tamilnadu(Banana Tree, Teak, Paddy, and Sugarcane). © 2019, Blue Eyes Intelligence Engineering and Sciences Publication. All rights reserved.","ArcGIS; Deep Learning(DL); Geographic Information System (GIS); Images Classification; Land Use Crop Type; Remote Sensing (RS); Vegetation","","","","","","","","Canziani A., Et al., An Analysis of Deep Neural Network Models for Practical Applications, (2016); Chen S.W., Et al., Counting Apples and Oranges With Deep Learning: A Data-Driven Approach, IEEE Robotics and Automation Letters, 2, 2, pp. 781-788, (2017); Chen Y., Et al., Deep Learning-Based Classification of Hyperspectral Data, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 7, 6, pp. 2094-2107, (2014); Chi M., Et al., Big data for remote sensing: Challenges and opportunities, Proceedings of the IEEE, 104, 11, pp. 2207-2219, (2016); Christiansen, Et al., Deep Anomaly: Combining Background Subtraction and Deep Learning for Detecting Obstacles and Anomalies in an Agricultural Field, Sensors, 16, 11, (2016); Deep Learning Based Rootsoil Segmentation from X-Ray Tomography, (2016); Dyrmann, Et al., Plant species classification using deep convolutional neural network, Biosystems Engineering, 151, pp. 72-80, (2016); Pixel-wise classification of weeds and crops in images by using a fully convolutional neural network, International Conference on Agricultural Engineering, (2016); Grinblat, Et al., Deep learning for plant identification using vein morphological patterns, Computers and Electronics in Agriculture, 127, pp. 418-424, (2016); Evaluation of Features for Leaf Classification in Challenging Conditions. Winter Conference on Applications of Computer Vision (WACV), pp. 797-804, (2015); Kamilaris, Et al., A review on the practice of big data analysis in agriculture, Computers and Electronics in Agriculture, 143, 1, pp. 23-37, (2017); Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geoscience and Remote Sensing Letters, 14, 5, pp. 778-782, (2017); Kuwata, Et al., Estimating crop yields with deep learning and remotely sensed data, Italy: IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 858-861, (2015); Milioto, Et al., Real-time blob-wise sugar beets vs weeds classification for monitoring fields using convolutional neural networks, Proceedings of the International Conference on Unmanned Aerial Vehicles in Geomatics, (2017); Deep Recurrent Neural Networks for Mapping Winter Vegetation Quality Coverage via Multitemporal SAR Sentinel-1, (2017); Mohanty, Et al., Using deep learning for image-based plant disease detection, Frontiers in Plant Science, (2016); Semantic segmentation of mixed crops using deep convolutional neural network, International Conference on Agricultural Engineering, (2016); Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, 37, 9, pp. 1904-1916, (2015); Yang, Et al., Exploit All the Layers: Fast and Accurate Cnn Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2129-2137, (2016); Deep Generative Image Models Using A Laplacian Pyramid of Adversarial Networks, Advances in Neural Information Processing Systems, pp. 1486-1494, (2015); Training Region-Based Object Detectors with Online Hard Example Mining, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 761-769, (2016); Mini-Batch Primal and Dual Methods for SVMs, ICML, 3, pp. 1022-1030, (2013); A-Fast-Rcnn: Hard Positive Generation via Adversary for Object Detection, (2017); Deformable Part Models Are Convolutional Neural Networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 437-446, (2015); Li W., Et al., End-to-End Integration of a Convolution Network, Deformable Parts Model and Non-Maximum Suppression, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 851-859, (2015); Girshick, Et al., Deep Learning Strong Parts for Pedestrian Detection, Proceedings of the IEEE International Conference on Computer Vision, pp. 1904-1912, (2015); Symbiotic Segmentation and Part Localization for Fine-Grained Categorization, In Computer Vision (ICCV), 2013 IEEE International Conference On, pp. 321-328, (2013); Goring, Et al., Nonparametric Part Transfer for Fine-Grained Recognition, CVPR, (2014); Di L., Et al., Deep Lac: Deep Localization, Alignment and Classification for Fine-Grained Recognition, In Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1666-1674, (2015); Zhang N., Et al., Part-Based R-CNNs for Fine-Grained Category Detection, European Conference on Computer Vision, pp. 834-849, (2014); Redmon J., YOLO9000: Better, Faster, Stronger, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6517-6525, (2017); Ya-Fang S., Et al., Deep Co-Occurrence Feature Learning for Visual Object Recognition, Proc. Conf. Computer Vision and Pattern Recognition, (2017)","","","Blue Eyes Intelligence Engineering and Sciences Publication","","","","","","22498958","","","","English","Int. J. Eng. Adv. Technol.","Article","Final","","Scopus","2-s2.0-85070714846"
"Hou B.; Yan D.; Hao W.; Huang Q.; Su X.; Li Q.","Hou, Bowen (57221522247); Yan, Dongmei (58187494700); Hao, Wei (57204185351); Huang, Qingqing (55467449900); Su, Xiuqin (18438569800); Li, Qingwen (58463061900)","57221522247; 58187494700; 57204185351; 55467449900; 18438569800; 58463061900","Urban built-up area extraction using high-resolution remote sensing images with an improved convolutional neural network; [改进卷积网络的高分遥感图像城镇建成区提取]","2020","Journal of Image and Graphics","25","12","","2677","2689","12","3","10.11834/jig.190539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099397158&doi=10.11834%2fjig.190539&partnerID=40&md5=78b8508f90185454235cdc0b80a88a8e","Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China; Institute of Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Key Laboratory of Earth Observation, Institute of Remote Sensing of Sanya, Sanya, 572029, China; University of Chinese Academy of Sciences, Beijing, 100049, China","Hou B., Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China, University of Chinese Academy of Sciences, Beijing, 100049, China; Yan D., Institute of Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, Key Laboratory of Earth Observation, Institute of Remote Sensing of Sanya, Sanya, 572029, China; Hao W., Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China; Huang Q., Institute of Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Su X., Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China; Li Q., Institute of Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, University of Chinese Academy of Sciences, Beijing, 100049, China","Objective: The urban built-up area is an important source of basic information for urban research and serves as a prerequisite for the regional planning and implementation of the spatial layout of urban functions. Given the recent developments in Earth observational technologies and improvements in the resolution of remote sensing images, accurately and efficiently extracting information on urban built-up areas has become possible. However, due to the complex environment of urban built-up areas in high-resolution remote sensing images and the variations in their locations and development scales, various forms of remote sensing image representations increase the difficulty of using traditional information extraction methods for urban built-up areas. Recent studies show that deep learning algorithms have significant advantages in processing of large-scale images. This paper then examines these deep learning algorithms and reviews previous research that apply deep convolutional neural network methods, which have been widely used in computer vision to extract information on urban built-up areas from high-resolution satellite images. This article also improves the application of computer image processing technology in the field of remote sensing. Method: Semantic image segmentation is crucial in image processing and computer vision. This process recognizes an image at the pixel level and then labels the object category to which each pixel in the image belongs. Based on the deep convolutional neural network oriented to semantic image segmentation, this paper uses the refinement module for the feature map and the attention module of the channel domain to improve the original DeepLab v3 network. The feature refinement module accurately obtains relevant information between pixels and reduces the grid effect. Afterward, the network model processes the feature map through atrous spatial pyramid pooling. The decoding part of the network extracts the attention information of the channel domain and then weighs the low-level features to achieve a better representation and to restore the detailed information. Afterward, the urban built-up area is extracted via the sliding window prediction and full connection conditional random fields methods, both of which can be applied to extract urban built-up areas with better accuracy. However, the use of deep learning algorithms is prone to overfitting and poor robustness. Accordingly, data augmentation and extension are used to enhance the capabilities of the model. Specifically, we use rotation and filter operations while cutting the original training and verification data into 256 × 256 samples. Result Extracting information from remote sensing images involves an effective mining and category judgment of such information. The experimental data are taken from Gaofen-2 remote sensing images of Sanya and Haikou cities in Hainan Province, China. These images are specifically taken at the Qiongshan District of Haikou City and at the Tianya District, Jiyang District, and the sea surrounding the Jiaotouding Island of Sanya City. Given their weak sample processing ability, traditional classification algorithms have achieved an accuracy rate of no higher than 85% in the experiments. Meanwhile, deep learning methods, such as SegNet and DeepLab v3, have relatively high accuracy and better performance in extracting urban built-up area information from remote sensing satellite images. By using the refinement module for the feature map and the attention module of the channel domain, this paper improves the accuracy rate of the original DeepLab v3 network by 1.95%. Meanwhile, the proposed method has an accuracy rate of above 93%, a Kappa coefficient of greater than 0.837, a missed detection rate of less than 4.9%, and a false alarm rate of below 2.1%. This method can effectively extract urban built-up areas from large-scale high spatial resolution remote sensing images, and its extraction results are the closest to the actual situation. Conclusion: The comparative experiment shows that the proposed method outperforms others in extracting urban built-up area information from high-resolution remote sensing satellite imagers with diverse spectral information and complex texture structure. Two processing methods are also proposed to significantly improve the accuracy of the model. Both the sliding window method and conditional random fields processing demonstrate an excellent performance in extracting information from high-resolution remote sensing images and show high application value for large-scale remote sensing images. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","Attention mechanism; Convolutional neural network (CNN); Information extraction; Remote sensing image; Urban built-up area","","","","","","","","Chen L C, Papandreou G, Kokkinos I, Murphy K, Yuille A L., Semantic image segmentation with deep convolutional nets and fully connected CRFs, (2014); Chen L C, Papandreou G, Schroff F, Adam H., Rethinking Atrous convolution for semantic image segmentation, (2017); Chen L S, Zhao J S, Dong Z W, Zhu Q F., Urban construction land information extraction based on deep learning by multi-spectral remote sensing imagery of Yunnan central urban agglomeration area, Software Guide, 17, 11, pp. 177-180, (2018); Chen Z J, Chen J, Shi P J, Tamura M., An IHS-based change detection approach for assessment of urban expansion impact on arable land loss in China, International Journal of Remote Sensing, 24, 6, pp. 1353-1360, (2003); Deng L Y, Shen Z F, Ke Y M., Built-up area extraction and urban expansion analysis based on remote sensing images, Journal of Geo-information Science, 20, 7, pp. 996-1003, (2018); Gao C X, Sang N., Deep learning for object detection in remote sensing image, Bulletin of Surveying and Mapping, pp. 108-111, (2014); Guo L, Ma L, Ye S C., Remote sensing image extraction of residential areas based on radon and tree-structured wavelet transformation, Geomatics and Spatial Information Technology, 35, 1, pp. 150-153, (2012); He K M, Zhang X Y, Ren S Q, Jian S., Deep residual learning for image recognition, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Hu F, Xia G S, Hu J W, Zhang L P., Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery, Remote Sensing, 7, 11, pp. 14680-14707, (2015); Hu J, Shen L, Sun G., Squeeze-and-excitation networks, Proceedings of 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7132-7141, (2018); Hu X X, Li Y S, Li H C, Xu Y N., The comparative analysis about spatial information access technologies based on light UAV low-altitude digital aerophotogrammetry and high-resolution satellite remote sensing, Engineering of Surveying and Mapping, 19, 4, pp. 68-70, (2010); Klonowski J, Koch K R., Two level image interpretation based on Markov random fields, Semantic Modeling for the Acquisition of Topographic Information from Images and Maps, pp. 37-55, (1997); Krizhevsky A, Sutskever I, Hinton G E., ImageNet classification with deep convolutional neural networks, Proceedings of the 25th International Conference on Neural Information Processing Systems, pp. 1097-1105, (2012); LeCun Y, Bottou L, Bengio Y, Haffner P., Gradient-based learning applied to document recognition, Proceedings of the IEEE, 86, 11, pp. 2278-2324, (1998); Li D R., Towards the development of remote sensing and GIS in the 21st century, Geomatics and Information Science of Wuhan University, 28, 2, pp. 127-131, (2003); Li S H, Wang J L, Bi Y, Chen Y, Zhu M Y, Yang S, Zhu J., A review of methods for classification of remote sensing images, Remote Sensing for Land and Resources, 17, 2, pp. 1-6, (2005); Lin C G, Nevatia R., Building detection and description from a single intensity image, Computer Vision and Image Understanding, 72, 2, pp. 101-121, (1998); Long J, Shelhamer E, Darrell T., Fully convolutional networks for semantic segmentation, Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, (2015); Mnih V, Hinton G E., Learning to detect roads in high-resolution aerial images, Proceedings of the 11th European Conference on Computer Vision, pp. 210-223, (2010); Pesaresi M, Gerhardinger A, Kayitakire F C., A robust built-up area presence index by anisotropic rotation-invariant textural measure, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 1, 3, pp. 180-192, (2008); Pu R L, Landry S, Yu Q., Object-based urban detailed land cover classification with high spatial resolution IKONOS imagery, International Journal of Remote Sensing, 32, 12, pp. 3285-3308, (2011); Rodriguez-Galiano V F, Ghimire B, Rogan J, Chica-Olmo M, Rigol-Sanchez J P., An assessment of the effectiveness of a random forest classifier for land-cover classification, ISPRS Journal of Photogrammetry and Remote Sensing, 67, pp. 93-104, (2012); Rougier S, Puissant A, Stumpf A, Lachiche N., Comparison of sampling strategies for object-based classification of urban vegetation from very high resolution satellite images, International Journal of Applied Earth Observation and Geoinformation, 51, pp. 60-73, (2016); Tao C, Zou Z R, Ding X L., Residential area detection from high-resolution remote sensing imagery using corner distribution, Acta Geodaetica et Cartographica Sinica, 43, 2, pp. 164-169, (2014); Unsalan C, Boyer K L., Classifying land development in high-resolution panchromatic satellite images using straight-line statistics, IEEE Transactions on Geoscience and Remote Sensing, 42, 4, pp. 907-919, (2004); Wang J Q, Li J S, Zhou H C, Zhang X., Typical element extraction method of remote sensing image based on Deeplabv3+ and CRF, Computer Engineering, 45, 10, pp. 260-265, (2019); Welch R., Monitoring urban population and energy utilization patterns from satellite data, Remote Sensing of Environment, 9, 1, pp. 1-9, (1980); Xia M, Cao G, Wang G Y, Shang Y F., Remote sensing image classification based on deep learning and conditional random fields, Journal of Image and Graphics, 22, 9, pp. 1289-1301, (2017); Xu H Q., Fast information extraction of urban built-up land based on the analysis of spectral signature and normalized difference index, Geographical Research, 24, 2, pp. 311-320, (2005); Yang C J, Zhuo C H., Extracting residential areas on the TM imagery, Journal of Remote Sensing, 4, 2, pp. 146-150, (2000); Yang H W., High Resolution Remote Sensing Imagery Classification Based on Deep Learning, (2018); Zhang X Y, Du S H., Learning selfhood scales for urban land cover mapping with very-high-resolution satellite images, Remote Sensing of Environment, 178, pp. 172-190, (2016); Zhao P, Feng X Z, Lin G F., The decision tree algorithm of automatically extracting residential information from SPOT images, Journal of Remote Sensing, 7, 4, pp. 309-315, (2003)","D. Yan; Institute of Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; email: yandm@radi.ac.cn; W. Hao; Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China; email: haowei@opt.ac.cn","","Editorial and Publishing Board of JIG","","","","","","10068961","","","","Chinese","J. Image and Graphics","Article","Final","","Scopus","2-s2.0-85099397158"
