"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Molecular Sequence Numbers","Chemicals/CAS","Tradenames","Manufacturers","Funding Details","Funding Texts","References","Correspondence Address","Editors","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Abbreviated Source Title","Document Type","Publication Stage","Open Access","Source","EID"
"Tan Y.; Cai R.; Li J.; Chen P.; Wang M.","Tan, Yi (57193550394); Cai, Ruying (57227610300); Li, Jingru (26660256200); Chen, Penglu (57226812966); Wang, Mingzhu (57202642024)","57193550394; 57227610300; 26660256200; 57226812966; 57202642024","Automatic detection of sewer defects based on improved you only look once algorithm","2021","Automation in Construction","131","","103912","","","","30","10.1016/j.autcon.2021.103912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113313097&doi=10.1016%2fj.autcon.2021.103912&partnerID=40&md5=8eb01c7ac30a82547e7a91a1efab9d92","Key Laboratory for Resilient Infrastructures of Coastal Cities (Shenzhen University), Ministry of Education, China; Sino-Australia Joint Research Center in BIM and Smart Construction, Shenzhen University, Shenzhen, China; College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","Tan Y., Key Laboratory for Resilient Infrastructures of Coastal Cities (Shenzhen University), Ministry of Education, China, Sino-Australia Joint Research Center in BIM and Smart Construction, Shenzhen University, Shenzhen, China, College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Cai R., College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Li J., Key Laboratory for Resilient Infrastructures of Coastal Cities (Shenzhen University), Ministry of Education, China, Sino-Australia Joint Research Center in BIM and Smart Construction, Shenzhen University, Shenzhen, China, College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Chen P., College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; Wang M., Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","The drainage system is an important part of civil infrastructure. However, the underground sewage pipe will gradually suffer from defects over time, such as tree roots, deposits, infiltrations and cracks, which heavily affect the performance of sewage pipes. Therefore, it is significant to timely inspect the condition of sewage pipes. Closed-circuit television (CCTV) inspection is a commonly employed underground infrastructure inspection technology requiring engineering experience that can be subjective and inefficient. Nowadays, object detection based on convolutional neural network (CNN) can automatically detect defects, showing high potential for improving inspection efficiency. This paper proposed an improved CNN-based You Only Look Once version 3 (YOLOv3) method for automatic detection of sewage pipe defects, where the improvements are mainly involved in loss function, data augmentation, bounding box prediction and network structure. Experiment results demonstrate that the improved model outperforms Faster R-CNN and YOLOv3, achieving a mean average precision (mAP) value of 92%, which is higher than the existing research on automatic detection of sewage pipe defects. © 2021","Automatic defect detection; CCTV; CNN; Computer vision; Deep learning; Object detection; Sewer defects; YOLO","Closed circuit television systems; Computer vision; Convolutional neural networks; Deep learning; Defects; Inspection; Object recognition; Sewage; Sewers; Automatic defect detections; Automatic Detection; Closed circuit television; Convolutional neural network; Deep learning; Objects detection; Pipe defects; Sewage pipes; Sewer defect; YOLO; Object detection","","","","","FDYT, (2020KQNCX060); Foundation for Distinguished Young Talents in Higher Education of Guangdong","This research is supported by Foundation for Distinguished Young Talents in Higher Education of Guangdong , China (FDYT), No. 2020KQNCX060 .","Harvey R.R., McBean E.A., Comparing the utility of decision trees and support vector machines when planning inspections of linear sewer infrastructure, J. Hydroinf., 16, 6, pp. 1265-1279, (2014); Elsawah H., Bakry I., Moselhi O., Decision support model for integrated risk assessment and prioritization of intervention plans of municipal infrastructure, J. Pipeline Syst. Eng. Pract., 7, 4, (2016); Cheng J.C.P., Wang M., Automated detection of sewer pipe defects in closed-circuit television images using deep learning techniques, Autom. Constr., 95, pp. 155-171, (2018); Kuliczkowska E., An analysis of road pavement collapses and traffic safety hazards resulting from leaky sewers, Baltic J. Road Bridge Eng., 11, 4, pp. 251-258, (2016); Tanaka Y., Ishihara S., I. Assoc Comp Machinery, Cooperative Video Data Transmission for Sewer Inspection Using Multiple Drifting Cameras. Adjunct Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing Networking and Services, pp. 195-200, (2016); Liu J., Yi Y., Wang X., Exploring factors influencing construction waste reduction: a structural equation modeling approach, J. Clean. Prod., 276, 4, (2020); Caradot N., Sonnenberg H., Kropp I., Ringe A., Denhez S., Hartmann A., Rouault P., The relevance of sewer deterioration modelling to support asset management strategies, Urban Water J., 14, 10, pp. 1007-1015, (2017); Ke Z., Qwa B., Lca B., Jy D., Zl B., Zyab C., Tao Y., Qin J.B., Ground observation-based analysis of soil moisture spatiotemporal variability across a humid to semi-humid transitional zone in China, J. Hydrol., 574, pp. 903-914, (2019); Halfawy M.R., Hengmeechai J., Efficient algorithm for crack detection in sewer images from closed-circuit television inspections, J. Infrastruct. Syst., 20, 2, (2014); Chao L., Zhang K., Li Z., Zhu Y., Wang J., Yu Z., Geographically weighted regression based methods for merging satellite and gauge precipitation, J. Hydrol., 558, pp. 275-289, (2018); Dong H., Zhao B., Deng Y., Instability phenomenon associated with two typical high speed railway vehicles, Int. J. Non-Linear Mechanics, 105, Oct, pp. 130-145, (2018); Kovalnogov V., Simos T., Tsitouras C., Runge–Kutta pairs suited for SIR-type epidemic models, Math. Meth. Appl. Sci., 44, (2020); Kovalnogov V.N., Simos T.E., Tsitouras C., Ninth-order, explicit, two-step methods for second-order inhomogeneous linear IVPs, Math. Meth. Appl. Sci., 168, (2020); Medvedeva M., Simos T., Tsitouras C., Katsikis V., Direct estimation of SIR model parameters through second-order finite differences, Math. Meth. Appl. Sci., 44, (2020); Zhong B.T., Wu H.T., Ding L.Y., Love P.E.D., Li H., Luo H.B., Jiao L., Mapping computer vision research in construction: developments, knowledge gaps and implications for research, Autom. Constr., 107, (2019); Xiong Z., Tang Z., Chen X., Zhang X.M., Ye C., Research on image retrieval algorithm based on combination of color and shape features, J. Signal Process. Syst., (2021); Walsh J., Mahony N.O., Campbell S., Carvalho A., Riordan D., Deep learning vs. traditional computer vision, Computer Vision Conference (CVC), 2019, (2019); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Comput. Therm. Sci., (2014); Deng L., Chu H.H., Shi P., Wang W., Kong X., Region-based CNN method with deformable modules for visually classifying concrete cracks, Appl. Sci., 10, 7, (2020); Wang M.Z., Cheng J.C.P., A unified convolutional neural network integrated with conditional random field for pipe defect segmentation, Computer-Aided Civil Infrastruct. Eng., 35, 2, pp. 162-177, (2020); Ren S.Q., He K.M., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: single shot multibox detector, European Conference on Computer Vision, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: unified, real-time object detection, 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Li Y., Qi H.Z., Dai J., Ji X.Y., Wei Y.C., Fully convolutional instance-aware semantic segmentation, 30th IEEE Conference on Computer Vision and Pattern Recognition, pp. 4438-4446, (2017); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, (2017); Kumar S.S., Wang M.Z., Abraham D.M., Jahanshahi M.R., Iseley T., Cheng J.C.P., Deep learning-based automated detection of sewer defects in CCTV videos, J. Comput. Civ. Eng., 34, 1, (2020); Medvedeva M., Katsikis V., Mourtas S., Simos T., Randomized time-varying knapsack problems via binary beetle antennae search algorithm: emphasis on applications in portfolio insurance, Math. Meth. Appl. Sci., (2020); Makar J.M., Diagnostic techniques for sewer systems, J. Infrastruct. Syst., (1999); Shehab T., Moselhi O., Automated detection and classification of infiltration in sewer pipes, J. Infrastruct. Syst., 11, 3, pp. 165-171, (2005); Costello S.B., Chapman D.N., Rogers C.D.F., Metje N., Underground asset location and condition assessment technologies, Tunnel. Undergr. Space Technol., 22, 5-6, pp. 524-542, (2007); Li Q., Liu X., Novel approach to pavement image segmentation based on neighboring difference histogram method, Cisp 2008: First International Congress on Image and Signal Processing, Vol 2, Proceedings, pp. 792-796, (2008); Ayenu-Prah A., Attoh-Okine N., Evaluating pavement cracks with bidimensional empirical mode decomposition, Eurasip J. Adv. Signal Process., (2008); Yan M., Bo S., Xu K., He Y., Pavement crack detection and analysis for high-grade highway, Electronic Measurement and Instruments, 2007. ICEMI '07. 8th International Conference on, (2007); Zhou J., Huang P.S., Chiang F.P., Wavelet-based pavement distress detection and evaluation, Opt. Eng., (2006); Cevallos-Torres L.J., Gilces D.M., Guijarro-Rodriguez A., Barriga-Diaz R., Leyva-Vazquez M., Botto-Tobar M., BottoTobar M., Pizarro G., ZunigaPrieto M., Darmas M., Sanchez M.Z., An approach to the detection of post-seismic structural damage based on image segmentation methods, Technology Trends, pp. 644-658, (2019); Huynh P., Ross R., Martchenko A., Devlin J., Dou-edge evaluation algorithm for automatic thin crack detection in pipelines, 2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), (2015); Su T.C., Segmentation of crack and open joint in sewer pipelines based on CCTV inspection images, Proceedings of the 2015 Aasri International Conference on Circuits and Systems, pp. 263-266, (2015); Halfawy M.R., Hengmeechai J., Automated defect detection in sewer closed circuit television images using histograms of oriented gradients and support vector machine, Autom. Constr., 38, pp. 1-13, (2014); Ahrary A., Ishikawa M., Detecting pipe feature points for sewer pipe system based on image information - art. no. 604130, ICMIT 2005: Information Systems and Signal Processing, (2005); Saranya R., Daniel J., Abudhahir A., Chermakani N., Comparison of segmentation techniques for detection of defects in non-destructive testing images, 2014 International Conference on Electronics and Communication Systems, (2014); Su T.C., Yang M.D., Application of morphological segmentation to leaking defect detection in sewer pipelines, Sensors, 14, 5, pp. 8686-8704, (2014); Hou C., Simos T.E., Famelis I.T., Neural network solution of pantograph type differential equations, Math. Meth. Appl. Sci., (2020); Voulodimos A., Doulamis N., Doulamis A., Protopapadakis E., Deep learning for computer vision: a brief review, Comput. Intellig. Neurosci., (2018); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Commun. ACM, 60, 6, pp. 84-90, (2017); Szegedy C., Liu W., Jia Y., Sermanet P., Rabinovich A., Going deeper with convolutions, Comput. Soc., (2014); Kaiming H., Xiangyu Z., Shaoqing R., Jian S., Deep residual learning for image recognition, (2015); Kumar S.S., Abraham D.M., Jahanshahi M.R., Iseley T., Starr J., Automated defect classification in sewer closed circuit television inspections using deep convolutional neural networks, Autom. Constr., 91, pp. 273-283, (2018); Meijer D., Scholten L., Clemens F., Knobbe A., A defect classification methodology for sewer image sets with convolutional neural networks, Autom. Constr., 104, pp. 281-298, (2019); Hassan S.I., Dang L.M., Mehmood I., Im S., Choi C., Kang J., Park Y.S., Moon H., Underground sewer pipe condition assessment based on convolutional neural networks, Autom. Constr., 106, (2019); Zuo X., Dai B., Shan Y.W., Shen J.F., Hu C.L., Huang S.C., Classifying cracks at sub-class level in closed circuit television sewer inspection videos, Autom. Constr., 118, (2020); Modarres C., Astorga N., Droguett E.L., Meruane V., Convolutional neural networks for automated damage recognition and damage type identification, Struct. Control. Health Monit., 25, 10, (2018); Zhou S.L., Song W., Deep learning-based roadway crack classification using laser-scanned range images: a comparative study on hyperparameter selection, Autom. Constr., 114, (2020); Li B., Wang K.C.P., Zhang A., Yang E., Wang G., Automatic classification of pavement crack using deep convolutional neural network, Int. J. Pavement Eng., 21, 4, pp. 457-463, (2020); Cha Y.J., Choi W., Buyukozturk O., Deep learning-based crack damage detection using convolutional neural networks, Computer-Aided Civil Infrastruct. Eng., 32, 5, pp. 361-378, (2017); Kim B., Cho S., Image-based concrete crack assessment using mask and region-based convolutional neural network, Struct. Control. Health Monit., 26, 8, (2019); Wang M.Z., Cheng J.C.P., Development and improvement of deep learning based automated defect detection for sewer pipe inspection using faster R-CNN, Advanced Computing Strategies for Engineering, Pt Ii, pp. 171-192, (2018); Mondal T.G., Jahanshahi M.R., Wu R.T., Wu Z.Y., Deep learning-based multi-class damage detection for autonomous post-disaster reconnaissance, Struct. Control. Health Monit., 27, 4, (2020); Kim I.H., Jeon H., Baek S.C., Hong W.H., Jung H.J., Application of crack identification techniques for an aging concrete bridge inspection using an unmanned aerial vehicle, Sensors, 18, 6, (2018); Zhang C., Chang C.C., Jamshidi M., Concrete bridge surface damage detection using a single-stage detector, Computer-Aided Civil Infrastruct. Eng., 35, 4, pp. 389-409, (2019); Du Y., Pan N., Xu Z., Deng F., Shen Y., Kang H., Pavement distress detection and classification based on YOLO network, Int. J. Pavement Eng., pp. 1-14, (2020); Yin X.F., Chen Y., Bouferguene A., Zaman H., Al-Hussein M., Kurach L., A deep learning-based framework for an automated defect detection system for sewer pipes, Autom. Constr., 109, (2020); Fang W., Ding L., Love P.E.D., Luo H., Li H., Pena-Mora F., Zhong B., Zhou C., Computer vision applications in construction safety assurance, Autom. Constr., 110, (2020); Mi C., Huang Y., Fu C., Zhang Z., Postolache O., Vision-based measurement: actualities and developing trends in automated container terminals, IEEE Instrument. Meas. Magaz., 24, 4, pp. 65-76, (2021); Wei F.J., Yao G., Yang Y., Sun Y.J., Instance-level recognition and quantification for concrete surface bughole based on deep learning, Autom. Constr., 107, (2019); Augustauskas R., Lipnickas A., Improved pixel-level pavement-defect segmentation using a deep autoencoder, Sensors, 20, 9, (2020); Redmon J., Farhadi A., YOLO9000: better, faster, stronger, 30th IEEE Conference on Computer Vision and Pattern Recognition, pp. 6517-6525, (2017); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, pp. 1-6, (2018); Bochkovskiy A., Chien-Yao W., Liao H.Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020); Jocher G., Kwon Y., Guigarfr J.V.-M., Perry T., Marc G.B., Baltaci F., Suess D., Tailang C., Peiwen Y., idow09, Wanna Sea U., Xinyu W., Shead T.M., Havlik T., Skalski P., NirZarrabi L.A.I., Coce L., Hu J., Ovodov I., Wiki G., Reveriano F., Falak, Kendall D., ultralytics/yolov3: 43.1mAP@0.5:0.95 on COCO2014, (2020); Yap M.H., Hachiuma R., Alavi A., Brungel R., Cassidy B., Goyal M., Zhu H., Ruckert J., Olshansky M., Huang X., Saito H., Hassanpour S., Friedrich C.M., Ascher D.B., Song A., Kajita H., Gillespie D., Reeves N.D., Pappachan J.M., O'Shea C., Frank E., Deep learning in diabetic foot ulcers detection: a comprehensive evaluation, Comput. Biol. Med., 135, (2021); Ioffe S., Szegedy C., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, (2015); Dubey A.K., Jain V., Comparative study of convolution neural network's Relu and leaky-Relu activation functions, Applications of Computing, Automation and Wireless Systems in Electrical Engineering. Proceedings of MARC 2018. Lecture Notes in Electrical Engineering, pp. 873-880, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, 30th IEEE Conference on Computer Vision and Pattern Recognition, pp. 936-944, (2017); Jocher G., Stoken A., Borovec J., NanoCode012 A.; Rezatofighi H., Tsoi N., Gwak J., Sadeghian A., Reid I., Savarese S., Generalized intersection over union: a metric and a loss for bounding box regression, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 658-666, (2019); Zhaohui Z., Ping W., Wei L., Jinze L., Rongguang Y., Dongwei R., Distance-IoU loss: faster and better learning for bounding box regression, (2019); Yun S., Han D., Oh S.J., Chun S., Choe J., Yoo Y., Choe J., CutMix: regularization strategy to train strong classifiers with localizable features, 2019 IEEE/CVF International Conference on Computer Vision, pp. 6022-6031, (2019); Wang C., Liao H.M., Wu Y., Chen P., Hsieh J., Yeh I., CSPNet: a new backbone that can enhance learning capability of CNN, IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), (2020); He K.M., Zhang X.Y., Ren S.Q., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 37, 9, pp. 1904-1916, (2015); Liu S., Qi L., Qin H., Shi J., Jia J., Path aggregation network for instance segmentation, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8759-8768, (2018); Tzutalin, LabelImg; Kingma D., Ba J., Adam: a method for stochastic optimization, Comput. Therm. Sci., (2014); Loshchilov I., Hutter F., SGDR: Stochastic Gradient Descent with Warm Restarts, (2016); Neubeck A., Gool L., Efficient non-maximum suppression, International Conference on Pattern Recognition, (2006); Li D.S., Cong A.R., Guo S., Sewer damage detection from imbalanced CCTV inspection data using deep convolutional neural networks with hierarchical classification, Autom. Constr., 101, pp. 199-208, (2019)","R. Cai; College of Civil and Transportation Engineering, Shenzhen University, Shenzhen, China; email: ruycaicai@163.com","","Elsevier B.V.","","","","","","09265805","","AUCOE","","English","Autom Constr","Article","Final","","Scopus","2-s2.0-85113313097"
"Maxwell A.E.; Warner T.A.; Guillén L.A.","Maxwell, Aaron E. (55183805600); Warner, Timothy A. (57204253092); Guillén, Luis Andrés (57221113552)","55183805600; 57204253092; 57221113552","Accuracy assessment in convolutional neural network-based deep learning remote sensing studies—part 2: Recommendations and best practices","2021","Remote Sensing","13","13","2591","","","","29","10.3390/rs13132591","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110226604&doi=10.3390%2frs13132591&partnerID=40&md5=feb385a3bd37480e2f9555e3680e83fa","Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States","Maxwell A.E., Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States; Warner T.A., Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States; Guillén L.A., Department of Geology and Geography, West Virginia University, Morgantown, 26505, WV, United States","Convolutional neural network (CNN)-based deep learning (DL) has a wide variety of applications in the geospatial and remote sensing (RS) sciences, and consequently has been a focus of many recent studies. However, a review of accuracy assessment methods used in recently published RS DL studies, focusing on scene classification, object detection, semantic segmentation, and instance segmentation, indicates that RS DL papers appear to follow an accuracy assessment approach that diverges from that of traditional RS studies. Papers reporting on RS DL studies have largely abandoned traditional RS accuracy assessment terminology; they rarely reported a complete confusion matrix; and sampling designs and analysis protocols generally did not provide a population-based confusion matrix, in which the table entries are estimates of the probabilities of occurrence of the mapped landscape. These issues indicate the need for the RS community to develop guidance on best practices for accuracy assessment for CNN-based DL thematic mapping and object detection. As a first step in that process, we explore key issues, including the observation that accuracy assessments should not be biased by the CNN-based training and inference processes that rely on image chips. Furthermore, accuracy assessments should be consistent with prior recommendations and standards in the field, should support the estimation of a population confusion matrix, and should allow for assessment of model generalization. This paper draws from our review of the RS DL literature and the rich record of traditional remote sensing accuracy assessment research while considering the unique nature of CNN-based deep learning to propose accuracy assessment best practices that use appropriate sampling methods, training and validation data partitioning, assessment metrics, and reporting standards. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Accuracy assessment; Deep learning; Feature extraction; Instance segmentation; Object detection; Semantic segmentation; Thematic mapping","Convolution; Convolutional neural networks; Image resolution; Object detection; Object recognition; Remote sensing; Semantics; Accuracy assessment; Assessment metrics; Confusion matrices; Inference process; Model generalization; Reporting standards; Scene classification; Semantic segmentation; Deep learning","","","","","National Science Foundation, NSF, (2046059)","This work was funded by the National Science Foundation (NSF) (Federal Award ID Number 2046059: “CAREER: Mapping Anthropocene Geomorphology with Deep Learning, Big Data Spatial Analytics, and LiDAR”).","Maxwell A., Warner T., Guillen L., Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies—Part 1: Literature Review, Remote Sens, 13, (2021); Foody G.M., Status of land cover classification accuracy assessment, Remote Sens. Environ, 80, pp. 185-201, (2002); Stehman S.V., Foody G.M., Key issues in rigorous accuracy assessment of land cover products, Remote Sens. Environ, 231, (2019); Zhu X.X., Tuia D., Mou L., Xia G.-S., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote Sens. Mag, 5, pp. 8-36, (2017); Zhang L., Zhang L., Du B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art, IEEE Geosci. Remote Sens. Mag, 4, pp. 22-40, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Hoeser T., Bachofer F., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review—Part II: Applications, Remote Sens, 12, (2020); Hoeser T., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part I: Evolution and Recent Trends, Remote Sens, 12, (2020); Basu S., Ganguly S., Mukhopadhyay S., DiBiano R., Karki M., Nemani R., DeepSat: A Learning Framework for Satellite Imagery, Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 1-10, (2015); Ren Z., Sudderth E.B., Three-Dimensional Object Detection and Layout Prediction Using Clouds of Oriented Gradients, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1525-1533; Wei X., Fu K., Gao X., Yan M., Sun X., Chen K., Sun H., Semantic pixel labelling in remote sensing images using a deep convolutional encoder-decoder model, Remote Sens. Lett, 9, pp. 199-208, (2017); Dai J., He K., Sun J., Instance-Aware Semantic Segmentation via Multi-task Network Cascades, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3150-3158; Li Y., Qi H., Dai J., Ji X., Wei Y., Fully Convolutional Instance-Aware Semantic Segmentation, Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4438-4446; Boguszewski A., Batorski D., Ziemba-Jankowska N., Zambrzycka A., Dziedzic T., LandCover. ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Commun. ACM, 60, pp. 84-90, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Chollet F., Xception: Deep Learning with Depthwise Separable Convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1251-1258; Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., SSD: Single Shot MultiBox Detector, Proceedings of the Transactions on Petri Nets and Other Models of Concurrency XV, pp. 21-37; Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 2481-2495, (2017); Badrinarayanan V., Handa A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling, (2015); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional Networks for Biomedical Image Segmentation, (2015); Zhou Z., Siddiquee M.R., Tajbakhsh N., Liang J., UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation, IEEE Trans. Med. Imaging, 39, pp. 1856-1867, (2020); Zhou Z., Rahman Siddiquee M.M., Tajbakhsh N., Liang J., UNet++: A Nested U-Net Architecture for Medical Image Segmenta-tion, Proceedings of the Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11, (2018); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs, IEEE Trans. Pattern Anal. Mach. Intell, 40, pp. 834-848, (2018); Chen L.-C., Zhu Y., Papandreou G., Schroff F., Adam H., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation, Proceedings of the European conference on computer vision (ECCV), (2018); Pinheiro P.O., Collobert R., Dollar P., Learning to Segment Object Candidates, (2015); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2961-2969; Matterport/Mask_RCNN, (2021); Cheng T., Wang X., Huang L., Liu W., Boundary-Preserving Mask R-CNN, Transactions on Petri Nets and Other Models of Concurrency XV, pp. 660-676, (2020); Foody G., Thematic Map Comparison, Photogramm. Eng. Remote Sens, 70, pp. 627-633, (2004); Foody G.M., Harshness in image classification accuracy assessment, Int. J. Remote Sens, 29, pp. 3137-3158, (2008); Stehman S.V., Thematic map accuracy assessment from the perspective of finite population sampling, Int. J. Remote Sens, 16, pp. 589-593, (1995); Stehman S., Statistical Rigor and Practical Utility in Thematic Map Accuracy Assessment, Photogramm. Eng. Remote Sens, 67, pp. 727-734, (2001); Stehman S.V., Comparison of Systematic and Random Sampling for Estimating the Accuracy of Maps Generated from Remotely Sensed Data, PE & RS-Photogramm. Eng. Remote Sens, 58, pp. 1343-1350, (1992); Stehman S.V., Foody G.M., Others Accuracy assessment, The SAGE Handbook of Remote Sensing, pp. 297-309, (2009); Stehman S.V., Selecting and interpreting measures of thematic classification accuracy, Remote Sens. Environ, 62, pp. 77-89, (1997); Stehman S.V., Basic probability sampling designs for thematic map accuracy assessment, Int. J. Remote Sens, 20, pp. 2423-2441, (1999); Stehman S.V., Practical Implications of Design-Based Sampling Inference for Thematic Map Accuracy Assessment, Remote Sens. Environ, 72, pp. 35-45, (2000); Stehman S.V., A Critical Evaluation of the Normalized Error Matrix in Map Accuracy Assessment, Photogramm. Eng. Remote Sens, 70, pp. 743-751, (2004); Stehman S.V., Sampling designs for accuracy assessment of land cover, Int. J. Remote Sens, 30, pp. 5243-5272, (2009); Stehman S.V., Estimating area and map accuracy for stratified random sampling when the strata are different from the map classes, Int. J. Remote Sens, 35, pp. 4923-4939, (2014); Stehman S.V., Czaplewski R.L., Design and Analysis for Thematic Map Accuracy Assessment: Fundamental Principles, Remote Sens. Environ, 64, pp. 331-344, (1998); Stehman S.V., Wickham J.D., Pixels, blocks of pixels, and polygons: Choosing a spatial unit for thematic accuracy assessment, Remote Sens. Environ, 115, pp. 3044-3055, (2011); Congalton R., Accuracy Assessment of Remotely Sensed Data: Future Needs and Directions, Proceedings of the Pecora, 12, pp. 383-388, (1994); Congalton R.G., A review of assessing the accuracy of classifications of remotely sensed data, Remote Sens. Environ, 37, pp. 35-46, (1991); Congalton R.G., Green K., Assessing the Accuracy of Remotely Sensed Data: Principles and Practices, (2019); Congalton R.G., Oderwald R.G., Mead R.A., Assessing Landsat Classification Accuracy Using Discrete Multivariate Analysis Statistical Techniques, Photogramm. Eng. Remote Sens, 49, pp. 1671-1678, (1983); Stehman S.V., Estimating area from an accuracy assessment error matrix, Remote Sens. Environ, 132, pp. 202-211, (2013); Stehman S.V., Impact of sample size allocation when using stratified random sampling to estimate accuracy and area of land-cover change, Remote Sens. Lett, 3, pp. 111-120, (2012); Clinton N., Holt A., Scarborough J., Yan L., Gong P., Accuracy Assessment Measures for Object-based Image Segmentation Goodness, Photogramm. Eng. Remote Sens, 76, pp. 289-299, (2010); Kucharczyk M., Hay G., Ghaffarian S., Hugenholtz C., Geographic Object-Based Image Analysis: A Primer and Future Directions, Remote Sens, 12, (2020); Lizarazo I., Accuracy assessment of object-based image classification: Another STEP, Int. J. Remote Sens, 35, pp. 6135-6156, (2014); Radoux J., Bogaert P., Fasbender D., Defourny P., Thematic accuracy assessment of geographic object-based image classification, Int. J. Geogr. Inf. Sci, 25, pp. 895-911, (2011); Radoux J., Bogaert P., Good Practices for Object-Based Accuracy Assessment, Remote Sens, 9, (2017); Maxwell A.E., Warner T.A., Thematic Classification Accuracy Assessment with Inherently Uncertain Boundaries: An Argument for Center-Weighted Accuracy Assessment Metrics, Remote Sens, 12, (2020); Li W., Wang Z., Wang Y., Wu J., Wang J., Jia Y., Gui G., Classification of High-Spatial-Resolution Remote Sensing Scenes Method Using Transfer Learning and Deep Convolutional Neural Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 1986-1995, (2020); Pontius R.G., Millones M., Death to Kappa: Birth of quantity disagreement and allocation disagreement for accuracy assessment, Int. J. Remote Sens, 32, pp. 4407-4429, (2011); Foody G.M., Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification, Remote Sens. Environ, 239, (2020); Howard J., Gugger S., Deep Learning for Coders with Fastai and PyTorch, (2020); Subramanian V., Deep Learning with PyTorch: A Practical Approach to Building Neural Network Models Using PyTorch, (2018); Howard J., Gugger S., Fastai: A Layered API for Deep Learning, Information, 11, (2020); Graf L., Bach H., Tiede D., Semantic Segmentation of Sentinel-2 Imagery for Mapping Irrigation Center Pivots, Remote Sens, 12, (2020); Tharwat A., Classification assessment methods, Appl. Comput. Inform, 17, pp. 168-192, (2021); Singh A., Kalke H., Loewen M., Ray N., River Ice Segmentation with Deep Learning, IEEE Trans. Geosci. Remote Sens, 58, pp. 7570-7579, (2020); Zhang W., Liljedahl A.K., Kanevskiy M., Epstein H.E., Jones B.M., Jorgenson M.T., Kent K., Transferability of the Deep Learning Mask R-CNN Model for Automated Mapping of Ice-Wedge Polygons in High-Resolution Satellite and UAV Images, Remote Sens, 12, (2020); Maxwell A.E., Bester M.S., Guillen L.A., Ramezan C.A., Carpinello D.J., Fan Y., Hartley F.M., Maynard S.M., Pyron J.L., Semantic Segmentation Deep Learning for Extracting Surface Mine Extents from Historic Topographic Maps, Remote Sens, 12, (2020); Maxwell A.E., Pourmohammadi P., Poyner J.D., Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data, Remote Sens, 12, (2020); Zhang W., Witharana C., Liljedahl A.K., Kanevskiy M., Deep Convolutional Neural Networks for Automated Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery, Remote Sens, 10, (2018); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark, Proceedings of the 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226-3229; Robinson C., Hou L., Malkin K., Soobitsky R., Czawlytko J., Dilkina B., Jojic N., Large Scale High-Resolution Land Cover Mapping with Multi-Resolution Data, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12726-12735; Qi K., Yang C., Hu C., Guan Q., Tian W., Shen S., Peng F., Polycentric Circle Pooling in Deep Convolutional Networks for High-Resolution Remote Sensing Image Recognition, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 632-641, (2020); Prakash N., Manconi A., Loew S., Mapping Landslides on EO Data: Performance of Deep Learning Models vs. Traditional Machine Learning Models, Remote Sens, 12, (2020); Ii D.J.G., Haupt S.E., Nychka D.W., Thompson G., Interpretable Deep Learning for Spatial Analysis of Severe Hailstorms, Mon. Weather Rev, 147, pp. 2827-2845, (2019); Ngo P.T.T., Panahi M., Khosravi K., Ghorbanzadeh O., Kariminejad N., Cerda A., Lee S., Evaluation of deep learning algorithms for national scale landslide susceptibility mapping of Iran, Geosci. Front, 12, pp. 505-519, (2021); Lobo J.M., Jimenez-Valverde A., Real R., AUC: A misleading measure of the performance of predictive distribution models, Glob. Ecol. Biogeogr, 17, pp. 145-151, (2008); Saito T., Rehmsmeier M., The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, PLoS ONE, 10, (2015); Pham M.-T., Courtrai L., Friguet C., Lefevre S., Baussard A., YOLO-Fine: One-Stage Detector of Small Objects under Various Backgrounds in Remote Sensing Images, Remote Sens, 12, (2020); Chen J., Wan L., Zhu J., Xu G., Deng M., Multi-Scale Spatial and Channel-wise Attention for Improving Object Detection in Remote Sensing Imagery, IEEE Geosci. Remote Sens. Lett, 17, pp. 681-685, (2020); Oh S., Chang A., Ashapure A., Jung J., Dube N., Maeda M., Gonzalez D., Landivar J., Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework, Remote Sens, 12, (2020); Henderson P., Ferrari V., End-to-End Training of Object Class Detectors for Mean Average Precision, Proceedings of the Asian Conference on Computer Vision, pp. 198-213; Zheng Z., Zhong Y., Ma A., Han X., Zhao J., Liu Y., Zhang L., HyNet: Hyper-scale object detection network framework for multiple spatial resolution remote sensing imagery, ISPRS J. Photogramm. Remote Sens, 166, pp. 1-14, (2020); COCO-Common Objects in Context; Wu T., Hu Y., Peng L., Chen R., Improved Anchor-Free Instance Segmentation for Building Extraction from High-Resolution Remote Sensing Images, Remote Sens, 12, (2020); Cheng G., Xie X., Han J., Guo L., Xia G.-S., Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 3735-3756, (2020); Maxwell A.E., Warner T.A., Fang F., Implementation of machine-learning classification in remote sensing: An applied review, Int. J. Remote Sens, 39, pp. 2784-2817, (2018); Koutsoukas A., Monaghan K.J., Li X., Huan J., Deep-learning: Investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data, J. Cheminform, 9, pp. 1-13, (2017); Musgrave K., Belongie S., Lim S.-N., A Metric Learning Reality Check, Proceedings of the European Conference on Computer Vision; Sejnowski T.J., The unreasonable effectiveness of deep learning in artificial intelligence, Proc. Natl. Acad. Sci. USA, 117, pp. 30033-30038, (2020); Haralick R.M., Statistical and structural approaches to texture, Proc. IEEE, 67, pp. 786-804, (1979); Warner T., Kernel-Based Texture in Remote Sensing Image Classification, Geogr. Compass, 5, pp. 781-798, (2011); Kim M., Madden M., Warner T.A., Forest Type Mapping using Object-specific Texture Measures from Multispectral Ikonos Imagery, Photogramm. Eng. Remote Sens, 75, pp. 819-829, (2009); Kim M., Warner T.A., Madden M., Atkinson D.S., Multi-scale GEOBIA with very high spatial resolution digital aerial imagery: Scale, texture and image objects, Int. J. Remote Sens, 32, pp. 2825-2850, (2011); Fern C., Warner T.A., Scale and Texture in Digital Image Classification, Photogramm. Eng. Remote Sens, 68, pp. 51-63, (2002); Foody G.M., Sample size determination for image classification accuracy assessment and comparison, Int. J. Remote Sens, 30, pp. 5273-5291, (2009); Cortes C., Mohri M., Confidence Intervals for the Area Under the ROC Curve, Adv. Neural Inf. Process. Syst, 17, pp. 305-312, (2005); Demler O.V., Pencina M.J., Misuse of DeLong test to compare AUCs for nested models, Stat. Med, 31, pp. 2577-2587, (2012); Maxwell A.E., Warner T.A., Is high spatial resolution DEM data necessary for mapping palustrine wetlands?, Int. J. Remote Sens, 40, pp. 118-137, (2018); Maxwell A.E., Warner T.A., Strager M.P., Predicting Palustrine Wetland Probability Using Random Forest Machine Learning and Digital Elevation Data-Derived Terrain Variables, Photogramm. Eng. Remote Sens, 82, pp. 437-447, (2016); Wright C., Gallant A., Improved wetland remote sensing in Yellowstone National Park using classification trees to combine TM imagery and ancillary environmental data, Remote Sens. Environ, 107, pp. 582-605, (2007); Fuller R., Groom G., Jones A., The Land-Cover Map of Great Britain: An Automated Classification of Landsat Thematic Mapper Data, Photogramm. Eng. Remote Sens, 60, pp. 553-562, (1994); Foody G., Approaches for the production and evaluation of fuzzy land cover classifications from remotely-sensed data, Int. J. Remote Sens, 17, pp. 1317-1340, (1996); Foody G.M., Local characterization of thematic classification accuracy through spatially constrained confusion matrices, Int. J. Remote Sens, 26, pp. 1217-1228, (2005); Berberoglu S., Lloyd C., Atkinson P., Curran P., The integration of spectral and textural information using neural networks for land cover mapping in the Mediterranean, Comput. Geosci, 26, pp. 385-396, (2000); Rodriguez-Galiano V.F., Olmo M.C., Abarca-Hernandez F., Atkinson P., Jeganathan C., Random Forest classification of Mediterranean land cover using multi-seasonal imagery and multi-seasonal texture, Remote Sens. Environ, 121, pp. 93-107, (2012); Rodriguez-Galiano V.F., Chica-Rivas M., Evaluation of different machine learning methods for land cover mapping of a Mediterranean area using multi-seasonal Landsat images and Digital Terrain Models, Int. J. Digit. Earth, 7, pp. 492-509, (2014); Senf C., Leitao P.J., Pflugmacher D., van der Linden S., Hostert P., Mapping land cover in complex Mediterranean landscapes using Landsat: Improved classification accuracies from integrating multi-seasonal and synthetic imagery, Remote Sens. Environ, 156, pp. 527-536, (2015); Stow D.A., Hope A., McGuire D., Verbyla D., Gamon J., Huemmrich F., Houston S., Racine C., Sturm M., Tape K., Et al., Remote sensing of vegetation and land-cover change in Arctic Tundra Ecosystems, Remote Sens. Environ, 89, pp. 281-308, (2004); Rees W., Williams M., Vitebsky P., Mapping land cover change in a reindeer herding area of the Russian Arctic using Landsat TM and ETM+ imagery and indigenous knowledge, Remote Sens. Environ, 85, pp. 441-452, (2003); Bartsch A., Hofler A., Kroisleitner C., Trofaier A.M., Land Cover Mapping in Northern High Latitude Permafrost Regions with Satellite Data: Achievements and Remaining Challenges, Remote Sens, 8, (2016); Cingolani A.M., Renison D., Zak M.R., Cabido M.R., Mapping Vegetation in a Heterogeneous Mountain Rangeland Using Landsat Data: An Alternative Method to Define and Classify Land-Cover Units, Remote Sens. Environ, 92, pp. 84-97, (2004); Rigge M., Homer C., Shi H., Wylie B., Departures of Rangeland Fractional Component Cover and Land Cover from Landsat-Based Ecological Potential in Wyoming, USA, Rangel. Ecol. Manag, 73, pp. 856-870, (2020); Herold M., Scepan J., Clarke K., The Use of Remote Sensing and Landscape Metrics to Describe Structures and Changes in Urban Land Uses, Environ. Plan. A Econ. Space, 34, pp. 1443-1458, (2002); Huang X., Wang Y., Li J., Chang X., Cao Y., Xie J., Gong J., High-resolution urban land-cover mapping and landscape analysis of the 42 major cities in China using ZY-3 satellite images, Sci. Bull, 65, pp. 1039-1048, (2020); Dennis M., Barlow D., Cavan G., Cook P.A., Gilchrist A., Handley J., James P., Thompson J., Tzoulas K., Wheater C.P., Et al., Mapping Urban Green Infrastructure: A Novel Landscape-Based Approach to Incorporating Land Use and Land Cover in the Mapping of Human-Dominated Systems, Land, 7, (2018); Li X., Shao G., Object-Based Land-Cover Mapping with High Resolution Aerial Photography at a County Scale in Midwestern USA, Remote Sens, 6, pp. 11372-11390, (2014); Witharana C., Bhuiyan A.E., Liljedahl A.K., Kanevskiy M., Epstein H.E., Jones B.M., Daanen R., Griffin C.G., Kent K., Jones M.K.W., Understanding the synergies of deep learning and data fusion of multispectral and panchromatic high resolution commercial satellite imagery for automated ice-wedge polygon detection, ISPRS J. Photogramm. Remote Sens, 170, pp. 174-191, (2020); Mou L., Hua Y., Zhu X.X., Relation Matters: Relational Context-Aware Fully Convolutional Network for Semantic Segmentation of High-Resolution Aerial Images, IEEE Trans. Geosci. Remote Sens, 58, pp. 7557-7569, (2020); Luo S., Li H., Shen H., Deeply supervised convolutional neural network for shadow detection based on a novel aerial shadow imagery dataset, ISPRS J. Photogramm. Remote Sens, 167, pp. 443-457, (2020)","A.E. Maxwell; Department of Geology and Geography, West Virginia University, Morgantown, 26505, United States; email: Aaron.Maxwell@mail.wvu.edu","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110226604"
"Wu Y.; Wang Y.; Zhang S.; Ogai H.","Wu, Yutian (57208878889); Wang, Yueyu (57221111832); Zhang, Shuwei (57211500543); Ogai, Harutoshi (6603121795)","57208878889; 57221111832; 57211500543; 6603121795","Deep 3D Object Detection Networks Using LiDAR Data: A Review","2021","IEEE Sensors Journal","21","2","9181591","1152","1171","19","50","10.1109/JSEN.2020.3020626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098213929&doi=10.1109%2fJSEN.2020.3020626&partnerID=40&md5=4fa795115fd94cf8fa1ffc3503500cb7","Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Horizon Robotics, Beijing, China","Wu Y., Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Wang Y., Horizon Robotics, Beijing, China; Zhang S., Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; Ogai H., Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan","As the foundation of intelligent systems, machine vision perceives the surrounding environment and provides a basis for decision-making. Object detection is the core task in machine vision. 3D object detection can provide object steric size and location information. Compared with the 2D object detection widely studied in image coordinates, it can provide more applications of detection systems. Accurate LiDAR data has a stronger spatial capture capability and is insensitive to natural light, which makes LiDAR a potential sensor for 3D detection. Recently, deep neural network has been developed to learn powerful object features from sensor data. However, the sparsity of LiDAR point cloud data poses challenges to the network processing. Plenty of emerged efforts have been made to address this difficulty, but a comprehensive review literature is still lacking. The purpose of this article is to review the challenges and methodologies of 3D object detection networks using LiDAR data. On this account, we first give an outline of 3D detection task and LiDAR sensing techniques. Then we unfold the review of deep 3D detection networks with three kinds of LiDAR point cloud representations and their challenges. We next summarize evaluation metrics and performance of algorithms on three authoritative 3D detection benchmarks. Finally, we provide valuable insights of challenges and open issues.  © 2001-2012 IEEE.","3D object detection; deep learning; LiDAR; neural network; point cloud","Benchmarking; Computer vision; Decision making; Deep neural networks; Intelligent systems; Object recognition; Optical radar; Capture capability; Evaluation metrics; Lidar point cloud datum; Lidar point clouds; Location information; Network processing; Performance of algorithm; Surrounding environment; Object detection","","","","","","","Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 580-587, (2014); Girshick R., Fast R-CNN, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards realtime object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); Amzajerdian F., Role of lidar technology in future nasa space missions, MRS Spring Meeting, (2008); Dubayah R.O., Drake J.B., LiDAR remote sensing for forestry, J. Forestry, 98, 6, pp. 44-46, (2000); Lim K., Treitz P., Wulder M., St-Onge B., Flood M., LiDAR remote sensing of forest structure, Prog. Phys. Geogra., 27, 1, pp. 88-106, (2003); Lefsky M.A., Cohen W.B., Parker G.G., Harding D.J., LiDAR remote sensing for ecosystem studies: LiDAR, an emerging remote sensing technology that directly measures the three-dimensional distribution of plant canopies, can accurately estimate vegetation structural attributes and should be of particular interest to forest, landscape, and global ecologists, BioScience, 52, 1, pp. 19-30, (2002); Dong P., Characterization of individual tree crowns using threedimensional shape signatures derived from LiDAR data, Int. J. Remote Sens., 30, 24, pp. 6621-6628, (2009); Henning J.G., Radtke P.J., Detailed stem measurements of standing trees from ground-based scanning LiDAR, Forest Sci., 52, 1, pp. 67-80, (2006); Amzajerdian F., Petway L., Hines G., Barnes B., Pierrottet D., Lockard G., Doppler LiDAR sensor for precision landing on the moon and Mars, Proc. IEEE Aerosp. Conf., pp. 1-7, (2012); Amzajerdian F., Et al., Imaging flash LiDAR for safe landing on solar system bodies and spacecraft rendezvous and docking, Proc. Spie, 9465, (2015); Jasiobedzki P., Se S., Pan T., Umasuthan M., Greenspan M., Autonomous satellite rendezvous and docking using LiDAR and model based vision, Proc. Spie, 5798, pp. 54-65, (2005); Lindner P., Wanielik G., 3D LiDAR processing for vehicle safety and environment recognition, Proc. IEEE Workshop Comput. Intell. Vehicles Veh. Syst., pp. 66-71, (2009); Asvadi A., Premebida C., Peixoto P., Nunes U., 3D LiDARbased static and moving obstacle detection in driving environments: An approach based on voxels and multi-region ground planes, Robot. Auto. Syst., 83, pp. 299-311, (2016); Larson J., Trivedi M., LiDAR based off-road negative obstacle detection and analysis, Proc. 14th Int. IEEE Conf. Intell. Transp. Syst. (ITSC), pp. 192-197, (2011); Sinha A., Papadakis P., Mind the gap: Detection and traversability analysis of terrain gaps using LiDAR for safe robot navigation, Robotica, 31, 7, pp. 1085-1101, (2013); Moghadam P., Wijesoma W.S., Jun Feng D., Improving path planning and mapping based on stereo vision and LiDAR, Proc. 10th Int. Conf. Control, Autom., Robot. Vis., pp. 384-389, (2008); Kang Y., Roh C., Suh S.-B., Song B., A LiDAR-based decisionmaking method for road boundary detection using multiple Kalman filters, IEEE Trans. Ind. Electron., 59, 11, pp. 4360-4368, (2012); Niclass C.L., Shpunt A., Agranov G.A., Waldon M.C., Rezk M.A., Oggier T., Multi-range time of flight sensing, U.S. Patent 15 586 286, (2018); Gargoum S., El-Basyouny K., Automated extraction of road features using LiDAR data: A review of LiDAR applications in transportation, Proc. 4th Int. Conf. Transp. Inf. Saf. (ICTIS), pp. 563-574, (2017); Arnold E., Al-Jarrah O.Y., Dianati M., Fallah S., Oxtoby D., Mouzakitis A., A survey on 3D object detection methods for autonomous driving applications, IEEE Trans. Intell. Transp. Syst., 20, 10, pp. 3782-3795, (2019); Guo Y., Wang H., Hu Q., Liu H., Liu L., Bennamoun M., Deep Learning for 3D Point Clouds: A Survey, (2019); Feng D., Et al., Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges, IEEE Trans. Intell. Transp. Syst., (2020); Zou Z., Shi Z., Guo Y., Ye J., Object Detection in 20 Years: A Survey, (2019); Qi C.R., Liu W., Wu C., Su H., Guibas L.J., Frustum PointNets for 3D object detection from RGB-D data, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 918-927, (2018); Zaheer M., Kottur S., Ravanbakhsh S., Poczos B., Salakhutdinov R.R., Smola A.J., Deep sets, Proc. Adv. Neural Inf. Process. Syst., pp. 3391-3401, (2017); Wang S., Suo S., Ma W.-C., Pokrovsky A., Urtasun R., Deep parametric continuous convolutional neural networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 2589-2597, (2018); Dean T., Ruzon M.A., Segal M., Shlens J., Vijayanarasimhan S., Yagnik J., Fast, accurate detection of 100, 000 object classes on a single machine, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 1814-1821, (2013); Felzenszwalb P.F., Girshick R.B., McAllester D., Ramanan D., Object detection with discriminatively trained part-based models, IEEE Trans. Pattern Anal. Mach. Intell., 32, 9, pp. 1627-1645, (2010); Sermanet P., Kavukcuoglu K., Chintala S., Lecun Y., Pedestrian detection with unsupervised multi-stage feature learning, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3626-3633, (2013); Uijlings J.R.R., Van De Sande K.E.A., Gevers T., Smeulders A.W.M., Selective search for object recognition, Int. J. Comput. Vis., 104, 2, pp. 154-171, (2013); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Proc. Adv. Neural Inf. Process. Syst., pp. 1097-1105, (2012); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Liu W., Et al., SSD: Single shot multibox detector, Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, pp. 21-37, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 779-788, (2016); Li B., Zhang T., Xia T., Vehicle Detection from 3D LiDAR Using Fully Convolutional Network, (2016); Chen X., Ma H., Wan J., Li B., Xia T., Multi-view 3D object detection network for autonomous driving, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1907-1915, (2017); Ku J., Mozifian M., Lee J., Harakeh A., Waslander S.L., Joint 3D proposal generation and object detection from view aggregation, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1-8, (2018); Zhou Y., Tuzel O., VoxelNet: End-to-end learning for point cloud based 3D object detection, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4490-4499, (2018); Geiger A., Lenz P., Urtasun R., Are we ready for autonomous driving? The KITTI vision benchmark suite, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3354-3361, (2012); Huang X., Et al., The ApolloScape dataset for autonomous driving, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 954-960, (2018); Qi C.R., Yi L., Su H., Guibas L.J., PointNet++: Deep hierarchical feature learning on point sets in a metric space, Proc. Adv. Neural Inf. Process. Syst., pp. 5099-5108, (2017); Caesar H., Et al., NuScenes: A Multimodal Dataset for Autonomous Driving, (2019); Sun P., Et al., Scalability in Perception for Autonomous Driving: Waymo Open Dataset, (2019); Patil A., Malla S., Gang H., Chen Y.-T., The H3D dataset for full-surround 3D multi-object detection and tracking in crowded urban scenes, Proc. Int. Conf. Robot. Autom. (ICRA), pp. 9552-9557, (2019); Kesten R., Et al., Lyft level 5 av dataset 2019, Tech. Rep., (2019); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3431-3440, (2015); Minemura K., Liau H., Monrroy A., Kato S., LMNet: Real-time multiclass object detection on CPU using 3D LiDAR, Proc. 3rd Asia-Pacific Conf. Intell. Robot Syst. (ACIRS), pp. 28-34, (2018); Yu F., Koltun V., Multi-scale Context Aggregation by Dilated Convolutions, (2015); Zhou J., Tan X., Shao Z., Ma L., FVNet: 3D Front-view Proposal Generation for Real-time Object Detection from Point Clouds, (2019); Charles R.Q., Su H., Kaichun M., Guibas L.J., PointNet: Deep learning on point sets for 3D classification and segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 652-660, (2017); Meyer G.P., Laddha A., Kee E., Vallespi-Gonzalez C., Wellington C.K., LaserNet: An efficient probabilistic 3D object detector for autonomous driving, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 12669-12678, (2019); Meyer G.P., Charland J., Hegde D., Laddha A., Vallespi-Gonzalez C., Sensor fusion for joint 3D object detection and semantic segmentation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 1230-1237, (2019); Yu F., Wang D., Shelhamer E., Darrell T., Deep layer aggregation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 2403-2412, (2018); Deng J., Czarnecki K., MLOD: A multi-view 3D object detection based on robust feature fusion method, Proc. IEEE Intell. Transp. Syst. Conf. (ITSC), pp. 279-284, (2019); Lu H., Chen X., Zhang G., Zhou Q., Ma Y., Zhao Y., Scanet: Spatial-channel attention network for 3D object detection, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), pp. 1992-1996, (2019); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2117-2125, (2017); Hu J., Shen L., Sun G., Squeeze-and-excitation networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 7132-7141, (2018); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 2980-2988, (2017); Wang Z., Zhan W., Tomizuka M., Fusing Bird's eye view LiDAR point cloud and front view camera image for 3D object detection, Proc. IEEE Intell. Vehicles Symp. (IV), pp. 1-6, (2018); Liang M., Yang B., Wang S., Urtasun R., Deep continuous fusion for multi-sensor 3D object detection, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 641-656, (2018); Liang M., Yang B., Chen Y., Hu R., Urtasun R., Multi-task multi-sensor fusion for 3D object detection, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7345-7353, (2019); Yu S.-L., Westfechtel T., Hamada R., Ohno K., Tadokoro S., Vehicle detection and localization on bird's eye view elevation images using convolutional neural network, Proc. IEEE Int. Symp. Saf., Secur. Rescue Robot. (SSRR), pp. 102-109, (2017); Beltran J., Guindel C., Moreno F.M., Cruzado D., Garcia F., De La Escalera A., BirdNet: A 3D object detection framework from LiDAR information, Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC), pp. 3517-3523, (2018); Barrera A., Guindel C., Beltran J., Garcia F., BirdNet+: Endto-end 3D Object Detection in LiDAR Bird's Eye View, (2020); Dai J., Li Y., He K., Sun J., R-FCN: Object detection via regionbased fully convolutional networks, Proc. Adv. Neural Inf. Process. Syst., pp. 379-387, (2016); Zeng Y., Et al., RT3D: Real-time 3-D vehicle detection in LiDAR point cloud for autonomous driving, IEEE Robot. Autom. Lett., 3, 4, pp. 3434-3440, (2018); Wirges S., Fischer T., Stiller C., Frias J.B., Object detection and classification in occupancy grid maps using deep convolutional networks, Proc. 21st Int. Conf. Intell. Transp. Syst. (ITSC), pp. 3530-3535, (2018); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7263-7271, (2017); Simony M., Milzy S., Amendey K., Gross H.-M., Complex-YOLO: An euler-region-proposal for real-time 3D object detection on point clouds, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 197-209, (2018); Ali W., Abdelkarim S., Zidan M., Zahran M., El Sallab A., YOLO3D: End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 716-728, (2018); Yang B., Luo W., Urtasun R., PIXOR: Real-time 3D object detection from point clouds, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 7652-7660, (2018); Yang B., Liang M., Urtasun R., HDNET: Exploiting HD maps for 3D object detection, Proc. Conf. Robot Learn., pp. 146-155, (2018); Luo W., Yang B., Urtasun R., Fast and furious: Real time endto-end 3D detection, tracking and motion forecasting with a single convolutional net, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 3569-3577, (2018); Wang D.Z., Posner I., Voting for voting in online point cloud object detection, Robot., Sci. Syst., 1, 3, pp. 156071-1560710, (2015); Westin C.-F., Geometrical diffusion measures for MRI from tensor basis analysis, Proc. Ismrm, (1997); Engelcke M., Rao D., Wang D.Z., Tong C.H., Posner I., Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 1355-1361, (2017); Li B., 3D fully convolutional network for vehicle detection in point cloud, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1513-1518, (2017); Graham B., Spatially-sparse Convolutional Neural Networks, (2014); Graham B., Sparse 3D Convolutional Neural Networks, (2015); Graham B., Maaten Der L.Van, Submanifold Sparse Convolutional Networks, (2017); Graham B., Engelcke M., Maaten L.V.D., 3D semantic segmentation with submanifold sparse convolutional networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 9224-9232, (2018); Yan Y., Mao Y., Li B., SECOND: Sparsely embedded convolutional detection, Sensors, 18, 10, (2018); Lang A.H., Vora S., Caesar H., Zhou L., Yang J., Beijbom O., PointPillars: Fast encoders for object detection from point clouds, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 12697-12705, (2019); Lehner J., Mitterecker A., Adler T., Hofmarcher M., Nessler B., Hochreiter S., Patch Refinement-localized 3D Object Detection, (2019); Chen Y., Liu S., Shen X., Jia J., Fast point R-CNN, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9775-9784, (2019); Kuang H., Wang B., An J., Zhang M., Zhang Z., Voxel-FPN: Multi-scale voxel feature aggregation for 3D object detection from LiDAR point clouds, Sensors, 20, 3, (2020); Ye Y., Chen H., Zhang C., Hao X., Zhang Z., SARPNET: Shape attention regional proposal network for LiDAR-based 3D object detection, Neurocomputing, 379, pp. 53-63, (2020); Zhou Y., Et al., End-to-end Multi-view Fusion for 3D Object Detection in LiDAR Point Clouds, (2019); Liu Z., Zhao X., Huang T., Hu R., Zhou Y., Bai X., TANet: Robust 3D Object Detection from Point Clouds with Triple Attention, (2019); Shi S., Wang Z., Shi J., Wang X., Li H., From points to parts: 3D object detection from point cloud with part-aware and partaggregation network, IEEE Trans. Pattern Anal. Mach. Intell., (2020); Ronneberger O., Fischer P., Brox T., U-Net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent. Cham, pp. 234-241, (2015); Shi S., Wang X., Li H., PointRCNN: 3D object proposal generation and detection from point cloud, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-779, (2019); Chen Q., Sun L., Wang Z., Jia K., Yuille A., Object As Hotspots: An Anchor-free 3D Object Detection Approach Via Firing of Hotspots, (2019); Yi H., Et al., SegVoxelNet: Exploring Semantic Context and Depthaware Features for 3D Vehicle Detection from Point Cloud, (2020); Sindagi V.A., Zhou Y., Tuzel O., MVX-Net: Multimodal Voxelnet for 3D object detection, Proc. Int. Conf. Robot. Autom. (ICRA), pp. 7276-7282, (2019); Jaderberg M., Simonyan K., Zisserman A., Spatial transformer networks, Proc. Adv. Neural Inf. Process. Syst., pp. 2017-2025, (2015); Wu Z., Et al., 3D ShapeNets: A deep representation for volumetric shapes, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1912-1920, (2015); Maturana D., Scherer S., VoxNet: A 3D convolutional neural network for real-time object recognition, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 922-928, (2015); Wang Z., Jia K., Frustum ConvNet: Sliding frustums to aggregate local point-wise features for amodal 3D object detection, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1742-1749, (2019); Shin K., Kwon Y.P., Tomizuka M., RoarNet: A robust 3D object detection based on RegiOn approximation refinement, Proc. IEEE Intell. Vehicles Symp. (IV), pp. 2510-2515, (2019); Xu D., Anguelov D., Jain A., PointFusion: Deep sensor fusion for 3D bounding box estimation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 244-253, (2018); Zhao X., Liu Z., Hu R., Huang K., 3D object detection using scale invariant and feature reweighting networks, Proc. Aaai Conf. Artif. Intell., 33, pp. 9267-9274, (2019); Jiang M., Wu Y., Zhao T., Zhao Z., Lu C., PointSIFT: A SIFTlike Network Module for 3D Point Cloud Semantic Segmentation, (2018); Tian Y., Wang K., Wang Y., Tian Y., Wang Z., Wang F.-Y., Adaptive and Azimuth-aware Fusion Network of Multimodal Local Features for 3D Object Detection, (2019); Yang Z., Sun Y., Liu S., Shen X., Jia J., IPOD: Intensive Point-based Object Detector for Point Cloud, (2018); Ngiam J., Et al., StarNet: Targeted Computation for Object Detection in Point Clouds, (2019); Xu K., Hu W., Leskovec J., Jegelka S., How Powerful Are Graph Neural Networks?, (2018); Qi C.R., Litany O., He K., Guibas L., Deep Hough voting for 3D object detection in point clouds, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 9277-9286, (2019); Yang Z., Sun Y., Liu S., Jia J., 3DSSD: Point-based 3D Single Stage Object Detector, (2020); Yang Z., Sun Y., Liu S., Shen X., Jia J., STD: Sparse-to-dense 3D object detector for point cloud, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 1951-1960, (2019); Shi S., Et al., PV-RCNN: Point-voxel Feature Set Abstraction for 3D Object Detection, (2019); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal visual object classes (VOC) challenge, Int. J. Comput. Vis., 88, 2, pp. 303-338, (2010); Lin T.-Y., Et al., Microsoft COCO: Common objects in context, Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, pp. 740-755, (2014); Salton G., McGill M.J., Introduction to Modern Information Retrieval, (1983); Simonelli A., Bulo S.R., Porzi L., Lopez-Antequera M., Kontschieder P., Disentangling monocular 3D object detection, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 1991-1999, (2019); Zhu B., Jiang Z., Zhou X., Li Z., Yu G., Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection, (2019); Yuan W., Khot T., Held D., Mertz C., Hebert M., PCN: Point completion network, Proc. Int. Conf. 3D Vis. (3DV), pp. 728-737, (2018); Liu M., Sheng L., Yang S., Shao J., Hu S.-M., Morphing and Sampling Network for Dense Point Cloud Completion, (2019); Groueix T., Fisher M., Kim V.G., Russell B.C., Aubry M., A papier-mache approach to learning 3D surface generation, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 216-224, (2018); Huang Z., Yu Y., Xu J., Ni F., Le X., PF-Net: Point fractal network for 3D point cloud completion, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7662-7670, (2020); Zarzar J., Giancola S., Ghanem B., PointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement, (2019); Shi W., Rajkumar R., Point-GNN: Graph neural network for 3D object detection in a point cloud, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1711-1719, (2020); Vora S., Lang A.H., Helou B., Beijbom O., PointPainting: Sequential Fusion for 3D Object Detection, (2019); Gao F., Wang C., Hybrid strategy for traffic light detection by combining classical and self-learning detectors, Iet Intell. Transp. Syst., 14, 7, pp. 735-741, (2020)","Y. Wu; Graduate School of Information, Production and Systems, Waseda University, Fukuoka, Japan; email: wuyutian@fuji.waseda.jp","","Institute of Electrical and Electronics Engineers Inc.","","","","","","1530437X","","","","English","IEEE Sensors J.","Review","Final","","Scopus","2-s2.0-85098213929"
"Gibril M.B.A.; Shafri H.Z.M.; Shanableh A.; Al-Ruzouq R.; Wayayok A.; Hashim S.J.B.; Sachit M.S.","Gibril, Mohamed Barakat A. (57188810669); Shafri, Helmi Zulhaidi Mohd (24072139200); Shanableh, Abdallah (7003825668); Al-Ruzouq, Rami (8520465800); Wayayok, Aimrun (57211373258); Hashim, Shaiful Jahari bin (55444812700); Sachit, Mourtadha Sarhan (57960732100)","57188810669; 24072139200; 7003825668; 8520465800; 57211373258; 55444812700; 57960732100","Deep convolutional neural networks and Swin transformer-based frameworks for individual date palm tree detection and mapping from large-scale UAV images","2022","Geocarto International","37","27","","18569","18599","30","1","10.1080/10106049.2022.2142966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141665201&doi=10.1080%2f10106049.2022.2142966&partnerID=40&md5=2f109d35910b62c9a9db243c5774737f","Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Department of Civil and Environmental Engineering, Faculty of Engineering, University of Sharjah, Sharjah, United Arab Emirates; GIS and Remote Sensing Center, Research Institute of Sciences and Engineering, University of Sharjah, Sharjah, United Arab Emirates; Department of Biological and Agricultural Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Department of Computer and Communication Systems Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia","Gibril M.B.A., Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Shafri H.Z.M., Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Shanableh A., Department of Civil and Environmental Engineering, Faculty of Engineering, University of Sharjah, Sharjah, United Arab Emirates, GIS and Remote Sensing Center, Research Institute of Sciences and Engineering, University of Sharjah, Sharjah, United Arab Emirates; Al-Ruzouq R., Department of Civil and Environmental Engineering, Faculty of Engineering, University of Sharjah, Sharjah, United Arab Emirates, GIS and Remote Sensing Center, Research Institute of Sciences and Engineering, University of Sharjah, Sharjah, United Arab Emirates; Wayayok A., Department of Biological and Agricultural Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Hashim S.J.B., Department of Computer and Communication Systems Engineering, Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia; Sachit M.S., Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Selangor, Serdang, Malaysia","Timely and reliable mapping of individual date palm trees is essential for their monitoring, health and risk assessment, pest control, and sustainable management of the date palm industry. This study presents an instance segmentation framework for large-scale detection and mapping of date palm trees using unmanned aerial vehicle (UAV)-based images. First, a data conversion framework is created to convert UAV image tiles and ground-truth vector data into annotation format of Common Objects in Context. Second, this study examines the efficacy of various instance segmentation models, namely, mask region convolutional neural network (Mask R-CNN), Mask Scoring R-CNN, You Only Look At CoefficientTs, Point-based Rendering, Segmenting Objects by Locations (SOLO), and SOLOv2) with varying residual learning networks (ResNets) in detecting and delineating individual date palm trees. Furthermore, the performance of two variants of Swin Transformer networks with a feature pyramid network (FPN) (Swin-small-FPN and Swin-tiny-FPN) as Mask R-CNN network backbones was also evaluated. Third, we assess the generalizability of the evaluated instance segmentation models and backbones on different testing datasets with varying spatial resolutions. Results show that Mask R-CNN models based on Swin Transformers backbones outperform those with ResNets in the detection and segmentation of date palm trees with mAP50 of 92% and 91% and F-measures of 94% and 93%. Moreover, the Mask scoring R-CNN-based ResNet-50 and Mask R-CNN with a Swin-small-FPN backbone outperform the evaluated models and demonstrate great generalizability in different datasets with diverse spatial resolutions. The proposed instance segmentation framework provides an efficient tool for date palm tree mapping from multi-scale UAV-based images and is valuable and suitable for individual tree crown delineations and other earth-related applications. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","individual tree crown delineation; Instance segmentation; mask R-CNN; mask scoring R-CNN; PointRend; SOLOv2; Swin transformer; YOLACT","aerial survey; agroindustry; alternative agriculture; angiosperm; artificial neural network; canopy architecture; crop plant; data set; detection method; developing world; fruit production; image analysis; performance assessment; reliability analysis; segmentation; spatial resolution; tree; unmanned vehicle; vegetation cover; vegetation mapping","","","","","University of Sharjah, UOS","The authors would like to acknowledge the Municipality of Ajman for supplying remotely sensed data of the study area and the University of Sharjah for providing the facilities used in this research.","Al-Khayri J.M., Naik P.M., Jain S.M., Johnson D.V., Advances in Date Palm (Phoenix dactylifera L.) breeding, Advances in plant breeding strategies: Fruits, 3, pp. 727-771, (2018); Al-Ruzouq R., Shanableh A., Barakat A. Gibril M., Al-Mansoori S., Image segmentation parameter selection and ant colony optimization for date palm tree detection and mapping from very-high-spatial-resolution aerial imagery, Remote Sens, 10, 9, (2018); Al-Saad M., Aburaed N., Mansoori S.A., Ahmad H.A., Autonomous palm tree detection from remote sensing images–UAE dataset, IGARSS 2022–2022 IEEE Int Geosci Remote Sens Symp, pp. 2191-2194, (2022); Ammar A., Koubaa A., Benjdira B., Deep-learning-based automated palm tree counting and geolocation in large farms from aerial geotagged images, Agronomy, 11, 8, (2021); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sens, 11, 4, (2019); Anagnostis A., Tagarakis A.C., Kateris D., Moysiadis V., Sorensen C.G., Pearson S., Bochtis D., Orchard mapping with deep learning semantic segmentation, Sensors, 21, 11, (2021); Bolya D., Zhou C., Xiao F., Lee Y.J., YOLACT: real-time instance segmentation, 2019 IEEE/CVF Int Conf Comput Vis, pp. 9156-9165, (2019); Braga J.R.G., Peripato V., Dalagnol R., Ferreira M.P., Tarabalka Y., Aragao L.E.O.C., de Campos Velho H.F., Shiguemori E.H., Wagner F.H., Tree crown delineation algorithm based on a convolutional neural network, Remote Sens, 12, 8, pp. 1-27, (2020); Briechle S., Krzystek P., Vosselman G., Silvi-Net–A dual-CNN approach for combined classification of tree species and standing dead trees from remote sensing data, Int J Appl Earth Obs Geoinf, 98, (2021); Cai L., Long T., Dai Y., Huang Y., Mask R-CNN-based detection and segmentation for pulmonary nodule 3D visualization diagnosis, IEEE Access, 8, pp. 44400-44409, (2020); Chao C.T., Krueger R.R., The Date Palm (Phoenix dactylifera L.): overview of biology, uses, and cultivation, horts, 42, 5, pp. 1077-1082, (2007); Chen K., Jiaqi W., Pang J., Cao Y., Xiong Y., Li X., Sun S., Feng W., Liu Z., Xu J., Et al., MMDetection: Open MMLab detection toolbox and benchmark, (2019); Cheng G., Han J., Lu X., Remote sensing image scene classification: benchmark and State of the Art, Proc IEEE, 105, 10, pp. 1865-1883, (2017); Cheng Z., Qi L., Cheng Y., Cherry tree crown extraction from natural Orchard images with complex backgrounds, Agriculture, 11, 5, (2021); Chen X., Jiang K., Zhu Y., Wang X., Yun T., Individual tree crown segmentation directly from uav-borne lidar data using the pointnet of deep learning, Forests, 12, 2, pp. 131-122, (2021); Choi K., Lim W., Chang B., Jeong J., Kim I., Park C.R., Ko D.W., An automatic approach for tree species detection and profile estimation of urban street trees using deep learning and Google street view images, ISPRS J Photogramm Remote Sens, 190, pp. 165-180, (2022); Chowdhury P.N., Shivakumara P., Nandanwar L., Samiron F., Pal U., Lu T., Oil palm tree counting in drone images, Pattern Recognit Lett, 153, pp. 1-9, (2022); Cubes M., A high resolution 3d surface construction algorithm/william e, Proc 14th Annu Conf Comput Graph Interact Tech, pp. 163-169, (1987); Culman M., Delalieux S., Van Tricht K., Individual palm tree detection using deep learning on RGB imagery to support tree inventory, Remote Sens, 12, 21, pp. 1-31, (2020); Dahy B., Issa S., Saleous N., Detecting and mapping mature, medium, and young age date palms in the arid lands of Abu Dhabi, using hierarchical integrated approach (HIA), Remote Sens Appl Soc Environ, 23, June, (2021); Dong Y., Zhang Y., Hou Y., Tong X., Wu Q., Zhou Z., Cao Y., Damage recognition of road auxiliary facilities based on deep convolution network for segmentation and image region correction. Ye Z, editor, Adv Civ Eng, 2022, pp. 1-10, (2022); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The pascal visual object classes (VOC) challenge, Int J Comput Vis, 88, 2, pp. 303-338, (2010); Food and Agriculture Organization. FAOSTAT, (2021); Ferreira M.P., Almeida D.D., Papa D., Minervino J.B.S., Veras H.F.P., Formighieri A., Santos C.A.N., Ferreira M.A.D., Figueiredo E.O., Ferreira E.J.L., Individual tree detection and species classification of Amazonian palms using UAV images and deep learning, For Ecol Manage, 475, April, (2020); Gibril M.B.A., Shafri H.Z.M., Shanableh A., Al-Ruzouq R., Wayayok A., Hashim S.J., Deep convolutional neural network for large-scale date palm tree mapping from uav-based images, Remote Sens, 13, 14, pp. 1-24, (2021); Gonzalez F., Mcfadyen A., Puig E., Advances in unmanned aerial systems and payload technologies for precision agriculture, In: Chen G, editor. Adv Agric Mach Technol, pp. 133-155, (2018); Gu W., Bai S., Kong L., Review article A review on 2D instance segmentation based on deep neural networks, Image Vis Comput, 120, (2022); Han P., Ma C., Chen J., Chen L., Bu S., Xu S., Zhao Y., Zhang C., Hagino T., Fast tree detection and counting on UAVs for sequential aerial images with generating orthophoto mosaicing, Remote Sens, 14, 16, (2022); Hao Z., Lin L., Post C.J., Mikhailova E.A., Li M., Chen Y., Yu K., Liu J., Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN), ISPRS J Photogramm Remote Sens, 178, June, pp. 112-123, (2021); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a worldview-2/3 and liDAR data fusion approach and deep learning, Sensors (Switzerland), 19, 6, pp. 1284-1223, (2019); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, 2017 IEEE Int Conf Comput Vis. Vol. 2017-Octob, pp. 2980-2988, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Vol. 2016-Decem, pp. 770-778, (2016); Hoang T.M., Nam G.P., Cho J., Kim I.J., DEFace: deep efficient face network for small scale variations, IEEE Access, 8, pp. 142423-142433, (2020); Huang Z., Huang L., Gong Y., Huang C., Wang X., Mask scoring R-CNN, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. Vol. 2019-June, pp. 6402-6411, (2019); Huang H., Li X., Chen C., Individual tree crown detection and delineation from very-high-resolution UAV images based on bias field and marker-controlled watershed segmentation algorithms, IEEE J Sel Top Appl Earth Observations Remote Sens, 11, 7, pp. 2253-2262, (2018); Hu G., Wang T., Wan M., Bao W., Zeng W., UAV remote sensing monitoring of pine forest diseases based on improved Mask R-CNN, Int J Remote Sens, 43, 4, pp. 1274-1305, (2022); Hu G., Yin C., Wan M., Zhang Y., Fang Y., Recognition of diseased Pinus trees in UAV images using deep learning and AdaBoost classifier, Biosyst Eng, 194, pp. 138-151, (2020); Hu G., Zhu Y., Wan M., Bao W., Zhang Y., Liang D., Yin C., Detection of diseased pine trees in unmanned aerial vehicle images by using deep convolutional neural networks, Geocarto Int, 37, 12, pp. 3520-3539, (2022); Jintasuttisak T., Edirisinghe E., Elbattay A., Deep neural network based date palm tree detection in drone imagery, Comput Electron Agric, 192, (2022); Kagan D., Fuhrmann Alpert G., Fire M., Automatic large scale detection of red palm weevil infestation using street view images, ISPRS J Photogramm Remote Sens, 182, pp. 122-133, (2021); Kentsch S., Karatsiolis S., Kamilaris A., Tomhave L., Caceres M.L.L., Identification of tree species in Japanese forests based on aerial photography and deep learning, Adv New Trends Environ Informatics, pp. 255-270, (2021); Khan N., Kamaruddin M.A., Sheikh U.U., Yusup Y., Bakht M.P., Oil palm and machine learning: reviewing one decade of ideas, innovations, applications, and gaps, Agric, 11, 9, pp. 1-26, (2021); Kirillov A., Wu Y., He K., Girshick R., Pointrend: image segmentation as rendering, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, pp. 9796-9805, (2020); Kolanuvada S.R., Ilango K.K., Automatic extraction of tree crown for the estimation of biomass from UAV imagery using neural networks, J Indian Soc Remote Sens, 49, 3, pp. 651-658, (2021); Kurup S.S., Hedar Y.S., Al Dhaheri M.A., El-Heawiety A.Y., Aly M.A.M., Alhadrami G., Morpho-physiological evaluation and RAPD markers-assisted characterization of date palm (Phoenix dactylifera L.) varieties for salinity tolerance, J Food Agric Environ, 7, 3-4, pp. 503-507, (2009); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks, Remote Sens, 11, 1, (2018); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc IEEE Conf Comput Vis Pattern Recognit, pp. 2117-2125, (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: common objects in context, Eur Conf Comput Vis, pp. 740-755, (2014); Liu X., Ghazali K.H., Han F., Mohamed I.I., Automatic detection of oil palm tree from UAV images based on the deep learning method, Appl Artif Intell, 35, 1, pp. 13-24, (2021); Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo B., Swin transformer: hierarchical vision transformer using shifted windows, 2021 IEEE/CVF Int Conf Comput Vis, pp. 9992-10002, (2021); Liu Y., Zhang Y., Wang Y., Hou F., Yuan J., Tian J., Zhang Y., Shi Z., Fan J., He Z., A survey of visual transformers, pp. 1-21, (2021); Li Y., Wang H., Dang L.M., Piran J., Moon H., A robust instance segmentation framework for underground sewer defect detection, Measurement, 190, January, (2022); Luo Y., Han J., Liu Z., Wang M., Xia G., An elliptic centerness for object instance segmentation in aerial images, J Remote Sens, 2022, pp. 1-14, (2022); Mao Z., Huang X., Gong Y., Xiang H., Zhang F., A dataset and ensemble model for glass façade segmentation in oblique aerial images, IEEE Geosci Remote Sensing Lett, 19, pp. 1-5, (2022); Marin W., Mondragon I.F., Colorado J.D., Aerial identification of amazonian palms in high-density forest using deep learning, Forests, 13, 5, (2022); Martins G.B., La Rosa L.E.C., Happ P.N., Filho L.C.T.C., Santos C.J.F., Feitosa R.Q., Ferreira M.P., Deep learning-based tree species mapping in a highly diverse tropical urban setting, Urban Urban Green, 64, March, (2021); Martins J.A.C., Nogueira K., Osco L.P., Gomes F.D.G., Furuya D.E.G., Goncalves W.N., Sant'ana D.A., Ramos A.P.M., Liesenberg V., Dos Santos J.A., Et al., Semantic segmentation of tree-canopy in urban environment with pixel-wise deep learning, Remote Sens, 13, 16, (2021); Mazloumzadeh S.M., Shamsi M., Nezamabadi-Pour H., Fuzzy logic to classify date palm trees based on some physical properties related to precision agriculture, Precision Agric, 11, 3, pp. 258-273, (2010); Mihi A., Nacer T., Chenchouni H., Monitoring dynamics of date palm plantations from 1984 to 2013 using Landsat time-series in Sahara desert oases of Algeria, Adv Sci Technol Innov, pp. 225-228, (2019); Miraki M., Sohrabi H., Fatehi P., Kneubuehler M., Individual tree crown delineation from high-resolution UAV images in broadleaf forest, Ecol Inform, 61, (2021); Mo J., Lan Y., Yang D., Wen F., Qiu H., Chen X., Deng X., Deep learning-based instance segmentation method of Litchi canopy from UAV-acquired images, Remote Sens, 13, 19, (2021); Mohan M., Silva C.A., Klauberg C., Jat P., Catts G., Cardil A., Hudak A.T., Dia M., Individual tree detection from unmanned aerial vehicle (UAV) derived canopy height model in an open canopy mixed conifer forest, Forests, 8, 9, pp. 340-317, (2017); Morales G., Kemper G., Sevillano G., Arteaga D., Ortega I., Telles J., Automatic segmentation of Mauritia flexuosa in unmanned aerial vehicle (UAV) imagery using deep learning, Forests, 9, 12, (2018); Moura M.M., de Oliveira L.E.S., Sanquetta C.R., Bastos A., Mohan M., Corte A.P.D., Towards Amazon forest restoration: automatic detection of species from UAV imagery, Remote Sens, 13, 13, (2021); Mulley M., Kooistra L., Bierens L., High-resolution multisensor remote sensing to support date palm farm management, Agric, 9, 2, (2019); Nasi R., Honkavaara E., Blomqvist M., Lyytikainen-Saarenmaa P., Hakala T., Viljanen N., Kantola T., Holopainen M., Remote sensing of bark beetle damage in urban forests at individual tree level using a novel hyperspectral camera from UAV and aircraft, Urban for Urban Green, 30, pp. 72-83, (2018); Nguyen H.T., Caceres M.L.L., Moritake K., Kentsch S., Shu H., Diez Y., Correction: nguyen et al. Individual sick fir tree (Abies mariesii) identification in insect infested forests by means of uav images and deep learning, Remote Sens, 13, 11, pp. 1-24, (2021); Ocer N.E., Kaplan G., Erdem F., Kucuk Matci D., Avdan U., Tree extraction from multi-scale UAV images using Mask R-CNN with FPN, Remote Sens Lett, 11, 9, pp. 847-856, (2020); Osco L.P., Arruda M., Marcato Junior J., da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J Photogramm Remote Sens, 160, pp. 97-106, (2020); Ozdarici-Ok A., Ok A.O., Zeybek M., Atesoglu A., Automated extraction and validation of Stone Pine (Pinus pinea L.) trees from UAV-based digital surface models, Geo-Spatial Inf Sci, 25, 3, pp. 1-21, (2022); Pearse G.D., Watt M.S., Soewarto J., Tan A.Y.S., Deep learning and phenology enhance large-scale tree species classification in aerial imagery during a biosecurity response, Remote Sens, 13, 9, (2021); Piyathilaka L., Preethichandra D.M.G., Izhar U., Kahandawa G., pp. 1-7, (2020); Qin J., Wang B., Wu Y., Lu Q., Zhu H., Identifying pine wood nematode disease using uav images and deep learning algorithms, Remote Sens, 13, 2, pp. 1-14, (2021); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans Pattern Anal Mach Intell, 39, 6, pp. 1137-1149, (2017); Safonova A., Guirado E., Maglinets Y., Alcaraz-Segura D., Tabik S., Olive tree biovolume from uav multi-resolution image segmentation with mask R-CNN, Sensors, 21, 5, (2021); Safonova A., Hamad Y., Dmitriev E., Georgiev G., Trenkin V., Georgieva M., Dimitrov S., Iliev M., Individual tree crown delineation for the species classification and assessment of vital status of forest stands from UAV images, Drones, 5, 3, (2021); Schiefer F., Kattenborn T., Frick A., Frey J., Schall P., Koch B., Schmidtlein S., Mapping forest tree species in high resolution UAV-based RGB-imagery by means of convolutional neural networks, ISPRS J Photogramm Remote Sens, 170, vember, pp. 205-215, (2020); eMotion 3 user manual, senseFly Parrot Gr, March, pp. 73-75, (2018); Shareef M.A., Estimation and mapping of dates palm using Landsat-8 Images: a case study in Baghdad City, 2018 Int Conf Adv Sci Eng, pp. 425-430, (2018); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Spennemann D.H.R., Review of the vertebrate-mediated dispersal of the Date Palm, Phoenix dactylifera, Zool Middle East, 64, 4, pp. 283-296, (2018); Stateras D., Kalivas D., Assessment of olive tree canopy characteristics and yield forecast model using high resolution UAV imagery, Agric, 10, 9, pp. 1-13, (2020); Sun Y., Li Z., He H., Guo L., Zhang X., Xin Q., Counting trees in a subtropical mega city using the instance segmentation method, Int J Appl Earth Obs Geoinf, 106, (2022); Torresan C., Carotenuto F., Chiavetta U., Miglietta F., Zaldei A., Gioli B., Individual tree crown segmentation in two-layered dense mixed forests from uav lidar data, Drones, 4, 2, pp. 10-20, (2020); Wang Z., Fan C., Xian M., Application and evaluation of a deep learning architecture to urban tree canopy mapping, Remote Sens, 13, 9, pp. 1-14, (2021); Wang X., Kong T., Shen C., Jiang Y., Li L., SOLO: segmenting objects by locations, Eur Conf Comput Vis. Vol. 12363 LNCS, pp. 649-665, (2020); Wang X., Zhang R., Kong T., Li L., Shen C., SOLOv2: dynamic and fast instance segmentation, Proc 34th Conf Neural Inf Process Syst (NeurIPS 2020). Vol. 2020-Decem., (2020); Wang X., Zhang R., Shen C., Kong T., Li L., SOLO: a simple framework for instance segmentation, IEEE Trans Pattern Anal Mach Intell, (2021); Weinstein B.G., Marconi S., Aubry-Kientz M., Vincent G., Senyondo H., White E.P., DeepForest: a Python package for RGB deep learning tree crown delineation, Methods Ecol Evol, 11, 12, pp. 1743-1751, (2020); Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., (2019); Xia K., Wang H., Yang Y., Du X., Feng H., Automatic detection and parameter estimation of Ginkgo biloba in urban environment based on RGB images, J Sensors, 2021, pp. 1-12, (2021); Xia L., Zhang R., Chen L., Li L., Yi T., Wen Y., Ding C., Xie C., Evaluation of deep learning segmentation models for detection of pine wilt disease in unmanned aerial vehicle images, Remote Sens, 13, 18, pp. 1-15, (2021); Xi X., Xia K., Yang Y., Du X., Feng H., Evaluation of dimensionality reduction methods for individual tree crown delineation using instance segmentation network and UAV multispectral imagery in urban forest, Comput Electron Agric, 191, October, (2021); Yang M., Mou Y., Liu S., Meng Y., Liu Z., Li P., Xiang W., Zhou X., Peng C., Detecting and mapping tree crowns based on convolutional neural network and Google Earth images, Int J Appl Earth Obs Geoinf, 108, (2022); Yarak K., Witayangkurn A., Kritiyutanont K., Arunplod C., Shibasaki R., Oil palm tree detection and health classification on high‐resolution imagery using deep learning, Agric, 11, 2, pp. 1-17, (2021); Yu K., Hao Z., Post C.J., Mikhailova E.A., Lin L., Zhao G., Tian S., Liu J., Comparison of classical methods and mask R-CNN for automatic tree detection and mapping using UAV imagery, Remote Sens, 14, 2, (2022); Yu R., Luo Y., Zhou Q., Zhang X., Wu D., Ren L., Early detection of pine wilt disease using deep learning algorithms and UAV-based multispectral imagery, For Ecol Manage, 497, (2021); Zamboni P., Junior J.M., de Silva J.A., Miyoshi G.T., Matsubara E.T., Nogueira K., Goncalves W.N., Benchmarking anchor-based and anchor-free state-of-the-art deep learning methods for individual tree detection in rgb high-resolution images, Remote Sens, 13, 13, (2021); Zhang Z., Huang S., Liu X., Zhang B., Dong D., Adversarial attacks on YOLACT instance segmentation, Comput Secur, 116, (2022); Zhang C., Wei S., Ji S., Detecting large-scale urban land cover changes from very high resolution remote sensing images using CNN-based classification, ISPRS Int J Geo-Inf, 8, 4, (2019); Zhang C., Xia K., Feng H., Yang Y., Du X., Tree species classification using deep learning and RGB optical images obtained by an unmanned aerial vehicle, J Res, 32, 5, pp. 1879-1888, (2020); Zhang C., Zhou J., Wang H., Tan T., Cui M., Huang Z., Wang P., Zhang L., Multi‐species individual tree segmentation and identification based on improved Mask R‐CNN and UAV imagery in mixed forests, Remote Sens, 14, 4, (2022); Zheng J., Fu H., Li W., Wu W., Yu L., Yuan S., Tao W.Y.W., Pang T.K., Kanniah K.D., Growing status observation for oil palm trees using Unmanned Aerial Vehicle (UAV) images, ISPRS J Photogramm Remote Sens, 173, pp. 95-121, (2021); Zheng J., Fu H., Li W., Wu W., Zhao Y., Dong R., Yu L., Cross-regional oil palm tree counting and detection via a multi-level attention domain adaptation network, ISPRS J Photogramm Remote Sens, 167, pp. 154-177, (2020); Zheng J., Wu W., Yuan S., Fu H., Li W., Yu L., Multisource-domain generalization-based oil palm, IEEE Geosci Remote Sensing Lett, 19, pp. 1-5, (2022); Zhong Z., Xiong J., Zheng Z., Liu B., Liao S., Huo Z., Yang Z., Original papers A method for litchi picking points calculation in natural environment based on main fruit bearing branch detection, Comput Electron Agric, 189, August, (2021); Zou Z., Shi Z., Guo Y., Ye J., Object detection in 20 years: a survey, (2019)","H.Z.M. Shafri; Department of Civil Engineering and Geospatial Information Science Research Centre (GISRC), Faculty of Engineering, Universiti Putra Malaysia (UPM), Serdang, Selangor, Malaysia; email: helmi@upm.edu.my","","Taylor and Francis Ltd.","","","","","","10106049","","","","English","Geocarto Int.","Article","Final","","Scopus","2-s2.0-85141665201"
"Timilsina S.; Aryal J.; Kirkpatrick J.B.","Timilsina, Shirisa (57216852299); Aryal, Jagannath (16315027400); Kirkpatrick, Jamie B. (7203081724)","57216852299; 16315027400; 7203081724","Mapping urban tree cover changes using object-based convolution neural network (OB-CNN)","2020","Remote Sensing","12","18","3017","","","","38","10.3390/RS12183017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092112272&doi=10.3390%2fRS12183017&partnerID=40&md5=4f1365f720006efbe141ff8f61c2250e","School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia; Melbourne School of Engineering, University of Melbourne, Parkville, 3010, VIC, Australia","Timilsina S., School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia; Aryal J., School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia, Melbourne School of Engineering, University of Melbourne, Parkville, 3010, VIC, Australia; Kirkpatrick J.B., School of Technology, Environments and Design, Discipline of Geography and Spatial Sciences, University of Tasmania, Hobart, 7001, TAS, Australia","Urban trees provide social, economic, environmental and ecosystem services benefits that improve the liveability of cities and contribute to individual and community wellbeing. There is thus a need for effective mapping, monitoring and maintenance of urban trees. Remote sensing technologies can effectively map and monitor urban tree coverage and changes over time as an efficient and low-cost alternative to field-based measurements, which are time consuming and costly. Automatic extraction of urban land cover features with high accuracy is a challenging task, and it demands object based artificial intelligence workflows for efficiency and thematic accuracy. The aim of this research is to effectively map urban tree cover changes and model the relationship of such changes with socioeconomic variables. The object-based convolutional neural network (CNN) method is illustrated by mapping urban tree cover changes between 2005 and 2015/16 using satellite, Google Earth imageries and Light Detection and Ranging (LiDAR) datasets. The training sample for CNN model was generated by Object Based Image Analysis (OBIA) using thresholds in a Canopy Height Model (CHM) and the Normalised Difference Vegetation Index (NDVI). The tree heatmap produced from the CNN model was further refined using OBIA. Tree cover loss, gain and persistence was extracted, and multiple regression analysis was applied to model the relationship with socioeconomic variables. The overall accuracy and kappa coefficient of tree cover extraction was 96% and 0.77 for 2005 images and 98% and 0.93 for 2015/16 images, indicating that the object-based CNN technique can be effectively implemented for urban tree coverage mapping and monitoring. There was a decline in tree coverage in all suburbs. Mean parcel size and median household income were significantly related to tree cover loss (R2 = 58.5%). Tree cover gain and persistence had positive relationship with tertiary education, parcel size and ownership change (gain: R2 = 67.8% and persistence: R2 = 75.3%). The research findings demonstrated that remote sensing data with intelligent processing can contribute to the development of policy input for management of tree coverage in cities. © 2020 by the authors.","Convolution neural networks (CNNs); Deep learning; GEOBIA; Object-based CNN; Socioeconomic predictor variables; Urban tree mapping","Convolution; Convolutional neural networks; Data handling; Ecosystems; Extraction; Heating; Lithium compounds; Mapping; Optical radar; Regression analysis; Remote sensing; Research and development management; Trees (mathematics); Convolution neural network; Field-based measurements; Intelligent processing; Light detection and ranging; Multiple regression analysis; Normalised difference vegetation index; Object based image analysis (OBIA); Remote sensing technology; Forestry","","","","","Google Earth and Land Information System Tasmania; Kingborough Council; LiDAR point cloud and cadastral parcel datasets; University of Tasmania, UTAS; AVL List, AVL","This research received no external funding. The authors wish to thank the Kingborough Council, Tasmania for providing the aerial imagery of the study area. We are grateful to the University of Tasmania for providing research facilities. We also thank Google Earth and Land Information System Tasmania (LIST) for providing imagery and LiDAR point cloud and cadastral parcel datasets.","Bolund P., Hunhammar S., Ecosystem services in urban areas., Ecol. Econ., 29, pp. 293-301, (1999); Lohr V., earson-Mi S.C., Tarnai J., Dillman D., How Urb n Residents Rate and Rank the Benefits and Problems Associated with Trees in Cities., J. Arboric., 1, pp. 28-35, (2004); Shackleton S., Chinyimba A., Hebinck P., Shackleton C., Kaoma H., Multiple benefits and values of trees in urban landscapes in two towns in northern South Africa., Landsc. Urban Plan., 136, pp. 76-86, (2015); Solecki W.D., Welchb J.M., Urban parks: Green spaces or green walls? Landsc., Urban Plan., 32, pp. 93-106, (1995); Tyrvainen L., Silvennoinen H., Kolehmainen O., Ecological and aesthetic values in urban forest management., Urban For. Urban Green., 1, pp. 15-149, (2003); Erker T., Wang L., Lorentz L., Stoltman A., Townsend P.A., A statewide urban tree canopy mapping ethod., Remote Sens. Environ., 229, pp. 148-158, (2019); Guo T., Morgenroth J., Conway T., Xu C., City- ide canopy cover decline due to residential property redevelopment in Christchurch, New Zealand., Sci. Total Environ., 681, pp. 202-210, (2019); Nowak D.J., Rowntree R.A., McPherson E.G., Sisinni S.M., Kerkmann E.R., Stevens J.C., Measuring and analyzing urban tree cover., Landsc. Urban Plan., 36, pp. 49-57, (1996); Schneider A., Monitoring land cover change in urban and peri-urban areas using dense time stacks of Landsat satellite data and a data mining approach., Remote Sens. Environ., 124, pp. 689-704, (2012); Stave J., Oba G., Stenseth N.C., Temporal changes in woody-plant use and the ekwar indigenous tree management system along the Turkwel River, Kenya., Environ. Conserv., 28, pp. 150-159, (2001); Tucker Lima J.M., Staudhammer C.L., Brandeis T.J., Escobedo F.J., Zipperer W., Temporal dynamics of a subtropical urban forest in San Juan, Puerto Rico, 2001-2010., Landsc. Urban Plan., 120, pp. 96-106, (2013); Bowden L.W., Urban environments: Inventory and analysis., Man. Remote Sens., 12, pp. 1815-1880, (1975); Grove J.M., Troy A.R., O'Neil-Dunne J.P.M., Burch W.R., Cadenasso M.L., Pickett S.T.A., Characterization of households and its implications for the vegetation of urban ecosystems., Ecosystems, 9, pp. 578-597, (2006); Iverson L.R., Cook E.A., Urban forest cover of the Chicago region and its relation to household density and income., Urban Ecosyst., 4, pp. 105-124, (2000); Kirkpatrick J.B., Daniels G.D., Zagorski T., Explaining variation in front gardens between suburbs of Hobart, Tasmania, Australia., Landsc. Urban Plan., 79, pp. 314-322, (2007); Kirkpatrick J.B., Daniels G.D., Davison A., Temporal and spatial variation in garden and street trees in six eastern Australian cities., Landsc. Urban Plan., 101, pp. 244-252, (2011); Martin C.A., Paige S.W., Kinzig A.P., Neighbourhood socioeconomic status is a useful predictor of perennial landscape vegetation in residential neighbourhoods and embedded small parks of Phoenix, AZ., Landsc. Urban Plan., 69, pp. 355-368, (2004); Talarchek G.M., The Urban forest of New Orleans: An exploratory analysis of relationship., Urban Geogr., 11, pp. 65-86, (1990); Moskal L.M., Styers D.M., Halabisky M., Monitoring urban tree cover using object-based image analysis and public domain remotely sensed data., Remote Sens., 3, pp. 2243-2262, (2011); Ehlers M., Gahler M., Janowsky R., Automated analysis of ultra high resolution remote sensing data for biotope type mapping: New possibilities and challenges., ISPRS J. Photogramm. Remote Sens., 57, pp. 315-326, (2003); Mikita T., Janata P., Surovy P., Forest stand inventory based on combined aerial and terrestrial close-range photogrammetry., Forests, 7, (2016); Ke Y., Quackenbush L.J., A review of methods for automatic individual tree-crown detection and delineation from passive remote sensing., Internatl. J. Remote Sens., 32, pp. 4725-4747, (2011); Xiao Q., McPherson E.G., Tree health mapping with multispectral remote sensing data at UC Davis, California., Urban Ecosyst., 8, pp. 349-361, (2005); Anees A., Aryal J., A Statistical Framework for Near-Real Time Detection of Beetle Infestation in Pine Forests Using MODIS Data., IEEE Geosci. Remote Sens. Lett., 11, pp. 1717-1721, (2014); Anees A., Aryal J., Near-real time detection of beetle infestation in pine forests using MODIS data., IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 7, pp. 3713-3723, (2014); Anees A., Aryal J., O'Reilly M.M., Gale T.J., A Relative Density Ratio-Based Framework for Detection of Land Cover Changes in MODIS NDVI Time Series., IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 9, pp. 3359-3371, (2016); Rogan J., Chen D.M., Remote sensing technology for mapping and monitoring land-cover and land-use change., Prog. Plann., 61, pp. 301-325, (2004); Ardila J.P., Bijker W., Tolpekin V.A., Stein A., Context-sensitive extraction of tree crown objects in urban areas using VHR satellite images., Int. J. Appl. Earth Obs. Geoinf., 15, pp. 57-69, (2012); O'Neil-Dunne J., MacFaden S., Royar A., A versatile, production-oriented approach to high-resolution tree-canopy mapping in urban and suburban landscapes using GEOBIA and data fusion., Remote Sens., 6, pp. 12837-12865, (2014); Walker J.S., Briggs J.M., An Object-oriented Approach to Urban Forest Mapping in Phoenix., Photogramm. Eng. Remote Sens., 73, pp. 577-583, (2007); Zhou W., Troy A., Grove M., Object-based Land Cover Classification and Change Analysis in the Baltimore Metropolitan Area Using Multitemporal High Resolution Remote Sensing Data., Sensors, 8, pp. 1613-1636, (2008); Blaschke T., Object based image analysis for remote sensing., ISPRS J. Photogramm. Remote Sens., 65, pp. 2-16, (2010); Walker J.S., Blaschke T., Object-based land-cover classification for the Phoenix metropolitan area: Optimization vs. transportability., Int. J. Remote Sens., 29, pp. 2021-2040, (2008); Zhou J., Yu B., Qin J., Multi-level spatial analysis for change detection of urban vegetation at individual tree scale., Remote Sens., 6, pp. 9086-9103, (2014); Banzhaf E., Kollai H., Monitoring the urban tree cover for urban ecosystem services-The case of Leipzig, Germany., In Proceedings of the 36th International Symposium on Remote Sensing of Environment, 40, pp. 301-305, (2015); Ejares J.A., Violanda R.R., Diola A.G., Dy D.T., Otadoy J.B., Otadoy R.E.S., Tree canopy cover mapping using LiDAR in urban barangays of Cebu City, central Philippines., In Proceedings of the XXIII ISPRS Congress, The International Archives of the Photogrammetry, 41, pp. 611-615, (2016); Blaschke T., Hay G.J., Kelly M., Lang S., Hofmann P., Addink E., Feitosa R.Q., Van der Meer F., Van der Werff H., Van Coillie F., Et al., Geographic Object-Based Image Analysis-Towards a new paradigm., ISPRS J. Photogramm. Remote Sens., 87, pp. 180-191, (2014); Belgiu M., Dragut L., Random forest in remote sensing: A review of applications and future directions., ISPRS J. Photogramm. Remote Sens., 114, pp. 24-31, (2016); Dragut L., Tiede D., Levick S.R., ESP: A tool to estimate scale parameter for multiresolution image segmentation of remotely sensed data., Int. J. Geogr. Inf. Sci., 24, pp. 859-871, (2010); Jin B., Ye P., Zhang X., Song W., Li S., Object-Oriented Method Combined with Deep Convolutional Neural Networks for Land-Use-Type Classification of Remote Sensing Images., J. Indian Soc. Remote Sens., 47, pp. 951-965, (2019); Ming D., Li J., Wang J., Zhang M., Scale parameter selection by spatial statistics for GeOBIA: Using mean-shift based multi-scale segmentation as an example., ISPRS J. Photogramm. Remote Sens., 106, pp. 28-41, (2015); Du S., Shy M., Wang Q., Modelling relational contexts in GEOBIA framework for improving urban land-cover mapping., GISci. Remote Sens., 56, pp. 184-209, (2019); Belgiu M., Tomljenovic I., Lampoltshammer T.J., Blaschke T., Hofle B., Ontology-based classification of building types detected from airborne laser scanning data., Remote Sens., 6, pp. 1347-1366, (2014); Duro D.C., Franklin S.E., Dube M.G., A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery., Remote Sens. Environ., 118, pp. 259-272, (2012); Heumann B.W., An object-based classification of mangroves using a hybrid decision tree-support vector machine approach., Remote Sens., 3, pp. 2440-2460, (2011); Fukushima K., Neocognitron: A hierarchical neural network capable of visual pattern recognition., Neural Netw., 1, pp. 119-130, (1988); Fu T., Ma L., Li M., Johnson B.A., Using convolutional neural network to identify irregular segmentation objects from very high-resolution remote sensing imagery., J. Appl. Remote Sens., 12, (2018); Zhang Q., Wang Y., Liu Q., Liu X., Wang W., CNN based suburban building detection using monocular high resolution Google Earth images., In Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 661-664, (2016); Zhu X.X., Tuia D., Mou L., Xia G.S., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources., IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36, (2017); Alom M.Z., Taha T.M., Yakopcic C., Westberg S., Sidike P., Nasrin M.S., Van Esesn B.C., Awwal A.A.S., Asari V.K., The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches., (2018); Zhou W., Newsam S., Li C., Shao Z., Learning low dimensional convolutional neural networks for high-resolution remote sensing image retrieval., Remote Sens., 9, (2017); Chen S.W., Shivakumar S.S., Dcunha S., Das J., Okon E., Qu C., Taylor C.J., Kumar V., Counting Apples and Oranges with Deep Learning: A Data-Driven Approach., IEEE Robot. Autom. Lett., 2, pp. 781-788, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks., Drones, 2, (2018); Sa I., Ge Z., Dayoub F., Upcroft B., Perez T., McCool C., Deepfruits: A fruit detection system using deep neural networks., Sensors, 16, (2016); Li W., Dong R., Fu H., Yu L., Large-scale oil palm tree detection from high-resolution satellite images using two-stage convolutional neural networks., Remote Sens., 11, (2019); Wang Z., Underwood J., Walsh K.B., Machine vision assessment of mango orchard flowering., Comput. Electron. Agric., 151, pp. 501-511, (2018); Timilsina S., Sharma S.K., Aryal J., Mapping Urban Trees Within Cadastral Parcels Using an Object-based Convolutional Neural Network., In Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 111-117, (2019); Fan C., Johnston M., Darling L., Scott L., Liao F.H., Land use and socio-economic determinants of urban forest structure and diversity., Landsc. Urban. Plan., 181, pp. 10-21, (2019); Steenberg J.W.N., Robinson P.J., Duinker P.N., A spatio-temporal analysis of the relationship between housing renovation, socioeconomic status, and urban forest ecosystems., Environ. Plan. B Urban. Anal. City Sci., 46, pp. 1115-1131, (2018); Grove J.M., Burch W.R.J., A social ecosystem approach and applications of urban ecosystem and landscape analyses: A case study of Baltimore, Maryland., Urban Ecosyst., 1, pp. 259-275, (1997); Kirkpatrick J.B., Davison A., Daniels G.D., Resident attitudes towards trees influence the planting and removal of different types of trees in eastern Australian cities., Landsc. Urban Plan., 107, pp. 147-158, (2012); Kirkpatrick J.B., Davison A., Daniels G.D., Sinners, scapegoats or fashion victims? Understanding the deaths of trees in the green city., Geoforum, 48, pp. 165-176, (2013); (2019); (2019); Bolstad P., GIS Fundamentals: A First Text on Geographic Information Systems, (2012); Yang C., A high-resolution airborne four-camera imaging system for agricultural remote sensing., Comput. Electron. Agric., 88, pp. 13-24, (2012); Bannari A., Morin D., Bonn F., A Review of Vegetation Indices., Remote Sens. Rev., 13, pp. 95-120, (1995); Dubayah R.O., Drake J.B., Lidar Remote Sensing for Forestry., J. For., 98, pp. 44-46, (2000); (2019); Ghorbanzadeh O., Blaschke T., Gholamnia K., Meena S.R., Tiede D., Aryal J., Evaluation of Different Machine Learning Methods and Deep-Learning Convolutional Neural Networks for Landslide Detection., Remote Sens., 11, (2019); Chen L.-C., Barron J.T., Papandreou G., Murphy K., Yuille A.L., Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform., In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 4545-4554, (2016); User's Guide: Data Analysis and Quality Tools, (1998); Ellis E.A., Mathews A.J., Object-based delineation of urban tree canopy: Assessing change in Oklahoma City, 2006-2013., Comput. Environ. Urban. Syst., 73, pp. 85-94, (2019); Branson S., Wegner J.D., Hall D., Lang N., Schindler K., Perona P., From Google Maps to a fine-grained catalog of street trees., ISPRS J. Photogramm. Remote Sens., 135, pp. 13-30, (2018); Ballantyne M., Pickering C.M., Differences in the impacts of formal and informal recreational trails on urban forest loss and tree structure., J. Environ. Manag., 159, pp. 94-105, (2015); Brunner J., Cozens P., Where Have All the Trees Gone? Urban Consolidation and the Demise of Urban Vegetation: A Case Study from Western Australia., Plan. Pract. Res., 28, pp. 231-255, (2013); Kaspar J., Kendal D., Sore R., Livesley S.J., Urban Forestry & Urban Greening Random point sampling to detect gain and loss in tree canopy cover in response to urban densification., Urban For. Urban Green., 24, pp. 26-34, (2017); Lin B., Meyers J., Barnett G., Understanding the potential loss and inequities of green space distribution with urban densification., Urban For. Urban Green., 14, pp. 952-958, (2015); Ossola A., Hopton M.E., Measuring urban tree loss dynamics across residential landscapes., Sci. Total Environ., 612, pp. 940-949, (2018); Pauleit S., Ennos R., Golding Y., Modeling the environmental impacts of urban land use and land cover change-A study in Merseyside, UK., Landscap. Urban Plan., 71, pp. 295-310, (2005); Potapov P.V., Turubanova S.A., Hansen M.C., Adusei B., Broich M., Altstatt A., Mane L., Justice C.O., Quantifying forest cover loss in Democratic Republic of the Congo, 2000-2010, with Landsat ETM+ data., Remote Sens. Environ., 122, pp. 106-116, (2012)","J. Aryal; Melbourne School of Engineering, University of Melbourne, Parkville, 3010, Australia; email: Jagannath.aryal@unimelb.edu.au","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092112272"
"Saqib M.A.; Aqib M.; Tahir M.N.; Hafeez Y.","Saqib, Muhammad Ali (57536657800); Aqib, Muhammad (57645901000); Tahir, Muhammad Naveed (57618935400); Hafeez, Yaser (58070344600)","57536657800; 57645901000; 57618935400; 58070344600","Towards deep learning based smart farming for intelligent weeds management in crops","2023","Frontiers in Plant Science","14","","1211235","","","","0","10.3389/fpls.2023.1211235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167585549&doi=10.3389%2ffpls.2023.1211235&partnerID=40&md5=eaac8c8cd597bc6f4fd3d16d2b61dc36","University Institute of Information Technology (UIIT), Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan; National Center of Industrial Biotechnology, Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan; Department of Agronomy, Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan; Pilot Project for Data Driven Smart Decision Platform for Increased Agriculture Productivity, Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan","Saqib M.A., University Institute of Information Technology (UIIT), Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan; Aqib M., University Institute of Information Technology (UIIT), Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan, National Center of Industrial Biotechnology, Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan; Tahir M.N., Department of Agronomy, Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan, Pilot Project for Data Driven Smart Decision Platform for Increased Agriculture Productivity, Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan; Hafeez Y., University Institute of Information Technology (UIIT), Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Punjab, Rawalpindi, Pakistan","Introduction: Deep learning (DL) is a core constituent for building an object detection system and provides a variety of algorithms to be used in a variety of applications. In agriculture, weed management is one of the major concerns, weed detection systems could be of great help to improve production. In this work, we have proposed a DL-based weed detection model that can efficiently be used for effective weed management in crops. Methods: Our proposed model uses Convolutional Neural Network based object detection system You Only Look Once (YOLO) for training and prediction. The collected dataset contains RGB images of four different weed species named Grass, Creeping Thistle, Bindweed, and California poppy. This dataset is manipulated by applying LAB (Lightness A and B) and HSV (Hue, Saturation, Value) image transformation techniques and then trained on four YOLO models (v3, v3-tiny, v4, v4-tiny). Results and discussion: The effects of image transformation are analyzed, and it is deduced that the model performance is not much affected by this transformation. Inferencing results obtained by making a comparison of correctly predicted weeds are quite promising, among all models implemented in this work, the YOLOv4 model has achieved the highest accuracy. It has correctly predicted 98.88% weeds with an average loss of 1.8 and 73.1% mean average precision value. Future work: In the future, we plan to integrate this model in a variable rate sprayer for precise weed management in real time. Copyright © 2023 Saqib, Aqib, Tahir and Hafeez.","artificial intelligence; digital agriculture; object detection; weed management; YOLO","","","","","","Higher Education Commission, Pakistan, HEC, (2230)","The authors are thankful to the Higher Education Commission (HEC), Islamabad, Pakistan to provide financial support for this study under project No 2230 “Pilot Project for Data Driven Smart Decision Platform for Increased Agriculture Productivity.” Acknowledgments ","Aqib M., Mehmood R., Alzahrani A., Katib I., Albeshri A., A deep learning model to predict vehicles occupancy on freeways for traffic management, Int. J. Comput. Sci. Netw. Secu, 18, pp. 1-8, (2018); Aqib M., Mehmood R., Alzahrani A., Katib I., Albeshri A., Altowaijri S.M., Rapid transit systems: Smarter urban planning using big data, in-memory computing, deep learning, and gpus, Sustainability, 11, pp. 27-36, (2019); Aqib M., Mehmood R., Alzahrani A., Katib I., Albeshri A., Altowaijri S.M., Smarter traffic prediction using big data, in-memory computing, deep learning and gpus, Sensors, 19, (2019); Archdeacon T.J., Correlation and regression analysis : a historian’s guide, (1994); Bakator M., Radosav D., Deep learning and medical diagnosis: A review of literature, Multimodal Technol. Interaction, 2, (2018); Balducci F., Impedovo D., Pirlo G., Machine learning applications on agricultural datasets for smart farm enhancement, Machines, 6, (2018); Bosilj P., Aptoula E., Duckett T., Cielniak G., Transfer learning between crop types for semantic segmentation of crops versus weeds in precision agriculture, J. Field Robotics, 37, pp. 7-19, (2020); Chebrolu N., Lottes P., Schaefer A., Winterhalter W., Burgard W., Stachniss C., Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields, Int. J. Robotics Res, 36, pp. 1045-1052, (2017); Dass A., Shekhawat K., Choudhary A.K., Sepat S., Rathore S.S., Mahajan G., Et al., Weed management in rice using crop competition-a review, Crop protection, 95, pp. 45-52, (2017); dos Santos Ferreira A., Matte Freitas D., Goncalves da Silva G., Pistori H., Theophilo Folhes M., Weed detection in soybean crops using ConvNets, Comput. Electron. Agric, 143, pp. 314-324, (2017); Espejo-Garcia B., Mylonas N., Athanasakos L., Fountas S., Vasilakoglou I., Towards weeds identification assistance through transfer learning, Comput. Electron. Agric, 171, (2020); Etienne A., Ahmad A., Aggarwal V., Saraswat D., Deep learning-based object detection system for identifying weeds using uas imagery, Remote Sens, 13, (2021); Fernandez-Quintanilla C., Pena J.M., Andujar D., Dorado J., Ribeiro A., Lopez-Granados F., Is the current state of the art of weed monitoring suitable for site-specific weed management in arable crops, (2018); Franco C., Pedersen S.M., Papaharalampos H., Orum J.E., The value of precision for image-based decision support in weed management, Precis. Agric, 18, pp. 366-382, (2017); Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters J.G., Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, pp. 1-12, (2020); Garcia L., Parra L., Jimenez J.M., Parra M., Lloret J., Mauri P.V., Et al., Deployment strategies of soil monitoring wsn for precision agriculture irrigation scheduling in rural areas, Sensors, 21, (2021); Gharde Y., Singh P.K., Dubey R.P., Gupta P.K., Assessment of yield and economic losses in agriculture due to weeds in India, Crop Prot, 107, pp. 12-18, (2018); Gilland B., World population and food supply: can food production keep pace with population growth in the next half-century, Food Policy, 27, pp. 47-63, (2002); Giselsson T.M., Jorgensen R.N., Jensen P.K., Dyrmann M., Midtiby H.S., A Public Image Database for Benchmark of Plant Seedling Classification Algorithms, arXiv e-prints, (2017); Guo Y., Liu Y., Oerlemans A., Lao S., Wu S., Lew M.S., Deep learning for visual understanding: A review, Neurocomputing, 187, pp. 27-48, (2016); Hague T., Tillett N.D., Wheeler H., Automated crop and weed monitoring in widely spaced cereals, Precis. Agric, 7, pp. 21-32, (2006); Hati A.J., Singh R.R., Artificial intelligence in smart farms: plant phenotyping for species recognition and health condition identification using deep learning, AI, 2, pp. 274-289, (2021); Hoang Trong V., Gwang-hyun Y., Thanh Vu D., Jin-young K., Late fusion of multimodal deep neural networks for weeds classification, Comput. Electron. Agric, 175, (2020); Jastrzebska M., Kostrzewska M., Saeid A., Conventional agrochemicals: Pros and cons, Smart Agrochemicals Sustain. Agric, pp. 1-28, (2022); Jiang H., Zhang C., Qiao Y., Zhang Z., Zhang W., Song C., CNN feature based graph convolutional network for weed and crop recognition in smart farming, Comput. Electron. Agric, 174, (2020); Jin X., Sun Y., Che J., Bagavathiannan M., Yu J., Chen Y., A novel deep learning-based method for detection of weeds in vegetables, Pest Manage. Sci, 78, pp. 1861-1869, (2022); Khalid S., Oqaibi H.M., Aqib M., Hafeez Y., Small pests detection in field crops using deep learning object detection, Sustainability, 15, (2023); Khan F., Zafar N., Tahir M.N., Aqib M., Saleem S., Haroon Z., Deep learning-based approach for weed detection in potato crops, Environ. Sci. Proc, 23, (2022); Lameski P., Zdravevski E., Trajkovik V., Kulakov A., Weed Detection dataset with RGB images taken under variable light conditions, ICT Innovations 2017: Data-Driven Innovation. 9th International Conference, 778, pp. 112-119, (2017); Le V.N.T., Ahderom S., Apopei B., Alameh K., A novel method for detecting morphologically similar crops and weeds based on the combination of contour masks and filtered Local Binary Pattern operators, GigaScience, 9, pp. 1-16, (2020); Liang M., Delahaye D., Drone fleet deployment strategy for large scale agriculture and forestry surveying, 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pp. 4495-4500, (2019); Madsen S.L., Mathiassen S.K., Dyrmann M., Laursen M.S., Paz L.C., Jorgensen R.N., Open plant phenotype database of common weeds in Denmark, Remote Sens, 12, pp. 12-46, (2020); Mavani N.R., Ali J.M., Othman S., Hussain M.A., Hashim H., Rahman N.A., Application of artificial intelligence in food industry—a guideline, Food Eng. Rev, 14, pp. 134-175, (2021); Mitra D., Emerging plant diseases: research status and challenges, Emerging Trends Plant Pathol, pp. 1-17, (2021); Mohamed E.S., Belal A., Abd-Elmabod S.K., El-Shirbeny M.A., Gad A., Zahran M.B., Smart farming for improving agricultural management, Egyptian J. Remote Sens. Space Sci, 24, pp. 971-981, (2021); Munz S., Reiser D., Approach for image-based semantic segmentation of canopy cover in PEA–OAT intercropping, Agric. (Switzerland), 10, pp. 1-12, (2020); Oerke E.C., Crop losses to pests, The Journal of Agricultural Science, 144, pp. 31-43, (2006); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Et al., DeepWeeds: A multiclass weed species image dataset for deep learning, Sci. Rep, 9, pp. 1-12, (2019); Partel V., Charan Kakarla S., Ampatzidis Y., Development and evaluation of a low-cost and smart technology for precision weed management utilizing artificial intelligence, Comput. Electron. Agric, 157, pp. 339-350, (2019); Potena C., Nardi D., Pretto A., Potena C., Nardi D., Pretto A., Et al., Intelligent autonomous systems, 14, pp. 105-121, (2017); Rajalakshmi T., Panikulam P., Sharad P.K., Nair R.R., Development of a small scale cartesian coordinate farming robot with deep learning based weed detection, J. Physics: Conf. Ser, 1969, (2021); Reginaldo L.T.R.T., Lins H.A., Sousa M.D.F., Teofilo T.M.D.S., Mendonҫa V., Silva D.V., Weed interference in carrot yield in two localized irrigation systems, Rev. Caatinga, 34, pp. 119-131, (2021); Shrestha A., Mahmood A., Review of deep learning algorithms and architectures, IEEE Access, 7, pp. 53040-53065, (2019); Sivakumar A.N.V., Li J., Scott S., Psota E., Jhala A.J., Luck J.D., Et al., Comparison of object detection and patch-based classification deep learning models on mid-to late-season weed detection in UAV imagery, Remote Sens, 12, pp. 21-36, (2020); Skovsen S., Dyrmann M., Mortensen A.K., Laursen M.S., Gislum R., Eriksen J., Et al., The grassClover image dataset for semantic and hierarchical species understanding in agriculture, Tech. Rep, (2019); Szegedy C., Wei L., Yangqing J., Pierre S., Scott R., Dragomir A., Et al., Going deeper with convolutions Christian, Population Health Manage, 18, pp. 186-191, (2015); van Dijk A.D.J., Kootstra G., Kruijer W., de Ridder D., Machine learning in plant science and plant breeding, iScience, 24, (2021); Yoo S.H., Geng H., Chiu T.L., Yu S.K., Cho D.C., Heo J., Et al., Deep learning-based decision-tree classifier for covid-19 diagnosis from chest x-ray imaging, Front. Med, 7, (2020); Yu J., Schumann A.W., Cao Z., Sharpe S.M., Boyd N.S., Weed detection in perennial ryegrass with deep learning convolutional neural network, Front. Plant Sci, 10, (2019); Zhang P., Guo Z., Ullah S., Melagraki G., Afantitis A., Lynch I., Nanotechnology and artificial intelligence to enable sustainable and precision agriculture, Nat. Plants, 7, pp. 7 7, 864-876, (2021); Zhu M., Recall, precision and average precision, 2, (2004); Zhuang J., Li X., Bagavathiannan M., Jin X., Yang J., Meng W., Et al., Evaluation of different deep convolutional neural networks for detection of broadleaf weed seedlings in wheat, Pest Manage. Sci, 78, pp. 521-529, (2022)","M. Aqib; University Institute of Information Technology (UIIT), Pir Mehr Ali Shah (PMAS), Arid Agriculture University Rawalpindi, Rawalpindi, Punjab, Pakistan; email: aqib.qazi@uaar.edu.pk","","Frontiers Media SA","","","","","","1664462X","","","","English","Front. Plant Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85167585549"
"Farjon G.; Huijun L.; Edan Y.","Farjon, Guy (57210638288); Huijun, Liu (58237906200); Edan, Yael (7004434501)","57210638288; 58237906200; 7004434501","Deep-learning-based counting methods, datasets, and applications in agriculture: a review","2023","Precision Agriculture","24","5","","1683","1711","28","0","10.1007/s11119-023-10034-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163144652&doi=10.1007%2fs11119-023-10034-8&partnerID=40&md5=bbb08b1bd0668341be09a0f534c5da34","Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; College of Metrological Technology and Engineering, China Jiliang University, Hangzhou, China","Farjon G., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; Huijun L., College of Metrological Technology and Engineering, China Jiliang University, Hangzhou, China; Edan Y., Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel","The number of objects is considered an important factor in a variety of tasks in the agricultural domain. Automated counting can improve farmers’ decisions regarding yield estimation, stress detection, disease prevention, and more. In recent years, deep learning has been increasingly applied to many agriculture-related applications, complementing conventional computer-vision algorithms for counting agricultural objects. This article reviews progress in the past decade and the state of the art for counting methods in agriculture, focusing on deep-learning methods. It presents an overview of counting algorithms, metrics, platforms and sensors, a list of all publicly available datasets, and an in-depth discussion of various deep-learning methods used for counting. Finally, it discusses open challenges in object counting using deep learning and gives a glimpse into new directions and future perspectives for counting research. The review reveals a major leap forward in object counting in agriculture in the past decade, led by the penetration of deep learning methods into counting platforms. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep learning; Precision agriculture; Visual counting","agricultural application; algorithm; artificial neural network; data set; literature review; precision agriculture; visualization","","","","","Marcus Endowment Fund; Ministry of Science, ICT and Future Planning, MSIP, (20187); Ben-Gurion University of the Negev, BGU","This research was partially supported by the Phenomics Consortium, Research Innovation Authority Grant, from the Ministry of Science Grant Number 20187 and from Ben-Gurion University of the Negev through the Agricultural, Biological and Cognitive Robotics Initiative, the Marcus Endowment Fund, and the W. Gunther Plaut Chair in Manufacturing Engineering. ","Afonso M., Fonteijn H., Fiorentin F.S., Lensink D., Mooij M., Faber N., Polder G., Wehrens R., Tomato fruit detection and counting in greenhouses using deep learning, Frontiers in Plant Science, 11, (2020); Albuquerque P.L.F., Garcia V., Junior A.D.S.O., Lewandowski T., Detweiler C., Goncalves A.B., Costa C.S., Naka M.H., Pistori H., Automatic live fingerlings counting using computer vision, Computers and Electronics in Agriculture, 167, (2019); Alharbi N., Zhou J., Wang W., Automatic counting of wheat spikes from wheat growth images, (2018); Almaazmi A., Palm trees detecting and counting from high-resolution worldview-3 satellite images in United Arab Emirates, Remote Sensing for Agriculture, Ecosystems, and Hydrology XX, International Society for Optics and Photonics, (2018); Anderson N.T., Walsh K.B., Koirala A., Wang Z., Amaral M.H., Dickinson G.R., Sinha P., Robson A.J., Estimation of fruit load in Australian mango orchards using machine vision, Agronomy, 11, (2021); Bach S., Binder A., Montavon G., Klauschen F., Muller K.R., Samek W., On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation, PLoS ONE, 10, (2015); Bao W., Lin Z., Hu G., Liang D., Huang L., Zhang X., Method for wheat ear counting based on frequency domain decomposition of msvf-isct, Information Processing in Agriculture, (2022); Barbedo J.G.A., Koenigkan L.V., Perspectives on the use of unmanned aerial systems to monitor cattle, Outlook on Agriculture, 47, pp. 214-222, (2018); Bellocchio E., Ciarfuglia T.A., Costante G., Valigi P., Weakly supervised fruit counting for yield estimation using spatial consistency, IEEE Robotics and Automation Letters, 4, pp. 2348-2355, (2019); Bellocchio E., Costante G., Cascianelli S., Fravolini M.L., Valigi P., Combining domain adaptation and spatial consistency for unseen fruits counting: A quasi-unsupervised approach, IEEE Robotics and Automation Letters, 5, pp. 1079-1086, (2020); Bhattarai U., Karkee M., A weakly-supervised approach for flower/fruit counting in apple orchards, Computers in Industry, 138, (2022); Brereton P., Kitchenham B.A., Budgen D., Turner M., Khalil M., Lessons from applying the systematic literature review process within the software engineering domain, Journal of Systems and Software, 80, pp. 571-583, (2007); Bruscolini M., Suttor B., Giustarini L., Zare M., Gaffinet B., Schumann G., Drone services for plant water-status mapping, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, pp. 8527-8530, (2021); Cao L., Xiao Z., Liao X., Yao Y., Wu K., Mu J., Li J., Pu H., Automated chicken counting in surveillance camera environments based on the point supervision algorithm: Lc-densefcn, Agriculture, 11, (2021); Chen C.H., Kung H.Y., Hwang F.J., Deep learning techniques for agronomy applications, Agronomy, 9, 3, (2019); Chen I.T., Lin H.Y., Detection, counting and maturity assessment of cherry tomatoes using multi-spectral images and machine learning techniques, In VISIGRAPP (5: VISAPP), pp. 759-766, (2020); Darwin B., Dharmaraj P., Prince S., Popescu D.E., Hemanth D.J., Recognition of bloom/yield in crop images using deep learning models for smart agriculture: A review, Agronomy, 11, (2021); David E., Serouart M., Smith D., Madec S., Velumani K., Liu S., Wang X., Pinto F., Shafiee S., Tahir I.S., Et al., Global wheat head detection 2021: An improved dataset for benchmarking wheat head detection methods, Plant Phenomics, (2021); Dhaka V.S., Meena S.V., Rani G., Sinwar D., Ijaz M.F., Wozniak M., Et al., A survey of deep convolutional neural networks applied for prediction of plant leaf diseases, Sensors, 21, (2021); Dijkstra K., Loosdrecht J., Schomaker L.R., Wiering M.A., Centroidnet: A deep neural network for joint object localization and counting, Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 585-601, (2018); Dobrescu A., Valerio Giuffrida M., Tsaftaris S.A., Leveraging multiple datasets for deep leaf counting, In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 2072-2079, (2017); Dobrescu A., Valerio Giuffrida M., Tsaftaris S.A., Understanding deep neural networks for regression in leaf counting, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops., (2019); Everingham M., Van Gool L., Williams C.K., Winn J., Zisserman A., The pascal visual object classes (voc) challenge, International Journal of Computer Vision, 88, pp. 303-338, (2010); Farjon G., Itzhaky Y., Khoroshevsky F., Bar-Hillel A., Leaf counting: Fusing network components for improved accuracy, Frontiers in Plant Science, 12, (2021); Farjon G., Krikeb O., Hillel A.B., Alchanatis V., Detection and counting of flowers on apple trees for better chemical thinning decisions, Precision Agriculture, pp. 1-19, (2019); Gao F., Fang W., Sun X., Wu Z., Zhao G., Li G., Zhang Q., A novel apple fruit detection and counting methodology based on deep learning and trunk tracking in modern orchard, Computers and Electronics in Agriculture, 197, (2022); Gao F., Fu L., Zhang X., Majeed Y., Li R., Karkee M., Zhang Q., Multi-class fruit-on-plant detection for apple in snap system using faster r-CNN, Computers and Electronics in Agriculture, 176, (2020); Gebbers R., Adamchuk V.I., Precision agriculture and food security, Science, 327, pp. 828-831, (2010); Gene-Mola J., Sanz-Cortiella R., Rosell-Polo J.R., Morros J.R., Ruiz-Hidalgo J., Vilaplana V., Gregorio E., Fuji-SfM dataset: A collection of annotated images and point clouds for fuji apple detection and location using structure-from-motion photogrammetry, Data in Brief, 30, (2020); Gene-Mola J., Vilaplana V., Rosell-Polo J.R., Morros J.R., Ruiz-Hidalgo J., Gregorio E., Kfuji RGB-ds database: Fuji apple multi-modal images for fruit detection with color, depth and range-corrected ir data, Data in Brief, 25, (2019); Gomez A.S., Aptoula E., Parsons S., Bosilj P., Deep regression versus detection for counting in robotic phenotyping, IEEE Robotics and Automation Letters, 6, pp. 2902-2907, (2021); Gutierrez S., Wendel A., Underwood J., Ground based hyperspectral imaging for extensive mango yield estimation, Computers and Electronics in Agriculture, 157, pp. 126-135, (2019); Hani N., Roy P., Isler V., Apple counting using convolutional neural networks, In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2559-2565, (2018); Hani N., Roy P., Isler V., A comparative study of fruit detection and counting methods for yield mapping in apple orchards, Journal of Field Robotics, 37, pp. 263-282, (2020); Harel B., Parmet Y., Edan Y., Maturity classification of sweet peppers using image datasets acquired in different times, Computers in Industry, 121, (2020); Harel B., van Essen R., Parmet Y., Edan Y., Viewpoint analysis for maturity classification of sweet peppers, Sensors, 20, (2020); Hassler S.C., Baysal-Gurel F., Unmanned aircraft system (UAS) technology and applications in agriculture, Agronomy, 9, (2019); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); He L., Fang W., Zhao G., Wu Z., Fu L., Li R., Majeed Y., Dhupia J., Fruit yield prediction and estimation in orchards: A state-of-the-art comprehensive review for both direct and indirect methods, Computers and Electronics in Agriculture, 195, (2022); Hemming J., Ruizendaal J., Hofstee J.W., Van Henten E.J., Fruit detectability analysis for different camera positions in sweet-pepper, Sensors, 14, pp. 6032-6044, (2014); Hobbs J., Paull R., Markowicz B., Rose G., Flowering density estimation from aerial imagery for automated pineapple flower counting, In AI for Social Good Workshop., (2020); Hollings T., Burgman M., van Andel M., Gilbert M., Robinson T., Robinson A., How do you find the green sheep? A critical review of the use of remotely sensed imagery to detect and count animals, Methods in Ecology and Evolution, 9, pp. 881-892, (2018); Hong S.J., Nam I., Kim S.Y., Kim E., Lee C.H., Ahn S., Park I.K., Kim G., Automatic pest counting from pheromone trap images using deep learning object detectors for Matsucoccus thunbergianae monitoring, Insects, 12, (2021); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Jayasinghe C., Badenhorst P., Jacobs J., Spangenberg G., Smith K., Image-based high-throughput phenotyping for the estimation of persistence of perennial ryegrass (Lolium perenne L.)—a review, Grass and Forage Science, 76, pp. 321-339, (2021); Jiang Y., Li C., Paterson A.H., Robertson J.S., Deepseedling: Deep convolutional network and Kalman filter for plant seedling detection and counting in the field, Plant Methods, 15, pp. 1-19, (2019); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Computers and Electronics in Agriculture, 147, pp. 70-90, (2018); Kendall A., Gal Y., What uncertainties do we need in Bayesian deep learning for computer vision?, In Advances in Neural Information Processing Systems, pp. 5574-5584, (2017); Kestur R., Meduri A., Narasipura O., Mangonet: A deep semantic segmentation architecture for a method to detect and count mangoes in an open orchard, Engineering Applications of Artificial Intelligence, 77, pp. 59-69, (2019); Kim D.W., Yun H.S., Jeong S.J., Kwon Y.S., Kim S.G., Lee W.S., Kim H.J., Modeling and testing of growth status for Chinese cabbage and white radish with UAV-based RGB imagery, Remote Sensing, 10, (2018); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning–method overview and review of use for fruit detection and yield estimation, Computers and Electronics in Agriculture, 162, pp. 219-234, (2019); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, (2012); Krogh Mortensen A., Skovsen S., Karstoft H., Gislum R., The oil radish growth dataset for semantic segmentation and yield estimation, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops., (2019); Kurtser P., Edan Y., Statistical models for fruit detectability: Spatial and temporal analyses of sweet peppers, Biosystems Engineering, 171, pp. 272-289, (2018); Lac L., Keresztes B., Louargant M., Donias M., Da Costa J.P., An annotated image dataset of vegetable crops at an early stage of growth for proximal sensing applications, Data in Brief, 42, (2022); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2980-2988, (2017); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft Coco: Common Objects in Context, pp. 740-755, (2014); Linker R., A procedure for estimating the number of green mature apples in night-time orchard images using light distribution and its application to yield estimation, Precision Agriculture, 18, pp. 59-75, (2017); Liu S., Zeng X., Whitty M., 3dbunch: A novel IOS-smartphone application to evaluate the number of grape berries per bunch using image analysis techniques, IEEE Access, 8, pp. 114663-114674, (2020); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, In: European Conference on Computer Vision, pp. 21-37, (2016); Maheswari P., Raja P., Apolo-Apolo O.E., Perez-Ruiz M., Intelligent fruit yield estimation for orchards using deep learning based semantic segmentation techniques—a review, Frontiers in Plant Science, 12, (2021); Malambo L., Popescu S., Ku N.W., Rooney W., Zhou T., Moore S., A deep learning semantic segmentation-based approach for field-level sorghum panicle counting, Remote Sensing, 11, (2019); Mavridou E., Vrochidou E., Papakostas G.A., Pachidis T., Kaburlasos V.G., Machine vision systems in precision agriculture for crop farming, Journal of Imaging, 5, (2019); Mokrane A., Braham A.C., Cherki B., UAV coverage path planning for supporting autonomous fruit counting systems, In 2019 International Conference on Applied Automation and Industrial Diagnostics (ICAAID, pp. 1-5, (2019); Mosley L., Pham H., Bansal Y., Hare E., Image-based sorghum head counting when you only look once., (2020); Nellithimaru A.K., Kantor G.A., Rols: Robust object-level slam for grape counting, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops., (2019); Ni X., Li C., Jiang H., Takeda F., Deep learning image segmentation and extraction of blueberry fruit traits associated with harvestability and yield, Horticulture Research, 7, (2020); Oghaz M.M.D., Razaak M., Kerdegari A., Argyriou V., Remagnino P., Scene and environment monitoring using aerial imagery and deep learning, . in 2019 15Th International Conference on Distributed Computing in Sensor Systems (DCOSS, pp. 362-369, (2019); Osco L.P., de Arruda M.D.S., Goncalves D.N., Dias A., Batistoti J., de Souza M., Gomes F.D.G., Ramos A.P.M., de Castro Jorge L.A., Liesenberg V., Et al., A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery, ISPRS Journal of Photogrammetry and Remote Sensing, 174, pp. 1-17, (2021); Osco L.P., De Arruda M.D.S., Junior J.M., Da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS Journal of Photogrammetry and Remote Sensing, 160, pp. 97-106, (2020); Palacios F., Bueno G., Salido J., Diago M.P., Hernandez I., Tardaguila J., Automated grapevine flower detection and quantification method based on computer vision and deep learning from on-the-go imaging using a mobile sensing platform under field conditions, Computers and Electronics in Agriculture, 178, (2020); Rahimzadeh M., Attar A., Detecting and counting pistachios based on deep learning, Iran Journal of Computer Science, 5, pp. 69-81, (2022); Rahnemoonfar M., Dobbs D., Yari M., Starek M.J., Discountnet: Discriminating and counting network for real-time counting and localization of sparse objects in high-resolution UAV imagery, Remote Sensing, 11, (2019); Rashid M., Bari B.S., Yusup Y., Kamaruddin M.A., Khan N., A comprehensive review of crop yield prediction using machine learning approaches with special emphasis on palm oil yield prediction, IEEE Access, 9, pp. 63406-63439, (2021); Redmon J., Farhadi A., Yolov3: An incremental improvement., (2018); Ren S., He K., Girshick R., Sun J., Faster r-CNN: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, (2015); Ringdahl O., Kurtser P., Edan Y., Strategies for selecting best approach direction for a sweet-pepper harvesting robot, In Annual Conference Towards Autonomous Robotic Systems (, pp. 516-525, (2017); Ruiz-Garcia L., Lunadei L., Barreiro P., Robla I., A review of wireless sensor technologies and applications in agriculture and food industry: State of the art and current trends, Sensors, 9, pp. 4728-4750, (2009); Sadeghi-Tehran P., Virlet N., Ampe E.M., Reyns P., Hawkesford M.J., Deepcount: in-field automatic quantification of wheat spikes using simple linear iterative clustering and deep convolutional neural networks, Frontiers in Plant Science, 10, (2019); Saleem M.H., Potgieter J., Arif K.M., Automation in agriculture by machine and deep learning techniques: A review of recent developments, Precision Agriculture, 22, pp. 2053-2091, (2021); Santoro F., Tarantino E., Figorito B., Gualano S., D'Onghia A.M., A tree counting algorithm for precision agriculture tasks, International Journal of Digital Earth, 6, pp. 94-102, (2013); Santos L., Santos F.N., Oliveira P.M., Shinde P., Deep learning applications in agriculture: A short review, Iberian Robotics Conference, pp. 139-151, (2019); Santos T.T., de Souza L.L., dos Santos A.A., Avila S., Grape detection, segmentation, and tracking using deep neural networks and three-dimensional association, Computers and Electronics in Agriculture, 170, (2020); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2014); Soares V.H.A., Ponti M.A., Goncalves R.A., Campello R.J., Cattle counting in the wild with geolocated aerial images in large pasture areas, Computers and Electronics in Agriculture, 189, (2021); Springenberg J.T., Dosovitskiy A., Brox T., Riedmiller M., Striving for simplicity: The all convolutional net., (2014); Syazwani R.W.N., Asraf H.M., Amin M.M.S., Dalila K.N., Automated image identification, detection and fruit counting of top-view pineapple crown using machine learning, Alexandria Engineering Journal, 61, pp. 1265-1276, (2022); Tan M., Pang R., Le Q.V., Efficientdet: Scalable and efficient object detection, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10781-10790, (2020); Tenorio G.L., Caarls W., Automatic visual estimation of tomato cluster maturity in plant rows, Machine Vision and Applications, 32, pp. 1-18, (2021); Tian M., Guo H., Chen H., Wang Q., Long C., Ma Y., Automated pig counting using deep learning, Computers and Electronics in Agriculture, 163, (2019); Tong P., Han P., Li S., Li N., Bu S., Li Q., Li K., Counting trees with point-wise supervised segmentation network, Engineering Applications of Artificial Intelligence, 100, (2021); Tu S., Pang J., Liu H., Zhuang N., Chen Y., Zheng C., Wan H., Xue Y., Passion fruit detection and counting based on multiple scale faster r-CNN using RGB-D images, Precision Agriculture, 21, pp. 1072-1091, (2020); Vermote E.F., Skakun S., Becker-Reshef I., Saito K., Remote sensing of coconut trees in Tonga using very high spatial resolution worldview-3 data, Remote Sensing, 12, (2020); Villacres J., Viscaino M., Delpiano J., Vougioukas S., Cheein F.A., Apple orchard production estimation using deep learning strategies: A comparison of tracking-by-detection algorithms, Computers and Electronics in Agriculture, 204, (2023); Vitzrabin E., Edan Y., Adaptive thresholding with fusion using a RGBD sensor for red sweet-pepper detection, Biosystems Engineering, 146, pp. 45-56, (2016); Vitzrabin E., Edan Y., Changing task objectives for improved sweet pepper detection for robotic harvesting, IEEE Robotics and Automation Letters, 1, pp. 578-584, (2016); Westling F., Underwood J., Bryson M., A procedure for automated tree pruning suggestion using lidar scans of fruit trees, Computers and Electronics in Agriculture, 187, (2021); Wosner O., Farjon G., Bar-Hillel A., Object detection in agricultural contexts: A multiple resolution benchmark and comparison to human, Computers and Electronics in Agriculture, 189, (2021); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network, Remote Sensing, 11, (2019); Xiong H., Cao Z., Lu H., Madec S., Liu L., Shen C., Tasselnetv2: In-field counting of wheat spikes with context-augmented local regression networks, Plant Methods, 15, (2019); Xu B., Wang W., Falzon G., Kwan P., Guo L., Chen G., Tait A., Schneider D., Automated cattle counting using mask r-CNN in quadcopter vision system, Computers and Electronics in Agriculture, 171, (2020); Yamamoto K., Guo W., Yoshioka Y., Ninomiya S., On plant detection of intact tomato fruits using image analysis and machine learning methods, Sensors, 14, pp. 12191-12206, (2014); Yang Y., Ramanan D., Articulated human detection with flexible mixtures of parts, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 12, pp. 2878-2890, (2012); Zhang C., Zhang K., Ge L., Zou K., Wang S., Zhang J., Li W., A method for organs classification and fruit counting on pomegranate trees based on multi-features fusion and support vector machine by 3d point cloud, Scientia Horticulturae, 278, (2021); Zhang L., Li W., Liu C., Zhou X., Duan Q., Automatic fish counting method using image density grading and local regression, Computers and Electronics in Agriculture, 179, (2020); Zhang Q., Liu Y., Gong C., Chen Y., Yu H., Applications of deep learning for dense scenes analysis in agriculture: A review, Sensors, 20, (2020); Zhong Y., Gao J., Lei Q., Zhou Y., A vision-based counting and recognition system for flying insects in intelligent agriculture, Sensors, 18, (2018); Zivkovic Z., Van Der Heijden F., Efficient adaptive density estimation per image pixel for the task of background subtraction, Pattern Recognition Letters, 27, pp. 773-780, (2006)","G. Farjon; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel; email: guyfar@post.bgu.ac.il","","Springer","","","","","","13852256","","PREAF","","English","Precis. Agric.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85163144652"
"Rukhovich D.I.; Koroleva P.V.; Rukhovich A.D.; Komissarov M.A.","Rukhovich, Dmitry I. (58576494100); Koroleva, Polina V. (15843738800); Rukhovich, Alexey D. (56145307100); Komissarov, Mikhail A. (56210687500)","58576494100; 15843738800; 56145307100; 56210687500","Informativeness of the Long-Term Average Spectral Characteristics of the Bare Soil Surface for the Detection of Soil Cover Degradation with the Neural Network Filtering of Remote Sensing Data","2023","Remote Sensing","15","1","124","","","","0","10.3390/rs15010124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145776089&doi=10.3390%2frs15010124&partnerID=40&md5=5eb27c9a5e8473beaa3b8d2132e86dd7","Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Ufa Institute of Biology UFRC RAS, Pr. Oktyabrya 69, Ufa, 450054, Russian Federation","Rukhovich D.I., Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Koroleva P.V., Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Rukhovich A.D., Dokuchaev Soil Science Institute, Pyzhevsky Lane 7, Moscow, 119017, Russian Federation; Komissarov M.A., Ufa Institute of Biology UFRC RAS, Pr. Oktyabrya 69, Ufa, 450054, Russian Federation","The long-term spectral characteristics of the bare soil surface (BSS) in the BLUE, GREEN, RED, NIR, SWIR1, and SWIR2 Landsat spectral bands are poorly studied. Most often, the RED and NIR spectral bands are used to analyze the spatial heterogeneity of the soil cover; in our opinion, it is outmoded and seems unreasonable. The study of multi-temporal spectral characteristics requires the processing of big remote sensing data based on artificial intelligence in the form of convolutional neural networks. The analysis of BSS belongs to the direct methods of analysis of the soil cover. Soil degradation can be detected by ground methods (field reconnaissance surveys), modeling, or digital methods, and based on the remote sensing data (RSD) analysis. Ground methods are laborious, and modeling gives indirect results. RSD analysis can be based on the principles of calculation of vegetation indices (VIs) and on the BSS identification. The calculation of VIs also provides indirect information about the soil cover through the state of vegetation. BSS analysis is a direct method for analyzing soil cover heterogeneity. In this work, the informativeness of the long-term (37 years) average spectral characteristics of the BLUE, GREEN, RED, NIR, SWIR1 and SWIR2 bands of the Landsat 4–8 satellites for detecting areas of soil degradation with recognition of the BSS using deep machine learning methods was estimated. The objects of study are the spectral characteristics of kastanozems (dark chestnut soils) in the south of Russia in the territory of the Morozovsky district of the Rostov region. Soil degradation in this area is mainly caused by erosion. The following methods were used: retrospective monitoring of soil and land cover, deep machine learning using convolutional neural networks, and cartographic analysis. Six new maps of the average long-term spectral brightness of the BSS have been obtained. The information content of the BSS for six spectral bands has been verified on the basis of ground surveys. The informativeness was determined by the percentage of coincidences of degradation facts identified during the RSD analysis, and those determined in the field. It has been established that the spectral bands line up in the following descending order of information content: RED, NIR, GREEN, BLUE, SWIR1, SWIR2. The accuracy of degradation maps by band was determined as: RED—84.6%, NIR—82.9%, GREEN—78.0%, BLUE—78.0%, SWIR1—75.5%, SWIR2—62.2%. © 2022 by the authors.","bare soil; deep machine learning; Landsat spectral bands; neural networks; soil degradation; soil water erosion","Convolution; Convolutional neural networks; Deep neural networks; Erosion; Infrared devices; Learning systems; Remote sensing; Soil moisture; Vegetation mapping; Bare soils; Deep machine learning; LANDSAT; Landsat spectral band; Machine-learning; Neural-networks; Soil degradation; Soil water; Soil water erosion; Spectral band; Water erosion; Landsat","","","","","Russian Science Foundation, RSF, (22-17-00071, FGUR-2022-0009)","The research was supported by Russian Science Foundation (project No. 22-17-00071, https://rscf.ru/project/22-17-00071/) (development of methodology for detection of soil degradation/erosion areas based on remote sensing data) and framework of state assignment No. FGUR-2022-0009 (field surveys and agrochemical analyses).","Ischenko T.A., All-Union Instruction on Soil Surveys and the Compilation of Large-Scale Soil Land Use Maps, (1973); Farifteh J., Van Der Meer F., Atzberger C., Carranza E.J.M., Quantitative analysis of salt-affected soil reflectance spectra: A comparison of two adaptive methods (PLSR and ANN), Remote Sens. Environ, 110, pp. 59-78, (2007); Higginbottom T.P., Symeonakis E., Assessing land degradation and desertification using vegetation index data: Current frameworks and future directions, Remote Sens, 6, pp. 9552-9575, (2014); Ibrahim Y.Z., Balzter H., Kaduk J., Tucker C.J., Land degradation assessment using residual trend analysis of GIMMS NDVI3g, soil moisture and rainfall in sub-Saharan west Africa from 1982 to 2012, Remote Sens, 7, pp. 5471-5494, (2015); Mendonca-Santos M.D.L., Dart R.O., Santos H.G., Coelho M.R., Berbara R.L.L., Lumbreras J.F., Digital soil mapping of topsoil organic carbon content of Rio de Janeiro state, Brazil, Digital Soil Mapping, pp. 255-266, (2010); Lozbenev N., Komissarov M., Zhidkin A., Gusarov A., Fomicheva D., Comparative assessment of digital and conventional soil mapping: A case study of the Southern Cis-Ural region, Russia, Soil Syst, 6, (2022); Glazunov G.P., Gendugov V.M., A full-scale model of wind erosion and its verification, Eurasian Soil Sci, 36, pp. 216-226, (2003); Larionov G.A., Dobrovol'skaya N.G., Krasnov S.F., Liu B.Y., The new equation for the relief factor in statistical models of water erosion, Eurasian Soil Sci, 36, pp. 1105-1113, (2003); Maltsev K.A., Yermolaev O.P., Potential soil loss from erosion on arable lands in the European part of Russia, Eurasian Soil Sci, 52, pp. 1588-1597, (2019); Sukhanovskii Y.P., Rainfall erosion model, Eurasian Soil Sci, 43, pp. 1036-1046, (2010); Shary P.A., Sharaya L.S., Mitusov A.V., Fundamental quantitative methods of land surface analysis, Geoderma, 107, pp. 1-32, (2002); Romanenkov V., Smith J., Smith P., Sirotenko O.D., Rukhovitch D.I., Romanenko I.A., Soil organic carbon dynamics of croplands in European Russia: Estimates from the “model of humus balance, Reg. Environ. Chang, 7, pp. 93-104, (2007); Rukhovich D.I., Koroleva P.V., Vilchevskaya E.V., Romanenkov V., Kolesnikova L.G., Constructing a spatially-resolved database for modelling soil organic carbon stocks of croplands in European Russia, Reg. Environ. Chang, 7, pp. 51-61, (2007); Xu H., Hu X., Guan H., Zhang B., Wang M., Chen S., Chen M., A remote sensing based method to detect soil erosion in forests, Remote. Sens, 11, (2019); Phinzi K., Ngetar N.S., Mapping soil erosion in a quaternary catchment in Eastern Cape using geographic information system and remote sensing, S. Afr. J. Geomat, 6, (2017); Eckert S., Husler F., Liniger H., Hodel E., Trend analysis of MODIS NDVI time series for detecting land degradation and regeneration in Mongolia, J. Arid. Environ, 113, pp. 16-28, (2015); Ayalew D.A., Deumlich D., Sarapatka B., Doktor D., Quantifying the sensitivity of NDVI-Based C factor estimation and potential soil erosion prediction using Spaceborne earth observation data, Remote Sens, 12, (2020); De Carvalho D.F., Durigon V.L., Antunes M.A.H., De Almeida W.S., Oliveira P.T.S., Predicting soil erosion using Rusle and NDVI time series from TM Landsat 5, Pesqui. Agropecuária Bras, 49, pp. 215-224, (2014); Yengoh G.T., Dent D., Olsson L., Tengberg A.E., Tucker C.J., Limits to the use of NDVI in land degradation assessment, Use of the Normalized Difference Vegetation Index (NDVI) to Assess Land Degradation at Multiple Scales, pp. 27-30, (2015); Khitrov N.B., Rukhovich D.I., Koroleva P.V., Kalinina N.V., Trubnikov A.V., Petukhov D.A., Kulyanitsa A.L., A study of the responsiveness of crops to fertilizers by zones of stable intra-field heterogeneity based on big satellite data analysis, Arch. Agron. Soil Sci, 66, pp. 1963-1975, (2020); Zhang Y., Walker J.P., Pauwels V.R.N., Sadeh Y., Assimilation of wheat and soil states into the APSIM-wheat crop model: A case study, Remote Sens, 14, (2022); Qi G., Chang C., Yang W., Gao P., Zhao G., Soil salinity inversion in coastal corn planting areas by the satellite-UAV-ground integration approach, Remote Sens, 13, (2021); Romano E., Bergonzoli S., Pecorella I., Bisaglia C., De Vita P., Methodology for the definition of durum wheat yield homogeneous zones by using satellite spectral indices, Remote Sens, 13, (2021); Iwahashi Y., Ye R., Kobayashi S., Yagura K., Hor S., Soben K., Homma K., Quantification of changes in rice production for 2003–2019 with MODIS LAI data in Pursat Province, Cambodia, Remote Sens, 13, (2021); Rukhovich D.I., Koroleva P.V., Rukhovich D.D., Kalinina N.V., The use of deep machine learning for the automated selection of remote sensing data for the determination of areas of arable land degradation processes distribution, Remote Sens, 13, (2021); Kulyanitsa A.L., Rukhovich D.I., Koroleva P.V., Vilchevskaya E.V., Kalinina N.V., Analysis of the informativity of big satellite precision-farming data processing for correcting large-scale soil maps, Eurasian Soil Sci, 53, pp. 1709-1725, (2020); Rukhovich D.I., Koroleva P.V., Kalinina N.V., Vilchevskaya E.V., Suleiman G.A., Chernousenko G.I., Detecting degraded arable land on the basis of remote sensing big data analysis, Eurasian Soil Sci, 54, pp. 161-175, (2021); Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., The informativeness of coefficients a and b of the soil line for the analysis of remote sensing materials, Eurasian Soil Sci, 49, pp. 831-845, (2016); Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Maps of averaged spectral deviations from soil lines and their comparison with traditional soil maps, Eurasian Soil Sci, 49, pp. 739-756, (2016); Kulyanitsa A.L., Rukhovich A.D., Rukhovich D.D., Koroleva P.V., Rukhovich D.I., Simakova M.S., The Application of the piecewise linear approximation to the spectral neighborhood of soil line for the analysis of the quality of normalization of remote sensing materials, Eurasian Soil Sci, 50, pp. 387-396, (2017); Koroleva P.V., Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Kulyanitsa A.L., Trubnikov A.V., Kalinina N.V., Simakova M.S., Location of bare soil surface and soil line on the RED–NIR spectral plane, Eurasian Soil Sci, 50, pp. 1375-1385, (2017); Koroleva P.V., Rukhovich D.I., Rukhovich A.D., Rukhovich D.D., Kulyanitsa A.L., Trubnikov A.V., Kalinina N.V., Simakova M.S., Characterization of soil types and subtypes in N-dimensional space of multitemporal (empirical) soil line, Eurasian Soil Sci, 51, pp. 1021-1033, (2018); Satellite Big Data: How It Is Changing the Face of Precision Farming; Koroleva P.V., Rukhovich D.I., Shapovalov D.A., Suleiman G.A., Dolinina E.A., Retrospective monitoring of soil waterlogging on arable land of Tambov oblast in 2018–1968, Eurasian Soil Sci, 52, pp. 834-852, (2019); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Chernousenko G.I., Vil'chevskaya E.V., Dolinina E.A., The influence of soil salinization on land use changes in Azov district of Rostov oblast, Eurasian Soil Sci, 50, pp. 276-295, (2017); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Chernousenko G.I., Vil'chevskaya E.V., Dolinina E.A., Rukhovich S.V., Methodology for comparing soil maps of different dates with the aim to reveal and describe changes in the soil cover (by the example of soil salinization monitoring), Eurasian Soil Sci, 49, pp. 145-162, (2016); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Vil'chveskaya E.V., Dolinina E.A., Rukhovich S.V., Retrospective analysis of changes in land uses on vertic soils of closed mesodepressions on the Azov plain, Eurasian Soil Sci, 48, pp. 1050-1075, (2015); Rukhovich D.I., Simakova M.S., Kulyanitsa A.L., Bryzzhev A.V., Koroleva P.V., Kalinina N.V., Vil'chevskaya E.V., Dolinina E.A., Rukhovich S.V., Impact of shelterbelts on the fragmentation of erosional networks and local soil waterlogging, Eurasian Soil Sci, 47, pp. 1086-1099, (2014); Zi Y., Xie F., Jiang Z., A cloud detection method for Landsat 8 images based on PCANet, Remote Sens, 10, (2018); Zeng X., Yang J., Deng X., An W., Li J., Cloud detection of remote sensing images on Landsat-8 by deep learning, Proceedings of the Tenth International Conference on Digital Image Processing (ICDIP 2018); Mateo-Garcia G., Gomez-Chova L., Convolutional neural networks for cloud screening: Transfer learning from Landsat-8 to Proba-V, Proceedings of the 2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 2103-2106; Shao Z., Pan Y., Diao C., Cai J., Cloud detection in remote sensing images based on multiscale features-convolutional neural network, IEEE Trans. Geosci. Remote Sens, 57, pp. 4062-4076, (2019); Goodfellow I., Bengio Y., Courville A., Deep learning, (2016); Porzi L., Bulo S.R., Colovic A., Kontschieder P., Seamless scene segmentation, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8269-8278, (2019); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 234-241, (2015); Zhou Z., Rahman Siddiquee M.M., Tajbakhsh N., Liang J., UNet++: A nested U-Net architecture for medical image segmentation, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11, (2018); Liu Y., Zhu Q., Cao F., Chen J., Lu G., High-resolution remote sensing image segmentation framework based on attention mechanism and adaptive weighting, ISPRS Int. J. Geo-Inf, 10, (2021); Zhang J., Zhu H., Wang P., Ling X., ATT squeeze U-Net: A lightweight network for forest fire detection and recognition, IEEE Access, 9, pp. 10858-10870, (2021); Sa I., Popovic M., Khanna R., Chen Z., Lottes P., Liebisch F., Nieto J., Stachniss C., Walter A., Siegwart R., WeedMap: A large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming, Remote Sens, 10, (2018); Lottes P., Behley J., Milioto A., Stachniss C., Fully convolutional networks with sequential information for robust crop and weed detection in precision farming, IEEE Robot. Autom. Lett, 3, pp. 2870-2877, (2018); Openshaw S., Geographical data mining: Key design issues, Proceedings of the 4th International Conference on GeoComputation; Hastie T.J., Tibshirani R., Friedman J.H., The Elements of Statistical Learning: Data Mining, Inference, and Prediction, (2008); Rukhovich D.I., Koroleva P.V., Rukhovich D.D., Rukhovich A.D., Recognition of the bare soil using deep machine learning methods to create maps of arable soil degradation based on the analysis of multi-temporal remote sensing data, Remote Sens, 14, (2022); Kauth R.J., Thomas G.S., The tasseled cap—A graphic description of the spectral-temporal development of agricultural crops as seen by LANDSAT, Proceedings of the Symposium on machine processing of remotely sensed data, (1976); Bajocco S., Ginaldi F., Savian F., Morelli D., Scaglione M., Fanchini D., Raparelli E., Bregaglio S.U.M., On the use of NDVI to estimate LAI in field crops: Implementing a conversion equation library, Remote Sens, 14, (2022); Dubbini M., Palumbo N., De Giglio M., Zucca F., Barbarella M., Tornato A., Sentinel-2 data and unmanned aerial system products to support crop and bare soil monitoring: Methodology based on a statistical comparison between remote sensing data with identical spectral bands, Remote Sens, 14, (2022); Lee K.-S., Cohen W.B., Kennedy R.E., Maiersperger T.K., Gower S.T., Hyperspectral versus multispectral data for estimating leaf area index in four different biomes, Remote Sens. Environ, 91, pp. 508-520, (2004); Darvishzadeh R., Atzberger C., Skidmore A.K., Abkar A.A., Leaf Area Index derivation from hyperspectral vegetation indices and the red edge position, Int. J. Remote Sens, 30, pp. 6199-6218, (2009); Bezuglova O.S., Nazarenko O.G., Ilyinskaya I.N., Land degradation dynamics in Rostov oblast, Arid Ecosyst, 10, pp. 93-97, (2020); Gaevaya E.A., Bezuglova O.S., Ilinskaya I.N., Taradin S.A., Nezhinskaya E.N., Mishchenko A.V., The experience in the implementation of adaptive-landscape systems of agriculture in Rostov Oblast, IOP Conf. Ser. Earth Environ. Sci, 629, (2021); Golosov V.N., Collins A.L., Dobrovolskaya N.G., Bazhenova O.I., Ryzhov Y.V., Sidorchuk A.Y., Soil loss on the arable lands of the forest-steppe and steppe zones of European Russia and Siberia during the period of intensive agriculture, Geoderma, 381, (2021); Gusarov A.V., Land-use/-cover changes and their effect on soil erosion and river suspended sediment load in different landscape zones of European Russia during 1970–2017, Water, 13, (2021); Litvin L.F., Kiryukhina Z.P., Krasnov S.F., Dobrovol'skaya N.G., Dynamics of agricultural soil erosion in European Russia, Eurasian Soil Sci, 50, pp. 1343-1352, (2017); Dokuchaev V.V., State Soil-Erosion Map of Russia (Asian Part), Scale 1:2,500,000, (2004); Beck H.E., Zimmermann N.E., McVicar T.R., Vergopolan N., Berg A., Wood E.F., Present and future Köppen-Geiger climate classification maps at 1–km resolution, Sci. Data, 5, pp. 180-214, (2018); Vysotskii G.N., Izbrannye Trudy (Selected Works), (1960); Selyaninov G.T., Methods of agricultural climatology, Agric. Meteorol, 22, pp. 4-20, (1930); Rukhovich D.I., Koroleva P.V., Vilchevskaya E.V., Kalinina N.V., Digital thematic cartography as a change in the available primary sources and ways of using them, Digital Soil Mapping: Theoretical and Experimental Studies, pp. 58-86, (2012); Bryzzhev A.V., Rukhovich D.I., Koroleva P.V., Kalinina N.V., Vilchevskaya E.V., Dolinina E.A., Rukhovich S.V., Organization of retrospective monitoring of the soil cover of Rostov oblast, Eurasian Soil Sci, 48, pp. 1029-1049, (2015); Shapovalov D.A., Koroleva P.V., Kalinina N.V., Rukhovich D.I., Suleiman G.A., Dolinina E.A., Differences in inventories of waterlogged territories in soil surveys of different years and in land management documents, Eurasian Soil Sci, 53, pp. 294-309, (2020); McCarty J.L., Ellicott E.A., Romanenkov V., Rukhovitch D., Koroleva P., Multi-year black carbon emissions from cropland burning in the Russian Federation, Atmos. Environ, 63, pp. 223-238, (2012); Rouse J.W., Haas R.H., Schell J.A., Deering D.W., Monitoring vegetation systems in the great plains with ERTS, Proceedings of the Third ERTS Symposium, 1, pp. 309-317, (1974); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, arXiv, (2015); Jadon S., A survey of loss functions for semantic segmentation, Proceedings of the 2020 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB), pp. 1-7; Kingma D.P., Ba J., Adam: A method for stochastic optimization, arXiv, (2014); Kohavi R., A study of cross-validation and bootstrap for accuracy estimation and model selection, Proceedings of the 14th international joint conference on Artificial intelligence-Volume 2 (IJCAI’95), pp. 1137-1143; Mullin M., Sukthankar R., Complete cross-validation for nearest neighbor classifiers, Proceedings of the Seventeenth International Conference on Machine Learning (ICML ’00), pp. 639-646; Soil Map of the Collective Farm Rodina, Morozovsky District, Rostov Region, Scale 1:25000, (1975); Arnold R., Blume H.P., Bockheim J., Boyadgiev T., Bridges E., Brinkman R., Broll G., Bronger A., Constantini E., Creutzberg D., Et al., World Reference Base for Soil Resources: IUSS Working Group WRB. FAO, (1998); (1993); Walkley A.J., Black I.A., Estimation of soil organic carbon by the chromic acid titration method, Soil Sci, 37, pp. 29-38, (1934); Egorov V.V., Classification and Diagnostics of Soils of the USSR (Russian Translations Series, 42), (1986); Vieira A.S., do Valle Junior R.F., Rodrigues V.S., da Silva Quinaia T.L., Mendes R.G., Valera C.A., Fernandes L.F.S., Pacheco F.A.L., Estimating water erosion from the brightness index of orbital images: A framework for the prognosis of degraded pastures, Sci. Total Environ, 776, (2021); Yuan Q., Shen H., Li T., Li Z., Li S., Jiang Y., Xu H., Tan W., Yang Q., Wang J., Et al., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ, 241, (2020); Cook K.L., An evaluation of the effectiveness of low-cost UAVs and structure from motion for geomorphic change detection, Geomorphology, 278, pp. 195-208, (2017); Rahmati O., Tahmasebipour N., Haghizadeh A., Pourghasemi H.R., Feizizadeh B., Evaluation of different machine learning models for predicting and mapping the susceptibility of gully erosion, Geomorphology, 298, pp. 118-137, (2017)","P.V. Koroleva; Dokuchaev Soil Science Institute, Moscow, Pyzhevsky Lane 7, 119017, Russian Federation; email: soilmap@yandex.ru","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145776089"
"Sapkota B.B.; Hu C.; Bagavathiannan M.V.","Sapkota, Bishwa B. (57199153629); Hu, Chengsong (57224534070); Bagavathiannan, Muthukumar V. (24179346400)","57199153629; 57224534070; 24179346400","Evaluating Cross-Applicability of Weed Detection Models Across Different Crops in Similar Production Environments","2022","Frontiers in Plant Science","13","","837726","","","","9","10.3389/fpls.2022.837726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130203987&doi=10.3389%2ffpls.2022.837726&partnerID=40&md5=349c38d59916181c6f508656aac1ec92","Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States; Department of Biological and Agricultural Engineering, College Station, TX, United States","Sapkota B.B., Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States; Hu C., Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States, Department of Biological and Agricultural Engineering, College Station, TX, United States; Bagavathiannan M.V., Department of Soil and Crop Sciences, Texas AM University, College Station, TX, United States","Convolutional neural networks (CNNs) have revolutionized the weed detection process with tremendous improvements in precision and accuracy. However, training these models is time-consuming and computationally demanding; thus, training weed detection models for every crop-weed environment may not be feasible. It is imperative to evaluate how a CNN-based weed detection model trained for a specific crop may perform in other crops. In this study, a CNN model was trained to detect morningglories and grasses in cotton. Assessments were made to gauge the potential of the very model in detecting the same weed species in soybean and corn under two levels of detection complexity (levels 1 and 2). Two popular object detection frameworks, YOLOv4 and Faster R-CNN, were trained to detect weeds under two schemes: Detect_Weed (detecting at weed/crop level) and Detect_Species (detecting at weed species level). In addition, the main cotton dataset was supplemented with different amounts of non-cotton crop images to see if cross-crop applicability can be improved. Both frameworks achieved reasonably high accuracy levels for the cotton test datasets under both schemes (Average Precision-AP: 0.83–0.88 and Mean Average Precision-mAP: 0.65–0.79). The same models performed differently over other crops under both frameworks (AP: 0.33–0.83 and mAP: 0.40–0.85). In particular, relatively higher accuracies were observed for soybean than for corn, and also for complexity level 1 than for level 2. Significant improvements in cross-crop applicability were further observed when additional corn and soybean images were added to the model training. These findings provide valuable insights into improving global applicability of weed detection models. Copyright © 2022 Sapkota, Hu and Bagavathiannan.","CNNs; deep learning; digital technologies; precision agriculture; precision weed control; site-specific weed management","","","","","","NRCS-CIG, (213A750013G017); USDA-Natural Resources Conservation Service-Conservation; Cotton Incorporated, (20-739)","This study was funded in part by the USDA-Natural Resources Conservation Service-Conservation Innovation Grant (NRCS-CIG) program (award #NR213A750013G017) and Cotton Incorporated (award #20-739). ","Abdalla A., Cen H., Wan L., Rashid R., Weng H., Zhou W., Et al., Fine-tuning convolutional neural network with transfer learning for semantic segmentation of ground-level oilseed rape images in a field with high weed pressure, Comput. Electron. Agric, 167, (2019); Adhikari S.P., Yang H., Kim H., Learning semantic graphics using convolutional encoder–decoder network for autonomous weeding in paddy, Front. Plant Sci, 10, (2019); Ahmad F., Qiu B., Dong X., Ma J., Huang X., Ahmed S., Et al., Effect of operational parameters of UAV sprayer on spray deposition pattern in target and off-target zones during outer field weed control application, Comput. Electron. Agric, 172, (2020); Ahmed F., Al-Mamun H.A., Bari A.S.M.H., Hossain E., Kwan P., Classification of crops and weeds from digital images: a support vector machine approach, Crop Prot, 40, pp. 98-104, (2012); Alchanatis V., Ridel L., Hetzroni A., Yaroslavsky L., Weed detection in multi-spectral images of cotton fields, Comput. Electron. Agric, 47, pp. 243-260, (2005); Aravind R., Daman M., Kariyappa B.S., (2015); Bagavathiannan M.V., Davis A.S., An ecological perspective on managing weeds during the great selection for herbicide resistance, Pest Manag. Sci, 74, pp. 2277-2286, (2018); Beckie H.J., Ashworth M.B., Flower K.C., Herbicide resistance management: recent developments and trends, Plants, 8, (2019); Berge T.W., Goldberg S., Kaspersen K., Netland J., Towards machine vision based site-specific weed management in cereals, Comput. Electron. Agric, 81, pp. 79-86, (2012); Bochkovskiy A., Wang C.-Y., Liao H.Y.M., YOLOv4: optimal speed and accuracy of object detection, (2020); Buchanan G.A., Burns E.R., Influence of weed competition on cotton, Weed Sci, 18, pp. 149-154, (1970); Czymmek V., Harders L.O., Knoll F.J., Hussmann S., (2019); Dutta A., Zisserman A., (2019); Fawakherji M., Youssef A., Bloisi D., Pretto A., Nardi D., (2019); Gai J., Tang L., Steward B.L., Automated crop plant detection based on the fusion of color and depth images for robotic weed control, J. Field Robot, 37, pp. 35-52, (2020); Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters J.G., Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, (2020); Garcia-Santillan I.D., Pajares G., On-line crop/weed discrimination through the Mahalanobis distance from images in maize fields, Biosyst. Eng, 166, pp. 28-43, (2018); Girshick R., (2015); Hu C., Sapkota B.B., Thomasson J.A., Bagavathiannan M.V., Influence of image quality and light consistency on the performance of convolutional neural networks for weed mapping, Remote Sens, 13, (2021); Jiang H., Zhang C., Qiao Y., Zhang Z., Zhang W., Song C., CNN feature based graph convolutional network for weed and crop recognition in smart farming, Comput. Electron. Agric, 174, (2020); Kargar B.A.H., Shirzadifar A.M., (2013); Lamm R.D., Slaughter D.C., Giles D.K., Precision weed control system for cotton, Transact. ASAE, 45, (2002); Le V.N.T., Ahderom S., Alameh K., Performances of the LBP based algorithm over CNN models for detecting crops and weeds with similar morphologies, Sensors, 20, (2020); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Et al., SSD: single shot multibox detector, Computer Vision – ECCV 2016, Lecture Notes in Computer Science, pp. 21-37, (2016); Liu B., Bruch R., Weed detection for selective spraying: a review, Curr. Robot. Rep, 1, pp. 19-26, (2020); Lopez-Granados F., Weed detection for site-specific weed management: mapping and real-time approaches, Weed Res, 51, pp. 1-11, (2011); Lottes P., Behley J., Chebrolu N., Milioto A., Stachniss C., (2018); Lottes P., Behley J., Chebrolu N., Milioto A., Stachniss C., Robust joint stem detection and crop-weed classification using image sequences for plant-specific treatment in precision farming, J. Field Robot, 37, pp. 20-34, (2019); Ma X., Deng X., Qi L., Jiang Y., Li H., Wang Y., Et al., Fully convolutional network for rice seedling and weed image segmentation at the seedling stage in paddy fields, PLoS One, 14, (2019); Machleb J., Peteinatos G.G., Kollenda B.L., Andujar D., Gerhards R., Sensor-based mechanical weed control: present state and prospects, Comput. Electron. Agric, 176, (2020); Martin D., Singh V., Latheef M.A., Bagavathiannan M., Spray deposition on weeds (Palmer amaranth and Morningglory) from a remotely piloted aerial application system and packpack sprayer, Drones, 4, (2020); Nave W.R., Wax L.M., Effect of weeds on soybean yield and harvesting efficiency, Weed Sci, 19, pp. 533-535, (1971); Oquab M., Bottou L., Laptev I., Sivic J., (2014); Osorio K., Puerto A., Pedraza C., Jamaica D., Rodriguez L., A deep learning approach for weed detection in lettuce crops using multispectral images, AgriEngineering, 2, pp. 471-488, (2020); Partel V., Kakarla S.C., Ampatzidis Y., Development and evaluation of a low-cost and smart technology for precision weed management utilizing artificial intelligence, Comput. Electron. Agric, 157, pp. 339-350, (2019); Redmon J., Divvala S., Girshick R., Farhadi A., (2016); Redmon J., Farhadi A., YOLOv3: an incremental improvement, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Rumpf T., Romer C., Weis M., Sokefeld M., Gerhards R., Plumer L., Sequential support vector machine classification for small-grain weed species discrimination with special regard to Cirsium arvense and Galium aparine, Comput. Electron. Agric, 80, pp. 89-96, (2012); Sabzi S., Abbaspour-Gilandeh Y., Garcia-Mateos G., A fast and accurate expert system for weed identification in potato crops using metaheuristic algorithms, Comput. Ind, 98, pp. 80-89, (2018); Sapkota B., Singh V., Neely C., Rajan N., Bagavathiannan M., Detection of Italian ryegrass in wheat and prediction of competitive interactions using remote-sensing and machine-learning techniques, Remote Sens, 12, (2020); Sharpe S.M., Schumann A.W., Boyd N.S., Goosegrass detection in strawberry and tomato using a convolutional neural network, Sci. Rep, 10, (2020); Suarez L.A., Apan A., Werth J., Detection of phenoxy herbicide dosage in cotton crops through the analysis of hyperspectral data, Int. J. Remote Sens, 38, pp. 6528-6553, (2017); Sujaritha M., Annadurai S., Satheeshkumar J., Kowshik Sharan S., Mahesh L., Weed detecting robot in sugarcane fields using fuzzy real time classifier, Comput. Electron. Agric, 134, pp. 160-171, (2017); Wu X., Xu W., Song Y., Cai M., A detection method of weed in wheat field on machine vision, Procedia Engin, 15, pp. 1998-2003, (2011); Xie S., Hu C., Bagavathiannan M., Song D., Toward robotic weed control: detection of nutsedge weed in bermudagrass turf using inaccurate and insufficient training data, IEEE Robot. Automat. Lett, 6, pp. 7365-7372, (2021); Yu J., Schumann A.W., Cao Z., Sharpe S.M., Boyd N.S., Weed detection in perennial ryegrass with deep learning convolutional neural network, Front. Plant Sci, 10, (2019)","M.V. Bagavathiannan; Department of Soil and Crop Sciences, Texas AM University, College Station, United States; email: muthu@tamu.edu","","Frontiers Media S.A.","","","","","","1664462X","","","","English","Front. Plant Sci.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130203987"
"Biffi L.J.; Mitishita E.; Liesenberg V.; Dos Santos A.A.; Gonçalves D.N.; Estrabis N.V.; Silva J.A.; Osco L.P.; Ramos A.P.M.; Centeno J.A.S.; Schimalski M.B.; Rufato L.; Neto S.L.R.; Junior J.M.; Gonçalves W.N.","Biffi, Leonardo Josoé (55942501700); Mitishita, Edson (12751886500); Liesenberg, Veraldo (15848875300); Dos Santos, Anderson Aparecido (57209615996); Gonçalves, Diogo Nunes (56797665900); Estrabis, Nayara Vasconcelos (57212376607); Silva, Jonathan de Andrade (23396702400); Osco, Lucas Prado (57196329154); Ramos, Ana Paula Marques (56198690100); Centeno, Jorge Antonio Silva (12752460700); Schimalski, Marcos Benedito (36505022100); Rufato, Leo (34573697800); Neto, Sílvio Luís Rafaeli (57194017468); Junior, José Marcato (55640064500); Gonçalves, Wesley Nunes (23396539500)","55942501700; 12751886500; 15848875300; 57209615996; 56797665900; 57212376607; 23396702400; 57196329154; 56198690100; 12752460700; 36505022100; 34573697800; 57194017468; 55640064500; 23396539500","Article atss deep learning-based approach to detect apple fruits","2021","Remote Sensing","13","1","54","1","23","22","32","10.3390/rs13010054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098759179&doi=10.3390%2frs13010054&partnerID=40&md5=f1b5fcf3d122a0872448bf68d1d4a5bb","Department of Environmental and Sanitation Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572—Limoeiro, Pres, Prudente, 19067-175, SP, Brazil; Department of Agronomy, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil","Biffi L.J., Department of Environmental and Sanitation Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil, Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Mitishita E., Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Liesenberg V., Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Dos Santos A.A., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Gonçalves D.N., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Estrabis N.V., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Silva J.A., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Osco L.P., Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572—Limoeiro, Pres, Prudente, 19067-175, SP, Brazil; Ramos A.P.M., Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572—Limoeiro, Pres, Prudente, 19067-175, SP, Brazil; Centeno J.A.S., Federal University of Paraná (UFPR), Avenida Coronel Francisco Heráclito dos Santos 210, Curitiba, 81531-990, PR, Brazil; Schimalski M.B., Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Rufato L., Department of Agronomy, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Neto S.L.R., Department of Environmental and Sanitation Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Avenida Luiz de Camões 2090, Lages, 88520-000, SC, Brazil; Junior J.M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil; Gonçalves W.N., Faculty of Computer Science, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil, Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul (UFMS), Cidade Universitária, Av. Costa e Silva-Pioneiros, Campo Grande, 79070-900, MS, Brazil","In recent years, many agriculture-related problems have been evaluated with the integration of artificial intelligence techniques and remote sensing systems. Specifically, in fruit detection problems, several recent works were developed using Deep Learning (DL) methods applied in images acquired in different acquisition levels. However, the increasing use of anti-hail plastic net cover in commercial orchards highlights the importance of terrestrial remote sensing systems. Apples are one of the most highly-challenging fruits to be detected in images, mainly because of the target occlusion problem occurrence. Additionally, the introduction of high-density apple tree orchards makes the identification of single fruits a real challenge. To support farmers to detect apple fruits efficiently, this paper presents an approach based on the Adaptive Training Sample Selection (ATSS) deep learning method applied to close-range and low-cost terrestrial RGB images. The correct identification supports apple production forecasting and gives local producers a better idea of forthcoming management practices. The main advantage of the ATSS method is that only the center point of the objects is labeled, which is much more practicable and realistic than boundingbox annotations in heavily dense fruit orchards. Additionally, we evaluated other object detection methods such as RetinaNet, Libra Regions with Convolutional Neural Network (R-CNN), Cascade RCNN, Faster R-CNN, Feature Selective Anchor-Free (FSAF), and High-Resolution Network (HRNet). The study area is a highly-dense apple orchard consisting of Fuji Suprema apple fruits (Malus domestica Borkh) located in a smallholder farm in the state of Santa Catarina (southern Brazil). A total of 398 terrestrial images were taken nearly perpendicularly in front of the trees by a professional camera, assuring both a good vertical coverage of the apple trees in terms of heights and overlapping between picture frames. After, the high-resolution RGB images were divided into several patches for helping the detection of small and/or occluded apples. A total of 3119, 840, and 2010 patches were used for training, validation, and testing, respectively. Moreover, the proposed method’s generalization capability was assessed by applying simulated image corruptions to the test set images with different severity levels, including noise, blurs, weather, and digital processing. Experiments were also conducted by varying the bounding box size (80, 100, 120, 140, 160, and 180 pixels) in the image original for the proposed approach. Our results showed that the ATSS-based method slightly outperformed all other deep learning methods, between 2.4% and 0.3%. Also, we verified that the best result was obtained with a bounding box size of 160 × 160 pixels. The proposed method was robust regarding most of the corruption, except for snow, frost, and fog weather conditions. Finally, a benchmark of the reported dataset is also generated and publicly available. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Object detection; Precision agriculture","Agricultural robots; Convolutional neural networks; Forestry; Fruits; Learning systems; Object detection; Orchards; Pixels; Precipitation (meteorology); Remote sensing; Agriculture-related; Artificial intelligence techniques; Generalization capability; Learning-based approach; Malus domestica Borkh; Management practices; Object detection method; Remote sensing system; Deep learning","","","","","Department of Environmental and Sanitation Engineering; Santa Catarina Research Foundation; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303279/2018-4, 303559/2019-5, 304052/2019-1, 307689/2013-1, 313887/2018-7, 433783/2018-4, 436863/2018-9); Fundação de Amparo à Pesquisa e Inovação do Estado de Santa Catarina, FAPESC, (2017TR1762, 2019TR816); Universidade do Estado de Santa Catarina, UDESC","Funding text 1: Funding: This research was partially funded by the Santa Catarina Research Foundation (FAPESC; 2017TR1762, 2019TR816), the Brazilian National Council for Scientific and Technological Development (CNPq; 307689/2013-1, 303279/2018-4, 433783/2018-4, 313887/2018-7, 436863/2018-9, 303559/2019-5, and 304052/2019-1) and the Coordination for the Improvement of Higher Education Personnel (CAPES; Finance Code 001). APC charges of this manuscript were covered by the Remote Sensing 2019 Outstanding Reviewer Award granted to V.L.; Funding text 2: Acknowledgments: We would like to thank the State University of Santa Catarina (UDESC), and the Department of Environmental and Sanitation Engineering, for supporting the doctoral dissertation of the first author. We would also like to thank the owner of the study site for allowing us access into the rural property, as well as for their availability and generosity. The authors also acknowledge the computational support of the Federal University of Mato Grosso do Sul (UFMS). Finally, we would like to thank the editors and two reviewers for providing very constructive concerns and suggestions. Such feedback has greatly helped us improve the quality of the manuscript.","Dian Bah M., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens, 10, pp. 1-20, (2018); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Tu S., Pang J., Liu H., Zhuang N., Chen Y., Zheng C., Wan H., Xue Y., Passion fruit detection and counting based on multiple scale faster R-CNN using RGB-D images, Precision Agric, 21, pp. 1072-1091, (2020); Chen J., Li F., Wang R., Fan Y., Raza M.A., Liu Q., Wang Z., Cheng Y., Wu X., Yang F., Yang W., Estimation of nitrogen and carbon content from soybean leaf reflectance spectra using wavelet analysis under shade stress, Comput. Electron. Agric, 156, pp. 482-489, (2019); Hasan M.M., Chopin J.P., Laga H., Miklavcic S.J., Detection and analysis of wheat spikes using Convolutional Neural Networks, Plant Methods, 14, pp. 1-13, (2018); Hunt M.L., Blackburn G.A., Carrasco L., Redhead J.W., Rowland C.S., High resolution wheat yield mapping using Sentinel-2, Remote Sens. Environ, 233, (2019); Salami E., Gallardo A., Skorobogatov G., Barrado C., On-the-fly olive tree counting using a UAS and cloud services, Remote Sens, (2019); Ball J.E., Anderson D.T., Sr C.S.C., Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community, J. Appl. Remote Sens, 11, pp. 1-54, (2017); Schmidhuber J., Deep Learning in neural networks: An overview, Neural Netw, 61, pp. 85-117, (2015); Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens, 146, pp. 124-136, (2018); Meng L., Peng Z., Zhou J., Zhang J., Lu Z., Baumann A., Du Y., Real-Time Detection of Ground Objects Based on Unmanned Aerial Vehicle Remote Sensing with Deep Learning: Application in Excavator Detection for Pipeline Safety, Remote Sens, 12, (2020); Zhang X., Han L., Han L., Zhu L., How Well Do Deep Learning-Based Methods for Land Cover Classification and Object Detection Perform on High Resolution Remote Sensing Imagery?, Remote Sens, 12, (2020); Yuan Q., Shen H., Li T., Li Z., Li S., Jiang Y., Xu H., Tan W., Yang Q., Wang J., Gao J., Zhang L., Deep learning in environmental remote sensing: Achievements and challenges, Remote Sens. Environ, 241, (2020); Chaudhuri U., Banerjee B., Bhattacharya A., Datcu M., CMIR-NET: A deep learning based model for cross-modal retrieval in remote sensing, Pattern Recognit. Lett, 131, pp. 456-462, (2020); Osco L.P., dos Santos de Arruda M., Marcato Junior J., da Silva N.B., Ramos A.P.M., Erika Akemi Saito Moryia, Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Li J., Goncalves W.N., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens, 160, pp. 97-106, (2020); Lobo Torres D., Queiroz Feitosa R., Nigri Happ P., Elena Cue La Rosa L., Marcato Junior J., Martins J., Ola Bressan P., Goncalves W.N., Liesenberg V., Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery, Sensors, 20, (2020); Zhu L., Huang L., Fan L., Huang J., Huang F., Chen J., Zhang Z., Wang Y., Landslide Susceptibility Prediction Modeling Based on Remote Sensing and a Novel Deep Learning Algorithm of a Cascade-Parallel Recurrent Neural Network, Sensors, 20, (2020); Castro W., Marcato Junior J., Polidoro C., Osco L.P., Goncalves W., Rodrigues L., Santos M., Jank L., Barrios S., Valle C., Simeao R., Carromeu C., Silveira E., Jorge L.A.d.C., Matsubara E., Deep Learning Applied to Phenotyping of Biomass in Forages with UAV-Based RGB Imagery, Sensors, 20, (2020); Lecun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Khamparia A., Singh K.M., A systematic review on deep learning architectures and applications, Expert Syst, 36, pp. 1-22, (2019); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS J. Photogramm. Remote Sens, 159, pp. 296-307, (2020); Apolo-Apolo O., Martinez-Guanter J., Egea G., Raja P., Perez-Ruiz M., Deep learning techniques for estimation of the yield and size of citrus fruits using a UAV, Eur. J. Agron, 115, (2020); Apolo-Apolo O.E., Perez-Ruiz M., Martinez-Guanter J., Valente J., A Cloud-Based Environment for Generating Yield Estimation Maps From Apple Orchards Using UAV Imagery and a Deep Learning Technique, Front. Plant Sci, 11, (2020); Veeranampalayam Sivakumar A.N., Li J., Scott S., Psota E., Jhala J., Luck J.D., Shi Y., Comparison of Object Detection and Patch-Based Classification Deep Learning Models on Mid-to Late-Season Weed Detection in UAV Imagery, Remote Sens, 12, (2020); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens, 9, (2016); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Habaragamuwa H., Ogawa Y., Suzuki T., Shiigi T., Ono M., Kondo N., Detecting greenhouse strawberries (mature and immature), using deep convolutional neural network, Eng. Agric. Environ. Food, 11, pp. 127-138, (2018); Kirk R., Cielniak G., Mangan M., L*a*b*Fruits: A rapid and robust outdoor fruit detection system combining bio-inspired features with one-stage deep learning networks, Sensors, 20, pp. 1-19, (2020); Liu X., Chen S.W., Aditya S., Sivakumar N., Dcunha S., Qu C., Taylor C.J., Das J., Kumar V., Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure from Motion, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1045-1052, (2018); Bargoti S., Underwood J.P., Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards, J. Field Robot, 34, pp. 1039-1060, (2017); Kestur R., Meduri A., Narasipura O., MangoNet: A deep semantic segmentation architecture for a method to detect and count mangoes in an open orchard, Eng. Appl. Artif. Intell, 77, pp. 59-69, (2019); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning – Method overview and review of use for fruit detection and yield estimation, Comput. Electron. Agric, 162, pp. 219-234, (2019); Dias P.A., Tabb A., Medeiros H., Apple flower detection using deep convolutional networks, Comput. Ind, 99, pp. 17-28, (2018); Wu D., Lv S., Jiang M., Song H., Using channel pruning-based YOLO v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments, Comput. Electron. Agric, 178, (2020); Jiang P., Chen Y., Liu B., He D., Liang C., Real-Time Detection of Apple Leaf Diseases Using Deep Learning Approach Based on Improved Convolutional Neural Networks, IEEE Access, 7, pp. 59069-59080, (2019); Wang D., Li C., Song H., Xiong H., Liu C., He D., Deep Learning Approach for Apple Edge Detection to Remotely Monitor Apple Growth in Orchards, IEEE Access, 8, pp. 26911-26925, (2020); Tian Y., Yang G., Wang Z., Wang H., Li E., Liang Z., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agric, 157, pp. 417-426, (2019); Kang H., Chen C., Fast implementation of real-time fruit detection in apple orchards using deep learning, Comput. Electron. Agric, 168, (2020); Gene-Mola J., Vilaplana V., Rosell-Polo J.R., Morros J.R., Ruiz-Hidalgo J., Gregorio E., Multi-modal deep learning for Fuji apple detection using RGB-D cameras and their radiometric capabilities, Comput. Electron. Agric, 162, pp. 689-698, (2019); Gao F., Fu L., Zhang X., Majeed Y., Li R., Karkee M., Zhang Q., Multi-class fruit-on-plant detection for apple in SNAP system using Faster R-CNN, Comput. Electron. Agric, 176, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, (2015); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal Loss for Dense Object Detection, (2017); Zhang S., Chi C., Yao Y., Lei Z., Li S.Z., Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition CVPR; Srivastava L.M., CHAPTER 17—Fruit Development and Ripening, Plant Growth and Development, pp. 413-429, (2002); Meszaros M., Belikova H., Conka P., Namestek J., Effect of hail nets and fertilization management on the nutritional status, growth and production of apple trees, Sci. Hortic, 255, pp. 134-144, (2019); Brglez Sever M., Tojnko S., Breznikar A., Skendrovic Babojelic M., Ivancic A., Sirk M., Unuk T., The influence of differently coloured anti-hail nets and geomorphologic characteristics on microclimatic and light conditions in apple orchards, J. Cent. Eur. Agric, 21, pp. 386-397, (2020); Bosco L.C., Bergamaschi H., Cardoso L.S., Paula V.A.d., Marodin G.A.B., Brauner P.C., Microclimate alterations caused by agricultural hail net coverage and effects on apple tree yield in subtropical climate of Southern Brazil, Bragantia, 77, pp. 181-192, (2018); Bosco L.C., Bergamaschi H., Marodin G.A., Solar radiation effects on growth, anatomy, and physiology of apple trees in a temperate climate of Brazil, Int. J. Biometeorol, pp. 1969-1980, (2020); Pang J., Chen K., Shi J., Feng H., Ouyang W., Lin D., Libra R-CNN: Towards Balanced Learning for Object Detection, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821-830; Cai Z., Vasconcelos N., Cascade R-CNN: Delving Into High Quality Object Detection, Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, pp. 6154-6162, (2018); Zhu C., He Y., Savvides M., Feature Selective Anchor-Free Module for Single-Shot Object Detection, Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 840-849; Wang J., Sun K., Cheng T., Jiang B., Deng C., Zhao Y., Liu D., Mu Y., Tan M., Wang X., Liu W., Xiao B., Deep High-Resolution Representation Learning for Visual Recognition, IEEE Trans. Pattern Anal. Mach. Intell, pp. 1-1, (2020); Alvares C.A., Stape J.L., Sentelhas P.C., de Moraes Goncalves J.L., Sparovek G., Köppen’s climate classification map for Brazil, Meteorol. Z, 22, pp. 711-728, (2013); Soil taxonomy: a basic system of soil classification for making and interpreting soil surveys, 436, (1999); dos Santos H.G., JACOMINE P.K.T., Dos Anjos L., De Oliveira V., LUMBRERAS J.F., COELHO M.R., De Almeida J., de Araujo Filho J., De Oliveira J., CUNHA T.J.F., Sistema Brasileiro de Classificação de Solos, (2018); HIDROWEB V3.1.1—Séries Históricas de Estações; Bittencourt C.C., Barone F.M., A cadeia produtiva da maçã em Santa Catarina: competitividade segundo produção e packing house, Rev. Admin. Pública, 45, pp. 1199-1222, (2011); Censo Agropecuário 2017: Resultados Definitivos, (2019); Denardi F., Kvitschal M.V.A.c., Hawerroth M.C., A brief history of the forty-five years of the Epagri apple breeding program in Brazil, Crop. Breed. Appl. Biotechnol, 19, pp. 347-355, (2019); Geosciences: Continuos Catographic Bases; Liang X., Jaakkola A., Wang Y., Hyyppa J., Honkavaara E., Liu J., Kaartinen H., The use of a hand-held camera for individual tree 3D mapping in forest sample plots, Remote Sens, 6, pp. 6587-6603, (2014); Petri J., Denardi F., SUZUKI A.E., 405-Fuji Suprema: Nova cultivar de macieira, Agropecu. Catarin. Florianópolis, 10, pp. 48-50, (1997); Dutta A., Zisserman A., The VIA Annotation Software for Images, Audio and Video, Proceedings of the 27th ACM International Conference on Multimedia (MM ’19), (2019); Lin T.Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature Pyramid Networks for Object Detection, (2017); Michaelis C., Mitzkus B., Geirhos R., Rusak E., Bringmann O., Ecker A.S., Bethge M., Brendel W., Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming, (2020); Hendrycks D., Dietterich T., Benchmarking Neural Network Robustness to Common Corruptions and Perturbations, Proceedings of the International Conference on Learning Representations, (2019); Koirala A., Walsh K.B., Wang Z., McCarthy C., Deep learning for real-time fruit detection and orchard fruit load estimation: benchmarking of ‘MangoYOLO’, Precis. Agric, 20, pp. 1107-1135, (2019); Underwood J.P., Hung C., Whelan B., Sukkarieh S., Mapping almond orchard canopy volume, flowers, fruit and yield using lidar and vision sensors, Comput. Electron. Agric, 130, pp. 83-96, (2016); Hani N., Roy P., Isler V., A comparative study of fruit detection and counting methods for yield mapping in apple orchards, J. Field Robot, 37, pp. 263-282, (2020); Fachinello J.A.C., Pasa M.d.S., Schmtiz J.D., Betemps D.A.L., Situação e perspectivas da fruticultura de clima temperado no Brasil, Rev. Bras. Frutic, 33, pp. 109-120, (2011); Schotsmans W., East A., Thorp G., Woolf A., 6—Feijoa (Acca sellowiana [Berg] Burret), Postharvest Biology and Technology of Tropical and Subtropical Fruits, pp. 115-135, (2011)","V. Liesenberg; Department of Forest Engineering, College of Agriculture and Veterinary, Santa Catarina State University (UDESC), Lages, Avenida Luiz de Camões 2090, 88520-000, Brazil; email: veraldo.liesenberg@udesc.br","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85098759179"
"Osio A.A.; Le H.-A.; Ayugi S.; Onyango F.; Odwe P.; Lefevre S.","Osio, A.A. (57218827748); Le, H.-A. (55440782700); Ayugi, S. (57668178700); Onyango, F. (57667520300); Odwe, P. (57215532945); Lefevre, S. (57203070803)","57218827748; 55440782700; 57668178700; 57667520300; 57215532945; 57203070803","Detection of degraded acacia tree species using deep neural networks on uav drone imagery","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","455","462","7","0","10.5194/isprs-Annals-V-3-2022-455-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132009450&doi=10.5194%2fisprs-Annals-V-3-2022-455-2022&partnerID=40&md5=d080e12723f4ffe04798691fc5ebcd5a","Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; IRISA, Universite Bretagne Sud (UBS), Vannes, France","Osio A.A., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Le H.-A., IRISA, Universite Bretagne Sud (UBS), Vannes, France; Ayugi S., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Onyango F., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Odwe P., Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; Lefevre S., IRISA, Universite Bretagne Sud (UBS), Vannes, France","Deep-learning-based image classification and object detection has been applied successfully to tree monitoring. However, studies of tree crowns and fallen trees, especially on flood inundated areas, remain largely unexplored. Detection of degraded tree trunks on natural environments such as water, mudflats, and natural vegetated areas is challenging due to the mixed colour image backgrounds. In this paper, Unmanned Aerial Vehicles (UAVs), or drones, with embedded RGB cameras were used to capture the fallen Acacia Xanthophloea trees from six designated plots around Lake Nakuru, Kenya. Motivated by the need to detect fallen trees around the lake, two well-established deep neural networks, i.e. Faster Region-based Convolution Neural Network (Faster R-CNN) and Retina-Net were used for fallen tree detection. A total of 7,590 annotations of three classes on 256×256 image patches were used for this study. Experimental results show the relevance of deep learning in this context, with Retina-Net model achieving 38.9% precision and 57.9% recall.  © Authors 2022.","Acacia degradation; Deep Learning; Object Detection; UAV","Aircraft detection; Antennas; Drones; Floods; Lakes; Object detection; Object recognition; Acacium degradation; Colour image; Deep learning; Fallen tree; Images classification; Mudflats; Natural environments; Objects detection; Tree crowns; Tree species; Deep neural networks","","","","","Kenya National Council for Science Technology & Innovation; Kenya Wildlife Services; Providence Health Care, PHC; National Research Fund, Kenya, NRF; Campus France","The authors acknowledge: Kenya National Research Fund (K-NRF) and Campus France through Pamoja PHC, Kenya National Council for Science Technology & Innovation (K-NACOSTI) and Kenya Wildlife Services for providing permit to enable Drone Surveys in Lake Nakuru.","Alon A. S., Festijo E. D., Juanico D. E. O., Tree detection using genus-specific retinanet from or-thophoto for segmentation access of airborne lidar data, 2019 IEEE 6th International Conference on Engineering Technologies and Applied Sciences (ICETAS), pp. 1-6, (2019); Ampatzidis Y., Partel V., Meyering B., Albrecht U., Citrus rootstock evaluation utilizing UAV-based remote sensing and artificial intelligence, Computers and Electronics in Agriculture, 164, (2019); Bisson P. A., Quinn T. P., Reeves G. H., Gregory S. V., Best management practices, cumulative effects, and long-Term trends in fish abundance in pacific northwest river systems, Watershed management, pp. 189-232, (1992); Cowden M. M., A study of the current range and habitat of fuzzy sandozi conks (Bridgeoporus nobilissimus) throughout Pacific Northwest forests, (2002); Dalponte M., Ene L. T., Gobakken T., Nasset E., Gianelle D., Predicting selected forest stand characteristics with multispectral ALS data, Remote Sensing, 10, 4, (2018); Davidson N., Dinesen L., Fennessy S., Finlayson C., Grillas P., Grobicki A., McInnes R., Stroud D., A review of the adequacy of reporting to the Ramsar Convention on change in the ecological character of wetlands, Marine and Freshwater Research, 71, 1, pp. 117-126, (2019); Galidaki G., Zianis D., Gitas I., Radoglou K., Karathanassi V., Tsakiri-Strati M., Woodhouse I., Mallinis G., Vegetation biomass estimation with remote sensing: focus on forest and other wooded land over the Mediterranean ecosystem, International Journal of Remote Sensing, 38, 7, pp. 1940-1966, (2017); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Gkioxari G., Girshick R., Malik J., Contextual action recognition with r*cnn, Proceedings of the IEEE International Conference on Computer Vvision, pp. 1080-1088, (2015); Gorelick N., Hancher M., Dixon M., Ilyushchenko S., Thau D., Moore R., Google Earth Engine: Planetary-scale geospatial analysis for everyone, Remote sensing of Environment, 202, pp. 18-27, (2017); Hagele M., Seegerer P., Lapuschkin S., Bockmayr M., Samek W., Klauschen F., Muller K.-R., Binder A., Resolving challenges in deep learning-based analyses of histopathological images using explanation methods, Scientific reports, 10, 1, pp. 1-12, (2020); Harmon M. E., Franklin J. F., Swanson F. J., Sollins P., Gregory S., Lattin J., Anderson N., Cline S., Aumen N., Sedell J., Et al., Ecology of coarse woody debris in temperate ecosystems, Advances in ecological research, 15, pp. 133-302, (1986); Iradukunda P., Sang J. K., Nyadawa M. O., Maina C.W., Sedimentation effect on the storage capacity in lake Nakuru, Kenya, Journal of Sustainable Research in Engineering, 5, 3, pp. 149-158, (2020); Jiang S., Yao W., Heurich M., Et al., Dead wood detection based on semantic segmentation of vhr aerial cir imagery using optimized fcn-densenet, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 42, pp. 127-133, (2019); Lin T.-Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision (ICCV), (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C. L., Microsoft COCO: Common objects in context, European Conference on Computer Vision, pp. 740-755, (2014); Manzanera J. A., Garc?a-Abril A., Pascual C., Tejera R., Martin-Fernandez S., Tokola T., Valbuena R., Fusion of airborne LiDAR and multispectral sensors reveals synergic capabilities in forest structure characterization, GIScience & Remote Sensing, 53, 6, pp. 723-738, (2016); Mubea K., Menz G., Monitoring land-use change in Nakuru (Kenya) using multi-sensor satellite data, (2012); Naik P., Dalponte M., Bruzzone L., Prediction of Forest Aboveground Biomass Using Multitemporal Multispectral Remote Sensing Data, Remote Sensing, 13, 7, (2021); Ngweno C. C., Mwasi S. M., Kairu J. K., Distribution, density and impact of invasive plants in Lake Nakuru National Park, Kenya, African Journal of Ecology, 48, 4, pp. 905-913, (2010); Norden B., Gotmark F., Ryberg M., Paltto H., Allmer J., Partial cutting reduces species richness of fungi on woody debris in oak-rich forests, Canadian Journal of Forest Research, 38, 7, pp. 1807-1816, (2008); Nzimande N., Mutanga O., Kiala Z., Sibanda M., Mapping the spatial distribution of the yellowwood tree (Podocarpus henkelii) in the Weza-Ngele forest using the newly launched Sentinel-2 multispectral imager data, South African Geographical Journal, 103, 2, pp. 204-222, (2021); Odada E., Raini J., Ndetei R., Experiences and lessons learned brief, Lake Nakuru, (2004); Osio A., Lefevre S., Object-Based Change Detection on Acacia Xanthophloea Species Degradation Along Lake Nakuru Riparian Reserve, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 43, pp. 347-352, (2021); Osio A., Lefevre S., Ogao P., Ayugi S., Obiabased monitoring of riparian vegetation applied to the identification of degraded acacia xanthophloea along lake nakuru, kenya, GEOBIA 2018-From pixels to ecosystems and global sustainability, pp. 18-22, (2018); Osio A., Pham M., Lefevre S., Spatial Processing of Sentinel Imagery for Monitoring of Acacia Forest Degradation in Lake Nakuru Riparian Reserve, ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, 3, pp. 525-532, (2020); Phantom D., RTK User Manual v1. 4, (2018); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-Time object detection with region proposal networks, Advances in Neural Information Processing Systems, 28, pp. 91-99, (2015); Santos A. A. d., Marcato Junior J., Araujo M. S., Di Martini D. R., Tetila E. C., Siqueira H. L., Aoki C., Eltner A., Matsubara E. T., Pistori H., Et al., Assessment of CNN-based methods for individual tree detection on images captured by RGB cameras attached to UAVs, Sensors, 19, 16, (2019); She X., Zhang L., Cen Y., Wu T., Huang C., Baig M. H. A., Comparison of the continuity of vegetation indices derived from Landsat 8 OLI and Landsat 7 ETM+ data among different vegetation types, Remote Sensing, 7, 10, pp. 13485-13506, (2015); Swanson F., Franklin J., Promoting the Science of Ecology, Ecological Applications, 2, 3, pp. 262-274, (1992); Thiel C., Mueller M. M., Epple L., Thau C., Hese S., Voltersen M., Henkel A., UAS Imagery-Based Mapping of Coarse Wood Debris in a Natural Deciduous Forest in Central Germany (Hainich National Park), Remote Sensing, 12, 20, (2020); Torres P., Rodes-Blanco M., Viana-Soto A., Nieto H., Garcia M., The Role of Remote Sensing for the Assessment and Monitoring of Forest Health: A Systematic Evidence Synthesis, Forests, 12, 8, (2021); Uijlings J., van de Sande K., Gevers T., Selective Search for Object Recognition, International Journal of Computer Vision, 104, pp. 154-171, (2013); Vareschi E., Jacobs J., The ecology of Lake Nakuru, Oecologia, 65, 3, pp. 412-424, (1985); Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., Detectron2, (2019); Zhang W., Liljedahl A. K., Kanevskiy M., Epstein H. E., Jones B. M., Jorgenson M. T., Kent K., Transferability of the deep learning mask R-CNN model for automated mapping of ice-wedge polygons in highresolution satellite and UAV images, Remote Sensing, 12, 7, (2020)","A.A. Osio; Technical University of Kenya (TUK), Faculty of Engineering & Built Environment, Nairobi, Kenya; email: osio@univ-ubs.fr","Jiang J.; Shaker A.; Zhang H.","Copernicus GmbH","","2022 24th ISPRS Congress on Imaging Today, Foreseeing Tomorrow, Commission III","6 June 2022 through 11 June 2022","Nice","179841","21949042","","","","English","ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132009450"
"Osco L.P.; dos Santos de Arruda M.; Gonçalves D.N.; Dias A.; Batistoti J.; de Souza M.; Gomes F.D.G.; Ramos A.P.M.; de Castro Jorge L.A.; Liesenberg V.; Li J.; Ma L.; Marcato J., Jr.; Gonçalves W.N.","Osco, Lucas Prado (57196329154); dos Santos de Arruda, Mauro (57221930407); Gonçalves, Diogo Nunes (56797665900); Dias, Alexandre (14025423500); Batistoti, Juliana (57211501362); de Souza, Mauricio (57211332159); Gomes, Felipe David Georges (57216938738); Ramos, Ana Paula Marques (56198690100); de Castro Jorge, Lúcio André (6503932315); Liesenberg, Veraldo (15848875300); Li, Jonathan (57235557700); Ma, Lingfei (57190372479); Marcato, José (53863759800); Gonçalves, Wesley Nunes (23396539500)","57196329154; 57221930407; 56797665900; 14025423500; 57211501362; 57211332159; 57216938738; 56198690100; 6503932315; 15848875300; 57235557700; 57190372479; 53863759800; 23396539500","A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery","2021","ISPRS Journal of Photogrammetry and Remote Sensing","174","","","1","17","16","41","10.1016/j.isprsjprs.2021.01.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100623552&doi=10.1016%2fj.isprsjprs.2021.01.024&partnerID=40&md5=407768dbca1b3c444534d94b2b26866f","Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo, R. José Bongiovanni, 700 - Cidade Universitária, Presidente Prudente, 19050-920, SP, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Av. Costa e Silva, s/n, Campo Grande, 79070-900, MS, Brazil; Faculty of Veterinary Medicine and Animal Science, Federal University of Mato Grosso do Sul, Avenida Senador Filinto Muller 2443, Campo Grande, 79074-960, MS, Brazil; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil; Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572 - Limoeiro, Pres. Prudente, 19067-175, SP, Brazil; National Research Center of Development of Agricultural Instrumentation, Brazilian Agricultural Research Agency (EMBRAPA), 13560-970, R. XV de Novembro, São Carlos, 1452, SP, Brazil; Forest Engineering Department, University of Santa Catarina State (UDESC), 88520-000, Av. Luiz de Camões, 2090, Conta Dinheiro, Lages, SC, Brazil; Department of Geography and Environmental Management, University of Waterloo, Waterloo, N2L 3G1, ON, Canada","Osco L.P., Faculty of Engineering and Architecture and Urbanism, University of Western São Paulo, R. José Bongiovanni, 700 - Cidade Universitária, Presidente Prudente, 19050-920, SP, Brazil; dos Santos de Arruda M., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Av. Costa e Silva, s/n, Campo Grande, 79070-900, MS, Brazil; Gonçalves D.N., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Av. Costa e Silva, s/n, Campo Grande, 79070-900, MS, Brazil; Dias A., Faculty of Veterinary Medicine and Animal Science, Federal University of Mato Grosso do Sul, Avenida Senador Filinto Muller 2443, Campo Grande, 79074-960, MS, Brazil; Batistoti J., Faculty of Veterinary Medicine and Animal Science, Federal University of Mato Grosso do Sul, Avenida Senador Filinto Muller 2443, Campo Grande, 79074-960, MS, Brazil; de Souza M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil; Gomes F.D.G., Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572 - Limoeiro, Pres. Prudente, 19067-175, SP, Brazil; Ramos A.P.M., Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, km 572 - Limoeiro, Pres. Prudente, 19067-175, SP, Brazil; de Castro Jorge L.A., National Research Center of Development of Agricultural Instrumentation, Brazilian Agricultural Research Agency (EMBRAPA), 13560-970, R. XV de Novembro, São Carlos, 1452, SP, Brazil; Liesenberg V., Forest Engineering Department, University of Santa Catarina State (UDESC), 88520-000, Av. Luiz de Camões, 2090, Conta Dinheiro, Lages, SC, Brazil; Li J., Department of Geography and Environmental Management, University of Waterloo, Waterloo, N2L 3G1, ON, Canada; Ma L., Department of Geography and Environmental Management, University of Waterloo, Waterloo, N2L 3G1, ON, Canada; Marcato J., Jr., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil; Gonçalves W.N., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Av. Costa e Silva, Campo Grande, 79070-900, MS, Brazil","Accurately mapping croplands is an important prerequisite for precision farming since it assists in field management, yield-prediction, and environmental management. Crops are sensitive to planting patterns and some have a limited capacity to compensate for gaps within a row. Optical imaging with sensors mounted on Unmanned Aerial Vehicles (UAV) is a cost-effective option for capturing images covering croplands nowadays. However, visual inspection of such images can be a challenging and biased task, specifically for detecting plants and rows on a one-step basis. Thus, developing an architecture capable of simultaneously extracting plant individually and plantation-rows from UAV-images is yet an important demand to support the management of agricultural systems. In this paper, we propose a novel deep learning method based on a Convolutional Neural Network (CNN) that simultaneously detects and geolocates plantation-rows while counting its plants considering highly-dense plantation configurations. The experimental setup was evaluated in (a) a cornfield (Zea mays L.) with different growth stages (i.e. recently planted and mature plants) and in a (b) Citrus orchard (Citrus Sinensis Pera). Both datasets characterize different plant density scenarios, in different locations, with different types of crops, and from different sensors and dates. This scheme was used to prove the robustness of the proposed approach, allowing a broader discussion of the method. A two-branch architecture was implemented in our CNN method, where the information obtained within the plantation-row is updated into the plant detection branch and retro-feed to the row branch; which are then refined by a Multi-Stage Refinement method. In the corn plantation datasets (with both growth phases – young and mature), our approach returned a mean absolute error (MAE) of 6.224 plants per image patch, a mean relative error (MRE) of 0.1038, precision and recall values of 0.856, and 0.905, respectively, and an F-measure equal to 0.876. These results were superior to the results from other deep networks (HRNet, Faster R-CNN, and RetinaNet) evaluated with the same task and dataset. For the plantation-row detection, our approach returned precision, recall, and F-measure scores of 0.913, 0.941, and 0.925, respectively. To test the robustness of our model with a different type of agriculture, we performed the same task in the citrus orchard dataset. It returned an MAE equal to 1.409 citrus-trees per patch, MRE of 0.0615, precision of 0.922, recall of 0.911, and F-measure of 0.965. For the citrus plantation-row detection, our approach resulted in precision, recall, and F-measure scores equal to 0.965, 0.970, and 0.964, respectively. The proposed method achieved state-of-the-art performance for counting and geolocating plants and plant-rows in UAV images from different types of crops. The method proposed here may be applied to future decision-making models and could contribute to the sustainable management of agricultural systems. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Object detection; Precision agriculture; Remote sensing; UAV imagery","Citrus; Citrus sinensis; Zea mays; Agricultural robots; Antennas; Convolutional neural networks; Cost effectiveness; Crops; Decision making; Deep learning; Environmental management; Network architecture; Orchards; Statistical tests; Sustainable development; Unmanned aerial vehicles (UAV); Agricultural system; Decision making models; Different growth stages; Mean absolute error; Mean relative error; Precision and recall; State-of-the-art performance; Sustainable management; data set; decision making; detection method; growth; image analysis; orchard; performance assessment; satellite imagery; unmanned vehicle; Learning systems","","","","","CAPES-Print, (59/300.066/2015, 59/300.095/2015, 88881.311850/2018-01); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES; Empresa Brasileira de Pesquisa Agropecuária, EMBRAPA, (23700.19/0192-9-1); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303559/2019-5, 304173/2016-9, 310517/2020-6, 313887/2018-7, 433783/2018-4); Fundação de Amparo à Pesquisa e Inovação do Estado de Santa Catarina, FAPESC, (2017TR1762); Universidade Federal de Mato Grosso do Sul, UFMS","Funding text 1: The authors acknowledge the support of UFMS (Federal University of Mato Grosso do Sul) and CAPES (Finance code 001), the donation of a Titan X and a Titan V by ©NVIDIA, the EMBRAPA (Brazilian Agricultural Research Corporation) for providing additional imagery datasets, and CAPES ( Coordination for the Improvement of Higher Education Personnel - Finance code 001);.; Funding text 2: The authors are funded by EMBRAPA (p: 23700.19/0192-9-1), National Council for Scientific and Technological Development (CNPq) (grant number 303559/2019-5, 433783/2018-4, 310517/2020-6, 313887/2018-7, and 304173/2016-9), CAPES-Print (p: 88881.311850/2018-01), and Fundect (p: 59/300.066/2015 and 59/300.095/2015). V. Liesenberg is supported by FAPESC (2017TR1762). ","Adhikari S.P., Yang H., Kim H., Learning semantic graphics using convolutional encoder–decoder network for autonomous weeding in paddy, Front. Plant Sci., 10, pp. 1-12, (2019); Aich S., Stavness I., (2018); Alshehhi R., Marpu P.R., Woon W.L., Mura M.D., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks, ISPRS J. Photogramm. Remote Sens., 130, pp. 139-149, (2017); Ampatzidis Y., Partel V., UAV-based high throughput phenotyping in citrus utilizing multispectral imaging and artificial intelligence, Remote Sens., 11, 4, pp. 410-429, (2019); An J., Li W., Li M., Cui S., Yue H., Identification and classification of maize drought stress using deep convolutional neural network, Symmetry, 11, 2, pp. 1-14, (2019); Bah M.D., Hafiane A., Canals R., CRowNet: Deep Network for Crop Row Detection in UAV Images, IEEE Access, 8, pp. 5189-5200, (2020); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Ball J.E., Anderson D.T., Chan C.S., (2017); Chen S.W., Shivakumar S.S., Dcunha S., Das J., Okon E., Qu C., Taylor C.J., Kumar V., Counting apples and oranges with deep learning: a data-driven approach, IEEE Robot. Autom. Lett., 2, 2, pp. 781-788, (2017); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks, Drones, 2, 4, pp. 39-55, (2018); Delloye C., Weiss M., Defourny P., Retrieval of the canopy chlorophyll content from Sentinel-2 spectral bands to estimate nitrogen uptake in intensive winter wheat cropping systems, Remote Sens. Environ., 216, pp. 245-261, (2018); Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens., 146, pp. 124-136, (2018); Dian Bah M., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens., 10, 11, pp. 1-20, (2018); Djerriri K., Ghabi M., Karoui M.S., Adjoudj R., (2018); Fan Z., Lu J., Gong M., Xie H., Goodman E.D., Automatic Tobacco Plant Detection in UAV Images via Deep Neural Networks, IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing, 11, 3, pp. 876-887, (2018); Freudenberg M., Nolke N., Agostini A., Urban K., Worgotter F., Kleinn C., Large scale palm tree detection in high resolution satellite images using U-Net, Remote Sens., 11, 3, pp. 1-18, (2019); Ghamisi P., Plaza J., Chen Y., Li J., Plaza A.J., Advanced Spectral Classifiers for Hyperspectral Images: A review, IEEE Geosci. Remote Sens. Mag., 5, 1, pp. 8-32, (2017); Gnadinger F., Schmidhalter U., (2017); Goldman E., Herzig R., Eisenschtat A., Ratzon O., Levi I., Goldberger J., Hassner T., (2019); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban tree species classification using a WorldView-2/3 and LiDAR data fusion approach and deep learning, Sensors, 19, 6, pp. 1-23, (2019); Hassanein M., Khedr M., El-Sheimy N., Crop row detection procedure using low-cost UAV imagery system, ISPRS Archives, 42, 2/W13, pp. 349-356, (2019); Ho Tong Minh D., Ienco D., Gaetano R., Lalande N., Ndikumana E., Osman F., Maurel P., Deep Recurrent Neural Networks for Winter Vegetation Quality Mapping via Multitemporal SAR Sentinel-1, IEEE Geosci. Remote Sensing Lett., 15, 3, pp. 464-468, (2018); Huang X., Liu X., Zhang L., (2014); Hunt E.R., Daughtry C.S.T., What good are unmanned aircraft systems for agricultural remote sensing and precision agriculture?, Int. J. Remote Sens., 39, 15-16, pp. 5345-5376, (2018); Hunt M.L., Blackburn G.A., Carrasco L., Redhead J.W., Rowland C.S., High resolution wheat yield mapping using Sentinel-2, Remote Sens. Environ., 233, (2019); Hussain M., Chen D., Cheng A., Wei H., Stanley D., Change detection from remotely sensed images: From pixel-based to object-based approaches, ISPRS J. Photogramm. Remote Sens., 80, pp. 91-106, (2013); Ioffe S., Szegedy C., 1, pp. 448-456, (2015); Jakubowski M.K., Li W., Guo Q., Kelly M., Delineating individual trees from lidar data: A comparison of vector- and raster-based segmentation approaches, Remote Sens., 5, 9, pp. 4163-4186, (2013); Jensen J.R., Introductory Digital Image Processing: A Remote Sensing Perspective, (2015); Jiang H., Chen S., Li D., Wang C., Yang J., Papaya Tree detection with UAV images using a GPU-accelerated scale-space filtering method, Remote Sens., 9, 7, pp. 721-734, (2017); Jin Z., Azzari G., You C., Di Tommaso S., Aston S., Burke M., Lobell D.B., Smallholder maize area and yield mapping at national scales with Google Earth Engine, Remote Sens. Environ., 228, pp. 115-128, (2019); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric., 147, pp. 70-90, (2018); Khamparia A., Singh K.M., A systematic review on deep learning architectures and applications, Expert Systems, 36, 3, (2019); Kitano B.T., Mendes C.C.T., Geus A.R., Oliveira H.C., Souza J.R., (2019); Krizhevsky A., (2014); Larsen M., Eriksson M., Descombes X., Perrin G., Brandtberg T., Gougeon F.A., (2011); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, 7553, pp. 436-444, (2015); Leiva J.N., Robbins J., Saraswat D., She Y., Ehsani R., Evaluating remotely sensed plant count accuracy with differing unmanned aircraft system altitudes, physical canopy separations, and ground covers, J. Appl. Remote Sens, 11, 3, (2017); Li D., Guo H., Wang C., Li W., Chen H., Zuo Z., Individual Tree Delineation in Windbreaks Using Airborne-Laser-Scanning Data and Unmanned Aerial Vehicle Stereo Images, IEEE Geosci. Remote Sensing Lett., 13, 9, pp. 1330-1334, (2016); Li W., Fu H., Yu L., Cracknell A., Deep learning based oil palm tree detection and counting for high-resolution remote sensing images, Remote Sens., 9, 1, pp. 22-35, (2017); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., (2017); Miyoshi G.T., Arruda M., (2020); Mochida K., Koda S., Inoue K., Hirayama T., Tanaka S., Nishii R., Melgani F., Computer vision-based phenotyping for improvement of plant productivity: A machine learning perspective, GigaScience, 8, 1, pp. 1-12, (2018); Mohanty S.K., Swain M.R., (2019); Ndikumana E., Minh D.H.T., Baghdadi N., Courault D., Hossard L., Deep recurrent neural network for agricultural classification using multitemporal SAR Sentinel-1 for Camargue, France. Remote Sens., 10, 8, pp. 1-16, (2018); Nevalainen O., Honkavaara E., Tuominen S., Viljanen N., Hakala T., Yu X., (2017); Oliveira H.C., Guizilini V.C., Nunes I.P., Souza J.R., Failure Detection in Row Crops From UAV Images Using Morphological Operators, IEEE Geosci. Remote Sensing Lett., 15, 7, pp. 991-995, (2018); Osco L.P., Paula A., Ramos M., Pereira D.R., (2019); Osco L.P., Ramos A.P.M., Pinheiro M.M.F.; Osco L.P., Arruda M.D.S.D., Marcato Junior J., da Silva N.B., Ramos A.P.M., Moryia E.A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E.T., Li J., Goncalves W.N., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens., 160, pp. 97-106, (2020); Ozcan A.H., Hisar D., Sayar Y., Unsalan C., Tree crown detection and delineation in satellite images using probabilistic voting, Remote Sensing Letters, 8, 8, pp. 761-770, (2017); Ozdarici-Ok A., Automatic detection and delineation of citrus trees from VHR satellite imagery, Int. J. Remote Sens., 36, 17, pp. 4275-4296, (2015); Paoletti M.E., Haut J.M., Plaza J., Plaza A., A new deep convolutional neural network for fast hyperspectral image classification, ISPRS J. Photogramm. Remote Sens., 145, pp. 120-147, (2018); Primicerio J., Caruso G., Comba L., Crisci A., Gay P., Guidoni S., Genesio L., Ricauda Aimonino D., Vaccari F.P., Individual plant definition and missing plant characterization in vineyards from high-resolution UAV imagery, Eur. J. Remote Sens., 50, 1, pp. 179-186, (2017); Quan L., Feng H., Lv Y., Wang Q.I., Zhang C., Liu J., Yuan Z., Maize seedling detection under different growth stages and complex field environments based on an improved Faster R–CNN, Biosyst. Eng., 184, pp. 1-23, (2019); Redmon J., Farhadi A., (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proc. NIPS, 28, pp. 91-99, (2015); Ribera J., Chen Y., Boomsma C., Delp E.J., Counting plants using deep learning, Prof. Global SIP, 2017, pp. 1344-1348, (2018); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Lecture Notes Comput. Sci. (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9351, pp. 234-241, (2015); Safonova A., Tabik S., Alcaraz-Segura D., Rubtsov A., Maglinets Y., Herrera F., Detection of fir trees (Abies sibirica) damaged by the bark beetle in unmanned aerial vehicle images with deep learning, Remote Sens., 11, 6, (2019); Salami E., Gallardo A., Skorobogatov G., Barrado C., On-the-fly olive tree counting using a UAS and cloud services, Remote Sens., 11, 3, pp. 316-337, (2019); Santos A.A., (2019); Simonyan K., Zisserman A., (2015); Sun J., Di L., Sun Z., Shen Y., Lai Z., County-level soybean yield prediction using deep CNN-LSTM model, Sensors, 19, 20, pp. 1-21, (2019); Sylvain J.-D., Drolet G., Brown N., Mapping dead forest cover using a deep convolutional neural network and digital aerial photography, ISPRS J. Photogramm. Remote Sens., 156, pp. 14-26, (2019); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., (2016); Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A., 2017, pp. 4278-4284, (2017); Tao S., Wu F., Guo Q., Wang Y., Li W., Xue B., Hu X., Li P., Tian D.I., Li C., Yao H., Li Y., Xu G., Fang J., Segmenting tree crowns from terrestrial and mobile LiDAR data by exploring ecological theories, ISPRS J. Photogramm. Remote Sens., 110, pp. 66-76, (2015); Varela S., Dhodda P.R., Hsu W.H., Prasad P.V.V., Assefa Y., Peralta N.R., (2018); Verma N.K., Lamb D.W., Reid N., Wilson B., Comparison of canopy volume measurements of scattered eucalypt farm trees derived from high spatial resolution imagery and LiDAR, Remote Sens., 8, 5, pp. 388-404, (2016); Wang H., Magagi R., Goita K., Trudel M., McNairn H., Powers J., Crop phenology retrieval via polarimetric SAR decomposition and Random Forest algorithm, Remote Sens. Environ., 231, (2019); Wang S., Azzari G., Lobell D.B., Crop type mapping without field-level labels: Random forest transfer and unsupervised clustering techniques, Remote Sens. Environ., 222, pp. 303-317, (2019); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks, Remote Sens., 11, 11, pp. 1309-1322, (2019); Weiss M., Jacob F., Duveiller G., Remote sensing for agricultural applications: A meta-review, Remote Sens. Environ., 236, (2020); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network, Remote Sens., 11, 6, pp. 691-710, (2019); Zhang T.Y., Suen C.Y., A fast parallel algorithm for thinning digital patterns, Commun. ACM, 27, 3, pp. 236-239, (1984); Zhang H., Li Y., Zhang Y., Shen Q., Spectral-spatial classification of hyperspectral imagery using a dual-channel convolutional neural network, Remote Sens. Lett., 8, 5, pp. 438-447, (2017); Zhao H., Shi J., Qi X., Wang X., Jia J., Pyramid scene parsing network, Proc. CVPR, pp. 2881-2890, (2017); Zhong L., Hu L., Zhou H., Deep learning based multi-temporal crop classification, Remote Sens. Environ., 221, pp. 430-443, (2019); Zhou C., Yang G., Liang D., Yang X., Xu B.O., An integrated skeleton extraction and pruning method for spatial recognition of maize seedlings in MGV and UAV remote Images, IEEE Trans. Geosci. Remote Sens., 56, 8, pp. 4618-4632, (2018)","A.P.M. Ramos; Post-Graduate Program of Environment and Regional Development, University of Western São Paulo (UNOESTE), Rodovia Raposo Tavares, Pres. Prudente, km 572 - Limoeiro, 19067-175, Brazil; email: anaramos@unoeste.br","","Elsevier B.V.","","","","","","09242716","","IRSEE","","English","ISPRS J. Photogramm. Remote Sens.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85100623552"
"Gallo I.; Rehman A.U.; Dehkordi R.H.; Landro N.; La Grassa R.; Boschetti M.","Gallo, Ignazio (7003336792); Rehman, Anwar Ur (58071783100); Dehkordi, Ramin Heidarian (57218627173); Landro, Nicola (57214364871); La Grassa, Riccardo (57204648786); Boschetti, Mirco (6701354038)","7003336792; 58071783100; 57218627173; 57214364871; 57204648786; 6701354038","Deep Object Detection of Crop Weeds: Performance of YOLOv7 on a Real Case Dataset from UAV Images","2023","Remote Sensing","15","2","539","","","","13","10.3390/rs15020539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146552598&doi=10.3390%2frs15020539&partnerID=40&md5=698f007824969bbc0ec9fed32dbc0d55","Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; Institute for Electromagnetic Sensing of the Environment, National Research Council, Milan, 20133, Italy; The Italian National Institute for Astrophysics, Rome, 00100, Italy","Gallo I., Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; Rehman A.U., Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; Dehkordi R.H., Institute for Electromagnetic Sensing of the Environment, National Research Council, Milan, 20133, Italy; Landro N., Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; La Grassa R., The Italian National Institute for Astrophysics, Rome, 00100, Italy; Boschetti M., Institute for Electromagnetic Sensing of the Environment, National Research Council, Milan, 20133, Italy","Weeds are a crucial threat to agriculture, and in order to preserve crop productivity, spreading agrochemicals is a common practice with a potential negative impact on the environment. Methods that can support intelligent application are needed. Therefore, identification and mapping is a critical step in performing site-specific weed management. Unmanned aerial vehicle (UAV) data streams are considered the best for weed detection due to the high resolution and flexibility of data acquisition and the spatial explicit dimensions of imagery. However, with the existence of unstructured crop conditions and the high biological variation of weeds, it remains a difficult challenge to generate accurate weed recognition and detection models. Two critical barriers to tackling this challenge are related to (1) a lack of case-specific, large, and comprehensive weed UAV image datasets for the crop of interest, (2) defining the most appropriate computer vision (CV) weed detection models to assess the operationality of detection approaches in real case conditions. Deep Learning (DL) algorithms, appropriately trained to deal with the real case complexity of UAV data in agriculture, can provide valid alternative solutions with respect to standard CV approaches for an accurate weed recognition model. In this framework, this paper first introduces a new weed and crop dataset named Chicory Plant (CP) and then tests state-of-the-art DL algorithms for object detection. A total of 12,113 bounding box annotations were generated to identify weed targets (Mercurialis annua) from more than 3000 RGB images of chicory plantations, collected using a UAV system at various stages of crop and weed growth. Deep weed object detection was conducted by testing the most recent You Only Look Once version 7 (YOLOv7) on both the CP and publicly available datasets (Lincoln beet (LB)), for which a previous version of YOLO was used to map weeds and crops. The YOLOv7 results obtained for the CP dataset were encouraging, outperforming the other YOLO variants by producing value metrics of 56.6%, 62.1%, and 61.3% for the mAP@0.5 scores, recall, and precision, respectively. Furthermore, the YOLOv7 model applied to the LB dataset surpassed the existing published results by increasing the mAP@0.5 scores from 51% to 61%, 67.5% to 74.1%, and 34.6% to 48% for the total mAP, mAP for weeds, and mAP for sugar beets, respectively. This study illustrates the potential of the YOLOv7 model for weed detection but remarks on the fundamental needs of large-scale, annotated weed datasets to develop and evaluate models in real-case field circumstances. © 2023 by the authors.","Convolutional Neural Network; Deep Learning; object detection; UAV imagery; YOLOv7","Agricultural chemicals; Agricultural robots; Aircraft detection; Antennas; Convolution; Convolutional neural networks; Crops; Data acquisition; Deep learning; Drones; Large dataset; Object recognition; Plants (botany); Statistical tests; Aerial vehicle; Convolutional neural network; Deep learning; Objects detection; Real case; Unmanned aerial vehicle imagery; Vehicle images; Weed detection; Weed recognition; You only look once version 7; Object detection","","","","","CNR-DIPARTIMENTO DI INGEGNERIA; Ministero dell’Istruzione, dell’Università e della Ricerca, MIUR, (ARS01_01136)","The work was supported by the project “E-crops—Technologies for Digital and Sustainable Agriculture”, funded by the Italian Ministry of University and Research (MUR) under the PON Agrifood Program (Contract ARS01_01136). Ramin Heidarian Dehkordi activity was founded by E-crops and by the CNR-DIPARTIMENTO DI INGEGNERIA, ICT E TECNOLOGIE PER L’ENERGIA E I TRASPORTI project “DIT.AD022.180 Transizione industriale e resilienza delle Società post-Covid19 (FOE 2020)”, sub task activity “Agro-Sensing”.","Young S.L., Beyond precision weed control: A model for true integration, Weed Technol, 32, pp. 7-10, (2018); Barnes E., Morgan G., Hake K., Devine J., Kurtz R., Ibendahl G., Sharda A., Rains G., Snider J., Maja J.M., Et al., Opportunities for robotic systems and automation in cotton production, AgriEngineering, 3, pp. 339-362, (2021); Pandey P., Dakshinamurthy H.N., Young S.N., Frontier: Autonomy in Detection, Actuation, and Planning for Robotic Weeding Systems, Trans. ASABE, 64, pp. 557-563, (2021); Bauer M.V., Marx C., Bauer F.V., Flury D.M., Ripken T., Streit B., Thermal weed control technologies for conservation agriculture—A review, Weed Res, 60, pp. 241-250, (2020); Kennedy H., Fennimore S.A., Slaughter D.C., Nguyen T.T., Vuong V.L., Raja R., Smith R.F., Crop signal markers facilitate crop detection and weed removal from lettuce and tomato by an intelligent cultivator, Weed Technol, 34, pp. 342-350, (2020); Van Der Weide R., Bleeker P., Achten V., Lotz L., Fogelberg F., Melander B., Innovation in mechanical weed control in crop rows, Weed Res, 48, pp. 215-224, (2008); Lamm R.D., Slaughter D.C., Giles D.K., Precision weed control system for cotton, Trans. ASAE, 45, (2002); Chostner B., See & Spray: The next generation of weed control, Resour. Mag, 24, pp. 4-5, (2017); Gerhards R., Andujar Sanchez D., Hamouz P., Peteinatos G.G., Christensen S., Fernandez-Quintanilla C., Advances in site-specific weed management in agriculture—A review, Weed Res, 62, pp. 123-133, (2022); Chen D., Lu Y., Li Z., Young S., Performance evaluation of deep transfer learning on multi-class identification of common weed species in cotton production systems, Comput. Electron. Agric, 198, (2022); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A multiclass weed species image dataset for deep learning, Sci. Rep, 9, pp. 1-12, (2019); Suh H.K., Ijsselmuiden J., Hofstee J.W., van Henten E.J., Transfer learning for the classification of sugar beet and volunteer potato under field conditions, Biosyst. Eng, 174, pp. 50-65, (2018); Espejo-Garcia B., Mylonas N., Athanasakos L., Fountas S., Vasilakoglou I., Towards weeds identification assistance through transfer learning, Comput. Electron. Agric, 171, (2020); Dyrmann M., Karstoft H., Midtiby H.S., Plant species classification using deep convolutional neural network, Biosyst. Eng, 151, pp. 72-80, (2016); Girshick R., Donahue J., Darrell T., Malik J., Region-based convolutional networks for accurate object detection and segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 38, pp. 142-158, (2015); Jiao L., Zhang F., Liu F., Yang S., Li L., Feng Z., Qu R., A survey of deep learning-based object detection, IEEE Access, 7, pp. 128837-128868, (2019); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Proceedings of the 28th International Conference on Neural Information Processing Systems; He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969; Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788; Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271; Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Bochkovskiy A., Wang C.Y., Liao H.Y.M., Yolov4: Optimal speed and accuracy of object detection, arXiv, (2020); Glenn J., What Is YOLOv5?, (2020); Li C., Li L., Jiang H., Weng K., Geng Y., Li L., Ke Z., Li Q., Cheng M., Nie W., Et al., Yolov6: A single-stage object detection framework for industrial applications, arXiv, (2022); Wang C.Y., Bochkovskiy A., Liao H.Y.M., YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, arXiv, (2022); Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters J.G., Deep convolutional neural networks for image-based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, pp. 1-12, (2020); Ahmad A., Saraswat D., Aggarwal V., Etienne A., Hancock B., Performance of deep learning models for classifying and detecting common weeds in corn and soybean production systems, Comput. Electron. Agric, 184, (2021); Sharpe S.M., Schumann A.W., Boyd N.S., Goosegrass detection in strawberry and tomato using a convolutional neural network, Sci. Rep, 10, pp. 1-8, (2020); Sun C., Shrivastava A., Singh S., Gupta A., Revisiting unreasonable effectiveness of data in deep learning era, Proceedings of the IEEE International Conference on Computer Vision, pp. 843-852; Lu Y., Young S., A survey of public datasets for computer vision tasks in precision agriculture, Comput. Electron. Agric, 178, (2020); Mylonas N., Malounas I., Mouseti S., Vali E., Espejo-Garcia B., Fountas S., Eden library: A long-term database for storing agricultural multi-sensor datasets from uav and proximal platforms, Smart Agric. Technol, 2, (2022); Wu Z., Chen Y., Zhao B., Kang X., Ding Y., Review of weed detection methods based on computer vision, Sensors, 21, (2021); Salazar-Gomez A., Darbyshire M., Gao J., Sklar E.I., Parsons S., Towards practical object detection for weed spraying in precision agriculture, arXiv, (2021); Gallo I., Rehman A.U., Dehkord R.H., Landro N., La Grassa R., Boschetti M., Weed Detection by UAV 416a Image Dataset—Chicory Crop Weed, (2022); Lottes P., Khanna R., Pfeifer J., Siegwart R., Stachniss C., UAV-based crop and weed classification for smart farming, Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 3024-3031; Gao J., Liao W., Nuyttens D., Lootens P., Vangeyte J., Pizurica A., He Y., Pieters J.G., Fusion of pixel and object-based features for weed mapping using unmanned aerial vehicle imagery, Int. J. Appl. Earth Obs. Geoinf, 67, pp. 43-53, (2018); Lottes P., Behley J., Milioto A., Stachniss C., Fully convolutional networks with sequential information for robust crop and weed detection in precision farming, IEEE Robot. Autom. Lett, 3, pp. 2870-2877, (2018); Le V.N.T., Apopei B., Alameh K., Effective plant discrimination based on the combination of local binary pattern operators and multiclass support vector machine methods, Inf. Process. Agric, 6, pp. 116-131, (2019); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Sa I., Chen Z., Popovic M., Khanna R., Liebisch F., Nieto J., Siegwart R., weednet: Dense semantic weed classification using multispectral images and mav for smart farming, IEEE Robot. Autom. Lett, 3, pp. 588-595, (2017); Jin X., Che J., Chen Y., Weed identification using deep learning and image processing in vegetable plantation, IEEE Access, 9, pp. 10940-10950, (2021); Milioto A., Lottes P., Stachniss C., Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in CNNs, Proceedings of the 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 2229-2235; Lottes P., Stachniss C., Semi-supervised online visual crop and weed classification in precision farming exploiting plant arrangement, Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5155-5161; Etienne A., Ahmad A., Aggarwal V., Saraswat D., Deep Learning-Based Object Detection System for Identifying Weeds Using UAS Imagery, Remote Sens, 13, (2021); Peteinatos G.G., Reichel P., Karouta J., Andujar D., Gerhards R., Weed identification in maize, sunflower, and potatoes with the aid of convolutional neural networks, Remote Sens, 12, (2020); Bah M.D., Hafiane A., Canals R., Deep learning with unsupervised data labeling for weed detection in line crops in UAV images, Remote Sens, 10, (2018); Di Cicco M., Potena C., Grisetti G., Pretto A., Automatic model based dataset generation for fast and accurate crop and weeds detection, Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5188-5195; Espejo-Garcia B., Mylonas N., Athanasakos L., Vali E., Fountas S., Combining generative adversarial networks and agricultural transfer learning for weeds identification, Biosyst. Eng, 204, pp. 79-89, (2021); Dwyer J., Quickly Label Training Data and Export To Any Format, (2020); Chien W., YOLOv7 Repositry with all Instruction, (2022); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Proceedings of the European Conference on Computer Vision, pp. 21-37; Chen Q., Wang Y., Yang T., Zhang X., Cheng J., Sun J., You only look one-level feature, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13039-13048; Jensen P.K., Survey of Weeds in Maize Crops in Europe, (2011)","I. Gallo; Department of Theoretical and Applied Science, University of Insubria, Varese, 20100, Italy; email: ignazio.gallo@uninsubria.it","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146552598"
"Li M.; Zhang Z.; Lei L.; Wang X.; Guo X.","Li, Min (58361218800); Zhang, Zhijie (57217729916); Lei, Liping (57203377707); Wang, Xiaofan (57208274062); Guo, Xudong (9336669100)","58361218800; 57217729916; 57203377707; 57208274062; 9336669100","Agricultural greenhouses detection in high‐resolution satellite images based on convolutional neural networks: Comparison of faster R‐CNN, YOLO v3 and SSD","2020","Sensors (Switzerland)","20","17","4938","1","14","13","68","10.3390/s20174938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090049390&doi=10.3390%2fs20174938&partnerID=40&md5=f2bce24e71989eb9b071eb0a4ac26161","Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; Key Laboratory of Land Use, Ministry of Natural Resources, China Land Surveying and Planning Institute, Beijing, 100035, China","Li M., Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; Zhang Z., Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China, College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; Lei L., Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; Wang X., Key Laboratory of Land Use, Ministry of Natural Resources, China Land Surveying and Planning Institute, Beijing, 100035, China; Guo X., Key Laboratory of Land Use, Ministry of Natural Resources, China Land Surveying and Planning Institute, Beijing, 100035, China","Agricultural greenhouses (AGs) are an important facility for the development of modern agriculture. Accurately and effectively detecting AGs is a necessity for the strategic planning of modern agriculture. With the advent of deep learning algorithms, various convolutional neural network (CNN)‐based models have been proposed for object detection with high spatial resolution images. In this paper, we conducted a comparative assessment of the three well‐established CNN‐based models, which are Faster R‐CNN, You Look Only Once‐v3 (YOLO v3), and Single Shot Multi‐Box Detector (SSD) for detecting AGs. The transfer learning and fine‐tuning approaches were implemented to train models. Accuracy and efficiency evaluation results show that YOLO v3 achieved the best performance according to the average precision (mAP), frames per second (FPS) metrics and visual inspection. The SSD demonstrated an advantage in detection speed with an FPS twice higher than Faster R‐CNN, although their mAP is close on the test set. The trained models were also applied to two independent test sets, which proved that these models have a certain transability and the higher resolution images are significant for accuracy improvement. Our study suggests YOLO v3 with superiorities in both accuracy and computational efficiency can be applied to detect AGs using high‐resolution satellite images operationally. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Agricultural greenhouse detection; Convolutional neural network; Faster R‐CNN; SSD; YOLO v3","Agricultural robots; Agriculture; Computational efficiency; Convolution; Deep learning; Efficiency; Greenhouses; Image enhancement; Learning algorithms; Learning systems; Object detection; Transfer learning; Accuracy Improvement; Agricultural greenhouse; Comparative assessment; Efficiency evaluation; Frames per seconds; High spatial resolution images; Higher resolution images; Modern agricultures; article; convolutional neural network; greenhouse; satellite imagery; transfer of learning; velocity; Convolutional neural networks","","","","","National Key Research and Development Program of China, (2016YFB0501505)","Funding: This research was financially supported by the National Key Research and Development Program of China (2016YFB0501505).","Cantliffe D.J., Protected agriculture—A regional solution for water scarcity and production of high‐value crops in the Jordan Valley, Proceedings of the Water in the Jordan Valley: Technical Solutions and Regional Cooperation Conference, (2001); Levin N., Lugassi R., Ramon U., Braun O., Ben-Dor E., Remote sensing as a tool for monitoring plasticulture in agricultural landscapes, Int. J. Remote Sens, 28, pp. 183-202, (2007); Picuno P., Innovative material and improved technical design for a sustainable exploitation of agricultural plastic film, Polym. Plast. Technol. Eng, 53, pp. 1000-1011, (2014); Picuno P., Tortora A., Capobianco R.L., Analysis of plasticulture landscapes in Southern Italy through remote sensing and solid modelling techniques, Landsc. Urban Plan, 100, pp. 45-56, (2011); Chaofan W., Jinsong D., Ke W., Ligang M., Tahmassebi A.R.S., Object‐based classification approach for greenhouse mapping using Landsat‐8 imagery, Int. J. Agric. Biol. Eng, 9, pp. 79-88, (2016); Knickel K., Changes in Farming Systems, Landscape, and Nature: Key Success Factors of Agri‐Environmental Schemes (AES), proceedings of the EUROMAB Symposium, (1999); Du X.M., Wu Z.H., Zhang Y.Q., PEI X.X., Study on changes of soil salt and nutrient in greenhouse of different planting years, J. Soil Water Conserv, 2, pp. 78-80, (2007); Hanan J.J., Holley W.D., Goldsberry K.L., Greenhouse Management, (2012); Arel I., Rose D.C., Karnowski T.P., Deep machine learning‐a new frontier in artificial intelligence research [research frontier], IEEE Comput. Intel. Mag, 5, pp. 13-18, (2010); Bishop C.M., Pattern Recognition and Machine Learning, (2007); Ma Y., Wu H., Wang L., Huang B., Ranjan R., Zomaya A., Jie W., Remote sensing big data computing: Challenges and opportunities, Future Gener. Comput. Sys, 51, pp. 47-60, (2015); Benediktsson J.A., Chanussot J., Moon W.M., Very High‐Resolution Remote Sensing: Challenges and Opportunities, Proc. IEEE, 100, pp. 1907-1910, (2012); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Li H., Deep learning for natural language processing: advantages and challenges, Natl. Sci. Rev, 5, pp. 24-26, (2017); Otter D.W., Medina J.R., Kalita J.K., A survey of the usages of deep learning for natural language processing, IEEE Trans. Neural Netw. Learn. Syst, pp. 1-21, (2020); Brunetti A., Buongiorno D., Trotta G.F., Bevilacqua V., Computer vision and deep learning techniques for pedestrian detection and tracking: A survey, Neurocomputing, 300, pp. 17-33, (2018); Silver D., Huang A., Maddison C.J., Guez A., Sifre L., Van Den Driessche G., Dieleman S., Mastering the game of Go with deep neural networks and tree search, Nature, 529, pp. 484-489, (2016); Chen Y., Fan R., Yang X., Wang J., Latif A., Extraction of urban water bodies from high‐resolution remote‐sensing imagery using deep learning, Water, 10, (2018); Gao L., Song W., Dai J., Chen Y., Road extraction from high‐resolution remote sensing imagery using refined deep residual convolutional neural network, Remote Sens, 11, (2019); Alshehhi R., Marpu P.R., Woon W.L., Dalla Mura M., Simultaneous extraction of roads and buildings in remote sensing imagery with convolutional neural networks, ISPRS J. Photogramm. Remote Sens, 130, pp. 139-149, (2017); Kamel A., Sheng B., Yang P., Li P., Shen R., Feng D.D., Deep convolutional neural networks for human action recognition using depth maps and postures, IEEE Trans. Sys. Man Cybern. Syst, 49, pp. 1806-1819, (2018); Xu Y., Xie Z., Feng Y., Chen Z., Road extraction from high‐resolution remote sensing imagery using deep learning, Remote Sens, 10, (2018); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional neural networks for large‐scale remote‐sensing image classification, IEEE Trans. Geosci. Remote Sens, 55, pp. 645-657, (2016); Pan X., Zhao J., High‐resolution remote sensing image classification method based on convolutional neural network and restricted conditional random field, Remote Sens, 10, (2018); Wang Y., Wang C., Zhang H., Dong Y., Wei S., Automatic Ship Detection Based on RetinaNet Using Multi‐Resolution Gaofen‐3 Imagery, Remote Sens, 11, (2019); Chen C., Gong W., Chen Y., Li W., Learning a two‐stage CNN model for multi‐sized building detection in remote sensing images, Remote Sens. Lett, 10, pp. 103-110, (2019); Koga Y., Miyazaki H., Shibasaki R., A Method for Vehicle Detection in High‐Resolution Satellite Images that Uses a Region‐Based Object Detector and Unsupervised Domain Adaptation, Remote Sens, 12, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2014); Girshick R., Fast r‐cnn, Proceedings of the IEEE International Conference on Computer Vision, (2015); Ren S., He K., Girshick R., Sun J., Faster r‐cnn: Towards real‐time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., Ssd: Single shot multibox detector, Proceedings of the European Conference on Computer Vision—ECCV2016, (2016); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real‐time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2016); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement; Everingham M., Van Gool L., Williams C.K., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, Int. J. Comput. Vis, 88, pp. 303-338, (2010); Deng J., Dong W., Socher R., Li L.J., Li K., Fei-Fei L., ImageNet: A large‐scale hierarchical image database, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255; Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Zitnick C.L., Microsoft COCO: Common Objects in Context, Lecture Notes in Computer Science, Proceedings of the European Conference on Computer Vision—ECCV2014, 8693, pp. 740-755, (2014); Cheng G., Zhou P., Han J., Learning rotation‐invariant convolutional neural networks for object detection in VHR optical remote sensing images, IEEE Trans. Geosci. Remote Sens, 54, pp. 7405-7415, (2016); Guo W., Yang W., Zhang H., Hua G., Geospatial object detection in high resolution satellite images based on multi‐scale convolutional neural network, Remote Sens, 10, (2018); Zhang S., Wu R., Xu K., Wang J., Sun W., R‐CNN‐Based Ship Detection from High Resolution Remote Sensing Imagery, Remote Sens, 11, (2019); Chen Z., Zhang T., Ouyang C., End‐to‐end airplane detection using transfer learning in remote sensing images, Remote Sens, 10, (2018); Ma H., Liu Y., Ren Y., Yu J., Detection of Collapsed Buildings in Post‐Earthquake Remote Sensing Images Based on the Improved YOLOv3, Remote Sens, 12, (2019); Paisitkriangkrai S., Sherrah J., Janney P., Hengel V.D., Effective semantic pixel labelling with convolutional networks and Conditional Random Fields, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 36-43; Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large‐Scale Image Recognition, (2014); West J., Ventura D., Warnick S., Spring Research Presentation: A Theoretical Foundation for Inductive Transfer, 1, (2007); Bochkovskiy A., Wang C.Y., Liao H.Y.M., YOLOv4: Optimal Speed and Accuracy of Object Detection, (2020)","Z. Zhang; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; email: zhangzj2018@radi.ac.cn; Z. Zhang; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, 100190, China; email: zhangzj2018@radi.ac.cn","","MDPI AG","","","","","","14248220","","","32878345","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090049390"
"Liu J.; Xiang J.; Jin Y.; Liu R.; Yan J.; Wang L.","Liu, Jia (56066228900); Xiang, Jianjian (57223242989); Jin, Yongjun (57358093300); Liu, Renhua (57223242856); Yan, Jining (55560283600); Wang, Lizhe (23029267900)","56066228900; 57223242989; 57358093300; 57223242856; 55560283600; 23029267900","Boost precision agriculture with unmanned aerial vehicle remote sensing and edge intelligence: A survey","2021","Remote Sensing","13","21","4387","","","","43","10.3390/rs13214387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120337164&doi=10.3390%2frs13214387&partnerID=40&md5=eb88473d904ad8374bc374d7c4012997","School of Computer Science, China University of Geosciences, Wuhan, 430078, China","Liu J., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Xiang J., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Jin Y., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Liu R., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Yan J., School of Computer Science, China University of Geosciences, Wuhan, 430078, China; Wang L., School of Computer Science, China University of Geosciences, Wuhan, 430078, China","In recent years unmanned aerial vehicles (UAVs) have emerged as a popular and costeffective technology to capture high spatial and temporal resolution remote sensing (RS) images for a wide range of precision agriculture applications, which can help reduce costs and environmental impacts by providing detailed agricultural information to optimize field practices. Furthermore, deep learning (DL) has been successfully applied in agricultural applications such as weed detection, crop pest and disease detection, etc. as an intelligent tool. However, most DL-based methods place high computation, memory and network demands on resources. Cloud computing can increase processing efficiency with high scalability and low cost, but results in high latency and great pressure on the network bandwidth. The emerging of edge intelligence, although still in the early stages, provides a promising solution for artificial intelligence (AI) applications on intelligent edge devices at the edge of the network close to data sources. These devices are with built-in processors enabling onboard analytics or AI (e.g., UAVs and Internet of Things gateways). Therefore, in this paper, a comprehensive survey on the latest developments of precision agriculture with UAV RS and edge intelligence is conducted for the first time. The major insights observed are as follows: (a) in terms of UAV systems, small or light, fixed-wing or industrial rotor-wing UAVs are widely used in precision agriculture; (b) sensors on UAVs can provide multi-source datasets, and there are only a few public UAV dataset for intelligent precision agriculture, mainly from RGB sensors and a few from multispectral and hyperspectral sensors; (c) DL-based UAV RS methods can be categorized into classification, object detection and segmentation tasks, and convolutional neural network and recurrent neural network are the mostly common used network architectures; (d) cloud computing is a common solution to UAV RS data processing, while edge computing brings the computing close to data sources; (e) edge intelligence is the convergence of artificial intelligence and edge computing, in which model compression especially parameter pruning and quantization is the most important and widely used technique at present, and typical edge resources include central processing units, graphics processing units and field programmable gate arrays. © 2021 by the authors.","Deep learning; Edge intelligence; High performance; Mobile devices; Model compression; Precision agriculture; Remote sensing; Unmanned aerial vehicles","Aircraft detection; Antennas; Classification (of information); Convolution; Convolutional neural networks; Data handling; Drones; Fixed wings; Graphics processing unit; Network architecture; Object detection; Recurrent neural networks; Remote sensing; Surveys; Cloud-computing; Data-source; Deep learning; Edge computing; Edge intelligence; High performance; Model compression; Performance; Precision Agriculture; Remote-sensing; Edge computing","","","","","National Natural Science Foundation of China, NSFC, (41901376, 42172333); China University of Geosciences, Wuhan, CUG; Fundamental Research Funds for the Central Universities","This research was funded in part by the National Natural Science Foundation of China under Grant No. 41901376 and No. 42172333, and in part by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan).","Precision Ag Definition; Messina G., Modica G., Applications of UAV Thermal Imagery in Precision Agriculture: State of the Art and Future Research Outlook, Remote Sens, 12, (2020); Schimmelpfennig D., Farm profits and adoption of precision agriculture, (2016); Maes W.H., Steppe K., Perspectives for Remote Sensing with Unmanned Aerial Vehicles in Precision Agriculture, Trends Plant Sci, 24, pp. 152-164, (2019); Lillesand T., Kiefer R.W., Chipman J., Remote Sensing and Image Interpretation, (2015); Mulla D.J., Twenty five years of remote sensing in precision agriculture: Key advances and remaining knowledge gaps, Biosyst. Eng, 114, pp. 358-371, (2013); Eskandari R., Mahdianpari M., Mohammadimanesh F., Salehi B., Brisco B., Homayouni S., Meta-Analysis of Unmanned Aerial Vehicle (UAV) Imagery for Agro-Environmental Monitoring Using Machine Learning and Statistical Models, Remote Sens, 12, (2020); Tsouros D.C., Bibi S., Sarigiannidis P.G., A Review on UAV-Based Applications for Precision Agriculture, Information, 10, (2019); Zhang H., Wang L., Tian T., Yin J., A Review of Unmanned Aerial Vehicle Low-Altitude Remote Sensing (UAV-LARS) Use in Agricultural Monitoring in China, Remote Sens, 13, (2021); Jang G., Kim J., Yu J.-K., Kim H.-J., Kim Y., Kim D.-W., Kim K.-H., Lee C.W., Chung Y.S., Review: Cost-Effective Unmanned Aerial Vehicle (UAV) Platform for Field Plant Breeding Application, Remote Sens, 12, (2020); Unmanned Aerial Vehicle; Deng L., Mao Z., Li X., Hu Z., Duan F., Yan Y., UAV-based multispectral remote sensing for precision agriculture: A comparison between different cameras, ISPRS J. Photogramm. Remote Sens, 146, pp. 124-136, (2018); Christiansen M.P., Laursen M.S., Jorgensen R.N., Skovsen S., Gislum R., Designing and Testing a UAV Mapping System for Agricultural Field Surveying, Sensors, 17, (2017); Popescu D., Stoican F., Stamatescu G., Ichim L., Dragana C., Advanced UAV-WSN System for Intelligent Monitoring in Precision Agriculture, Sensors, 20, (2020); Zhou X., Zheng H., Xu X., He J., Ge X., Yao X., Cheng T., Zhu Y., Cao W., Tian Y., Predicting grain yield in rice using multi-temporal vegetation indices from UAV-based multispectral and digital imagery, ISPRS J. Photogramm. Remote Sens, 130, pp. 246-255, (2017); Yang Q., Shi L., Han J., Zha Y., Zhu P., Deep convolutional neural networks for rice grain yield estimation at the ripening stage using UAV-based remotely sensed images, Field Crops Res, 235, pp. 142-153, (2019); Su J., Liu C., Coombes M., Hu X., Wang C., Xu X., Li Q., Guo L., Chen W.-H., Wheat yellow rust monitoring by learning from multispectral UAV aerial imagery, Comput. Electron. Agric, 155, pp. 157-166, (2018); Guo A., Huang W., Dong Y., Ye H., Ma H., Liu B., Wu W., Ren Y., Ruan C., Geng Y., Wheat Yellow Rust Detection Using UAV-Based Hyperspectral Technology, Remote Sens, 13, (2021); Bajwa A., Mahajan G., Chauhan B., NonconventionalWeed Management Strategies for Modern Agriculture, Weed Sci, 63, pp. 723-747, (2015); Huang Y., Reddy K.N., Fletcher R.S., Pennington D., UAV Low-Altitude Remote Sensing for Precision Weed Management, Weed Technol, 32, pp. 2-6, (2018); Van Klompenburg T., Kassahun A., Catal C., Crop yield prediction using machine learning: A systematic literature review, Comput. Electron. Agric, 177, (2020); Su Y.-X., Xu H., Yan L.-J., Support vector machine-based open crop model (SBOCM): Case of rice production in China, Saudi J. Biol. Sci, 24, pp. 537-547, (2017); Everingham Y., Sexton J., Skocaj D., Inman-Bamber G., Accurate prediction of sugarcane yield using a random forest algorithm, Agron. Sustain. Dev, 36, (2016); Chandra A.L., Desai S.V., Guo W., Balasubramanian V.N., Computer vision with deep learning for plant phenotyping in agriculture: A survey, (2020); Zhou L., Zhang C., Liu F., Qiu Z., He Y., Application of Deep Learning in Food: A Review, Compr. Rev. Food Sci. Food Saf, 18, pp. 1793-1811, (2019); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Bah M.D., Hafiane A., Canals R., Deep Learning with Unsupervised Data Labeling forWeed Detection in Line Crops in UAV Images, Remote Sens, 10, (2018); Kitano B.T., Mendes C.C.T., Geus A.R., Oliveira H.C., Souza J.R., Corn plant counting using deep learning and UAV images, IEEE Geosci. Remote. Sens. Lett, pp. 1-5, (2019); Nowakowski A., Mrziglod J., Spiller D., Bonifacio R., Ferrari I., Mathieu P.P., Garcia-Herranz M., Kim D.-H., Crop type mapping by using transfer learning, Int. J. Appl. Earth Obs. Geoinf, 98, (2021); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnarson B.A., Deep learning in remote sensing applications: A meta-analysis and review, ISPRS J. Photogramm. Remote Sens, 152, pp. 166-177, (2019); Chen J., Ran X., Deep Learning with Edge Computing: A Review, Proc. IEEE, 107, pp. 1655-1674, (2019); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, Proceedings of the International Conference on Learning Representations, (2015); Liu J., Liu R., Ren K., Li X., Xiang J., Qiu S., High-Performance Object Detection for Optical Remote Sensing Images with Lightweight Convolutional Neural Networks, Proceedings of the 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), pp. 585-592, (2020); Zhou Z., Chen X., Li E., Zeng L., Luo K., Zhang J., Edge Intelligence: Paving the Last Mile of Artificial Intelligence with Edge Computing, Proc. IEEE, 107, pp. 1738-1762, (2019); Pu Q., Ananthanarayanan G., Bodik P., Kandula S., Akella A., Bahl P., Stoica I., Low latency geo-distributed data analytics, ACM SIGCOMM Comp. Com. Rev, 45, pp. 421-434, (2015); Sitton-Candanedo I., Alonso R.S., Rodriguez-Gonzalez S., Coria J.A.G., De La Prieta F., Edge Computing Architectures in Industry 4.0: A General Survey and Comparison, International Workshop on Soft Computing Models in Industrial and Environmental Applications, pp. 121-131, (2019); Plastiras G., Terzi M., Kyrkou C., Theocharidcs T., Edge intelligence: Challenges and opportunities of near-sensor machine learning applications, Proceedings of the 2018 IEEE 29th International Conference on Application-Specific Systems, Architectures and Processors (ASAP), pp. 1-7, (2018); Deng S., Zhao H., Fang W., Yin J., Dustdar S., Zomaya A.Y., Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence, IEEE Internet Things J, 7, pp. 7457-7469, (2020); Boursianis A.D., Papadopoulou M.S., Diamantoulakis P., Liopa-Tsakalidi A., Barouchas P., Salahas G., Karagiannidis G., Wan S., Goudos S.K., Internet of Things (IoT) and Agricultural Unmanned Aerial Vehicles (UAVs) in smart farming: A comprehensive review, Internet Things, (2020); Kim J., Kim S., Ju C., Son H.I., Unmanned Aerial Vehicles in Agriculture: A Review of Perspective of Platform, Control, and Applications, IEEE Access, 7, pp. 105100-105115, (2019); Mogili U.R., Deepak B.B.V.L., Review on Application of Drone Systems in Precision Agriculture, Procedia Comput. Sci, 133, pp. 502-509, (2018); Kamilaris A., Prenafeta-Boldu F.X., A review of the use of convolutional neural networks in agriculture, J. Agric. Sci, 156, pp. 312-322, (2018); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Santos L., Santos F.N., Oliveira P.M., Shinde P., Deep Learning Applications in Agriculture: A Short Review, Iberian Robotics Conference, pp. 139-151, (2019); Interim Regulations on Flight Management of Unmanned Aerial Vehicles, 2021, (2018); Park M., Lee S., Lee S., Dynamic topology reconstruction protocol for uav swarm networking, Symmetry, 12, (2020); Radoglou-Grammatikis P., Sarigiannidis P., Lagkas T., Moscholios I., A compilation of UAV applications for precision agriculture, Comput. Netw, 172, (2020); Hayat S., Yanmaz E., Muzaffar R., Survey on Unmanned Aerial Vehicle Networks for Civil Applications: A Communications Viewpoint, IEEE Commun. Surv. Tutor, 18, pp. 2624-2661, (2016); Xie C., Yang C., A review on plant high-throughput phenotyping traits using UAV-based sensors, Comput. Electron. Agric, 178, (2020); Delavarpour N., Koparan C., Nowatzki J., Bajwa S., Sun X., A Technical Study on UAV Characteristics for Precision Agriculture Applications and Associated Practical Challenges, Remote Sens, 13, (2021); Tsouros D.C., Triantafyllou A., Bibi S., Sarigannidis P.G., Data acquisition and analysis methods in UAV-based applications for Precision Agriculture, Proceedings of the 2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS), pp. 377-384, (2019); Tahir M.N., Lan Y., Zhang Y., Wang Y., Nawaz F., Shah M.A.A., Gulzar A., Qureshi W.S., Naqvi S.M., Naqvi S.Z.A., Real time estimation of leaf area index and groundnut yield using multispectral UAV, Int. J. Precis. Agric. Aviat, (2020); Stroppiana D., Villa P., Sona G., Ronchetti G., Candiani G., Pepe M., Busetto L., Migliazzi M., Boschetti M., Early season weed mapping in rice crops using multi-spectral UAV data, Int. J. Remote Sens, 39, pp. 5432-5452, (2018); Wang H., Mortensen A.K., Mao P., Boelt B., Gislum R., Estimating the nitrogen nutrition index in grass seed crops using a UAV-mounted multispectral camera, Int. J. Remote Sens, 40, pp. 2467-2482, (2019); Ishida T., Kurihara J., Viray F.A., Namuco S.B., Paringit E.C., Perez G.J., Takahashi Y., Marciano J.J., A novel approach for vegetation classification using UAV-based hyperspectral imaging, Comput. Electron. Agric, 144, pp. 80-85, (2018); Ge X., Wang J., Ding J., Cao X., Zhang Z., Liu J., Li X., Combining UAV-based hyperspectral imagery and machine learning algorithms for soil moisture content monitoring, PeerJ, 7, (2019); Zhao X., Yang G., Liu J., Zhang X., Xu B., Wang Y., Zhao C., Gai J., Estimation of soybean breeding yield based on optimization of spatial scale of UAV hyperspectral image, Trans. Chin. Soc. Agric. Eng, 33, pp. 110-116, (2017); Prakash A., Thermal remote sensing: Concepts, issues and applications, Int. Arch. Photogramm. Remote Sens, 33, pp. 239-243, (2000); Weng Q., Thermal infrared remote sensing for urban climate and environmental studies: Methods, applications, and trends, ISPRS J. Photogramm. Remote Sens, 64, pp. 335-344, (2009); Khanal S., Fulton J., Shearer S., An overview of current and potential applications of thermal remote sensing in precision agriculture, Comput. Electron. Agric, 139, pp. 22-32, (2017); Dong P., Chen Q., LiDAR Remote Sensing and Applications, (2017); Zhou L., Gu X., Cheng S., Yang G., Shu M., Sun Q., Analysis of plant height changes of lodged maize using UAV-LiDAR data, Agriculture, 10, (2020); Shendryk Y., Sofonia J., Garrard R., Rist Y., Skocaj D., Thorburn P., Fine-scale prediction of biomass and leaf nitrogen content in sugarcane using UAV LiDAR and multispectral imaging, Int. J. Appl. Earth Obs. Geoinf, 92, (2020); Ndikumana E., Minh D.H.T., Baghdadi N., Courault D., Hossard L., Deep Recurrent Neural Network for Agricultural Classification using multitemporal SAR Sentinel-1 for Camargue, France, Remote Sens, 10, (2018); Lyalin K.S., Biryuk A.A., Sheremet A.Y., Tsvetkov V.K., Prikhodko D.V., UAV synthetic aperture radar system for control of vegetation and soil moisture, Proceedings of the 2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus), pp. 1673-1675, (2018); Liu C.-A., Chen Z.-X., Shao Y., Chen J.-S., Hasi T., Pan H.-Z., Research advances of SAR remote sensing for agriculture applications: A review, J. Integr. Agric, 18, pp. 506-525, (2019); Padua L., Vanko J., Hruska J., Adao T., Sousa J.J., Peres E., Morais R., UAS, sensors, and data processing in agroforestry: A review towards practical applications, Int. J. Remote Sens, 38, pp. 2349-2391, (2017); Allred B., Eash N., Freeland R., Martinez L., Wishart D., Effective and efficient agricultural drainage pipe mapping with UAS thermal infrared imagery: A case study, Agric. Water Manag, 197, pp. 132-137, (2018); Santesteban L.G., Di Gennaro S.F., Herrero-Langreo A., Miranda C., Royo J., Matese A., High-resolution UAV-based thermal imaging to estimate the instantaneous and seasonal variability of plant water status within a vineyard, Agric. Water Manag, 183, pp. 49-59, (2017); Xue J., Su B., Significant Remote Sensing Vegetation Indices: A Review of Developments and Applications, J. Sensors, 2017, pp. 1-17, (2017); Dai B., He Y., Gu F., Yang L., Han J., Xu W., A vision-based autonomous aerial spray system for precision agriculture, Proceedings of the 2017 IEEE International Conference on Robotics and Biomimetics (ROBIO), pp. 507-513, (2017); Faical B.S., Freitas H., Gomes P.H., Mano L., Pessin G., de Carvalho A., Krishnamachari B., Ueyama J., An adaptive approach for UAV-based pesticide spraying in dynamic environments, Comput. Electron. Agric, 138, pp. 210-223, (2017); Faical B.S., Pessin G., Filho G.P.R., Carvalho A.C.P.L.F., Gomes P.H., Ueyama J., Fine-Tuning of UAV Control Rules for Spraying Pesticides on Crop Fields: An Approach for Dynamic Environments, Int. J. Artif. Intell. Tools, 25, (2016); Esposito M., Crimaldi M., Cirillo V., Sarghini F., Maggio A., Drone and sensor technology for sustainable weed management: A review, Chem. Biol. Technol. Agric, 8, (2021); Bah M.D., Dericquebourg E., Hafiane A., Canals R., Deep Learning based Classification System for Identifying Weeds using High-Resolution UAV Imagery, Science and Information Conference, pp. 176-187, (2018); Huang H., Deng J., Lan Y., Yang A., Deng X., Zhang L., A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery, PLoS ONE, 13, (2018); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning, Sci. Rep. UK, 9, pp. 1-12, (2019); Sa I., Popovic M., Khanna R., Chen Z., Lottes P., Liebisch F., Nieto J., Stachniss C., Walter A., Siegwart R., WeedMap: A Large-Scale SemanticWeed Mapping Framework Using Aerial Multispectral Imaging and Deep Neural Network for Precision Farming, Remote Sens, 10, (2018); Scherrer B., Sheppard J., Jha P., Shaw J.A., Hyperspectral imaging and neural networks to classify herbicide-resistant weeds, J. Appl. Remote Sens, 13, (2019); Huang H., Lan Y., Yang A., Zhang Y., Wen S., Deng J., Deep learning versus Object-based Image Analysis (OBIA) in weed mapping of UAV imagery, Int. J. Remote Sens, 41, pp. 3446-3479, (2020); Hasan R.I., Yusuf S.M., Alzubaidi L., Review of the State of the Art of Deep Learning for Plant Diseases: A Broad Analysis and Discussion, Plants, 9, (2020); Abdulridha J., Batuman O., Ampatzidis Y., UAV-Based Remote Sensing Technique to Detect Citrus Canker Disease Utilizing Hyperspectral Imaging and Machine Learning, Remote Sens, 11, (2019); Tetila E.C., Machado B.B., Astolfi G., Belete N.A.D.S., Amorim W.P., Roel A.R., Pistori H., Detection and classification of soybean pests using deep learning with UAV images, Comput. Electron. Agric, 179, (2020); Zhang X., Han L., Dong Y., Shi Y., Huang W., Han L., Gonzalez-Moreno P., Ma H., Ye H., Sobeih T., A Deep Learning-Based Approach for Automated Yellow Rust Disease Detection from High-Resolution Hyperspectral UAV Images, Remote Sens, 11, (2019); Hu G., Yin C., Wan M., Zhang Y., Fang Y., Recognition of diseased Pinus trees in UAV images using deep learning and AdaBoost classifier, Biosyst. Eng, 194, pp. 138-151, (2020); Tetila E.C., Machado B.B., Menezes G.K., Oliveira A.D.S., Alvarez M., Amorim W.P., Belete N.A.D.S., Da Silva G.G., Pistori H., Automatic Recognition of Soybean Leaf Diseases Using UAV Images and Deep Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett, 17, pp. 903-907, (2019); Wiesner-Hanks T., Wu H., Stewart E., DeChant C., Kaczmar N., Lipson H., Gore M.A., Nelson R.J., Millimeter-Level Plant Disease Detection from Aerial Photographs via Deep Learning and Crowdsourced Data, Front. Plant Sci, 10, (2019); Albetis J., Jacquin A., Goulard M., Poilve H., Rousseau J., Clenet H., Dedieu G., Duthoit S., On the Potentiality of UAV Multispectral Imagery to Detect Flavescence dorée and Grapevine Trunk Diseases, Remote Sens, 11, (2018); Kerkech M., Hafiane A., Canals R., Vine disease detection in UAV multispectral images using optimized image registration and deep learning segmentation approach, Comput. Electron. Agric, 174, (2020); Bendig J., Willkomm M., Tilly N., Gnyp M.L., Bennertz S., Qiang C., Miao Y., Lenz-Wiedemann V.I.S., Bareth G., Very high resolution crop surface models (CSMs) from UAV-based stereo images for rice growth monitoring In Northeast China, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 40, pp. 45-50, (2013); Ni J., Yao L., Zhang J., Cao W., Zhu Y., Tai X., Development of an Unmanned Aerial Vehicle-Borne Crop-Growth Monitoring System, Sensors, 17, (2017); Fu Z., Jiang J., Gao Y., Krienke B., Wang M., Zhong K., Cao Q., Tian Y., Zhu Y., Cao W., Et al., Wheat Growth Monitoring and Yield Estimation based on Multi-Rotor Unmanned Aerial Vehicle, Remote Sens, 12, (2020); Zhao J., Zhang X., Gao C., Qiu X., Tian Y., Zhu Y., Cao W., Rapid Mosaicking of Unmanned Aerial Vehicle (UAV) Images for Crop Growth Monitoring Using the SIFT Algorithm, Remote Sens, 11, (2019); Li B., Xu X., Zhang L., Han J., Bian C., Li G., Liu J., Jin L., Above-ground biomass estimation and yield prediction in potato by using UAV-based RGB and hyperspectral imaging, ISPRS J. Photogramm. Remote Sens, 162, pp. 161-172, (2020); Maimaitijiang M., Sagan V., Sidike P., Hartling S., Esposito F., Fritschi F.B., Soybean yield prediction from UAV using multimodal data fusion and deep learning, Remote Sens. Environ, 237, (2020); Nebiker S., Lack N., Abacherli M., Laderach S., Light-weight multispectral UAV sensors and their capabilities for predicting grain yield and detecting plant diseases, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 41, (2016); Stroppiana D., Migliazzi M., Chiarabini V., Crema A., Musanti M., Franchino C., Villa P., Rice yield estimation using multispectral data from UAV: A preliminary experiment in northern Italy, Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 4467-4664, (2015); Kussul N., Lavreniuk M., Skakun S., Shelestov A., Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data, IEEE Geosci. Remote Sens. Lett, 14, pp. 778-782, (2017); Teimouri N., Dyrmann M., Jorgensen R.N., A Novel Spatio-Temporal FCN-LSTM Network for Recognizing Various Crop Types Using Multi-Temporal Radar Images, Remote Sens, 11, (2019); Wang S., Di Tommaso S., Faulkner J., Friedel T., Kennepohl A., Strey R., Lobell D., Mapping Crop Types in Southeast India with Smartphone Crowdsourcing and Deep Learning, Remote Sens, 12, (2020); Rebetez J., Satizabal H.F., Mota M., Noll D., Buchi L., Wendling M., Cannelle B., Perez-Uribe A., Burgos S., Augmenting a Convolutional Neural Network with Local Histograms-A Case Study in Crop Classification from High-Resolution UAV Imagery, (2016); Zhao L., Shi Y., Liu B., Hovis C., Duan Y., Shi Z., Finer Classification of Crops by Fusing UAV Images and Sentinel-2A Data, Remote Sens, 11, (2019); Hinton G.E., Salakhutdinov R.R., Reducing the Dimensionality of Data with Neural Networks, Science, 313, pp. 504-507, (2006); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet classification with deep convolutional neural networks, Adv. Neural Inf. Process. Syst, 25, pp. 1097-1105, (2012); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Goodfellow I., Pouget-Abadie J., Mirza M., Xu B., Warde-Farley D., Ozair S., Courville A., Bengio Y., Generative adversarial nets, Adv. Neural Inf. Process. Syst, 27, (2014); Reyes M.F., Auer S., Merkle N.M., Henry C., Schmitt M., SAR-to-Optical Image Translation Based on Conditional Generative Adversarial Networks - Optimization, Opportunities and Limits, Remote Sens, 11, (2019); Wang X., Yan H., Huo C., Yu J., Pant C., Enhancing Pix2Pix for Remote Sensing Image Classification, Proceedings of the International Conference on Pattern Recognition, pp. 2332-2336, (2018); Lv N., Ma H., Chen C., Pei Q., Zhou Y., Xiao F., Li J., Remote Sensing Data Augmentation Through Adversarial Training, Int. Geosci. Remote Sens. Symp, pp. 2511-2514, (2020); Ren C.X., Ziemann A., Theiler J., Durieux A.M.S., Deep snow: Synthesizing remote sensing imagery with generative adversarial nets, Proceedings of the 2020 Algorithms, Technologies, and Applications for Multispectral and Hyperspectral Imagery XXVI, pp. 196-205; Everingham M., Eslami S.M.A., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes Challenge: A Retrospective, Int. J. Comput. Vis, 111, pp. 98-136, (2015); Ha J.G., Moon H., Kwak J.T., Hassan S.I., Dang M., Lee O.N., Park H.Y., Deep convolutional neural network for classifying Fusarium wilt of radish from unmanned aerial vehicles, J. Appl. Remote Sens, (2017); Huang H., Deng J., Lan Y., Yang A., Zhang L., Wen S., Zhang H., Zhang Y., Deng Y., Detection of Helminthosporium Leaf Blotch Disease Based on UAV Imagery, Appl. Sci, 9, (2019); De Camargo T., Schirrmann M., Landwehr N., Dammer K.-H., Pflanz M., Optimized Deep Learning Model as a Basis for Fast UAV Mapping ofWeed Species in Winter Wheat Crops, Remote Sens, 13, (2021); Ukaegbu U., Tartibu L., Okwu M., Olayode I., Development of a Light-Weight Unmanned Aerial Vehicle for Precision Agriculture, Sensors, 21, (2021); Onishi M., Ise T., Automatic classification of trees using a UAV onboard camera and deep learning, (2018); Zhao J., Zhong Y., Hu X., Wei L., Zhang L., A robust spectral-spatial approach to identifying heterogeneous crops using remote sensing imagery with high spectral and spatial resolutions, Remote Sens. Environ, 239, (2020); Chen C.-J., Huang Y.-Y., Li Y.-S., Chen Y.-C., Chang C.-Y., Huang Y.-M., Identification of Fruit Tree Pests with Deep Learning on Embedded Drone to Achieve Accurate Pesticide Spraying, IEEE Access, 9, pp. 21986-21997, (2021); Li F., Liu Z., Shen W., Wang Y., Wang Y., Ge C., Sun F., Lan P., A Remote Sensing and Airborne Edge-Computing Based Detection System for Pine Wilt Disease, IEEE Access, 9, pp. 66346-66360, (2021); Valente J., Doldersum M., Roers C., Kooistra L., Detecting rumex obtusifolius weed plants in grasslands from UAV RGB imagery using deep learning, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 4, pp. 179-185, (2019); Veeranampalayam Sivakumar A.N., Li J., Scott S., Psota E., Jhala A.J., Luck J.D., Shi Y., Comparison of object detection and patch-based classification deep learning models on mid-to late-season weed detection in UAV imagery, Remote Sens, 12, (2020); Apolo-Apolo O., Martinez-Guanter J., Egea G., Raja P., Perez-Ruiz M., Deep learning techniques for estimation of the yield and size of citrus fruits using a UAV, Eur. J. Agron, 115, (2020); Chen Y., Lee W.S., Gan H., Peres N., Fraisse C., Zhang Y., He Y., Strawberry Yield Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoimages, Remote Sens, 11, (2019); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Zhang Z., Flores P., Igathinathane C., Naik D.L., Kiran R., Ransom J.K., Wheat Lodging Detection from UAS Imagery Using Machine Learning Algorithms, Remote Sens, 12, (2020); Stewart E.L., Wiesner-Hanks T., Kaczmar N., DeChant C., Wu H., Lipson H., Nelson R.J., Gore M.A., Quantitative Phenotyping of Northern Leaf Blight in UAV Images Using Deep Learning, Remote Sens, 11, (2019); Kerkech M., Hafiane A., Canals R., VddNet: Vine Disease Detection Network Based on Multispectral Images and Depth Map, Remote Sens, 12, (2020); Zou K., Chen X., Zhang F., Zhou H., Zhang C., A FieldWeed Density Evaluation Method Based on UAV Imaging and Modified U-Net, Remote Sens, 13, (2021); Osco L.P., Nogueira K., Ramos A.P.M., Pinheiro M.M.F., Furuya D.E.G., Goncalves W.N., Jorge L.A.D.C., Junior J.M., dos Santos J.A., Semantic segmentation of citrus-orchard using deep neural networks and multispectral UAV-based imagery, Precis. Agric, 22, pp. 1-18, (2021); Zhang J., Xie T., Yang C., Song H., Jiang Z., Zhou G., Zhang D., Feng H., Xie J., Segmenting Purple Rapeseed Leaves in the Field from UAV RGB Imagery Using Deep Learning as an Auxiliary Means for Nitrogen Stress Detection, Remote Sens, 12, (2020); Xu W., Yang W., Chen S., Wu C., Chen P., Lan Y., Establishing a model to predict the single boll weight of cotton in northern Xinjiang by using high resolution UAV remote sensing data, Comput. Electron. Agric, 179, (2020); Champ J., Mora-Fallas A., Goeau H., Mata-Montero E., Bonnet P., Joly A., Instance segmentation for the fine detection of crop and weed plants by precision agricultural robots, Appl. Plant Sci, 8, (2020); Mora-Fallas A., Goeau H., Joly A., Bonnet P., Mata-Montero E., Instance segmentation for automated weeds and crops detection in farmlands, A first approach to Acoustic Characterization of Costa Rican Children’s Speech, (2020); Toda Y., Okura F., Ito J., Okada S., Kinoshita T., Tsuji H., Saisho D., Training instance segmentation neural network with synthetic datasets for crop seed phenotyping, Commun. Biol, 3, (2020); Khan S., Tufail M., Khan M.T., Khan Z.A., Iqbal J., Wasim A., Real-time recognition of spraying area for UAV sprayers using a deep learning approach, PLoS ONE, 16, (2021); Deng J., Zhong Z., Huang H., Lan Y., Han Y., Zhang Y., Lightweight Semantic Segmentation Network for Real-TimeWeed Mapping Using Unmanned Aerial Vehicles, Appl. Sci, 10, (2020); Liu C., Li H., Su A., Chen S., Li W., Identification and Grading of Maize Drought on RGB Images of UAV Based on Improved U-Net, IEEE Geosci. Remote Sens. Lett, 18, pp. 198-202, (2020); Tri N.C., Duong H.N., Van Hoai T., Van Hoa T., Nguyen V.H., Toan N.T., Snasel V., A novel approach based on deep learning techniques and UAVs to yield assessment of paddy fields, Proceedings of the 2017 9th International Conference on Knowledge and Systems Engineering (KSE), pp. 257-262, (2017); Osco L.P., Arruda M.D.S.D., Goncalves D.N., Dias A., Batistoti J., de Souza M., Gomes F.D.G., Ramos A.P.M., Jorge L.A.D.C., Liesenberg V., Et al., A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery, ISPRS J. Photogramm. Remote Sens, 174, pp. 1-17, (2021); Osco L.P., Arruda M.D.S.D., Junior J.M., da Silva N.B., Ramos A.P.M., Moryia A.S., Imai N.N., Pereira D.R., Creste J.E., Matsubara E., Et al., A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery, ISPRS J. Photogramm. Remote Sens, 160, pp. 97-106, (2020); Zheng J., Fu H., Li W., Wu W., Yu L., Yuan S., Tao W.Y.W., Pang T.K., Kanniah K.D., Growing status observation for oil palm trees using Unmanned Aerial Vehicle (UAV) images, ISPRS J. Photogramm. Remote Sens, 173, pp. 95-121, (2021); Ampatzidis Y., Partel V., Costa L., Agroview: Cloud-based application to process, analyze and visualize UAV-collected data for precision agriculture applications utilizing artificial intelligence, Comput. Electron. Agric, 174, (2020); Pang Y., Shi Y., Gao S., Jiang F., Veeranampalayam-Sivakumar A.-N., Thompson L., Luck J., Liu C., Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery, Comput. Electron. Agric, 178, (2020); Wu J., Yang G., Yang X., Xu B., Han L., Zhu Y., Automatic Counting of in situ Rice Seedlings from UAV Images Based on a Deep Fully Convolutional Neural Network, Remote Sens, 11, (2019); Yang M.-D., Tseng H.-H., Hsu Y.-C., Yang C.-Y., Lai M.-H., Wu D.-H., A UAV Open Dataset of Rice Paddies for Deep Learning Practice, Remote Sens, 13, (2021); Zhao W., Yamada W., Li T., Digman M., Runge T., Augmenting Crop Detection for Precision Agriculture with Deep Visual Transfer Learning-A Case Study of Bale Detection, Remote Sens, 13, (2020); Ampatzidis Y., Partel V., UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence, Remote Sens, 11, (2019); Aeberli A., Johansen K., Robson A., Lamb D., Phinn S., Detection of Banana Plants Using Multi-Temporal Multispectral UAV Imagery, Remote Sens, 13, (2021); Fan Z., Lu J., Gong M., Xie H., Goodman E.D., Automatic Tobacco Plant Detection in UAV Images via Deep Neural Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 11, pp. 876-887, (2018); Zan X., Zhang X., Xing Z., Liu W., Zhang X., Su W., Liu Z., Zhao Y., Li S., Automatic Detection of Maize Tassels from UAV Images by Combining Random Forest Classifier and VGG16, Remote Sens, 12, (2020); Liu Y., Cen C., Che Y., Ke R., Ma Y., Ma Y., Detection of Maize Tassels from UAV RGB Imagery with Faster R-CNN, Remote Sens, 12, (2020); Yuan W., Choi D., UAV-Based Heating Requirement Determination for Frost Management in Apple Orchard, Remote Sens, 13, (2021); Dyson J., Mancini A., Frontoni E., Zingaretti P., Deep Learning for Soil and Crop Segmentation from Remotely Sensed Data, Remote Sens, 11, (2019); Feng Q., Yang J., Liu Y., Ou C., Zhu D., Niu B., Liu J., Li B., Multi-Temporal Unmanned Aerial Vehicle Remote Sensing for Vegetable Mapping Using an Attention-Based Recurrent Convolutional Neural Network, Remote Sens, 12, (2020); Der Yang M., Tseng H.H., Hsu Y.C., Tseng W.C., Real-time Crop Classification Using Edge Computing and Deep Learning, Proceedings of the 2020 IEEE 17th Annual Consumer Communications & Networking Conference, pp. 1-4, (2020); Yang M.-D., Boubin J.G., Tsai H.P., Tseng H.-H., Hsu Y.-C., Stewart C.C., Adaptive autonomous UAV scouting for rice lodging assessment using edge computing with deep learning EDANet, Comput. Electron. Agric, 179, (2020); Zhang Q., Liu Y., Gong C., Chen Y., Yu H., Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review, Sensors, 20, (2020); Zhong Y., Hu X., Luo C., Wang X., Zhao J., Zhang L., WHU-Hi: UAV-borne hyperspdectral with high spatial resolution (H2) benchmark datasets and classifier for precise crop identification based on deep convolutional neural network with CRF, Remote Sens. Environ, 250, (2020); Wiesner-Hanks T., Stewart E.L., Kaczmar N., DeChant C., Wu H., Nelson R.J., Lipson H., Gore M.A., Image set for deep learning: Field images of maize annotated with disease symptoms, BMC Res. Notes, 11, (2018); Daudt R.C., Le Saux B., Boulch A., Gousseau Y., Multitask learning for large-scale semantic change detection, Comput. Vis. Image Underst, 187, (2019); Zhang Y., CSIF. figshare. Dataset, (2018); Oldoni L.V., Sanches I.D., Picoli M.C.A., Covre R.M., Fronza J.G., LEM+ dataset: For agricultural remote sensing applications, Data Brief, 33, (2020); Ferreira A., Felipussi S.C., Pires R., Avila S., Santos G., Lambert J., Huang J., Rocha A., Eyes in the Skies: A Data-Driven Fusion Approach to Identifying Drug Crops from Remote Sensing Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 12, pp. 4773-4786, (2019); Russwurm M., Pelletier C., Zollner M., Lefevre S., Korner M., BreizhCrops: A time series dataset for crop type mapping, (2019); Rustowicz R., Cheong R., Wang L., Ermon S., Burke M., Lobell D., Semantic Segmentation of Crop Type in Ghana Dataset; Rustowicz R., Cheong R., Wang L., Ermon S., Burke M., Lobell D., Semantic Segmentation of Crop Type in South Sudan Dataset; Torre M., Remeseiro B., Radeva P., Martinez F., DeepNEM: Deep Network Energy-Minimization for Agricultural Field Segmentation, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 13, pp. 726-737, (2020); EarthExplorer; Copernicus Open Access Hub; Weikmann G., Paris C., Bruzzone L., TimeSen2Crop: A Million Labeled Samples Dataset of Sentinel 2 Image Time Series for Crop-Type Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 14, pp. 4699-4708, (2021); Khan W.Z., Ahmed E., Hakak S., Yaqoob I., Ahmed A., Edge computing: A survey, Future Gener. Comput. Syst, 97, pp. 219-235, (2019); Liu J., Xue Y., Ren K., Song J., Windmill C., Merritt P., High-Performance Time-Series Quantitative Retrieval from Satellite Images on a GPU Cluster, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens, 12, pp. 2810-2821, (2019)","J. Liu; School of Computer Science, China University of Geosciences, Wuhan, 430078, China; email: liujia@cug.edu.cn","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120337164"
"Rakhmatuiln I.; Kamilaris A.; Andreasen C.","Rakhmatuiln, Ildar (57226603012); Kamilaris, Andreas (36189564000); Andreasen, Christian (7005358690)","57226603012; 36189564000; 7005358690","Deep neural networks to detectweeds from crops in agricultural environments in real-time: A review","2021","Remote Sensing","13","21","4486","","","","24","10.3390/rs13214486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120067412&doi=10.3390%2frs13214486&partnerID=40&md5=37a2e27ab61e42b73ba77205db55f1a2","Department of Power Plant Networks and Systems, South Ural State University, Chelyabinsk City, 454080, Russian Federation; CYENS Center of Excellence, Dimarchias Square 23, Nicosia, 1016, Cyprus; Department of Computer Science, University of Twente, Enschede, 7522 NB, Netherlands; Department of Plant and Environmental Sciences, University of Copenhagen, Højbakkegaard Allé 13, Taastrup, DK 2630, Denmark","Rakhmatuiln I., Department of Power Plant Networks and Systems, South Ural State University, Chelyabinsk City, 454080, Russian Federation; Kamilaris A., CYENS Center of Excellence, Dimarchias Square 23, Nicosia, 1016, Cyprus, Department of Computer Science, University of Twente, Enschede, 7522 NB, Netherlands; Andreasen C., Department of Plant and Environmental Sciences, University of Copenhagen, Højbakkegaard Allé 13, Taastrup, DK 2630, Denmark","Automation, including machine learning technologies, are becoming increasingly crucial in agriculture to increase productivity. Machine vision is one of the most popular parts of machine learning and has been widely used where advanced automation and control have been required. The trend has shifted from classical image processing and machine learning techniques to modern artificial intelligence (AI) and deep learning (DL) methods. Based on large training datasets and pre-trained models, DL-based methods have proven to be more accurate than previous traditional techniques. Machine vision has wide applications in agriculture, including the detection of weeds and pests in crops. Variation in lighting conditions, failures to transfer learning, and object occlusion constitute key challenges in this domain. Recently, DL has gained much attention due to its advantages in object detection, classification, and feature extraction. DL algorithms can automatically extract information from large amounts of data used to model complex problems and is, therefore, suitable for detecting and classifying weeds and crops. We present a systematic review of AI-based systems to detect weeds, emphasizing recent trends in DL. Various DL methods are discussed to clarify their overall potential, usefulness, and performance. This study indicates that several limitations obstruct the widespread adoption of AI/DL in commercial applications. Recommendations for overcoming these challenges are summarized. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning in agriculture; Machine vision for weed control; Precision agriculture; Robotic weed control; Weed detection","Classification (of information); Computer vision; Crops; Deep neural networks; Feature extraction; Large dataset; Object detection; Precision agriculture; Agricultural environments; Deep learning in agriculture; Learning methods; Machine learning technology; Machine vision for weed control; Machine-vision; Precision Agriculture; Real- time; Robotic weed control; Weed detection; Weed control","","","","","Deputy Ministry of Research, Innovation and Digital Policy; Horizon 2020 Framework Programme, H2020, (101000256, 739578); European Commission, EC","This review was mainly funded by the EU–project WeLASER “Sustainable Weed Management in Agriculture with Laser-Based Autonomous Tools,” Grant agreement ID: 101000256, funded under H2020-EU.3.2.1.1. AK received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No. 739578 and from the Government of the Republic of Cyprus through the Deputy Ministry of Research, Innovation and Digital Policy.","NSP-Weeds; Kudsk P., Streibig J.S., Herbicides and two edge-sword, Weed Res, 43, pp. 90-102, (2003); Harrison J.L., Pesticide Drift and the Pursuit of Environmental Justice, (2011); Lemtiri A., Colinet G., Alabi T., Cluzeau D., Zirbes L., Haubruge E., Francis F., Impacts of earthworms on soil components and dynamics. A review, Biotechnol. Agron. Soc. Environ, 18, pp. 121-133, (2014); Pannacci E., Farneselli M., Guiducci M., Tei F., Mechanical weed control in onion seed production, Crop. Prot, 135, (2020); Rehman T., Qamar U., Zaman Q.Z., Chang Y.K., Schumann A.W., Corscadden K.W., Development and field evaluation of a machine vision based in-season weed detection system for wild blueberry, Comput. Electron. Agric, 162, pp. 1-3, (2019); Rakhmatulin I., Andreasen C., A concept of a compact and inexpensive device for controlling weeds with laser beams, Agron, 10, (2020); Raj R., Rajiv P., Kumar P., Khari M., Feature based video stabilization based on boosted HAAR Cascade and representative point matching algorithm, Image Vis. Comput, 101, (2020); Kaur J., Sinha P., Shukla R., Tiwari V., Automatic Cataract Detection Using Haar Cascade Classifier, Data Intelligence Cognitive Informatics, (2021); Abouzahir A., Sadik M., Sabir E., Bag-of-visual-words-augmented Histogram of Oriented Gradients for efficient weed detection, Biosyst. Eng, 202, pp. 179-194, (2021); Che'Ya N., Dunwoody E., Gupta M., Assessment of Weed Classification Using Hyperspectral Reflectance and Optimal Multispectral UAV Imagery, Agronomy, 11, (2021); De Rainville F.M., Durand A., Fortin F.A., Tanguy K., Maldague X., Panneton B., Simard M.J., Bayesian classification and unsupervised learning for isolating weeds in row crops, Pattern Anal. Applic, 17, pp. 401-414, (2014); Islam N., Rashid M., Wibowo S., Xu C.Y., Morshed A., Wasimi S.A., Moore S., Rahman S.M., Early Weed Detection Using Image Processing and Machine Learning Techniques in an Australian Chilli Farm, Agriculture, 11, (2021); Hung C., Xu Z., Sukkarieh S., Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV, Remote Sens, 6, pp. 12037-12054, (2014); Pourghassemi B., Zhang C., Lee J., On the Limits of Parallelizing Convolutional Neural Networks on GPUs, Proceedings of the SPAA ‘20: 32nd ACM Symposium on Parallelism in Algorithms and Architectures; Kulkarni A., Deshmukh G., Advanced Agriculture Robotic Weed Control System, Int. J. Adv. Res. Electr. Electron. Instrum. Eng, 2, (2013); Wang N., Zhang E., Dowell Y., Sun D., Design of an optical weed sensor using plant spectral characteristic, Am. Soc. Agric. Biol. Eng, 44, pp. 409-419, (2001); Gikunda P., Jouandeau N., Modern CNNs for IoT Based Farms, (2019); Jouandeau N., Gikunda P., State-Of-The-Art Convolutional Neural Networks for Smart Farms: A Review, Science and Information (SAI) Conference, (2017); Saleem M., Potgieter J., Arif K., Automation in Agriculture by Machine and Deep Learning Techniques: A Review of Recent Developments, Precis. Agric, 22, pp. 2053-2091, (2021); Kamilaris A., Prenafeta-Boldu F., A review of the use of convolutional neural networks in agriculture, J. Agric. Sci, 156, pp. 312-322, (2018); Jiang B., He J., Yang S., Fu H., Li H., Fusion of machine vision technology and AlexNet-CNNs deep learning network for the detection of postharvest apple pesticide residues, Artif. Intell. Agric, 1, pp. 1-8, (2019); Liu H., Lee S., Saunders C., Development of a machine vision system for weed detection during both of off-season, Amer. J. Agric. Biol. Sci, 9, pp. 174-193, (2014); Watchareeruetai U., Takeuchi Y., Matsumoto T., Kudo H., Ohnishi N., Computer Vision Based Methods for Detecting Weeds in Lawns, Mach. Vis. Applic, 17, pp. 287-296, (2006); Padmapriya S., Bhuvaneshwari P., Real time Identification of Crops, Weeds, Diseases, Pest Damage and Nutrient Deficiency, Internat. J. Adv. Res. Educ. Technol, 5, (2018); Olsen A., Konovalov D.A., Philippa B., Ridd P., Wood J.C., Johns J., Banks W., Girgenti B., Kenny O., Whinney J., Et al., DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning, Sci. Rep, 9, pp. 118-124, (2019); Downey D., Slaughter K., David C., Weeds accurately mapped using DGPS and ground-based vision identification, Calif. Agric, 58, pp. 218-221, (2004); Cun Y., Boser B., Dencker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation Applied to Handwritten Zip Code Recognition, Neural Comput, 1, pp. 541-551, (1989); Wen X., Jing H., Yanfeng S., Hui Z., Advances in Convolutional Neural Networks, Advances in Deep Learning, (2020); Gothai P., Natesan S., Weed Identification using Convolutional Neural Network and Convolutional Neural Network Architectures, Conference, Proceedings of the 2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC); Su W.-H., Crop plant signalling for real-time plant identification in smart farm: A systematic review and new concept in artificial intelligence for automated weed control, Artif. Intelli. Agric, 4, pp. 262-271, (2020); Li Y., Nie J., Chao X., Do we really need deep CNN for plant diseases identification?, Comput. Electron. Agric, 178, (2020); Kattenborn T., Leitloff J., Schiefer F., Hinz S., Review on Convolutional Neural Networks (CNN) in vegetation remote sensing, SPRS J. Photogram. Remote Sens, 173, pp. 24-49, (2021); O'Mahony N., Campbell S., Carvalho A., Harapanahalli S., Hernandez G.V., Krpalkova L., Riordan D., Walsh J., Deep Learning vs. Traditional Computer Vision, Advances in Computer Vision. CVC 2019. Advances in Intelligent Systems and Computing, 943, (2020); Wang A., Zhang W., Wei X., A review on weed detection using ground based machine vision and image processing techniques, Comput. Electron. Agric, 158, pp. 226-240, (2019); Dhillon A., Verma G., Convolutional neural network: A review of models, methodologies and applications to object detection, Prog. Artif. Intell, 9, pp. 85-112, (2020); Ren Y., Cheng X., Review of convolutional neural network optimization and training in image processing, Tenth International Symposium on Precision Engineering Measurements and Instrumentation 2018, (2019); Gorach T., Deep convolution neural networks—A review, Intern. Res. J. Eng. Technol, 5, pp. 439-452, (2018); Naranjo-Torres J., Mora M., Hernandez-Garcia R., Barrientos R., Review of Convolutional Neural Network Applied to Fruit Image Processing, Appl. Sci, 10, (2020); Jiao J., Zhao M., Lin J., Liang K., A comprehensive review on convolutional neural network in machine fault diagnosis, Neurocomputing, 417, pp. 36-63, (2020); He T., Kong R., Holmes A., Nguyen M., Sabuncu M.R., Eickhoff S.B., Bzdok D., Feng J., Yeo B.T.T., Deep neural networks and kernel regression achieve comparable accuracies for functional connectivity prediction of behaviour and demographics, NeuroImage, 206, (2020); Ma X., Kittikunakorn N., Sorman B., Xi H., Chen A., Marsh M., Mongeau A., Piche N., Williams R.O., Skomski D., Application of Deep Learning Convolutional Neural Networks for Internal Tablet Defect Detection: High Accuracy, Throughput, and Adaptability, J. Pharma. Sci, 109, pp. 1547-1557, (2020); Aydogan M., Karci A., Improving the accuracy using pre-trained word embeddings on deep neural networks for Turkish text classification, Phys. A Stat. Mech. Its Appl, 541, (2020); Agarwal M., Gupta S., Biswas K., Development of Efficient CNN model for Tomato crop disease identification, Sustain. Comput. Inform. Syst, 28, (2020); Boulent J., Foucher S., Theau J., Charles P., Convolutional Neural Networks for the Automatic Identification of Plant Diseases, Front. Plant Sci, 10, (2019); Jiang Y., Li C., Convolutional Neural Networks for Image-Based High Throughput Plant Phenotyping: A Review, Plant Phenomics 2020, 2020; Noon S., Amjad M., Qureshi M., Mannan A., Use of deep learning techniques for identification of plant leaf stresses: A review, Sustain. Comput. Inf. Systems, 28, (2020); Mishra S., Sachan R., Rajpal D., Deep Convolutional Neural Network based Detection System for Real-time Corn Plant Disease Recognition, Procedia Comput. Sci, 167, pp. 2003-2010, (2020); Badhan S.K., Dsilva D.M., Sonkusare R., Weakey S., Real-Time Weed Detection using Machine Learning and Stereo-Vision, Proceedings of the 2021 6th International Conference for Convergence in Technology (I2CT), pp. 1-5; Gai J., Plants Detection, Localization and Discrimination using 3D Machine Vision for Robotic Intra-row Weed Control, (2016); Gottardi M., A CMOS/CCD image sensor for 2D real time motion estimation, Sens. Actuators A Phys, 46, pp. 251-256, (1995); Helmers H., Schellenberg M., CMOS vs. CCD sensors in speckle interferometry, Opt. Laser Technol, 35, pp. 587-593, (2003); Silfhout R., Kachatkou A., Fibre-optic coupling to high-resolution CCD and CMOS image sensors, Nucl. Instr. Methods Phys. Res. Sect. A Accel. Spectrum. Detect. Ass. Equip, 597, pp. 266-269, (2008); Krishna B., Rekulapellim N., Kauda B.P., Materials Today: Proceedings. Comparison of different deep learning frameworks, Mater. Today Proc, (2020); Trung W., Maleki F., Romero F., Forghani R., Kadoury S., Overview of Machine Learning: Part 2: Deep Learning for Medical Image Analysis, Neuroimaging Clin. N. Am, 30, pp. 417-431, (2020); Wang P., Fan E., Wang P., Comparative analysis of image classification algorithms based on traditional machine learning and deep learning, Pattern Recognit. Lett, 141, pp. 61-67, (2021); Bui D., Tsangaratos P., Nguyen V., Liem N., Trinh P., Comparing the prediction performance of a Deep Learning Neural Network model with conventional machine learning models in landslide susceptibility assessment, CATENA, 188, (2020); Kamilaris A., Brik C., Karatsiolis S., Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles, Proceedings of the CAIP 2019, the Workshop on Deep-Learning Based Computer Vision for UAV, (2019); Barth R., IJsselmuiden J., Hemming J., Van Henten E.J., Data synthesis methods for semantic segmentation in agriculture: A Capsicum annuum dataset, Comput. Electron. Agri, 144, pp. 284-296, (2018); Zichao J., A Novel Crop Weed Recognition Method Based on Transfer Learning from VGG16 Implemented by Keras, OP Conf. Ser. Mater. Sci. Eng, 677, (2019); Chen D., Lu Y., Yong S., Performance Evaluation of Deep Transfer Learning on Multiclass Identification of Common Weed Species in Cotton Production Systems; Espejo-Garcia B., Mylonas N., Athanasakos L., Spyros Fountas S., Vasilakoglou I., Towards weeds identification assistance through transfer learning, Comput. Electron. Agric, 171, (2020); Al-Qurran R., Al-Ayyoub M., Shatnawi A., Plant Classification in the Wild: A Transfer Learning Approach, Proceedings of the 2018 International Arab Conference on Information Technology (ACIT), pp. 1-5; Pajares G., Garcia-Santillam I., Campos Y., Montalo M., Machine-vision systems selection for agricultural vehicles: A guide, Imaging, 2, (2016); Shorten C., Khoshgoftaar T., A survey on Image Data Augmentation for Deep Learning, J. Big Data, 6, (2019); Zheng Y., Kong J., Jin X., Wang X., CropDeep: The Crop Vision Dataset for Deep-Learning-Based Classification and Detection in Precision Agriculture, Sensors, 19, (2019); Sudars K., Jasko J., Namatevsa I., Ozola L., Badaukis N., Dataset of annotated food crops and weed images for robotic computer vision control, Data Brief, 31, (2020); Cap Q.H., Tani H., Uga H., Kagiwada S., Lyatomi H., LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis; Zhu J., Park T., Isola P., Efros A., Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, (2020); Huang Z., Ke W., Huang D., Improving Object Detection with Inverted Attention, Proceedings of the 2020 IEEE Winter Conference on Applications of Computer Vision (WACV); He C., Lai S., Lam K., Object Detection with Relation Graph Inference, Proceedings of the ICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), (2019); Champ J., Mora-Fallas A., Goeau H., Mata-Montero E., Bonnet P., Joly A., Instance segmentation for the fine detection of crop and weed plants by precision agricultural robots, Appl. Plant Sci, 8, (2020); Lameski P., Zdravevski E., Trajkovik V., Kulakov A., Weed Detection Dataset with RGB Images Taken Under Variable Light Conditions, ICT Innovations 2017. Communications in Computer and Information Science, 778, (2017); Giselsson T.M., Jorgensen R.N., Jensen P.K., Dyrmann M., Midtiby H.S., A Public Image Database for Benchmark of Plant Seedling Classification Algorithms, (2017); Cicco M., Potena C., Grisetti G., Pretto A., Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection, (2016); Lu Y., Young S., A survey of public datasets for computer vision tasks in precision agriculture, Comput. Electron. Agric, 178, (2020); Faisal F., Hossain B., Emam H., Performance Analysis of Support Vector Machine and Bayesian Classifier for Crop and Weed Classification from Digital Images, World Appl. Sci, 12, pp. 432-440, (2011); Dyrmann M., Automatic Detection and Classification of Weed Seedlings under Natural Light Conditions, (2017); Chang C., Lin K., Smart Agricultural Machine with a Computer Vision Based Weeding and Variable-Rate Irrigation Scheme, Robotics, 7, (2018); Slaughter D.C., Giles D.K., Downey D., Autonomous robotic weed control systems: A review, Comput. Electron. Agric, 61, pp. 63-78, (2008); Abhisesh S., Machine Vision System for Robotic Apple Harvesting in Fruiting Wall Orchards, (2016); Qiu Q., Fan Z., Meng Z., Zhang Q., Cong Y., Li B., Wang N., Zhao C., Extended Ackerman Steering Principle for the coordinated movement control of a four wheel drive agricultural mobile robot, Comput. Electron. Agric, 152, pp. 40-50, (2018); Ren G., Lin T., Ying Y., Chowdhary G., Ting K.C., Agricultural robotics research applicable to poultry production: A review, Comput. Electron. Agric, 169, (2020); Asha R., Aman M., Pankaj M., Singh A., Robotics-automation and sensor based approaches in weed detection and control: A review, Intern. J. Chem. Stud, 8, pp. 542-550, (2020); Shinde A., Shukla M., Crop detection by machine vision for weed management, Intern. J. Adv. Eng. Technol, 7, pp. 818-826, (2014); Raja R., Nguyen T., Vuong V.L., Slaughter D.C., Fennimore S.A., RTD-SEPs: Real-time detection of stem emerging points and classification of crop-weed for robotic weed control in producing tomato, Biosyst. Eng, 195, pp. 152-171, (2020); Sirikunkitti S., Chongcharoen K., Yoongsuntia P., Ratanavis A., Progress in a Development of a Laser-Based Weed Control System, Proceedings of the 2019 Research, Invention, and Innovation Congress (RI2C), pp. 1-4; Mathiassen S., Bak T., Christensen S., Kudsk P., The effect of laser treatment as a weed control method, Biosyst. Eng, 95, pp. 497-505, (2006); Xiong Y., Ge Y., Liang Y., Blackmore S., Development of a prototype robot and fast path-planning algorithm for static laser weeding, Comput. Electron. Agric, 142, pp. 494-503, (2017); Marx C., Barcikowski S., Hustedt M., Haferkamp H., Rath T., Design and application of a weed damage model for laser-based weed control, Biosyst. Eng, 113, pp. 148-157, (2012); Libran-Embid F., Klaus F., Tscharntke T., Grass I., Unmanned aerial vehicles for biodiversity-friendly agricultural landscapes—A systematic review, Sci. Total Environ, 732, (2020); Boursianis A., Papadopoulou M., Diamantoulakis P., Liopa-Tsakalidi A., Barouchas P., Salahas G., Karagiannidis G., Wan S., Goudos S.K., Internet of Things (IoT) and Agricultural Unmanned Aerial Vehicles (UAVs) in smart farming: A comprehensive review, Internet Things, 7, (2020); Huang H., Deng J., Lan Y., Yang A., Deng X., Zhang L., A fully convolutional network for weed mapping of unmanned aerial vehicle (UAV) imagery, PLoS ONE, 13, (2018); Hunter J., Gannon T.W., Richardson R.J., Yelverton F.H., Leon R.G., Integration of remote-weed mapping and an autonomous spraying unmanned aerial vehicle for site-specific weed management, Pest. Manag. Sci, 76, pp. 1386-1392, (2020); Cerro J., Ulloa C., Barrientos A., Rivas J., Unmanned Aerial Vehicles in Agriculture: A Survey, Agronomy, 11, (2021); Rasmussen J., Nielsen J., A novel approach to estimating the competitive ability of Cirsium arvense in cereals using unmanned aerial vehicle imagery, Weed Res, 60, pp. 150-160, (2020); Rijk L., Beedie S., Precision Weed Spraying using a Multirotor UAV, Proceedings of the10th International Micro-Air Vehicles Conference, (2018); Liang Y., Yang Y., Chao C., Low-Cost Weed Identification System Using Drones, Proceedings of the Seventh International Symposium on Computing and Networking Workshops (CANDARW), pp. 260-263; Zhang Q., Liu Y., Gong C., Chen Y., Yu H., Applications of deep learning for dense scenes, analysis in agriculture: A review, Sensors, 20, (2020); Asad M., Bais A., Weed Detection in Canola Fields Using Maximum Likelihood Classification and Deep Convolutional Neural Network, Inform. Process. Agric, 7, pp. 535-545, (2020); Shawky O., Hagag A., Dahshan E., Ismail M., Remote sensing image scene classification using CNN-MLP with data augmentation, Optik, 221, (2020); Zhuoyao Z., Lei S., Qiang H., Improved localization accuracy by LocNet for Faster R-CNN based text detection in natural scene images, Pattern Recognit, 96, (2019); Liakos K., Busato P., Moshou D., Pearson S., Bochtis D., Machine learning in agriculture: A review, Sensors, 18, (2018); Hasan A.S.M.M., Sohel F., Diepeveen D., Laga H., Jones M.G.K., A survey of deep learning techniques for weed detection from images, Comput. Electron. Agric, 184, (2021); Rehman T.U., Mahmud M.S., Chang Y.K., Jin J., Shin J., Current and future applications of statistical machine learning algorithms for agricultural machine vision systems, Comput. Electron. Agricult, 156, pp. 585-605, (2019); Osorio K., Puerto A., Pedraza C., Jamaica D., Rodriguez L., A Deep Learning Approach for Weed Detection in Lettuce Crops Using Multispectral Images, AgriEngineering, 2, (2020); Ferreira A.S., Freitas D.M., Goncalves da Silva G., Pistori H., Folhes M.T., Weed detection in soybean crops using ConvNets, Comput. Electron. Agric, 143, pp. 314-324, (2017); Santos L., Santos F.N., Oliveira P.M., Shinde P., Deep Learning Applications in Agriculture: A Short Review, Robot 2019: Fourth Iberian Robotics Conference. Advances in Intelligent Systems and Computing, 1092, (2019); Dokic K., Blaskovic L., Mandusic D., From machine learning to deep learning in agriculture—The quantitative review of trends, IOP Conf. Ser. Earth Environ. Sci, 614, (2020); Tian H., Wang T., Yadong Y., Qiao X., Li Y., Computer vision technology in agricultural automation —A review, Inform. Process. Agric, 7, pp. 1-19, (2020); Khaki S., Pham H., Han Y., Kuhl A., Convolutional Neural Networks for Image-Based Corn Kernel Detection and Counting; Yu J., Sharpe S., Schumann A., Boyd N., Deep learning for image-based weed detection in turfgrass, Eur. J. Agron, 104, pp. 78-84, (2019); Yu J., Schumann A., Cao Z., Sharpe S., Weed detection in perennial ryegrass with deep learning convolutional neural network, Front. Plant Sci, 10, (2019); Gao J., French A., Pound M., Deep convolutional neural networks for image based Convolvulus sepium detection in sugar beet fields, Plant Methods, 16, (2020); Scott S., Comparison of Object Detection and Patch-Based Classification Deep Learning Models on Mid- to Late-Season Weed Detection in UAV Imagery, Remote Sens, 12, (2020); Narvekar C., Rao M., Flower classification using CNN and transfer learning in CNN- Agriculture Perspective, Proceedings of the 3rd International Conference on Intelligent Sustainable Systems (ICISS), pp. 660-664; Sharma P., Berwal Y., Ghai W., Performance analysis of deep learning CNN models for disease detection in plants using image segmentation, Information. Process. Agric, 7, pp. 566-574, (2020); Du X., Lin T., Jin P., SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization, Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); Koh J., Spangenberg G., Kant S., Automated Machine Learning for High-Throughput Image-Based Plant Phenotyping, Remote Sens, 13, (2021); Shah S., Wu W., Lu Q., AmoebaNet: An SDN-enabled network service for big data science, J. Netw. Comput. Appl, 119, pp. 70-82, (2018); Yao L., Xu H., Zhang W., SM-NAS: Structural-to-Modular Neural Architecture Search for Object Detection, Proc. AAAI Conf. Artif. Intell, 34, pp. 12661-12668, (2020); Jia X., Yang X., Yu X., Gao H., A Modified CenterNet for Crack Detection of Sanitary Ceramics, Proceedings of the IECON 2020—46th Annual Conference of the IEEE Industrial Electronics Society; Zhao K., Yan W.Q., Fruit Detection from Digital Images Using CenterNet, Geom. Vis, 1386, pp. 313-326, (2021); Xu M., Deng Z., Qi L., Jiang Y., Li H., Wang Y., Xing X., Fully convolutional network for rice seedling and weed image segmentation at the seedling stage in paddy fields, PLoS ONE, 14, (2019); Kong J., Wang H., Wang X., Jin X., Fang X., Lin S., Multi-stream hybrid architecture based on cross-level fusion strategy for fine-grained crop species recognition in precision agriculture, Comput. Electron. Agric, 185, (2021); Wosner O., Detection in Agricultural Contexts: Are We Close to Human Level? Computer Vision—ECCV 2020 Workshops, Lect. Notes Comput. Sci, (2020); Wu D., Lv S., Jiang M., Song H., Using channel pruning-based YOLO v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments, Comput. Electron. Agric, 178, (2020); Kuznetsova A., Maleva T., Soloviev V., Detecting Apples in Orchards Using YOLOv3 and YOLOv5 in General and Close-Up Images; Advances in Neural Networks—ISNN, (2020); Tian Y., Yang G., Wang Z., Apple detection during different growth stages in orchards using the improved YOLO-V3 model, Comput. Electron. Agric, 157, pp. 417-426, (2019); Wu D., Wu Q., Yin X., BoJiang B., Wang H., He D., Song H., Lameness detection of dairy cows based on the YOLOv3 deep learning algorithm and a relative step size characteristic vector, Biosyst. Eng, 189, pp. 150-163, (2020); Waheed A., Goyal M., Gupta D., Khanna A., Hassanien A.E., Pandey H.M., An optimized dense convolutional neural network model for disease recognition and classification in corn leaf, Comput. Electron. Agric, 175, (2020); Atila U., Ucar M., Akyol K., Ucar E., Plant leaf disease classification using EfficientNet deep learning model, Ecol. Inform, 61, (2021); Pang Y., Shi Y., Gao S., Jiang F., Veeranampalayam-Sivakumar A.-N., Thomson L., Luck J., Liu C., Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery, Comput. Electron. Agric, 178, (2020); Liang F., Tian Z., Dong M., Cheng S., Sun L., Li H., Chen Y., Zhang G., Efficient neural network using pointwise convolution kernels with linear phase constraint, Neurocomputing, 423, pp. 572-579, (2021); Taravat A., Wagner M.P., Bonifacio R., Petit D., Advanced Fully Convolutional Networks for Agricultural Field Boundary Detection, Remote Sens, 13, (2021); Isufi E., Pocchiari M., Hanjalic A., Accuracy-diversity trade-off in recommender systems via graph convolutions, Inf. Process. Managem, 58, (2021); Wei Y., Gu K., Tan L., A positioning method for maize seed laser-cutting slice using linear discriminant analysis based on isometric distance measurement, Inf. Process. Agric, (2021); Koo J., Klabjan D., Utke J., Combined Convolutional and Recurrent Neural Networks for Hierarchical Classification of Images, (2019); Agarap A.F.M., An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification, (2017); Khaki S., Wang L., Archontoulis S., A CNN-RNN Framework for Crop Yield Prediction, Front. Plant Sci, 10, (2020); Dyrmann M., Jorgensen R.H., Midtiby H.S., RoboWeedSupport—Detection of weed locations in leaf occluded cereal crops using a fully convolutional neural network, Adv. Anim. Biosci, 8, pp. 842-847, (2017); Barth R., Hemming J., Henten V., Optimising realism of synthetic images using cycle generative adversarial networks for improved part segmentation, Comput. Electron. Agric, 173, (2020); Nguyen N., Tien D., Thanh D., An Evaluation of Deep Learning Methods for Small Object Detection, J. Electr. Comput. Eng, (2020); Chen C., Liu M., Tuzel O., Xiao J., R-CNN for Small Object Detection, Comput. Vis, (2017); Yu Y., Zhang K., Li Y., Zhang D., Fruit detection for strawberry harvesting robot in non-structural environment based on Mask-RCNN, Comput. Electron. Agric, 163, (2019); Boukhris L., Abderrazak J., Besbes H., Tailored Deep Learning based Architecture for Smart Agriculture, Proceedings of the 2020 International Wireless Communications and Mobile Computing (IWCMC); Basodi S., Chunya C., Zhang H., Pan Y., Gradient Amplification: An efficient way to train deep neural networks; Kurniawan A., Administering NVIDIA Jetson Nano, IoT Projects with NVIDIA Jetson Nano, (2021); Kurniawan A., NVIDIA Jetson Nano, IoT Projects with NVIDIA Jetson Nano, (2021); Verucchi M., Brilli G., Sapienza D., Verasani M., Arena M., Gatti F., Capotondi A., Cavicchioli R., Bertogna M., Solieri M., A Systematic Assessment of Embedded Neural Networks for Object Detection, Proceedings of the 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA), pp. 937-944; Gasparovic M., Zrinjski M., Barkovic D., Radocaj D., An automatic method for weed mapping in oat fields based on UAV imagery, Comput. Electron. Agric, 173, (2020); Yano I.H., Alves J.R., Santiago W.E., Mederos B.J.T., Identification of weeds in sugarcane fields through images taken by UAV and random forest classifier, IFAC-Pap, 49, pp. 415-420, (2016); Zhou H., Zhang C., A Field Weed Density Evaluation Method Based on UAV Imaging and Modified U-Net, Remote Sens, 13, (2021); Bakhshipour A., Jafari A., Evaluation of support vector machine and artificial neural networks in weed detection using shape features, Comput. Electron. Agric, 145, pp. 153-160, (2018); Sudars K., Data For: Dataset of Annotated Food Crops and Weed Images for Robotic Computer Vision Control, Mendeley Data, (2021); Xu Y., He R., Gao Z., Li C., Zhai Y., Jiao Y., Weed density detection method based on absolute feature corner points in field, Agronomy, 10, (2020); Shorewala S., Ashfaque A.R.S., Verma U., Weed Density and Distribution Estimation for Precision Agriculture Using Semi-Supervised Learning","C. Andreasen; Department of Plant and Environmental Sciences, University of Copenhagen, Taastrup, Højbakkegaard Allé 13, DK 2630, Denmark; email: can@plen.ku.dk","","MDPI","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120067412"
"Arshad M.A.; Khan S.H.; Qamar S.; Khan M.W.; Murtza I.; Gwak J.; Khan A.","Arshad, Muhammad Arif (57212308371); Khan, Saddam Hussain (57219645789); Qamar, Suleman (57462119000); Khan, Muhammad Waleed (57211373770); Murtza, Iqbal (56046240600); Gwak, Jeonghwan (36620985200); Khan, Asifullah (57421424200)","57212308371; 57219645789; 57462119000; 57211373770; 56046240600; 36620985200; 57421424200","Drone Navigation Using Region and Edge Exploitation-Based Deep CNN","2022","IEEE Access","10","","","95441","95450","9","4","10.1109/ACCESS.2022.3204876","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137869465&doi=10.1109%2fACCESS.2022.3204876&partnerID=40&md5=6a9a0c13aa87f3f06259f3e939283940","Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan; Centres of Excellence in Science and Applied Technologies, Islamabad, 44000, Pakistan; Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan; University of Engineering and Applied Sciences, Department of Computer Systems Engineering, Swat, 19060, Pakistan; Department of Mechanical and Aerospace Engineering, Columbus, 43210, OH, United States; The Ohio State University, Center for Automotive Research, Columbus, 43212, OH, United States; Air University, Faculty of Computing and Ai, Department of Creative Technologies, Islamabad, 44230, Pakistan; Department of Software, Korea National University of Transportation, Chungju, 27469, South Korea; Korea National University of Transportation, Department of Biomedical Engineering, Chungju, 27469, South Korea; Korea National University of Transportation, Department of Ai Robotics Engineering, Chungju, 27469, South Korea; Korea National University of Transportation, Department of It and Energy Convergence (BK21 FOUR), Chungju, 27469, South Korea; Pakistan Institute of Engineering and Applied Sciences, Deep Learning Laboratory, Center for Mathematical Sciences, Nilore, Islamabad, 45650, Pakistan","Arshad M.A., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Centres of Excellence in Science and Applied Technologies, Islamabad, 44000, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan; Khan S.H., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, University of Engineering and Applied Sciences, Department of Computer Systems Engineering, Swat, 19060, Pakistan; Qamar S., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan; Khan M.W., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan, Department of Mechanical and Aerospace Engineering, Columbus, 43210, OH, United States, The Ohio State University, Center for Automotive Research, Columbus, 43212, OH, United States; Murtza I., Air University, Faculty of Computing and Ai, Department of Creative Technologies, Islamabad, 44230, Pakistan; Gwak J., Department of Software, Korea National University of Transportation, Chungju, 27469, South Korea, Korea National University of Transportation, Department of Biomedical Engineering, Chungju, 27469, South Korea, Korea National University of Transportation, Department of Ai Robotics Engineering, Chungju, 27469, South Korea, Korea National University of Transportation, Department of It and Energy Convergence (BK21 FOUR), Chungju, 27469, South Korea; Khan A., Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Pieas Artificial Intelligence Center, Nilore, Islamabad, 45650, Pakistan, Pakistan Institute of Engineering and Applied Sciences, Deep Learning Laboratory, Center for Mathematical Sciences, Nilore, Islamabad, 45650, Pakistan","Drones are unmanned aerial vehicles (UAV) utilized for a broad range of functions, including delivery, aerial surveillance, traffic monitoring, architecture monitoring, and even War-field. Drones confront significant obstacles while navigating independently in complex and highly dynamic environments. Moreover, the targeted objects within a dynamic environment have irregular morphology, occlusion, and minor contrast variation with the background. In this regard, a novel deep Convolutional Neural Network(CNN) based data-driven strategy is proposed for drone navigation in the complex and dynamic environment. The proposed Drone Split-Transform-and-Merge Region-and-Edge (Drone-STM-RENet) CNN is comprised of convolutional blocks where each block methodically implements region and edge operations to preserve a diverse set of targeted properties at multi-levels, especially in the congested environment. In each block, the systematic implementation of the average and max-pooling operations can deal with the region homogeneity and edge properties. Additionally, these convolutional blocks are merged at a multi-level to learn texture variation that efficiently discriminates the target from the background and helps obstacle avoidance. Finally, the Drone-STM-RENet generates steering angle and collision probability for each input image to control the drone moving while avoiding hindrances and allowing the UAV to spot risky situations and respond quickly, respectively. The proposed Drone-STM-RENet has been validated on two urban cars and bicycles datasets: udacity and collision-sequence, and achieved considerable performance in terms of explained variance (0.99), recall (95.47%), accuracy (96.26%), and F-score (91.95%). The promising performance of Drone-STM-RENet on urban road datasets suggests that the proposed model is generalizable and can be deployed for real-time autonomous drones navigation and real-world flights.  © 2013 IEEE.","convolutional neural network; drone; drone split transform merge; perception and autonomy; Residual network","Air navigation; Aircraft detection; Antennas; Complex networks; Convolution; Deep neural networks; Drones; Object recognition; Aerial vehicle; Convolutional neural network; Drone split transform merge; Dynamic environments; Multilevels; Objects recognition; Perception and autonomy; Performance; Residual network; Robots","","","","","","","Mushtaq A., Haq I.U., Imtiaz M.U., Khan A., Shafiq O., Traffic flow management of autonomous vehicles using deep reinforcement learning and smart rerouting, IEEE Access, 9, pp. 51005-51019, (2021); Scherer S., Rehder J., Achar S., Cover H., Chambers A., Nuske S., Singh S., River mapping from a flying robot: State estimation, river detec- tion, and obstacle mapping, Auto. Robots, 33, 1, pp. 189-214, (2012); Faessler M., Fontana F., Forster C., Mueggler E., Pizzoli M., Scaramuzza D., Autonomous, vision-based flight and live dense 3D map- ping with a quadrotor micro aerial vehicle, J. Field Robot., 33, 4, pp. 431-450, (2016); Qamar S., Khan S.H., Arshad M.A., Qamar M., Khan A., Autonomous drone swarm navigation and multi-target tracking in 3D environments with dynamic obstacles, (2022); Shen S., Mulgaonkar Y., Michael N., Kumar V., Multi-sensor fusion for robust autonomous flight in indoor and outdoor environments with a rotorcraft MAV, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 4974-4981, (2014); Lee T., McKeever S., Courtney J., Flying free: A research overview of deep learning in drone navigation autonomy, Drones, 5, 2, (2021); Khan A., Shamsi M.H., Choi T.-S., Correlating dynamical mechan- ical properties with temperature and clay composition of polymer-clay nanocomposites, Comput. Mater. Sci., 45, 2, pp. 257-265, (2009); Tahir M., Khan A., Majid A., Protein subcellular localization offluorescence imagery using spatial and transform domain features, Bioin- formatics, 28, 1, pp. 91-97, (2012); Khan S.H., Sohail A., Khan A., Hassan M., Lee Y.S., Alam J., Basit A., Zubair S., COVID-19 detection in chest X-ray images using deep boosted hybrid learning, Comput. Biol. Med., 137, (2021); Giusti A., Guzzi J., Ciresan D.C., He F.L., Rodriguez J.P., Fontana F., Faessler M., Forster C., Schmidhuber J., Caro G.D., Scaramuzza D., A machine learning approach to visual perception of forest trails for mobile robots, IEEE Robot. Automat. Lett., 1, 2, pp. 661-667, (2015); Ross S., Melik-Barkhudarov N., Shankar K.S., Wendel A., Dey D., Bagnell J.A., Hebert M., Learning monocular reactive UAV control in cluttered natural environments, Proc. IEEE Int. Conf. Robot. Autom., pp. 1765-1772, (2013); Khan A., Sohail A., Ali A., A new channel boosted convolutional neural network using transfer learning, (2018); Lillicrap T.P., Hunt J.J., Pritzel A., Heess N., Erez T., Tassa Y., Silver D., Wierstra D., Continuous control with deep reinforcement learning, (2015); Khan S.H., Sohail A., Zafar M.M., Khan A., Coronavirus dis- ease analysis using chest X-ray images and a novel deep convolutional neural network, Photodiagnosis Photodyn. Therapy, 35, (2021); Khan A., Khan S.H., Saif M., Batool A., Sohail A., Khan M.W., A survey of deep learning techniques for the analysis of COVID-19 and their usability for detecting omicron, (2022); Long J., Et al., A novel self-training semi-supervised deep learning approach for machinery fault diagnosis, Int. J. Prod. Res., pp. 1-14, (2022); Asam M., Hussain S.J., Mohatram M., Khan S.H., Jamal T., Zafar A., Khan A., Ali M.U., Zahoora U., Detection of exceptional malware variants using deep boosted feature spaces and machine learning, Appl. Sci., 11, 21, (2021); Asam M., Khan S.H., Jamal T., Zahoora U., Khan A., Malware classification using deep boosted learning, (2021); Chamlawi R., Khan A., Idris A., Wavelet based image authentication and recovery, J. Comput. Sci. Technol., 22, 6, pp. 795-804, (2007); Asam M., Khan S.H., Jamal T., Khan A., IoT malware detection architecture using a novel channel boosted and squeezed CNN, (2022); Gandhi D., Pinto L., Gupta A., Learning to fly by crashing, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 3948-3955, (2017); Yang Z., Long J., Zi Y., Zhang S., Li C., Incremental novelty identification from initially one-class learning to unknown abnormality classification, IEEE Trans. Ind. Electron., 69, 7, pp. 7394-7404, (2022); Loquercio A., Maqueda A.I., Blanco C.R.D., Scaramuzza D., DroNet: Learning to fly by driving, IEEE Robot. Autom. Lett., 3, 2, pp. 1088-1095, (2018); Javadi S., Dahl M., Pettersson M.I., Vehicle detection in aerial images based on 3D depth maps and deep neural networks, IEEE Access, 9, pp. 8381-8391, (2021); Lin H.-Y., Peng X.-Z., Autonomous quadrotor navigation with vision based obstacle avoidance and path planning, IEEE Access, 9, pp. 102450-102459, (2021); Lynen S., Sattler T., Bosse M., Hesch J.A., Pollefeys M., Siegwart R., Get out of my lab: Large-scale, real-time visual-inertial localization, Robotics: Science and Systems, 1, (2015); Kouris A., Bouganis C.-S., Learning to fly by MySelf: A self supervised CNN-based approach for autonomous navigation, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 1-9, (2018); Taha B., Shoufan A., Machine learning-based drone detection and classification: State-of-The-art in research, IEEE Access, 7, pp. 138669-138682, (2019); Badrinarayanan V., Kendall A., Cipolla R., SegNet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 39, 12, pp. 2481-2495, (2017); Butt M.Q., Rahman A.U., Audiovisual saliency prediction in uncategorized video sequences based on audio-video correlation, (2021); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 779-788, (2016); Girshick R., Fast r-cnn, Proc. IEEE Int. Conf. Comput. Vis. (ICCV), pp. 1440-1448, (2015); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., LeCun Y., OverFeat: Integrated recognition, localization and detection using convo lutional networks, (2013); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, Proc. Int. Conf. Med. Image Com- put. Comput.-Assist. Intervent. Cham, Switzerland: Springer, pp. 234-241, (2015); Wang X., Cheng P., Liu X., Uzochukwu B., Fast and accurate, convolutional neural network based approach for object detection from UAV, Proc. 44th Annu. Conf. IEEE Ind. Electron. Soc. (IECON), pp. 3171-3175, (2018); Saripalli S., Montgomery J.F., Sukhatme G.S., Vision-based autonomous landing of an unmanned aerial vehicle, Proc. IEEE Int. Conf. Robot. Autom., pp. 2799-2804, (2002); Scaramuzza D., Achtelik M.C., Doitsidis L., Friedrich F., Kosmatopoulos E., Martinelli A., Achtelik M.W., Chli M., Chatzichristofis S., Kneip L., Gurdan D., Vision-controlled micro flying robots: From system design to autonomous navigation and mapping in GPS-denied environments, IEEE Robot. Automat. Mag., 21, 3, pp. 26-40, (2014); Meier L., Tanskanen P., Heng L., Lee G.H., Fraundorfer F., Pollefeys M., PIXHAWK: A micro aerial vehicle design for autonomous flight using onboard computer vision, Auto. Robots, 33, 1-2, pp. 21-39, (2012); Kendoul F., Fantoni I., Nonami K., Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles, Robot. Auto. Syst., 57, 6-7, pp. 591-602, (2009); Ashraf M.W., Sultani W., Shah M., Dogfight: Detecting drones from drones videos, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7067-7076, (2021); Xie S., Girshick R., Dollar P., Tu Z., He K., Aggregated residual transformations for deep neural networks, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1492-1500, (2017); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Jiang P., Chen Y., Liu B., He D., Liang C., Real-time detection of apple leaf diseases using deep learning approach based on improved convolutional neural networks, IEEE Access, 7, pp. 59069-59080, (2019); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1-9, (2015); Pan Z., Yu W., Yi X., Khan A., Yuan F., Zheng Y., Recent progress on generative adversarial networks (GANs): A survey, IEEE Access, 7, pp. 36322-36333, (2019); Khan S.H., Sohail A., Khan A., Lee Y.-S., COVID-19 detection in chest X-ray images using a new channel boosted CNN, Diagnostics, 12, 2, (2022); Khan S.H., Shah N.S., Nuzhat R., Majid A., Alquhayz H., Khan A., Malaria parasite classification framework using a novel channel squeezed and boosted CNN, Microscopy, (2022); Tian G., Sun Y., Liu Y., Zeng X., Wang M., Liu Y., Zhang J., Chen J., Adding before pruning: Sparse filter fusion for deep convolutional neural networks via auxiliary attention, IEEE Trans. Neural Netw. Learn. Syst., (2021); Christodoulidis S., Anthimopoulos M., Ebner L., Christe A., Mougiakakou S., Multisource transfer learning with convolutional neural networks for lung pattern analysis, IEEE J. Biomed. Health Inform., 21, 1, pp. 76-84, (2017); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Mancini M., Costante G., Valigi P., Ciarfuglia T.A., Delmerico J., Scaramuzza D., Toward domain independence for learning-based monocular depth estimation, IEEE Robot. Autom. Lett., 2, 3, pp. 1778-1785, (2017); Chew R., Rineer J., Beach R., O'Neil M., Ujeneza N., Lapidus D., Miano T., Hegarty-Craver M., Polly J., Temple D.S., Deep neural net- works and transfer learning for food crop identification in UAV images, Drones, 4, 1, (2020); Back S., Cho G., Oh J., Tran X.-T., Oh H., Autonomous UAV trail navigation with obstacle avoidance using deep neural networks, J. Intell. Robotic Syst., 100, 3-4, pp. 1195-1211, (2020); Kazakova A., Parallelization of autonomous driving tasks for safe high-speed vehicle control; Smolyanskiy N., Kamenev A., Smith J., Birchfield S., Toward low- flying autonomous MAV trail navigation using deep neural networks for environmental awareness, Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS), pp. 4241-4247, (2017); Yang S., Konam S., Ma C., Rosenthal S., Veloso M., Scherer S., Obstacle avoidance through deep networks based intermediate percep- tion, (2017); Pfeiffer M., Schaeuble M., Nieto J., Siegwart R., Cadena C., From perception to decision: A data-driven approach to end-to-end motion plan- ning for autonomous ground robots, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 1527-1533, (2017); Gao W., Hsu D., Lee W.S., Shen S., Subramanian K., Intention- Net: Integrating planning and deep learning for goal-directed autonomous navigation, Proc. Conf. Robot Learn., pp. 185-194, (2017); Richter C., Roy N., Safe visual navigation via deep learning and novelty detection, Proc. Robot., Sci. Syst. XIII, (2017)","A. Khan; Pattern Recognition Lab, Department of Computer and Information Sciences, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Nilore, 45650, Pakistan; email: asif@pieas.edu.pk; J. Gwak; Department of Software, Korea National University of Transportation, Chungju, 27469, South Korea; email: jgwak@ut.ac.kr","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137869465"
"Hoeser T.; Bachofer F.; Kuenzer C.","Hoeser, Thorsten (57216967331); Bachofer, Felix (36960527400); Kuenzer, Claudia (55927784300)","57216967331; 36960527400; 55927784300","Object detection and image segmentation with deep learning on earth observation data: A review-part II: Applications","2020","Remote Sensing","12","18","3053","","","","98","10.3390/RS12183053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092269929&doi=10.3390%2fRS12183053&partnerID=40&md5=d10875362df915bd0ef97635a5bf923e","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany; Department of Remote Sensing, Institute of Geography and Geology, University Würzburg, Am Huband, Wuerzburg, D-97074, Germany","Hoeser T., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany; Bachofer F., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany; Kuenzer C., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Münchner Straße 20, Wessling, D-82234, Germany, Department of Remote Sensing, Institute of Geography and Geology, University Würzburg, Am Huband, Wuerzburg, D-97074, Germany","In Earth observation (EO), large-scale land-surface dynamics are traditionally analyzed by investigating aggregated classes. The increase in data with a very high spatial resolution enables investigations on a fine-grained feature level which can help us to better understand the dynamics of land surfaces by taking object dynamics into account. To extract fine-grained features and objects, the most popular deep-learning model for image analysis is commonly used: the convolutional neural network (CNN). In this review, we provide a comprehensive overview of the impact of deep learning on EO applications by reviewing 429 studies on image segmentation and object detection with CNNs. We extensively examine the spatial distribution of study sites, employed sensors, used datasets and CNN architectures, and give a thorough overview of applications in EO which used CNNs. Our main finding is that CNNs are in an advanced transition phase from computer vision to EO. Upon this, we argue that in the near future, investigations which analyze object dynamics with CNNs will have a significant impact on EO research. With a focus on EO applications in this Part II, we complete the methodological review provided in Part I. © 2020 by the authors.","AI; Artificial intelligence; CNN; Convolutional neural networks; Deep learning; Earth observation; Image segmentation; Machine learning; Neural networks; Object detection","Convolutional neural networks; Dynamics; Image segmentation; Object detection; Object recognition; Observatories; Surface measurement; Earth observation data; Earth observations; Feature level; Land surface; Learning models; Object dynamics; Transition phase; Very high spatial resolutions; Deep learning","","","","","","","Marconcini M., Metz-Marconcini A., Ureyen S., Palacios-Lopez D., Hanke W., Bachofer F., Zeidler J., Esch T., Gorelick N., Kakarla A., Et al., Outlining where humans live, the World Settlement Footprint 2015, Sci. Data., 7, pp. 1-14, (2020); Zhu Z., Bi J., Pan Y., Ganguly S., Anav A., Xu L., Samanta A., Piao S., Nemani R.R., Myneni R.B., Global Data Sets of Vegetation Leaf Area Index (LAI)3g and Fraction of Photosynthetically Active Radiation (FPAR)3g Derived from Global Inventory Modeling and Mapping Studies (GIMMS) Normalized Difference Vegetation Index (NDVI3g) for the Period 1981 to 2011, Remote Sens., 5, pp. 927-948, (2013); Klein I., Gessner U., Dietz A.J., Kuenzer C., Global WaterPack-A 250 m resolution dataset revealing the daily dynamics of global inland water bodies, Remote Sens. Environ., 198, pp. 345-362, (2017); Reichstein M., Camps-Valls G., Stevens B., Jung M., Denzler J., Carvalhais N., Prabhat. Deep learning and process understanding for data-driven Earth system science, Nature, 566, pp. 195-204, (2019); Long Y., Xia G.S., Li S., Yang W., Yang M.Y., Zhu X.X., Zhang L., Li D., DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation, arXiv., (2020); Krizhevsky A., Sutskever I., Hinton G.E., ImageNet Classification with Deep Convolutional Neural Networks, In Advances in Neural Information Processing Systems 25, pp. 1097-1105, (2012); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Et al., ImageNet Large Scale Visual Recognition Challenge, Int. J. Comput. Vis. (Ijcv), 115, pp. 211-252, (2015); Vinuesa R., Azizpour H., Leite I., Balaam M., Dignum V., Domisch S., Fellander A., Langhans S.D., Tegmark M., Nerini F.F., The role of artificial intelligence in achieving the Sustainable Development Goals, Nat. Commun., 11, pp. 1-10, (2020); Copernicus Masters. ESA Digital Twin Earth Challenge.; Ball J.E., Anderson D.T., Chan C.S., Comprehensive survey of deep learning in remote sensing: Theories, tools, and challenges for the community, J. Appl. Remote Sens., 11, pp. 1-54, (2017); Zhu X.X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote Sens. Mag., 5, pp. 8-36, (2017); Zhang L., Zhang L., Du B., Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art, IEEE Geosci. Remote Sens. Mag., 4, pp. 22-40, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B.A., Deep learning in remote sensing applications: A meta-analysis and review, Isprs J. Photogramm. Remote Sens., 152, pp. 166-177, (2019); Tsagkatakis G., Aidini A., Fotiadou K., Giannopoulos M., Pentari A., Tsakalides P., Survey of Deep-Learning Approaches for Remote Sensing Observation Enhancement, Sensors, 19, (2019); Petersson H., Gustafsson D., Bergstrom D., Hyperspectral image analysis using deep learning-A review, In Proceedings of the 2016 Sixth International Conference on Image Processing Theory, pp. 1-6, (2016); Audebert N., Le Saux B., Lefevre S., Deep Learning for Classification of Hyperspectral Data: A Comparative Review, IEEE Geosci. Remote Sens. Mag., 7, pp. 159-173, (2019); Paoletti M., Haut J., Plaza J., Plaza A., Deep learning classifiers for hyperspectral imaging: A review, Isprs J. Photogramm. Remote Sens., 158, pp. 279-317, (2019); Zhu X.X., Montazeri S., Ali M., Hua Y., Wang Y., Mou L., Shi Y., Xu F., Bamler R., Deep Learning Meets SAR, arXiv, (2020); Khelifi L., Mignotte M., Deep Learning for Change Detection in Remote Sensing Images: Comprehensive Review and Meta-Analysis, arXiv, (2020); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Hoeser T., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part I: Evolution and Recent Trends, Remote Sens., 12, (2020); 2D Semantic Labeling Challenge.; Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, Int. J. Comput. Vis., 88, pp. 303-338, (2010); Everingham M., Eslami S.M., Gool L., Williams C.K., Winn J., Zisserman A., The Pascal Visual Object Classes Challenge: A Retrospective, Int. J. Comput. Vis., 111, pp. 98-136, (2015); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common Objects in Context, In Computer Vision - ECCV 2014, pp. 740-755, (2014); Cordts M., Omran M., Ramos S., Scharwachter T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The Cityscapes Dataset, In Proceedings of the CVPR Workshop on the Future of Datasets in Vision, 2, (2015); Cordts M., Omran M., Ramos S., Rehfeld T., Enzweiler M., Benenson R., Franke U., Roth S., Schiele B., The Cityscapes Dataset for Semantic Urban Scene Understanding, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213-3223, (2016); GRSS Data Fusion Contest; Mnih V., Machine Learning for Aerial Image Labeling, (2013); SpaceNet 1: Building Detection v1; SpaceNet 2: Building Detection v2; SpaceNet 4: Off-Nadir Buildings; Shermeyer J., Hogan D., Brown J., Etten A.V., Weir N., Pacifici F., Haensch R., Bastidas A., Soenen S., Bacastow T., Lewis R., SpaceNet 6: Multi-Sensor All Weather Mapping Dataset, arXiv, (2020); Ji S., Wei S., Lu M., Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set, IEEE Trans. Geosci. Remote Sens., 57, pp. 574-586, (2019); Demir I., Koperski K., Lindenbaum D., Pang G., Huang J., Basu S., Hughes F., Tuia D., Raskar R., DeepGlobe 2018: A Challenge to Parse the Earth Through Satellite Images, In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 172-181, (2018); SpaceNet 3: Road Network Detection; Etten A.V., City-Scale Road Extraction from Satellite Imagery v2: Road Speeds and Travel Times., In Proceedings of the The IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1786-1795, (2020); Cheng G., Wang Y., Xu S., Wang H., Xiang S., Pan C., Automatic Road Detection and Centerline Extraction via Cascaded End-to-End Convolutional Neural Network, IEEE Trans. Geosci. Remote Sens., 55, pp. 3322-3337, (2017); Cheng G., Han J., Zhou P., Guo L., Multi-class geospatial object detection and geographic image classification based on collection of part detectors, Isprs J. Photogramm. Remote Sens., 98, pp. 119-132, (2014); Xia G.S., Bai X., Ding J., Zhu Z., Belongie S., Luo J., Datcu M., Pelillo M., Zhang L., DOTA: A Large-Scale Dataset for Object Detection in Aerial Images, In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3974-3983, (2018); Long Y., Gong Y., Xiao Z., Liu Q., Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks, IEEE Trans. Geosci. Remote Sens., 55, pp. 2486-2498, (2017); Li J., Qu C., Shao J., Ship detection in SAR images based on an improved faster R-CNN, In Proceedings of the 2017 SAR in Big Data Era: Models, pp. 1-6, (2017); Huang L., Liu B., Li B., Guo W., Yu W., Zhang Z., Yu W., OpenSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 195-208, (2018); Li B., Liu B., Huang L., Guo W., Zhang Z., Yu W., OpenSARShip 2.0: A large-volume dataset for deeper interpretation of ship targets in Sentinel-1 imagery, In Proceedings of the 2017 SAR in Big Data Era: Models, pp. 1-5, (2017); Liu K., Mattyus G., Fast Multiclass Vehicle Detection on Aerial Images, IEEE Geosci. Remote Sens. Lett., 12, pp. 1938-1942, (2015); Razakarivony S., Jurie F., Vehicle detection in aerial imagery: A small target detection benchmark, J. Vis. Commun. Image Represent., 34, pp. 187-203, (2016); Mou L., Zhu X.X., Vehicle Instance Segmentation From Aerial Image and Video Using a Multitask Learning Residual Fully Convolutional Network, IEEE Trans. Geosci. Remote Sens., 56, pp. 6699-6711, (2018); Isikdogan F., Bovik A., Passalacqua P., Learning a River Network Extractor Using an Adaptive Loss Function, IEEE Geosci. Remote Sens. Lett., 15, pp. 813-817, (2018); Kong F., Huang B., Bradbury K., Malof J.M., The Synthinel-1 dataset: A collection of high resolution synthetic overhead imagery for building segmentation, In Proceedings of the 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1803-1812, (2020); Zhang F., Du B., Zhang L., Xu M., Weakly Supervised Learning Based on Coupled Convolutional Neural Networks for Aircraft Detection, IEEE Trans. Geosci. Remote Sens., 54, pp. 5553-5563, (2016); Ji J., Zhang T., Yang Z., Jiang L., Zhong W., Xiong H., Aircraft Detection from Remote Sensing Image Based on A Weakly Supervised Attention Model, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 322-325, (2019); Wu X., Hong D., Tian J., Kiefl R., Tao R., A Weakly-Supervised Deep Network for DSM-Aided Vehicle Detection, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1318-1321, (2019); Kaiser P., Wegner J.D., Lucchi A., Jaggi M., Hofmann T., Schindler K., Learning Aerial Image Segmentation From Online Maps, IEEE Trans. Geosci. Remote Sens., 55, pp. 6054-6068, (2017); Krylov V.A., de Martino M., Moser G., Serpico S.B., Large urban zone classification on SPOT-5 imagery with convolutional neural networks, In Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), pp. 1796-1799, (2016); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification, IEEE Trans. Geosci. Remote Sens., 55, pp. 645-657, (2017); Bittner K., Adam F., Cui S., Korner M., Reinartz P., Building Footprint Extraction From VHR Remote Sensing Images Combined With Normalized DSMs Using Fused Fully Convolutional Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 2615-2629, (2018); Voinov S., Krause D., Schwarz E., Towards Automated Vessel Detection and Type Recognition from VHR Optical Satellite Images, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 4823-4826, (2018); Piramanayagam S., Saber E., Schwartzkopf W., Koehler F.W., Supervised Classification of Multisensor Remotely Sensed Images Using a Deep Learning Framework, Remote Sens., 10, (2018); Kim J.H., Lee H., Hong S.J., Kim S., Park J., Hwang J.Y., Choi J.P., Objects Segmentation From High-Resolution Aerial Images Using U-Net With Pyramid Pooling Layers, IEEE Geosci. Remote Sens. Lett., 16, pp. 115-119, (2019); Shahzad M., Maurer M., Fraundorfer F., Wang Y., Zhu X.X., Buildings Detection in VHR SAR Images Using Fully Convolution Neural Networks, IEEE Trans. Geosci. Remote Sens., 57, pp. 1100-1116, (2019); Shi Y., Li Q., Zhu X., Building Footprint Extraction with Graph Convolutional Network, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5136-5139, (2019); Wu S., Du C., Chen H., Xu Y., Guo N., Jing N., Road Extraction from Very High Resolution Images Using Weakly labeled OpenStreetMap Centerline, Isprs Int. J. -Geo, 8, (2019); Vargas-Munoz J.E., Lobry S., Falcao A.X., Tuia D., Correcting rural building annotations in OpenStreetMap using convolutional neural networks, Isprs J. Photogramm. Remote Sens., 147, pp. 283-293, (2019); Huang J., Zhang X., Xin Q., Sun Y., Zhang P., Automatic building extraction from high-resolution aerial images and LiDAR data using gated residual refinement network, Isprs J. Photogramm. Remote Sens., 151, pp. 91-105, (2019); Griffiths D., Boehm J., Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours, Isprs J. Photogramm. Remote Sens., 154, pp. 70-83, (2019); Li W., He C., Fang J., Zheng J., Fu H., Yu L., Semantic Segmentation-Based Building Footprint Extraction Using Very High-Resolution Satellite Images and Multi-Source GIS Data, Remote Sens., 11, (2019); Manandhar P., Marpu P.R., Aung Z., Melgani F., Towards Automatic Extraction and Updating of VGI-Based Road Networks Using Deep Learning, Remote Sens., 11, (2019); Zeng F., Cheng L., Li N., Xia N., Ma L., Zhou X., Li M., A Hierarchical Airport Detection Method Using Spatial Analysis and Deep Learning, Remote Sens., 11, (2019); Schmitt M., Prexl J., Ebel P., Liebel L., Zhu X.X., Weakly Supervised Semantic Segmentation of Satellite Images for Land Cover Mapping - Challenges and Opportunities, arXiv, (2020); Airborne Synthetic Aperture Radar (AIRSAR); Xiao Z., Liu Q., Tang G., Zhai X., Elliptic Fourier transformation-based histograms of oriented gradients for rotationally invariant object detection in remote-sensing images, Int. J. Remote Sens., 36, pp. 618-644, (2015); Liu Z., Yuan L., Weng L., Yang Y., A High Resolution Optical Satellite Image Dataset for Ship Recognition and Some New Baselines, In Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods - Volume 1: ICPRAM, pp. 324-331, (2017); Tong X., Lu Q., Xia G., Zhang L., Large-Scale Land Cover Classification in Gaofen-2 Satellite Imagery, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 3599-3602, (2018); Zhu H., Chen X., Dai W., Fu K., Ye Q., Jiao J., Orientation robust object detection in aerial images using deep convolutional neural network, In Proceedings of the 2015 IEEE International Conference on Image Processing (ICIP), pp. 3735-3739, (2015); Zhu X., Hu J., Qiu C., Shi Y., Kang J., Mou L., Bagheri H., Haberle M., Hua Y., Huang R., Et al., So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones Classification, IEEE Geosci. Remote Sens. Mag., (2020); Cheng G., Han J., Lu X., Remote Sensing Image Scene Classification: Benchmark and State of the Art, Proc. IEEE, 105, pp. 1865-1883, (2017); Maggiori E., Tarabalka Y., Charpiat G., Alliez P., Can Semantic Labeling Methods Generalize to Any City? The Inria Aerial Image Labeling Benchmark, In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, (2017); Kang M., Ji K., Leng X., Lin Z., Contextual Region-Based Convolutional Neural Network with Multilayer Fusion for SAR Ship Detection, Remote Sens., 9, (2017); Liu W., Ma L., Chen H., Arbitrary-Oriented Ship Detection Framework in Optical Remote-Sensing Images, IEEE Geosci. Remote Sens. Lett., 15, pp. 937-941, (2018); Li Q., Mou L., Liu Q., Wang Y., Zhu X.X., HSF-Net: Multiscale Deep Feature Embedding for Ship Detection in Optical Remote Sensing Imagery, IEEE Trans. Geosci. Remote Sens., 56, pp. 7147-7161, (2018); Zhang X., Wang H., Xu C., Lv Y., Fu C., Xiao H., He Y., A Lightweight Feature Optimizing Network for Ship Detection in SAR Image, IEEE Access, 7, pp. 141662-141678, (2019); Zhang T., Zhang X., Shi J., Wei S., Depthwise Separable Convolution Neural Network for High-Speed SAR Ship Detection, Remote Sens., 11, (2019); Jiao J., Zhang Y., Sun H., Yang X., Gao X., Hong W., Fu K., Sun X., A Densely Connected End-to-End Neural Network for Multiscale and Multiscene SAR Ship Detection, IEEE Access, 6, pp. 20881-20892, (2018); Wu F., Zhou Z., Wang B., Ma J., Inshore Ship Detection Based on Convolutional Neural Network in Optical Satellite Images, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 4005-4015, (2018); Deng Z., Sun H., Zhou S., Zhao J., Learning Deep Ship Detector in SAR Images From Scratch, IEEE Trans. Geosci. Remote Sens., 57, pp. 4021-4039, (2019); You Y., Cao J., Zhang Y., Liu F., Zhou W., Nearshore Ship Detection on High-Resolution Remote Sensing Image via Scene-Mask R-CNN, IEEE Access, 7, pp. 128431-128444, (2019); You Y., Li Z., Ran B., Cao J., Lv S., Liu F., Broad Area Target Search System for Ship Detection via Deep Convolutional Neural Network, Remote Sens., 11, (2019); Zhang S., Wu R., Xu K., Wang J., Sun W., R-CNN-Based Ship Detection from High Resolution Remote Sensing Imagery, Remote Sens., 11, (2019); Fan W., Zhou F., Bai X., Tao M., Tian T., Ship Detection Using Deep Convolutional Neural Networks for PolSAR Images, Remote Sens., 11, (2019); Chen C., He C., Hu C., Pei H., Jiao L., A Deep Neural Network Based on an Attention Mechanism for SAR Ship Detection in Multiscale and Complex Scenarios, IEEE Access, 7, pp. 104848-104863, (2019); He Y., Sun X., Gao L., Zhang B., Ship Detection Without Sea-Land Segmentation for Large-Scale High-Resolution Optical Satellite Images, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 717-720, (2018); Gao L., He Y., Sun X., Jia X., Zhang B., Incorporating Negative Sample Training for Ship Detection Based on Deep Learning, Sensors, 19, (2019); Zhang Z., Guo W., Zhu S., Yu W., Toward Arbitrary-Oriented Ship Detection With Rotated Region Proposal and Discrimination Networks, IEEE Geosci. Remote Sens. Lett., 15, pp. 1745-1749, (2018); Wang J., Lu C., Jiang W., Simultaneous Ship Detection and Orientation Estimation in SAR Images Based on Attention Module and Angle Regression, Sensors, 18, (2018); Zhang Y., Zhang Y., Shi Z., Zhang J., Wei M., Rotationally Unconstrained Region Proposals for Ship Target Segmentation in Optical Remote Sensing, IEEE Access, 7, pp. 87049-87058, (2019); Chen J., Xie F., Lu Y., Jiang Z., Finding Arbitrary-Oriented Ships From Remote Sensing Images Using Corner Detection, IEEE Geosci. Remote Sens. Lett., (2019); Xiao X., Zhou Z., Wang B., Li L., Miao L., Ship Detection under Complex Backgrounds Based on Accurate Rotated Anchor Boxes from Paired Semantic Segmentation, Remote Sens., 11, (2019); Li M., Guo W., Zhang Z., Yu W., Zhang T., Rotated Region Based Fully Convolutional Network for Ship Detection, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 673-676, (2018); Wang T., Gu Y., Cnn Based Renormalization Method for Ship Detection in Vhr Remote Sensing Images, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 1252-1255, (2018); Fu K., Li Y., Sun H., Yang X., Xu G., Li Y., Sun X., A Ship Rotation Detection Model in Remote Sensing Images Based on Feature Fusion Pyramid Network and Deep Reinforcement Learning, Remote Sens., 10, (2018); Li S., Zhang Z., Li B., Li C., Multiscale Rotated Bounding Box-Based Deep Learning Method for Detecting Ship Targets in Remote Sensing Images, Sensors, 18, (2018); Sun J., Zou H., Deng Z., Cao X., Li M., Ma Q., Multiclass Oriented Ship Localization and Recognition In High Resolution Remote Sensing Images, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1288-1291, (2019); Voinov S., Heymann F., Bill R., Schwarz E., Multiclass Vessel Detection From High Resolution Optical Satellite Images Based On Deep Neural Networks, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 166-169, (2019); Ma J., Zhou Z., Wang B., Zong H., Wu F., Ship Detection in Optical Satellite Images via Directional Bounding Boxes Based on Ship Center and Orientation Prediction, Remote Sens., 11, (2019); Bi F., Hou J., Chen L., Yang Z., Wang Y., Ship Detection for Optical Remote Sensing Images Based on Visual Attention Enhanced Network, Sensors, 19, (2019); Feng Y., Diao W., Zhang Y., Li H., Chang Z., Yan M., Sun X., Gao X., Ship Instance Segmentation from Remote Sensing Images Using Sequence Local Context Module, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1025-1028, (2019); Dechesne C., Lefevre S., Vadaine R., Hajduch G., Fablet R., Ship Identification and Characterization in Sentinel-1 SAR Images with Multi-Task Deep Learning, Remote Sens., 11, (2019); Ma M., Chen J., Liu W., Yang W., Ship Classification and Detection Based on CNN Using GF-3 SAR Images, Remote Sens., 10, (2018); Lin H., Shi Z., Zou Z., Fully Convolutional Network With Task Partitioning for Inshore Ship Detection in Optical Remote Sensing Images, IEEE Geosci. Remote Sens. Lett., 14, pp. 1665-1669, (2017); Sun S., Lu Z., Liu W., Hu W., Li R., Shipnet for Semantic Segmentation on VHR Maritime Imagery, In Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6911-6914, (2018); Tang T., Zhou S., Deng Z., Lei L., Zou H., Arbitrary-Oriented Vehicle Detection in Aerial Imagery with Single Convolutional Neural Networks, Remote Sens., 9, (2017); Li Q., Mou L., Xu Q., Zhang Y., Zhu X.X., R3-Net: A Deep Network for Multioriented Vehicle Detection in Aerial Images and Videos, IEEE Trans. Geosci. Remote Sens., 57, pp. 5028-5042, (2019); Deng Z., Sun H., Zhou S., Zhao J., Zou H., Toward Fast and Accurate Vehicle Detection in Aerial Images Using Coupled Region-Based Convolutional Neural Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10, pp. 3652-3664, (2017); Schilling H., Bulatov D., Niessner R., Middelmann W., Soergel U., Detection of Vehicles in Multisensor Data via Multibranch Convolutional Neural Networks, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 4299-4316, (2018); Audebert N., Le Saux B., Lefevre S., Segment-before-Detect: Vehicle Detection and Classification through Semantic Segmentation of Aerial Images, Remote Sens., 9, (2017); Merkle N., Azimi S.M., Pless S., Kurz F., Semantic Vehicle Segmentation in Very High Resolution Multispectral Aerial Images Using Deep Neural Networks, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5045-5048, (2019); Koga Y., Miyazaki H., Shibasaki R., A CNN-Based Method of Vehicle Detection from Aerial Images Using Hard Example Mining, Remote Sens., 10, (2018); Gao Z., Ji H., Mei T., Ramesh B., Liu X., EOVNet: Earth-Observation Image-Based Vehicle Detection Network, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 3552-3561, (2019); Li S., Xu Y., Zhu M., Ma S., Tang H., Remote Sensing Airport Detection Based on End-to-End Deep Transferable Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett., 16, pp. 1640-1644, (2019); Zhang P., Niu X., Dou Y., Xia F., Airport Detection on Optical Satellite Images Using Deep Convolutional Neural Networks, IEEE Geosci. Remote Sens. Lett., 14, pp. 1183-1187, (2017); Chen F., Ren R., Van de Voorde T., Xu W., Zhou G., Zhou Y., Fast Automatic Airport Detection in Remote Sensing Images Using Convolutional Neural Networks, Remote Sens., 10, (2018); Cai B., Jiang Z., Zhang H., Yao Y., Nie S., Online Exemplar-Based Fully Convolutional Network for Aircraft Detection in Remote Sensing Images, IEEE Geosci. Remote Sens. Lett., 15, pp. 1095-1099, (2018); Chen Z., Zhang T., Ouyang C., End-to-End Airplane Detection Using Transfer Learning in Remote Sensing Images, Remote Sens., 10, (2018); Wang Y., Li H., Jia P., Zhang G., Wang T., Hao X., Multi-Scale DenseNets-Based Aircraft Detection from Remote Sensing Images, Sensors, 19, (2019); Zhao P., Gao H., Zhang Y., Li H., Yang R., An Aircraft Detection Method Based on Improved Mask R-CNN in Remotely Sensed Imagery, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1370-1373, (2019); Wang H., Gong Y., Wang Y., Wang L., Pan C., DeepPlane: A unified deep model for aircraft detection and recognition in remote sensing images, J. Appl. Remote Sens., 11, pp. 1-10, (2017); Hou B., Li J., Zhang X., Wang S., Jiao L., Object Detection and Trcacking Based on Convolutional Neural Networks for High-Resolution Optical Remote Sensing Video, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5433-5436, (2019); Xu Y., Wu L., Xie Z., Chen Z., Building Extraction in Very High Resolution Remote Sensing Imagery Using Deep Learning and Guided Filters, Remote Sens., 10, (2018); Lu X., Zhong Y., Zheng Z., Liu Y., Zhao J., Ma A., Yang J., Multi-Scale and Multi-Task Deep Learning Framework for Automatic Road Extraction, IEEE Trans. Geosci. Remote Sens., 57, pp. 9362-9377, (2019); Wei Y., Wang Z., Xu M., Road Structure Refined CNN for Road Extraction in Aerial Image, IEEE Geosci. Remote Sens. Lett., 14, pp. 709-713, (2017); Li Y., Guo L., Rao J., Xu L., Jin S., Road Segmentation Based on Hybrid Convolutional Network for High-Resolution Visible Remote Sensing Image, IEEE Geosci. Remote Sens. Lett., 16, pp. 613-617, (2019); Zhang X., Ma W., Li C., Wu J., Tang X., Jiao L., Fully Convolutional Network-Based Ensemble Method for Road Extraction From Aerial Images, IEEE Geosci. Remote Sens. Lett., (2019); He H., Yang D., Wang S., Wang S., Li Y., Road Extraction by Using Atrous Spatial Pyramid Pooling Integrated Encoder-Decoder Network and Structural Similarity Loss, Remote Sens., 11, (2019); Liu Y., Yao J., Lu X., Xia M., Wang X., Liu Y., RoadNet: Learning to Comprehensively Analyze Road Networks in Complex Urban Scenes From High-Resolution Remotely Sensed Images, IEEE Trans. Geosci. Remote Sens., 57, pp. 2043-2056, (2019); Hong Z., Ming D., Zhou K., Guo Y., Lu T., Road Extraction From a High Spatial Resolution Remote Sensing Image Based on Richer Convolutional Features, IEEE Access, 6, pp. 46988-47000, (2018); Yang X., Li X., Ye Y., Lau R.Y.K., Zhang X., Huang X., Road Detection and Centerline Extraction Via Deep Recurrent Convolutional Neural Network U-Net, IEEE Trans. Geosci. Remote Sens., 57, pp. 7209-7220, (2019); Azimi S.M., Fischer P., Korner M., Reinartz P., Aerial LaneNet: Lane-Marking Semantic Segmentation in Aerial Imagery Using Wavelet-Enhanced Cost-Sensitive Symmetric Fully Convolutional Neural Networks, IEEE Trans. Geosci. Remote Sens., 57, pp. 2920-2938, (2019); Zhao W., Du S., Emery W.J., Object-Based Convolutional Neural Network for High-Resolution Imagery Classification, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 10, pp. 3386-3396, (2017); Volpi M., Tuia D., Dense Semantic Labeling of Subdecimeter Resolution Images With Convolutional Neural Networks, IEEE Trans. Geosci. Remote Sens., 55, pp. 881-893, (2017); Audebert N., Le Saux B., Lefevre S., Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks, ISPRS J. Photogramm. Remote Sens., 140, pp. 20-32, (2018); Wang Y., Liang B., Ding M., Li J., Dense Semantic Labeling with Atrous Spatial Pyramid Pooling and Decoder for High-Resolution Remote Sensing Imagery, Remote Sens., 11, (2019); Liu S., Ding W., Liu C., Liu Y., Wang Y., Li H., ERN: Edge Loss Reinforced Semantic Segmentation Network for Remote Sensing Images, Remote Sens., 10, (2018); Luo H., Chen C., Fang L., Zhu X., Lu L., High-Resolution Aerial Images Semantic Segmentation Using Deep Fully Convolutional Network With Channel Attention Mechanism, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 12, pp. 3492-3507, (2019); Chen K., Fu K., Gao X., Yan M., Zhang W., Zhang Y., Sun X., Effective Fusion of Multi-Modal Data with Group Convolutions for Semantic Segmentation of Aerial Imagery, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 3911-3914, (2019); Zhang G., Lei T., Cui Y., Jiang P., A Dual-Path and Lightweight Convolutional Neural Network for High-Resolution Aerial Image Segmentation, ISPRS Int. J. Geo-Inf., 8, (2019); Cao Z., Diao W., Zhang Y., Yan M., Yu H., Sun X., Fu K., Semantic Labeling for High-Resolution Aerial Images Based on the DMFFNet, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 1021-1024, (2019); Jia Y., Ge Y., Chen Y., Li S., Heuvelink G.B., Ling F., Super-Resolution Land Cover Mapping Based on the Convolutional Neural Network, Remote Sens., 11, (2019); Guo S., Jin Q., Wang H., Wang X., Wang Y., Xiang S., Learnable Gated Convolutional Neural Network for Semantic Segmentation in Remote-Sensing Images, Remote Sens., 11, (2019); Basaeed E., Bhaskar H., Hill P., Al-Mualla M., Bull D., A supervised hierarchical segmentation of remote-sensing images using a committee of multi-scale convolutional neural networks, Int. J. Remote Sens., 37, pp. 1671-1691, (2016); Mou L., Hua Y., Zhu X.X., Spatial Relational Reasoning in Networks for Improving Semantic Segmentation of Aerial Images, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 5232-5235, (2019); Nogueira K., Dalla Mura M., Chanussot J., Schwartz W.R., dos Santos J.A., Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks, IEEE Trans. Geosci. Remote Sens., 57, pp. 7503-7520, (2019); Sun Y., Zhang X., Xin Q., Huang J., Developing a multi-filter convolutional neural network for semantic segmentation using high-resolution aerial imagery and LiDAR data, ISPRS J. Photogramm. Remote Sens., 143, pp. 3-14, (2018); Papadomanolaki M., Vakalopoulou M., Karantzalos K., A Novel Object-Based Deep Learning Framework for Semantic Segmentation of Very High-Resolution Remote Sensing Data: Comparison with Convolutional and Fully Convolutional Networks, Remote Sens., 11, (2019); Yue K., Yang L., Li R., Hu W., Zhang F., Li W., TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation, ISPRS J. Photogramm. Remote Sens., 156, pp. 1-13, (2019); Liu Y., Gross L., Li Z., Li X., Fan X., Qi W., Automatic Building Extraction on High-Resolution Remote Sensing Imagery Using Deep Convolutional Encoder-Decoder With Spatial Pyramid Pooling, IEEE Access, 7, pp. 128774-128786, (2019); Zhang Y., Gong W., Sun J., Li W., Web-Net: A Novel Nest Networks with Ultra-Hierarchical Sampling for Building Extraction from Aerial Imageries, Remote Sens., 11, (2019); Yang H.L., Yuan J., Lunga D., Laverdiere M., Rose A., Bhaduri B., Building Extraction at Scale Using Convolutional Neural Network: Mapping of the United States, IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 11, pp. 2600-2614, (2018); Hui J., Du M., Ye X., Qin Q., Sui J., Effective Building Extraction From High-Resolution Remote Sensing Images With Multitask Driven Deep Neural Network, IEEE Geosci. Remote Sens. Lett., 16, pp. 786-790, (2019); Wang S., Zhou L., He P., Quan D., Zhao Q., Liang X., Hou B., An Improved Fully Convolutional Network for Learning Rich Building Features, In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium, pp. 6444-6447, (2019); Guo Z., Wu G., Song X., Yuan W., Chen Q., Zhang H., Shi X., Xu M., Xu Y., Shibasaki R., Et al., Super-Resolution Integrated Building Semantic Segmentation for Multi-Source Remote Sensing Imagery, IEEE Access, 7, pp. 99381-99397, (2019); Lin J., Jing W., Song H., Chen G., ESFNet: Efficient Network for Building Extraction From High-Resolution Aerial Images, IEEE Access, 7, pp. 54285-54294, (2019); Schuegraf P., Bittner K., Automatic Building Footprint Extraction from Multi-Resolution Remote Sensing Images Using a Hybrid FCN, ISPRS Int. J. Geo-Inf., 8, (2019); Liu P., Liu X., Liu M., Shi Q., Yang J., Xu X., Zhang Y., Building Footprint Extraction from High-Resolution Images via Spatial Residual Inception Convolutional Neural Network, Remote Sens., 11, (2019); Ye Z., Fu Y., Gan M., Deng J., Comber A., Wang K., Building Extraction from Very High Resolution Aerial Imagery Using Joint Attention Deep Neural Network, Remote Sens., 11, (2019); Shrestha S., Vanneschi L., Improved Fully Convolutional Network with Conditional Random Fields for Building Extraction, Remote Sens., 10, (2018); Ji S., Wei S., Lu M., A scale robust convolutional neural network for automatic building extraction from aerial and satellite imagery, Int. J. Remote Sens., 40, pp. 3308-3322, (2019); Wen Q., Jiang K., Wang W., Liu Q., Guo Q., Li L., Wang P., Automatic Building Extraction from Google Earth Images under Complex Backgrounds Based on Deep Instance Segmentation Network, Sensors, 19, (2019); Shi Q., Liu M., Liu X., Liu P., Zhang P., Yang J., Li X., Domain Adaption for Fine-Grained Urban Village Extraction From Satellite Images, IEEE Geosci. Remote Sens. Lett., 17, pp. 1430-1434, (2020); Wang J., Kuffer M., Roy D., Pfeffer K., Deprivation pockets through the lens of convolutional neural networks, Remote Sens. Environ., 234, (2019); Persello C., Stein A., Deep Fully Convolutional Networks for the Detection of Informal Settlements in VHR Images, IEEE Geosci. Remote Sens. Lett., 14, pp. 2325-2329, (2017); Mboga N., Persello C., Bergado J.R., Stein A., Detection of Informal Settlements from VHR Images Using Convolutional Neural Networks, Remote Sens., 9, (2017)","T. Hoeser; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, Münchner Straße 20, D-82234, Germany; email: Thorsten.Hoeser@dlr.de","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092269929"
"Shill A.; Rahman M.A.","Shill, Apu (57272064000); Rahman, Md. Asifur (57207376228)","57272064000; 57207376228","Plant disease detection based on YOLOv3 and YOLOv4","2021","2021 International Conference on Automation, Control and Mechatronics for Industry 4.0, ACMI 2021","","","","","","","10","10.1109/ACMI53878.2021.9528179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115699851&doi=10.1109%2fACMI53878.2021.9528179&partnerID=40&md5=f6eb0cf6306b5c3f7b0410a941cff93b","Rajshahi University of Engineering and Technology, Computer Science and Engineering, Rajshahi, Bangladesh","Shill A., Rajshahi University of Engineering and Technology, Computer Science and Engineering, Rajshahi, Bangladesh; Rahman M.A., Rajshahi University of Engineering and Technology, Computer Science and Engineering, Rajshahi, Bangladesh","Although the loss of yield from plant disease can be precluded with early detection and treatment, they cause significant economic losses every year due to the dearth of cost-effective expert knowledge. Not only they are a great threat to the agricultural economy but also they are pernicious to the balance of the natural ecosystem. Many studies have been conducted to develop expert systems to alleviate losses by detecting the diseases at their earlier stage. However, most of these studies are limited to one or two plant species and a few varieties of diseases. This paper introduces a computer vision approach using the latest state of art You Only Look Once (YOLO) algorithms in developing an expert system, which can accurately identify numerous plant diseases from a diverse set of plant species. The latest algorithms from the YOLO family namely YOLO V3 and YOLO V4 have been incorporated in this research and the experimental result presented in this paper clearly reflects on the accuracy and suitability of the plant disease detection system.  © 2021 IEEE.","Computer Vision; Convolutional Neural Networks; Deep Learning; Object detection and recognition; Plant disease detection; PlantDoc dataset; YOLOv3; YOLOv4; You Only Look Once","Agricultural robots; Agriculture; Cost effectiveness; Expert systems; Industry 4.0; Losses; Agricultural economy; Cost effective; Economic loss; Expert knowledge; Natural ecosystem; Plant disease; Plant species; Arts computing","","","","","","","Chandra A.L., Desai S.V., Guo W., Balasubramanian V.N., Computer Vision with Deep Learning for Plant Phenotyping in Agriculture: A Survey, (2020); Oerke E.-C., Dehne H.-W., Schonbeck F., Weber A., Crop Production and Crop Protection: Estimated Losses in Major Food and Cash Crops, (2012); Agrios G., Plant Pathology 5th Edition: Elsevier Academic Press, pp. 79-103, (2005); Mohapatra T., Alagusundaram K., Jena J., Rathore N., Singh A., Singh S., Icar News, (2018); Dusseldorf M., Save Food; Sankaran S., Mishra A., Ehsani R., Davis C., A review of advanced techniques for detecting plant diseases, Computers and Electronics in Agriculture, 72, 1, pp. 1-13, (2010); Patil S.B., Bodhe S.K., Leaf disease severity measurement using image processing, International Journal of Engineering and Technology, 3, 5, pp. 297-301, (2011); Patil S.B., Bodhe S.K., Betel leaf area measurement using image processing, International Journal on Computer Science and Engineering, 3, 7, pp. 2656-2660, (2011); Grinblat G.L., Uzal L.C., Larese M.G., Granitto P.M., Deep learning for plant identification using vein morphological patterns, Computers and Electronics in Agriculture, 127, pp. 418-424, (2016); Fuentes A., Yoon S., Kim S.C., Park D.S., A robust deep-learningbased detector for real-time tomato plant diseases and pests recognition, Sensors, 17, 9, (2017); Barbedo J.G., Factors influencing the use of deep learning for plant disease recognition, Biosystems Engineering, 172, pp. 84-91, (2018); Chaari R., Ellouze F., Koubaa A., Qureshi B., Pereira N., Youssef H., Tovar E., Cyber-physical systems clouds: A survey, Computer Networks, 108, pp. 260-278, (2016); Koubaa A., Qureshi B., Sriti M.-F., Javed Y., Tovar E., A serviceoriented cloud-based management system for the internet-of-drones, 2017 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), pp. 329-335, (2017); Benjdira B., Khursheed T., Koubaa A., Ammar A., Ouni K., Car detection using unmanned aerial vehicles: Comparison between faster r-cnn and yolov3, 2019 1st International Conference on Unmanned Vehicle Systems-Oman (UVS), pp. 1-6, (2019); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A., Yolo9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271, (2017); Redmon J., Farhadi A., Yolov3: An Incremental Improvement, (2018); Bochkovskiy A., Wang C.-Y., Liao H.-Y.M., Yolov4: Optimal Speed and Accuracy of Object Detection, (2020); Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, (2014); Girshick R., Fast r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-cnn: Towards Realtime Object Detection with Region Proposal Networks, (2015); Wang C.-Y., Bochkovskiy A., Liao H.-Y.M., Scaled-yolov4: Scaling Cross Stage Partial Network, (2020); Singh D., Jain N., Jain P., Kayal P., Kumawat S., Batra N., Plantdoc: A dataset for visual plant disease detection, Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, pp. 249-253, (2020); Adigun J.G., Ground Truth Data for Object Detection in Autonomous Vehicle from A Driving Simulator; Mohanty S.P., Hughes D.P., Salathe M., Using deep learning for image-based plant disease detection, Frontiers in Plant Science, 7, (2016)","","","Institute of Electrical and Electronics Engineers Inc.","","2021 International Conference on Automation, Control and Mechatronics for Industry 4.0, ACMI 2021","8 July 2021 through 9 July 2021","Rajshahi","171642","","978-166543843-8","","","English","Int. Conf. Autom., Control Mechatronics Ind. 4.0, ACMI","Conference paper","Final","","Scopus","2-s2.0-85115699851"
"Türkdamar M.U.; Taşyürek M.; Öztürk C.","Türkdamar, Mehmet Uğur (57986057900); Taşyürek, Murat (57215586146); Öztürk, Celal (23091274400)","57986057900; 57215586146; 23091274400","Field Detection from Satellite Images with Deep Learning Methods","2023","6th International Conference on Inventive Computation Technologies, ICICT 2023 - Proceedings","","","","1","8","7","0","10.1109/ICICT57646.2023.10134299","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163449346&doi=10.1109%2fICICT57646.2023.10134299&partnerID=40&md5=1d6154742a3ee36f1c889e43ef3316d8","Department of Computer Engineering, Niğde Ömer Halisdemir University, Niğde, Turkey; Department of Computer Engineering, Kayseri University, Kayseri, Turkey; Department of Computer Engineering, Erciyes University, Kayseri, Turkey","Türkdamar M.U., Department of Computer Engineering, Niğde Ömer Halisdemir University, Niğde, Turkey; Taşyürek M., Department of Computer Engineering, Kayseri University, Kayseri, Turkey; Öztürk C., Department of Computer Engineering, Erciyes University, Kayseri, Turkey","Today, effective production is interrupted unless land ownership disputes are resolved. The state cannot make the necessary investments due to these disputes not being concluded, and the borders of the fields remain unclear. Artificial intelligence-based methods can be suggested to eliminate disagreements and uncertainty. By using convolutional neural network (CNN) based deep learning networks in which image data are meaningful, areas with primary importance in crop production have been identified in this study. With the CNN networks used by computer vision technology, meaningful information can be extracted from the image. Field detection processes were carried out in this study by using deep learning networks that learn from data. As remote sensing studies gain speed, the number of deep learning studies also increases. For this purpose, satellite images were first collected from the Google Earth website, and then these collected images were used in Faster R-CNN and SSD training, which gained a reputation for accuracy and speed. It is aimed to provide more efficient production and resolve disputes by detecting the fields from satellite images. From two different networks running, SSD outperformed Faster R-CNN in terms of both accuracy and run time. With an f1 score of %97.32, SSD gave Faster R-CNN %3.18 superiority. In the field object results in the test images, the SSD outperformed by detecting 12 more fields. In terms of run times, the SSD performed faster detections with a difference of 285.5ms in the experiments tried in one-third of the test images. © 2023 IEEE.","CNN; crop field detection; deep learning; faster region-CNN; satellite images; single shot detector","Convolutional neural networks; Cultivation; Deep learning; Learning systems; Remote sensing; Satellites; Convolutional neural network; Crop field detection; Crop fields; Deep learning; Fast region-convolutional neural network; Field detections; Satellite images; Single shot detector; Single-shot; Crops","","","","","","","Ozturk C., Tasyurek M., Turkdamar M.U., Transfer learning and fine-tuned transfer learning methods' effectiveness analyse in the cnnbased deep learning models, Concurrency and Computation: Practice and Experience, 35, 4; Berezina K., Ciftci O., Cobanoglu C., Robots, artificial intelligence, and service automation in restaurants, Robots, artificial intelligence, and service automation in travel, tourism and hospitality, (2019); Lu C., Tang X., Surpassing human-level face verification performance on lfw with gaussianface, Twenty-ninth AAAI conference on artificial intelligence, (2015); Spanaki K., Karafili E., Sivarajah U., Despoudi S., Irani Z., Artificial intelligence and food security: swarm intelligence of agritech drones for smart agrifood operations, Production Planning & Control, 33, 16, pp. 1498-1516, (2022); Biehl K., Danis D., Migration studies in Turkey from a gender perspective, (2020); Emrah A., Arzu K.A.N., Support Program for Rural Development Investments: The Case of Rural Economic Infrastructure Projects in Bolu in 2021, Kirsehir Ahi Evran University Journal of the Faculty of Agriculture, 2, 2, pp. 121-132; Aydin B., Ozkan E., Evaluation of fertilizer and soil analysis support in terms of producers: The case of Kırklareli province, Turkish Journal of Agriculture and Natural Sciences, 4, 3, pp. 302-310, (2017); Ozdemir Z., Supporting practices and results in Turkish agriculture, Istanbul University Faculty of Economics Journal, 47, pp. 1-4, (1989); Tuna Y., State intervention in agricultural product prices and the history of intervention price policy in Turkey, Istanbul University, Faculty of Economics, Journal, 47, pp. 1-4, (2011); Ulker E., Yuksel O., Ergul S., Status of Forage Crops Agriculture, Seed Production and Foreign Trade in Our Country, Uşak University Journal of Science and Natural Sciences, 4, 2, pp. 127-138; Sahin G., Cabuk S.N., Cetin M., The change detection in coastal settlements using image processing techniques: a case study of korfez, Environmental Science and Pollution Research, 29, 10, pp. 15172-15187, (2022); Cienciala A., Sobolewska-Mikulska K., Sobura S., Credibility of the cadastral data on land use and the methodology for their verification and update, Land Use Policy, 102, (2021); Li W., Wang D., Li M., Gao Y., Wu J., Yang X., Field detection of tiny pests from sticky trap images using deep learning in agricultural greenhouse, Computers and Electronics in Agriculture, 183, (2021); Chen F., Zhang Y., Zhang J., Liu L., Wu K., Rice false smut detection and prescription map generation in a complex planting environment, with mixed methods, based on near earth remote sensing, Remote Sensing, 14, 4, (2022); Dallaqua F., Rosa R., Schultz B., Faria L., Rodrigues T., Oliveira C., Kieser M., Malhotra V., Dwyer T., Wolfe D., Forest plantation detection through deep semantic segmentation, The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 43, pp. 77-84, (2022); Mahakalanda I., Demotte P., Perera I., Meedeniya D., Wijesuriya W., Rodrigo L., Deep learning-based prediction for stand age and land utilization of rubber plantation, Application of Machine Learning in Agriculture, pp. 131-156, (2022); Jin X., Che J., Chen Y., Weed identification using deep learning and image processing in vegetable plantation, IEEE Access, 9, pp. 10940-10950, (2021); Lee H., Ho H.W., Zhou Y., Deep learning-based monocular obstacle avoidance for unmanned aerial vehicle navigation in tree plantations, Journal of Intelligent & Robotic Systems, 101, 1, pp. 1-18, (2021); Liu X., Ghazali K.H., Han F., Mohamed I.I., Automatic detection of oil palm tree from uav images based on the deep learning method, Applied Artificial Intelligence, 35, 1, pp. 13-24, (2021); Fu L., Duan J., Zou X., Lin J., Zhao L., Li J., Yang Z., Fast and accurate detection of banana fruits in complex background orchards, IEEE Access, 8, pp. 196835-196846, (2020); Singh P., Verma A., Alex J.S.R., Disease and pest infection detection in coconut tree through deep learning techniques, Computers and electronics in agriculture, 182, (2021); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255, (2009); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in neural information processing systems, 28, (2015); Howard A.G., Some improvements on deep convolutional neural network based image classification, (2013); Chorowski J.K., Bahdanau D., Serdyuk D., Cho K., Bengio Y., Attention-based models for speech recognition, Advances in neural information processing systems, 28, (2015); Kocer B., New approaches to transfer learning, (2012); Turkdamar, Ugur M., Tasyurek M., Ozturk C., Helmet detection on the construction site with transfer-learned and non-transfer-learning deep networks, Niğde Ömer Halisdemir University Journal of Engineering Sciences, 12, 1, pp. 39-51, (2023); Skalski P., Make Sense, (2019); Paszke A., Gross S., Chintala S., Chanan G., Yang E., DeVito Z., Lin Z., Desmaison A., Antiga L., Lerer A., Automatic differentiation in pytorch, (2017); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, European conference on computer vision, pp. 740-755, (2014); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., Imagenet: A large-scale hierarchical image database, 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255, (2009)","","","Institute of Electrical and Electronics Engineers Inc.","","6th International Conference on Inventive Computation Technologies, ICICT 2023","26 April 2023 through 28 April 2023","Lalitpur","189117","","979-835039849-6","","","English","Int. Conf. Inven. Comput. Technol., ICICT - Proc.","Conference paper","Final","","Scopus","2-s2.0-85163449346"
"Seo D.; Cho B.-H.; Kim K.","Seo, Dasom (57347660200); Cho, Byeong-Hyo (57191619019); Kim, Kyoungchul (57225077819)","57347660200; 57191619019; 57225077819","Development of monitoring robot system for tomato fruits in hydroponic greenhouses","2021","Agronomy","11","11","2211","","","","20","10.3390/agronomy11112211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119475648&doi=10.3390%2fagronomy11112211&partnerID=40&md5=999c32c8f9b2eb7abb6b9710fa2fa360","Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea","Seo D., Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; Cho B.-H., Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; Kim K., Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea","Crop monitoring is highly important in terms of the efficient and stable performance of tasks such as planting, spraying, and harvesting, and for this reason, several studies are being conducted to develop and improve crop monitoring robots. In addition, the applications of deep learning algorithms are increasing in the development of agricultural robots since deep learning algorithms that use convolutional neural networks have been proven to show outstanding performance in image classification, segmentation, and object detection. However, most of these applications are focused on the development of harvesting robots, and thus, there are only a few studies that improve and develop monitoring robots through the use of deep learning. For this reason, we aimed to develop a real-time robot monitoring system for the generative growth of tomatoes. The presented method detects tomato fruits grown in hydroponic greenhouses using the Faster R-CNN (region-based convolutional neural network). In addition, we sought to select a color model that was robust to external light, and we used hue values to develop an image-based maturity standard for tomato fruits; furthermore, the developed maturity standard was verified through comparison with expert classification. Finally, the number of tomatoes was counted using a centroid-based tracking algorithm. We trained the detection model using an open dataset and tested the whole system in real-time in a hydroponic greenhouse. A total of 53 tomato fruits were used to verify the developed system, and the developed system achieved 88.6% detection accuracy when completely obscured fruits not captured by the camera were included. When excluding obscured fruits, the system’s accuracy was 90.2%. For the maturity classification, we conducted qualitative evaluations with the assistance of experts. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Hydroponic greenhouse; Maturity levels; Monitoring robot; Object detection","","","","","","Korea Smart Farm R&D Foundation; MSICT; Ministry of Agriculture, Food and Rural Affairs, MAFRA; Rural Development Administration, RDA, (421031-04); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries, iPET","Funding: This work was supported by Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) and Korea Smart Farm R&D Foundation through Smart Farm Innovation Technology Development Program, funded by MAFRA, MSICT and RDA (421031-04).","Yang D., Li H., Zhang L., Study on the fruit recognition system based on machine vision, Adv. J. Food Sci. Technol, 10, pp. 18-21, (2016); Tang Y., Chen M., Wang C., Luo L., Li J., Lian G., Zou X., Recognition and localization methods for vision-based fruit picking robots: A review, Front. Plant Sci, 11, pp. 1-17, (2020); Zhang Q., Karkee M., Tabb A., The use of agricultural robots in orchard management, Robotics and Automation for Improving Agriculture, pp. 187-214, (2019); Srinivasan N., Prabhu P., Smruthi S.S., Sivaraman N.V., Gladwin S.J., Rajavel R., Natarajan A.R., Design of an autonomous seed plating robot, Proceedings of the 2016 IEEE Region 10 Humanitarian Technology Conference (R10-HTC), pp. 1-4; Santhi P.V., Kapileswar N., Chenchela V.K.R., Prasad C.H.V.S., Sensor and vision based autonomous AGRIBOT for sowing seeds, Proceedings of the 2017 International Conference on Energy, Communication, Data Analysis and Soft Computing (ICECDS), pp. 242-245; Khuantham C., Sonthitham A., Spraying robot controlled by application smartphone for pepper farm, Proceedings of the 2020 International Conference on Power, Energy and Innovations (ICPEI), pp. 225-228; Cantelli L., Bonaccorse F., Longo D., Melita C.D., Schillaci G., Muscato G., A small versatile electrical robot for autonomous spraying in agriculture, Agric. Eng, 1, pp. 391-402, (2019); Danton A., Roux J.C., Dance B., Cariou C., Lenain R., Development of a spraying robot for precision agriculture: An edge following approach, Proceedings of the 2020 IEEE Conference on Control Technology and Applications (CCTA), pp. 267-272; Murugan K., Shankar B.J., Sumanth A., Sudharshan C.V., Reddy G.V., Smart automated pesticide spraying bot, Proceedings of the 2020 3rd International Conference on Intelligent Sustainable Systems (ICISS), pp. 864-868; Mu L., Cui G., Liu Y., Cui Y., Fu L., Gejima Y., Design and simulation of an integrated end-effector for picking kiwifruit by robot, Inf. Process. Agric, 7, pp. 58-71, (2020); Arad B., Balendonck J., Barth R., Ben-Shahar O., Edan Y., Hellstrom T., Hemming J., Kurtser P., Ringdahl O., Tielen T., Et al., Development of a sweet pepper harvesting robot, J. Field Robot, 37, pp. 1027-1039, (2020); Xiong Y., Ge Y., Grimstad L., From P.J., An autonomous strawberry-harvesting robot: Design, development, integration, and field evaluation, J. Field Robot, 37, pp. 202-224, (2020); Kuznetsova A., Maleva T., Soloviev V., Using YOLOv3 algorithm with pre-and post-processing for apple detection in fruit-harvesting robot, Agronomy, 10, (2020); Taqi F., Al-Langawi F., Abdulraheem H., El-Abd M., A cherry-tomato harvesting robot, Proceedings of the 2017 18th International Conference on Advanced Robotics (ICAR), pp. 463-468; Badeka E., Vrochidou E., Papakostas G.A., Pachidis T., Kaburlasos V.G., Harvest crate detection for grapes harvesting robot based on YOLOv3 model, Proceedings of the 2020 Fourth International Conference On Intelligent Computing in Data Sciences (ICDS), pp. 1-5; Chou W.C., Tsai W.R., Chang H.H., Lu S.Y., Lin K.F., Lin P.L., Prioritization of pesticides in crops with a semi-quantitative risk ranking method for Taiwan postmarket monitoring program, J. Food Drug Anal, 27, pp. 347-354, (2019); Ravankar A., Ravankar A.A., Watanabe M., Hoshino Y., Rawankar A., Development of a low-cost semantic monitoring system for vineyards using autonomous robots, Agriculture, 10, (2020); Kim W.S., Lee D.H., Kim Y.J., Kim T., Lee W.S., Choi C.H., Stereo-vision-based crop height estimation for agricultural robots, Comput. Electron. Agric, 181, (2021); Fernando S., Nethmi R., Silva A., Perera A., De Silva R., Abeygunawardhana P.K.W., Intelligent disease detection system for greenhouse with a robotic monitoring system, Proceedings of the 2020 2nd International Conference on Advancements in Computing (ICAC), pp. 204-209; Yoon C., Lim D., Park C., Factors affecting adoption of smart farms: The case of Korea, Comput. Hum. Behav, 108, (2020); Santos L.C., Aguiar A.S., Santos F.N., Valente A., Petry M., Occupancy grid and topological maps extraction from satellite images for path planning in agricultural robots, Robotics, 9, (2020); Moysiadis V., Sarigiannidis P., Vitsas V., Khelifi A., Smart farming in Europe, Comput. Sci. Rev, 39, (2021); Rong J., Wang P., Yang Q., Huang F., A field-tested harvesting robot for oyster mushroom in greenhouse, Agronomy, 11, (2021); Liu G., Nouaze J.C., Mbouembe P.L.T., Kim J.H., YOLO-tomato: A robust algorithm for tomato detection based on YOLOv3, Sensors, 20, (2020); Lawal M.O., Tomato detection based on modified YOLOv3 framework, Sci. Rep, 11, (2021); Afonso M., Fonteijn H., Fiorentin F.S., Lensink D., Mooij M., Faber N., Polder G., Wehrens R., Tomato fruit detection and counting in greenhouses using deep learning, Front. Plant Sci, 11, (2020); Hu C., Liu X., Pan Z., Li P., Automatic detection of single ripe tomato on plant combining Faster R-CNN and intuitionistic Fuzzy set, IEEE Access, 7, pp. 154683-154696, (2019); Iwasaki Y., Yamane A., Itoh M., Goto C., Matsumuto H., Takaichi M., Demonstration of year-round production of tomato fruits with high soluble-solids content by low node-order pinching and high-density planting, Bull. NARO Crop. Sci, 3, pp. 41-51, (2019); Alexander L., Grierson D., Ethylene biosynthesis and action in tomato: A model for climacteric fruit ripening, J. Exp. Bot, 53, pp. 2039-2055, (2002); Garcia M.B., Ambat S., Adao R.T., Tomayto, tomahto: A machine learning approach for tomato ripening stage identification using pixel-based color image classification, Proceedings of the 2019 IEEE 11th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM), pp. 1-6; Rupanagudi S.R., Ranjani B.S., Nagaraj P., Bhat V.G., A cost effective tomato maturity grading system using image processing for farmers, Proceedings of the 2014 International Conference on Contemporary Computing and Informatics (IC3I), pp. 7-12; Pacheco W.D.N., Lopez F.R.J., Tomato classification according to organoleptic maturity (coloration) using machine learning algorithms K-NN, MLP, and K-Means Clustering, Proceedings of the 2019 XXII Symposium on Image, Signal Processing and Artificial Vision (STSIVA), pp. 1-5; Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587; Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the 2016 IEEE Conference of Computer Vision and Pattern Recogniton, pp. 779-788; Ren S., He R., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detectin with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, 39, pp. 1137-1149, (2017); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single shot multibox detector, Proceedings of the European Conference on Computer Vision, pp. 21-37; He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recogniton, Proceedings of the 2016 IEEE Conference of Computer Vision and Pattern Recogniton, pp. 770-778; Make ML, Tomato Dataset, Make ML; He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision, pp. 2980-2988; Hallett S.H., Jones R.J.A., Compilation of an accumulated temperature databased for use in an environmental information system, Agric. For. Meteorol, 63, pp. 21-34, (1993); Harvest Timer; Hirsch R., Exploring Colour Photography: A Complete Guide, (2004)","B.-H. Cho; Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; email: cho2519@korea.kr; K. Kim; Department of Agricultural Engineering, National Institute of Agricultural Sciences, Jeonju, 54875, South Korea; email: kkcmole@korea.kr","","MDPI","","","","","","20734395","","","","English","Agronomy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119475648"
"Hoeser T.; Feuerstein S.; Kuenzer C.","Hoeser, Thorsten (57216967331); Feuerstein, Stefanie (57205419456); Kuenzer, Claudia (55927784300)","57216967331; 57205419456; 55927784300","DeepOWT: A global offshore wind turbine data set derived with deep learning from Sentinel-1 data","2022","Earth System Science Data","14","9","","4251","4270","19","2","10.5194/essd-14-4251-2022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140395043&doi=10.5194%2fessd-14-4251-2022&partnerID=40&md5=84a3e1cf6415a2b7fae930f60636fa42","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; Department of Remote Sensing, Institute of Geography and Geology, University of Würzburg, Würzburg, 97074, Germany","Hoeser T., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; Feuerstein S., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; Kuenzer C., German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany, Department of Remote Sensing, Institute of Geography and Geology, University of Würzburg, Würzburg, 97074, Germany","Offshore wind energy is at the advent of a massive global expansion. To investigate the development of the offshore wind energy sector, optimal offshore wind farm locations, or the impact of offshore wind farm projects, a freely accessible spatiotemporal data set of offshore wind energy infrastructure is necessary. With free and direct access to such data, it is more likely that all stakeholders who operate in marine and coastal environments will become involved in the upcoming massive expansion of offshore wind farms. To that end, we introduce the DeepOWT (Deep-learning-derived Offshore Wind Turbines) data set (available at 10.5281/zenodo.5933967, ), which provides 9941 offshore wind energy infrastructure locations along with their deployment stages on a global scale. DeepOWT is based on freely accessible Earth observation data from the Sentinel-1 radar mission. The offshore wind energy infrastructure locations were derived by applying deep-learning-based object detection with two cascading convolutional neural networks (CNNs) to search the entire Sentinel-1 archive on a global scale. The two successive CNNs have previously been optimised solely on synthetic training examples to detect the offshore wind energy infrastructures in real-world imagery. With subsequent temporal analysis of the radar signal at the detected locations, the DeepOWT data set reports the deployment stages of each infrastructure with a quarterly frequency from July 2016 until June 2021. The spatiotemporal information is compiled in a ready-to-use geographic information system (GIS) format to make the usability of the data set as accessible as possible.  © 2022 Thorsten Hoeser et al.","","","","","","","","","4C Offshore: 4C Offshorewind, (2021); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Kudlur M., Levenberg J., Monga R., Moore S., Murray D. G., Steiner B., Tucker P., Vasudevan V., Warden P., Wicke M., Yu Y., Zheng X., TensorFlow: A System for Large-Scale Machine Learning, 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265-283, (2016); Aschbacher J., ESA's earth observation strategy and Copernicus, Satellite earth observations and their impact on society and policy, pp. 81-86, (2017); Bailey H., Brookes K. L., Thompson P. M., Assessing environmental impacts of offshore wind farms: Lessons learned and recommendations for the future, Aquatic Biosystems, 10, (2014); Baumhoer C. A., Dietz A. J., Kneisel C., Kuenzer C., Automated Extraction of Antarctic Glacier and Ice Shelf Fronts from Sentinel-1 Imagery Using Deep Learning, Remote Sens, 11, (2019); Bazzi H., Ienco D., Baghdadi N., Zribi M., Demarez V., Distilling Before Refine: Spatio-Temporal Transfer Learning for Mapping Irrigated Areas Using Sentinel-1 Time Series, IEEE Geosci. Remote S, 17, pp. 1909-1913, (2020); Belenguer-Plomer M. A., Tanase M. A., Chuvieco E., Bovolo F., CNN-based burned area mapping using radar and optical data, Remote Sens. Environ, 260, (2021); Bergstr'm L., Kautsky L., Malm T., Rosenberg R., Wahlberg M., Capetillo N. A., Wilhelmsson D., Effects of offshore wind farms on marine wildlife-a generalized impact assessment, Environ. Res. Lett, 9, (2014); Boeck M., Voinov S., Keim S., Volkmann R., Langbein M., MA1/4hlbauer M., Frontend Libraries for DLR UKIS (Map) Applications, (2022); Cavazzi S., Dutton A., An Offshore Wind Energy Geographic Information System (OWE-GIS) for assessment of the UK's offshore wind energy potential, Renew. Energ, 87, pp. 212-228, (2016); COP26: Global coal to clean power transition statement, (2021); Cu La Rosa L. E., Happ P. N., Feitosa R. Q., Dense Fully Convolutional Networks for Crop Recognition from Multitemporal SAR Image Sequences, IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 7460-7463, (2018); Dirscherl M., Dietz A. J., Kneisel C., Kuenzer C., A Novel Method for Automated Supraglacial Lake Mapping in Antarctica Using Sentinel-1 SAR Imagery and Deep Learning, Remote Sens, 13, (2021); Drewitt A. L., Langston R. H. W., Assessing the impacts of wind farms on birds, Ibis, 148, pp. 29-42, (2006); Esteban M. D., Diez J. J., LApez J. S., Negro V., Why offshore wind energy?, Renew. Energ, 36, pp. 444-450, (2011); European Commission: An EU Strategy to harness the potential of offshore renewable energy for a climate neutral future, (2020); Fox A., Desholm M., Kahlert J., Christensen T. K., Krag Petersen I., Information needs to support environmental impact assessment of the effects of European marine offshore wind farms on birds, Ibis, 148, pp. 129-144, (2006); Gorelick N., Hancher M., Dixon M., Ilyushchenko S., Thau D., Moore R., Google Earth Engine: Planetary-scale geospatial analysis for everyone, Remote Sens. Environ, 202, pp. 18-27, (2017); Gusatu L. F., Yamu C., Zuidema C., Faaij A., A spatial analysis of the potentials for offshore wind farm locations in the North Sea region: Challenges and opportunities, ISPRS Int. J. Geo-Inf, 9, (2020); Gus, atu L., Menegon S., Depellegrin D., Zuidema C., Faaij A., Yamu C., Spatial and temporal analysis of cumulative environmental effects of offshore wind farms in the North Sea basin, Sci. Rep, 11, (2021); He K., Zhang X., Ren S., Sun J., Deep Residual Learning for Image Recognition, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, (2016); Henderson A. R., Morgan C., Smith B., SArensen H. C., Barthelmie R. J., Boesmans B., Offshore Wind Energy in Europe-A Review of the State-of-the-Art, Wind Energy, 6, pp. 35-52, (2003); Hoeser T., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part I: Evolution and Recent Trends, Remote Sens, 12, (2020); Hoeser T., Kuenzer C., SyntEO: Synthetic dataset generation for earth observation and deep learning-Demonstrated for offshore wind farm detection, ISPRS J. Photogramm, 189, pp. 163-184, (2022); Hoeser T., Kuenzer C., DeepOWT: A global offshore wind turbine data set, Zenodo [data set], (2022); Hoeser T., Bachofer F., Kuenzer C., Object Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review-Part II: Applications, Remote Sens, 12, (2020); Johnson A. F., Dawson C. L., Conners M. G., Locke C. C., Maxwell S. M., Offshore renewables need an experimental mindset, Science, 376, pp. 361-361, (2022); Kang M., Ji K., Leng X., Lin Z., Contextual RegionBased Convolutional Neural Network with Multilayer Fusion for SAR Ship Detection, Remote Sens, 9, (2017); Krizhevsky A., Sutskever I., Hinton G. E., ImageNet Classification with Deep Convolutional Neural Networks, pp. 1097-1105, (2012); Krizhevsky A., Sutskever I., Hinton G. E., ImageNet Classification with Deep Convolutional Neural Networks, Commun. ACM, 60, pp. 84-90, (2017); LeCun Y., Bengio Y., Hinton G., Deep Learning, Nature, 521, pp. 436-444, (2015); Loshchilov I., Hutter F., SGDR: Stochastic Gradient Descent with Warm Restarts, (2016); Ma L., Liu Y., Zhang X., Ye Y., Yin G., Johnson B. A., Deep learning in remote sensing applications: A metaanalysis and review, ISPRS J. Photogramm, 152, pp. 166-177, (2019); Majidi Nezhad M., Groppi D., Marzialetti P., Fusilli L., Laneve G., Cumo F., Garcia D. A., Wind energy potential analysis using Sentinel-1 satellite: A review and a case study on Mediterranean islands, Renew. Sust. Energ. Rev, 109, pp. 499-513, (2019); Mullissa A. G., Persello C., Tolpekin V., Fully Convolutional Networks for Multi-Temporal SAR Image Classification, IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pp. 6635-6638, (2018); Opitz J., Burst S., Macro F1 and Macro F1, (2019); Padilla R., Passos W. L., Dias T. L. B., Netto S. L., da Silva E. A. B., A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit, Electronics, 10, (2021); Reichstein M., Camps-Valls G., Stevens B., Jung M., Denzler J., Carvalhais N., Prabhat, Deep learning and process understanding for data-driven Earth system science, Nature, 566, pp. 195-204, (2019); Ren S., He K., Girshick R. B., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE T. Pattern Anal, 39, pp. 1137-1149, (2015); Rodrigues S., Restrepo C., Kontos E., Teixeira Pinto R., Bauer P., Trends of offshore wind projects, Renew. Sust. Energ. Rev, 49, pp. 1114-1135, (2015); Slavik K., Lemmen C., Zhang W., Kerimoglu O., Klingbeil K., Wirtz K. W., The large-scale impact of offshore wind farm structures on pelagic primary productivity in the southern North Sea, Hydrobiologia, 845, pp. 35-53, (2019); Torres R., Snoeij P., Geudtner D., Bibby D., Davidson M., Attema E., Potin P., Rommen B., Floury N., Brown M., Traver I. N., Deghaye P., Duesmann B., Rosich B., Miranda N., Bruno C., L'Abbate M., Croci R., Pietropaolo A., Huchler M., Rostan F., GMES Sentinel-1 mission, Remote Sens. Environ, 120, pp. 9-24, (2012); Virtanen E., Lappalainen J., Nurmi M., Viitasalo M., TikanmA¤ki M., Heinonen J., Atlaskin E., Kallasvuo M., Tikkanen H., Moilanen A., Balancing profitability of energy production, societal impacts and biodiversity in offshore wind farm design, Renew. Sust. Energ. Rev, 158, (2022); Virtanen P., Gommers R., Oliphant T. E., Haberland M., Reddy T., Cournapeau D., Burovski E., Peterson P., Weckesser W., Bright J., van der Walt S. J., Brett M., Wilson J., Millman K. J., Mayorov N., Nelson A. R. J., Jones E., Kern R., Larson E., Carey C. J., Polat E, Feng Y., Moore E. W., VanderPlas J., Laxalde D., Perktold J., Cimrman R., Henriksen I., Quintero E. A., Harris C. R., Archibald A. M., Ribeiro A. H., Pedregosa F., van Mulbregt P., SciPy 1.0 Contributors: SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python, Nature Methods, 17, pp. 261-272, (2020); Wever L., Krause G., Buck B. H., Lessons from stakeholder dialogues on marine aquaculture in offshore wind farms: Perceived potentials, constraints and research gaps, Mar. Policy, 51, pp. 251-259, (2015); Wilson J. C., Elliott M., The habitat-creation potential of offshore wind farms, Wind Energy, 12, pp. 203-212, (2009); Wong B. A., Thomas C., Halpin P., Automating offshore infrastructure extractions using synthetic aperture radar and Google Earth Engine, Remote Sens. Environ, 233, (2019); Xu W., Liu Y., Wu W., Dong Y., Lu W., Liu Y., Zhao B., Li H., Yang R., Proliferation of offshore wind farms in the North Sea and surrounding waters revealed by satellite image time series, Renew. Sust. Energ. Rev, 133, (2020); Zhang J., Wang Q., Su F., Automatic extraction of offshore platforms in single SAR images based on a dual-step-modified model, Sensors, 19, (2019); Zhang T., Tian B., Sengupta D., Zhang L., Si Y., Global offshore wind turbine dataset, Sci. Data, 8, (2021); Zhu X. X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F., Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources, IEEE Geosci. Remote S, 5, pp. 8-36, (2017); Zhu X. X., Montazeri S., Ali M., Hua Y., Wang Y., Mou L., Shi Y., Xu F., Bamler R., Deep Learning Meets SAR: Concepts, models, pitfalls, and perspectives, IEEE Geosci. Remote S, 9, pp. 143-172, (2021)","T. Hoeser; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Wessling, 82234, Germany; email: thorsten.hoeser@dlr.de","","Copernicus Publications","","","","","","18663508","","","","English","Earth Sys. Sci. Data","Data paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85140395043"
"Tugrul B.; Elfatimi E.; Eryigit R.","Tugrul, Bulent (55258504400); Elfatimi, Elhoucine (57422481000); Eryigit, Recep (6602419330)","55258504400; 57422481000; 6602419330","Convolutional Neural Networks in Detection of Plant Leaf Diseases: A Review","2022","Agriculture (Switzerland)","12","8","1192","","","","28","10.3390/agriculture12081192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140456325&doi=10.3390%2fagriculture12081192&partnerID=40&md5=700480525007781baa7ca6033ddaecd3","Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey","Tugrul B., Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey; Elfatimi E., Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey; Eryigit R., Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey","Rapid improvements in deep learning (DL) techniques have made it possible to detect and recognize objects from images. DL approaches have recently entered various agricultural and farming applications after being successfully employed in various fields. Automatic identification of plant diseases can help farmers manage their crops more effectively, resulting in higher yields. Detecting plant disease in crops using images is an intrinsically difficult task. In addition to their detection, individual species identification is necessary for applying tailored control methods. A survey of research initiatives that use convolutional neural networks (CNN), a type of DL, to address various plant disease detection concerns was undertaken in the current publication. In this work, we have reviewed 100 of the most relevant CNN articles on detecting various plant leaf diseases over the last five years. In addition, we identified and summarized several problems and solutions corresponding to the CNN used in plant leaf disease detection. Moreover, Deep convolutional neural networks (DCNN) trained on image data were the most effective method for detecting early disease detection. We expressed the benefits and drawbacks of utilizing CNN in agriculture, and we discussed the direction of future developments in plant disease detection. © 2022 by the authors.","deep learning; machine learning; plant leaf diseases","","","","","","","","Altieri M.A., Agroecology: The Science of Sustainable Agriculture, (2018); Gebbers R., Adamchuk V.I., Precision agriculture and food security, Science, 327, pp. 828-831, (2010); Carvalho F.P., Agriculture, pesticides, food security and food safety, Environ. Sci. Policy, 9, pp. 685-692, (2006); Mohanty S.P., Hughes D.P., Salathe M., Using deep learning for image-based plant disease detection, Front. Plant Sci, 7, (2016); Miller S.A., Beed F.D., Harmon C.L., Plant disease diagnostic capabilities and networks, Annu. Rev. Phytopathol, 47, pp. 15-38, (2009); LeCun Y., Bengio Y., Hinton G., Deep learning, Nature, 521, pp. 436-444, (2015); Najafabadi M.M., Villanustre F., Khoshgoftaar T.M., Seliya N., Wald R., Muharemagic E., Deep learning applications and challenges in big data analytics, J. Big Data, 2, pp. 1-21, (2015); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A., Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9; Abade A., Ferreira P.A., de Barros Vidal F., Plant diseases recognition on images using convolutional neural networks: A systematic review, Comput. Electron. Agric, 185, (2021); Dhaka V.S., Meena S.V., Rani G., Sinwar D., Ijaz M.F., Wozniak M., A survey of deep convolutional neural networks applied for prediction of plant leaf diseases, Sensors, 21, (2021); Nagaraju M., Chawla P., Systematic review of deep learning techniques in plant disease detection, Int. J. Syst. Assur. Eng. Manag, 11, pp. 547-560, (2020); Kamilaris A., Prenafeta-Boldu F.X., Deep learning in agriculture: A survey, Comput. Electron. Agric, 147, pp. 70-90, (2018); Fernandez-Quintanilla C., Pena J., Andujar D., Dorado J., Ribeiro A., Lopez-Granados F., Is the current state of the art of weed monitoring suitable for site-specific weed management in arable crops?, Weed Res, 58, pp. 259-272, (2018); Lu J., Tan L., Jiang H., Review on convolutional neural network (CNN) applied to plant leaf disease classification, Agriculture, 11, (2021); Golhani K., Balasundram S.K., Vadamalai G., Pradhan B., A review of neural networks in plant disease detection using hyperspectral data, Inf. Process. Agric, 5, pp. 354-371, (2018); Bangari S., Rachana P., Gupta N., Sudi P.S., Baniya K.K., A Survey on Disease Detection of a potato Leaf Using CNN, Proceedings of the 2nd IEEE International Conference on Artificial Intelligence and Smart Energy (ICAIS), pp. 144-149; LeCun Y., Bottou L., Bengio Y., Haffner P., Gradient-based learning applied to document recognition, Proc. IEEE, 86, pp. 2278-2324, (1998); Le Cun Y., Jackel L.D., Boser B., Denker J.S., Graf H.P., Guyon I., Henderson D., Howard R.E., Hubbard W., Handwritten digit recognition: Applications of neural network chips and automatic learning, IEEE Commun. Mag, 27, pp. 41-46, (1989); Abdel-Hamid O., Mohamed A.R., Jiang H., Deng L., Penn G., Yu D., Convolutional neural networks for speech recognition, IEEE/ACM Trans. Audio Speech Lang. Process, 22, pp. 1533-1545, (2014); Kamilaris A., Prenafeta-Boldu F.X., Disaster monitoring using unmanned aerial vehicles and deep learning, arXiv, (2018); Canziani A., Paszke A., Culurciello E., An analysis of deep neural network models for practical applications, arXiv, (2016); Schmidhuber J., Deep learning in neural networks: An overview, Neural Netw, 61, pp. 85-117, (2015); Alom M.Z., Taha T.M., Yakopcic C., Westberg S., Sidike P., Nasrin M.S., Hasan M., Van Essen B.C., Awwal A.A., Asari V.K., A state-of-the-art survey on deep learning theory and architectures, Electronics, 8, (2019); Bahrampour S., Ramakrishnan N., Schott L., Shah M., Comparative study of deep learning software frameworks, arXiv, (2015); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258; Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., Darrell T., Caffe: Convolutional architecture for fast feature embedding, Proceedings of the 22nd ACM International Conference on Multimedia, pp. 675-678; Ferentinos K.P., Deep learning models for plant disease detection and diagnosis, Comput. Electron. Agric, 145, pp. 311-318, (2018); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S., Irving G., Isard M., Et al., {TensorFlow}: A system for {Large-Scale} machine learning, Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265-283; Bastien F., Lamblin P., Pascanu R., Bergstra J., Goodfellow I., Bergeron A., Bouchard N., Warde-Farley D., Bengio Y., Theano: New features and speed improvements, arXiv, (2012); Kim P., Matlab Deep Learning: With Machine Learning, Neural Networks and Artificial Intelligence, (2017); Amara J., Bouaziz B., Algergawy A., A deep learning-based approach for banana leaf diseases classification, Proceedings of the Datenbanksysteme für Business, Technologie und Web (BTW 2017)—Workshopband; Naik B.N., Ramanathan M., Palanisamy P., Detection and classification of chilli leaf disease using a squeeze-and-excitation-based CNN model, Ecol. Inform, 69, (2022); Partel V., Kim J., Costa L., Pardalos P.M., Ampatzidis Y., Smart Sprayer for Precision Weed Control Using Artificial Intelligence: Comparison of Deep Learning Frameworks, Proceedings of the International Symposium on Artificial Intelligence and Mathematics, ISAIM 2020; Redmon J., Farhadi A., Yolov3: An incremental improvement, arXiv, (2018); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Proceedings of the Advances in Neural Information Processing Systems 28 (NIPS 2015); Redmon J., Darknet: Open Source Neural Networks in C, (2013); Binguitcha-Fare A.A., Sharma P., Crops and weeds classification using convolutional neural networks via optimization of transfer learning parameters, Int. J. Eng. Adv. Technol. (IJEAT), 8, pp. 2249-8958, (2019); Sahu P., Chug A., Singh A.P., Singh D., Singh R.P., Deep Learning Models for Beans Crop Diseases: Classification and Visualization Techniques, Int. J. Mod. Agric, 10, pp. 796-812, (2021); Mukti I.Z., Biswas D., Transfer learning based plant diseases detection using ResNet50, Proceedings of the 4th IEEE International Conference on Electrical Information and Communication Technology (EICT), pp. 1-6; Arya S., Singh R., A Comparative Study of CNN and AlexNet for Detection of Disease in Potato and Mango leaf, Proceedings of the IEEE International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT), 1, pp. 1-6; Milioto A., Lottes P., Stachniss C., Real-Time Blob-Wise Sugar Beets VS Weeds Classification for Monitoring Fields Using Convolutional Neural Networks, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 4, pp. 41-48, (2017); Lu Y., Yi S., Zeng N., Liu Y., Zhang Y., Identification of rice diseases using deep convolutional neural networks, Neurocomputing, 267, pp. 378-384, (2017); Zhang W., Hansen M.F., Volonakis T.N., Smith M., Smith L., Wilson J., Ralston G., Broadbent L., Wright G., Broad-leaf weed detection in pasture, Proceedings of the 3rd IEEE International Conference on Image, Vision and Computing (ICIVC), pp. 101-105; Liang W.C., Yang Y.J., Chao C.M., Low-cost weed identification system using drones, Proceedings of the 7th IEEE International Symposium on Computing and Networking Workshops (CANDARW), pp. 260-263; Dyrmann M., Karstoft H., Midtiby H.S., Plant species classification using deep convolutional neural network, Biosyst. Eng, 151, pp. 72-80, (2016); Chen J., Liu Q., Gao L., Visual tea leaf disease recognition using a convolutional neural network model, Symmetry, 11, (2019); Nkemelu D.K., Omeiza D., Lubalo N., Deep convolutional neural network for plant seedlings classification, arXiv, (2018); Pearlstein L., Kim M., Seto W., Convolutional neural network application to plant detection, based on synthetic imagery, Proceedings of the 2016 IEEE Applied Imagery Pattern Recognition Workshop (AIPR), pp. 1-4; Jiang Z., Dong Z., Jiang W., Yang Y., Recognition of rice leaf diseases and wheat leaf diseases based on multi-task deep transfer learning, Comput. Electron. Agric, 186, (2021); Sravan V., Swaraj K., Meenakshi K., Kora P., A deep learning based crop disease classification using transfer learning, Mater. Today Proc, (2021); Shin J., Chang Y.K., Heung B., Nguyen-Quang T., Price G.W., Al-Mallahi A., A deep learning approach for RGB image-based powdery mildew disease detection on strawberry leaves, Comput. Electron. Agric, 183, (2021); Brahimi M., Boukhalfa K., Moussaoui A., Deep learning for tomato diseases: Classification and symptoms visualization, Appl. Artif. Intell, 31, pp. 299-315, (2017); Darwish A., Ezzat D., Hassanien A.E., An optimized model based on convolutional neural networks and orthogonal learning particle swarm optimization algorithm for plant diseases diagnosis, Swarm Evol. Comput, 52, (2020); DeChant C., Wiesner-Hanks T., Chen S., Stewart E.L., Yosinski J., Gore M.A., Nelson R.J., Lipson H., Automated identification of northern leaf blight-infected maize plants from field imagery using deep learning, Phytopathology, 107, pp. 1426-1432, (2017); Sibiya M., Sumbwanyambe M., A computational procedure for the recognition and classification of maize leaf diseases out of healthy leaves using convolutional neural networks, AgriEngineering, 1, pp. 119-131, (2019); Liu B., Zhang Y., He D., Li Y., Identification of apple leaf diseases based on deep convolutional neural networks, Symmetry, 10, (2017); Nachtigall L.G., Araujo R.M., Nachtigall G.R., Classification of apple tree disorders using convolutional neural networks, Proceedings of the 28th IEEE International Conference on Tools with Artificial Intelligence (ICTAI), pp. 472-476; Yuwana R.S., Suryawati E., Zilvan V., Ramdan A., Pardede H.F., Fauziah F., Multi-condition training on deep convolutional neural networks for robust plant diseases detection, Proceedings of the 2019 IEEE International Conference on Computer, Control, Informatics and Its Applications (IC3INA), pp. 30-35; Kawasaki Y., Uga H., Kagiwada S., Iyatomi H., Basic study of automated diagnosis of viral plant diseases using convolutional neural networks, Proceedings of the International Symposium on Visual Computing, pp. 638-645, (2015); Cruz A.C., Luvisi A., De Bellis L., Ampatzidis Y., X-FIDO: An effective application for detecting olive quick decline syndrome with deep learning and data fusion, Front. Plant Sci, 8, (2017); Ha J.G., Moon H., Kwak J.T., Hassan S.I., Dang M., Lee O.N., Park H.Y., Deep convolutional neural network for classifying Fusarium wilt of radish from unmanned aerial vehicles, J. Appl. Remote Sens, 11, (2017); Dang L.M., Syed I.H., Suhyeon I., Drone agriculture imagery system for radish wilt, J. Appl. Remote Sens, 11, (2017); Liang W., Zhang H., Zhang G.F., Cao H., Rice blast disease recognition using a deep convolutional neural network, Sci. Rep, 9, pp. 1-10, (2019); Ioffe S., Szegedy C., Batch normalization: Accelerating deep network training by reducing internal covariate shift, Proceedings of the International Conference on Machine Learning, pp. 448-456; Lu J., Hu J., Zhao G., Mei F., Zhang C., An In-field Automatic Wheat Disease Diagnosis System, Comput. Electron. Agric, 142, pp. 369-379, (2017); Ramcharan A., Baranowski K., McCloskey P., Ahmed B., Legg J., Hughes D.P., Deep learning for image-based cassava disease detection, Front. Plant Sci, 8, (2017); Wang G., Sun Y., Wang J., Automatic image-based plant disease severity estimation using deep learning, Comput. Intell. Neurosci, 2017, (2017); Oppenheim D., Shani G., Potato disease classification using convolution neural networks, Adv. Anim. Biosci, 8, pp. 244-249, (2017); Durmus H., Gunes E.O., Kirci M., Disease detection on the leaves of the tomato plants by using deep learning, Proceedings of the 6th International Conference on Agro-Geoinformatics, pp. 1-5; Arivazhagan S., Ligi S.V., Mango leaf diseases identification using convolutional neural network, Int. J. Pure Appl. Math, 120, pp. 11067-11079, (2018); Rangarajan A.K., Purushothaman R., Ramesh A., Tomato crop disease classification using pre-trained deep learning algorithm, Procedia Comput. Sci, 133, pp. 1040-1047, (2018); Nandhini M., Kala K.U., Thangadarshini M., Verma S.M., Deep Learning model of sequential image classifier for crop disease detection in plantain tree cultivation, Comput. Electron. Agric, 197, (2022); Picon A., Alvarez-Gila A., Seitz M., Ortiz-Barredo A., Echazarra J., Johannes A., Deep convolutional neural networks for mobile capture device-based crop disease classification in the wild, Comput. Electron. Agric, 161, pp. 280-290, (2019); Howlader M.R., Habiba U., Faisal R.H., Rahman M.M., Automatic Recognition of Guava Leaf Diseases using Deep Convolution Neural Network, Proceedings of the 2019 International Conference on Electrical, Computer and Communication Engineering (ECCE), pp. 1-5; Singh U.P., Chouhan S.S., Jain S., Jain S., Multilayer Convolution Neural Network for the Classification of Mango Leaves Infected by Anthracnose Disease, IEEE Access, 7, pp. 43721-43729, (2019); GeethaRamani R., ArunPandian J., Identification of plant leaf diseases using a nine-layer deep convolutional neural network, Comput. Electr. Eng, 76, pp. 323-338, (2019); Fang T., Chen P., Zhang J., Wang B., Crop leaf disease grade identification based on an improved convolutional neural network, J. Electron. Imaging, 29, (2020); Mishra S., Sachan R., Rajpal D., Deep convolutional neural network based detection system for real-time corn plant disease recognition, Procedia Comput. Sci, 167, pp. 2003-2010, (2020); Mkonyi L., Rubanga D., Richard M., Zekeya N., Sawahiko S., Maiseli B., Machuve D., Early identification of Tuta absoluta in tomato plants using deep learning, Sci. Afr, 10, (2020); Karlekar A., Seal A., SoyNet: Soybean leaf diseases classification, Comput. Electron. Agric, 172, (2020); Liu B., Ding Z., Tian L., He D., Li S., Wang H., Grape Leaf Disease Identification Using Improved Deep Convolutional Neural Networks, Front. Plant Sci, 11, (2020); Ahmad J., Jan B., Farman H., Ahmad W., Ullah A., Disease Detection in Plum Using Convolutional Neural Network under True Field Conditions, Sensors, 20, (2020); Rangarajan A.K., Purushothaman R., Disease Classification in Eggplant Using Pre-trained VGG16 and MSVM, Sci. Rep, 10, (2020); Yin H., Gu Y.H., Park C.J., Park J.H., Yoo S.J., Transfer Learning-Based Search Model for Hot Pepper Diseases and Pests, Agriculture, 10, (2020); Wang F., Rao Y.L., Luo Q., Jin X., Jiang Z.H., Zhang W., Li S., Practical cucumber leaf disease recognition using improved Swin Transformer and small sample size, Comput. Electron. Agric, 199, (2022); Subetha T., Khilar R., Christo M.S., A comparative analysis on plant pathology classification using deep learning architecture—Resnet and VGG19, Mater. Today Proc, (2021); Indu V.T., Priyadharsini S.S., Crossover-based wind-driven optimized convolutional neural network model for tomato leaf disease classification, J. Plant Dis. Prot, 129, pp. 559-578, (2021); Vallabhajosyula S., Sistla V., Kolli V.K.K., Transfer learning-based deep ensemble neural network for plant leaf disease detection, J. Plant Dis. Prot, 129, pp. 545-558, (2021); Hassan S.M., Maji A.K., Jasinski M.F., Leonowicz Z., Jasinska E., Identification of Plant-Leaf Diseases Using CNN and Transfer-Learning Approach, Electronics, 10, (2021); Yadav S., Sengar N., Singh A., Singh A., Dutta M.K., Identification of disease using deep learning and evaluation of bacteriosis in peach leaf, Ecol. Inform, 61, (2021); Atila U., Ucar M., Akyol K., Ucar E., Plant leaf disease classification using EfficientNet deep learning model, Ecol. Inform, 61, (2021); Ahmad I., Hamid M., Yousaf S., Shah S.T., Ahmad M.O., Optimizing Pretrained Convolutional Neural Networks for Tomato Leaf Disease Detection, Complexity, 2020, pp. 8812019:1-8812019:6, (2020); Zhang K., Wu Q., Chen Y., Detecting soybean leaf disease from synthetic image using multi-feature fusion faster R-CNN, Comput. Electron. Agric, 183, (2021); Pandey A., Jain K., A robust deep attention dense convolutional neural network for plant leaf disease identification and classification from smart phone captured real world images, Ecol. Inform, 70, (2022); Jin H., Li Y., Qi J., Feng J., Tian D., Mu W., GrapeGAN: Unsupervised image enhancement for improved grape leaf disease recognition, Comput. Electron. Agric, 198, (2022); Javidan S.M., Banakar A., Vakilian K.A., Ampatzidis Y., Diagnosis of Grape Leaf Diseases Using Automatic K-Means Clustering and Machine Learning, SSRN Electron. J, 3, (2022); Zeng W., Li H., Hu G., Liang D., Lightweight dense-scale network (LDSNet) for corn leaf disease identification, Comput. Electron. Agric, 197, (2022); Yu H., Cheng X., Chen C., Heidari A.A., Liu J., Cai Z., Chen H., Apple leaf disease recognition method with improved residual network, Multimed. Tools Appl, 81, pp. 7759-7782, (2022); Wei K., Chen B., Zhang J., Fan S., Wu K., Liu G., Chen D., Explainable Deep Learning Study for Leaf Disease Classification, Agronomy, 12, (2022); Hanh B.T., Manh H.V., Nguyen N.V., Enhancing the performance of transferred efficientnet models in leaf image-based plant disease classification, J. Plant Dis. Prot, 129, pp. 623-634, (2022); Ravi V., Acharya V., Pham T.D., Attention deep learning-based large-scale learning classifier for Cassava leaf disease classification, Expert Syst, 39, (2022); Li X., Li S., Transformer Help CNN See Better: A Lightweight Hybrid Apple Disease Identification Model Based on Transformers, Agriculture, 12, (2022); Sun X., Li G., Qu P., Xie X., Pan X., Zhang W., Research on plant disease identification based on CNN, Cogn. Robot, 2, pp. 155-163, (2022); Jiang J., Liu H., Zhao C., He C., Ma J., Cheng T., Zhu Y., Cao W., Yao X., Evaluation of Diverse Convolutional Neural Networks and Training Strategies for Wheat Leaf Disease Identification with Field-Acquired Photographs, Remote Sens, 14, (2022); Memon M.S., Kumar P., Iqbal R., Meta Deep Learn Leaf Disease Identification Model for Cotton Crop, Computers, 11, (2022); Chen Y., Xu K., Zhou P., Ban X., He D., Improved cross entropy loss for noisy labels in vision leaf disease classification, IET Image Process, 16, pp. 1511-1519, (2022); Russel N.S., Selvaraj A., Leaf species and disease classification using multiscale parallel deep CNN architecture, Neural Comput. Appl, (2022); Gaikwad S.S., Rumma S.S., Hangarge M., Fungi affected fruit leaf disease classification using deep CNN architecture, Int. J. Inf. Technol, (2022); Prabu M., Chelliah B.J., Mango leaf disease identification and classification using a CNN architecture optimized by crossover-based levy flight distribution algorithm, Neural Comput. Appl, 34, pp. 7311-7324, (2022); Kurmi Y., Saxena P., Kirar B.S., Gangwar S., Chaurasia V., Goel A., Deep CNN model for crops’ diseases detection using leaf images, Multidimens. Syst. Signal Process, 33, pp. 981-1000, (2022); Nagi R., Tripathy S.S., Deep convolutional neural network based disease identification in grapevine leaf images, Multimed. Tools Appl, 81, pp. 24995-25006, (2022); Subramanian M., Shanmugavadivel K., Nandhini P.S., On fine-tuning deep learning models using transfer learning and hyper-parameters optimization for disease identification in maize leaves, Neural Comput. Appl, 34, pp. 13951-13968, (2022); Gajjar R., Gajjar N.P., Thakor V.J., Patel N.P., Ruparelia S., Real-time detection and identification of plant leaf diseases using convolutional neural networks on an embedded platform, Vis. Comput, 38, pp. 2923-2938, (2022); Xu Y., Kong S., Gao Z., Chen Q., Jiao Y.B., Li C., HLNet Model and Application in Crop Leaf Diseases Identification, Sustainability, 14, (2022); Singh R.K., Tiwari A., Gupta R.K., Deep transfer modeling for classification of Maize Plant Leaf Disease, Multimed. Tools Appl, 81, pp. 6051-6067, (2022); Ruth J.A., Uma R., Meenakshi A., Ramkumar P., Meta-Heuristic Based Deep Learning Model for Leaf Diseases Detection, Neural Process. Lett, (2022); Pandian J.A., Kanchanadevi K., Kumar V.D., Jasinska E., Gono R., Leonowicz Z., Jasinski M.L., A Five Convolutional Layer Deep Convolutional Neural Network for Plant Leaf Disease Detection, Electronics, 11, (2022); Pandian J.A., Kumar V.D., Geman O., Hnatiuc M., Arif M., Kanchanadevi K., Plant Disease Detection Using Deep Convolutional Neural Network, Appl. Sci, 12, (2022); Borhani Y., Khoramdel J., Najafi E., A deep learning based approach for automated plant disease classification using vision transformer, Sci. Rep, 12, (2022); Yakkundimath R., Saunshi G., Anami B.S., Palaiah S., Classification of Rice Diseases using Convolutional Neural Network Models, J. Inst. Eng. Ser. B, 103, pp. 1047-1059, (2022); Chen W., Chen J., Zeb A., Yang S., Zhang D., Mobile convolution neural network for the recognition of potato leaf disease images, Multimed. Tools Appl, 81, pp. 20797-20816, (2022); Yogeswararao G., Naresh V., Malmathanraj R., Palanisamy P., An efficient densely connected convolutional neural network for identification of plant diseases, Multimed. Tools Appl, (2022); Paymode A.S., Malode V.B., Transfer learning for multi-crop leaf disease image classification using convolutional neural networks VGG, Artif. Intell. Agric, 6, pp. 23-33, (2022); Thakur P.S., Sheorey T., Ojha A., VGG-ICNN: A Lightweight CNN model for crop disease identification, Multimed. Tools Appl, (2022); Math R.M., Dharwadkar D.N.V., Early detection and identification of grape diseases using convolutional neural networks, J. Plant Dis. Prot, 129, pp. 521-532, (2022); Kamal K., Yin Z., Wu M., Wu Z., Depthwise separable convolution architectures for plant disease classification, Comput. Electron. Agric, 165, (2019); Elfatimi E., Eryigit R., Elfatimi L., Beans Leaf Diseases Classification Using MobileNet Models, IEEE Access, 10, pp. 9471-9482, (2022); Lim S., Kim I., Kim T., Kim C., Kim S., Fast autoaugment, Adv. Neural Inf. Process. Syst, 32, pp. 6665-6675, (2019); Hendrycks D., Mu N., Cubuk E.D., Zoph B., Gilmer J., Lakshminarayanan B., Augmix: A simple data processing method to improve robustness and uncertainty, arXiv, (2019); Cubuk E.D., Zoph B., Shlens J., Le Q.V., Randaugment: Practical automated data augmentation with a reduced search space, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 702-703; Ho D., Liang E., Chen X., Stoica I., Abbeel P., Population based augmentation: Efficient learning of augmentation policy schedules, Proceedings of the International Conference on Machine Learning, pp. 2731-2741; Sladojevic S., Arsenovic M., Anderla A., Culibrk D., Stefanovic D., Deep neural networks based recognition of plant diseases by leaf image classification, Comput. Intell. Neurosci, 2016, (2016); Barbedo J.G.A., Plant disease identification from individual lesions and spots using deep learning, Biosyst. Eng, 180, pp. 96-107, (2019); Coulibaly S., Kamsu-Foguem B., Kamissoko D., Traore D., Deep neural networks with transfer learning in millet crop images, Comput. Ind, 108, pp. 115-120, (2019); Barbedo J.G.A., A review on the main challenges in automatic plant disease identification based on visible range images, Biosyst. Eng, 144, pp. 52-60, (2016); Zhang H., Tang Z., Xie Y., Gao X., Chen Q., A watershed segmentation algorithm based on an optimal marker for bubble size measurement, Measurement, 138, pp. 182-193, (2019); Barbedo J.G., Factors influencing the use of deep learning for plant disease recognition, Biosyst. Eng, 172, pp. 84-91, (2018); Taherkhani A., Cosma G., McGinnity T.M., AdaBoost-CNN: An adaptive boosting algorithm for convolutional neural networks to classify multi-class imbalanced datasets using transfer learning, Neurocomputing, 404, pp. 351-366, (2020); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Proceedings of the 26th Annual Conference on Neural Information Processing Systems 25; He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778; Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, arXiv, (2014); LeCun Y., Boser B., Denker J.S., Henderson D., Howard R.E., Hubbard W., Jackel L.D., Backpropagation applied to handwritten zip code recognition, Neural Comput, 1, pp. 541-551, (1989)","R. Eryigit; Department of Computer Engineering, Ankara University, Ankara, 06830, Turkey; email: reryigit@eng.ankara.edu.tr","","MDPI","","","","","","20770472","","","","English","Agric.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140456325"
"Zamboni P.; Junior J.M.; Silva J.A.; Miyoshi G.T.; Matsubara E.T.; Nogueira K.; Gonçalves W.N.","Zamboni, Pedro (57204799515); Junior, José Marcato (55640064500); Silva, Jonathan de Andrade (23396702400); Miyoshi, Gabriela Takahashi (57192677861); Matsubara, Edson Takashi (23393072700); Nogueira, Keiller (56385805000); Gonçalves, Wesley Nunes (23396539500)","57204799515; 55640064500; 23396702400; 57192677861; 23393072700; 56385805000; 23396539500","Benchmarking anchor-based and anchor-free state-of-the-art deep learning methods for individual tree detection in rgb high-resolution images","2021","Remote Sensing","13","13","2482","","","","16","10.3390/rs13132482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109397264&doi=10.3390%2frs13132482&partnerID=40&md5=73bc26bba41674be78d58353790c0236","Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, 19060-900, Brazil; Computing Science and Mathematics Division, Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, United Kingdom","Zamboni P., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Junior J.M., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Silva J.A., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Miyoshi G.T., Department of Cartography, São Paulo State University (UNESP), Presidente Prudente, 19060-900, Brazil; Matsubara E.T., Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; Nogueira K., Computing Science and Mathematics Division, Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, United Kingdom; Gonçalves W.N., Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil, Faculty of Computer Science, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil","Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 × 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Object detection; Remote sensing","Antennas; Climate change; Forestry; Geographical distribution; Information management; Learning systems; Population statistics; Remote sensing; Urban growth; Ground sample distances; High resolution image; High-resolution aerial images; Individual tree detections; Population growth; Remote sensing applications; Statistically significant difference; Urban environments; Deep learning","","","","","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES, (59/300.066/2015, 88881.311850/2018-01); Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, (303559/2019-5, 304052/2019-1, 433783/2018-4); Universidade Federal de Mato Grosso do Sul, UFMS","Funding text 1: This research was partially funded by CNPq (p: 433783/2018-4, 303559/2019-5 and 304052/2019-1), CAPES Print (p: 88881.311850/2018-01) and Fundect (p: 59/300.066/2015).The authors acknowledge the support of the UFMS (Federal University of Mato Grosso do Sul) and CAPES (Finance Code 001).; Funding text 2: Acknowledgments: The authors acknowledge the support of the UFMS (Federal University of Mato Grosso do Sul) and CAPES (Finance Code 001).","McDonald R.I., Mansur A.V., Ascensao F., Colbert M., Crossman K., Elmqvist T., Gonzalez A., Guneralp B., Haase D., Hamann M., Et al., Research gaps in knowledge of the impact of urban growth on biodiversity, Nat. Sustain, 3, pp. 16-24, (2020); Ke J., Zhang J., Tang M., Does city air pollution affect the attitudes of working residents on work, government, and the city? An examination of a multi-level model with subjective well-being as a mediator, J. Clean. Prod, 265, (2021); Khomenko S., Cirach M., Pereira-Barboza E., Mueller N., Barrera-Gomez J., Rojas-Rueda D., de Hoogh K., Hoek G., Nieuwenhuijsen M., Premature mortality due to air pollution in European cities: A health impact assessment, Lancet Planet. Health, (2021); Abass K., Buor D., Afriyie K., Dumedah G., Segbefi A.Y., Guodaar L., Garsonu E.K., Adu-Gyamfi S., Forkuor D., Ofosu A., Et al., Urban sprawl and green space depletion: Implications for flood incidence in Kumasi, Ghana, Int. J. Disaster Risk Reduct, 51, (2020); (2015); Li H., Zhang S., Qian Z., Xie X.H., Luo Y., Han R., Hou J., Wang C., McMillin S.E., Wu S., Et al., Short-term effects of air pollution on cause-specific mental disorders in three subtropical Chinese cities, Environ. Res, 191, (2020); Heinz A., Deserno L., Reininghaus U., Urbanicity, social adversity and psychosis, World Psychiatry, 12, pp. 187-197, (2013); Summary for Policymakers. Climate Change 2013: The Physical Science Basis Contribution of Working Group I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change, (2013); Managing the Risks of Extreme Events and Disasters to Advance Climate Change Adaptation, (2012); Fasihi H., Parizadi T., Analysis of spatial equity and access to urban parks in Ilam, Iran, J. Environ. Manag, 15, (2020); U.N. Transforming Our World: The 2030 Agenda for Sustainable Development, (2015); Roy S., Byrne J., Pickering C., A systematic quantitative review of urban tree benefits, costs, and assessment methods across cities in different climatic zones, Urban For. Urban Green, 11, pp. 351-363, (2012); Endreny T.A., Strategically growing the urban forest will improve our world, Nat. Commun, 9, (2018); Fassnacht F.E., Latifi H., Sterenczak K., Modzelewska A., Lefsky M., Waser L.T., Straub C., Ghosh A., Review of studies on tree species classification from remotely sensed data, Remote Sens. Environ, 186, pp. 64-77, (2016); Padayaahce A., Irlich U., Faulklner K., Gaertner M., Proches S., Wilson J., Rouget M., How do invasive species travel to and through urban environments?, Biol. Invasions, 19, pp. 3557-3570, (2017); Nielsen A., Ostberg J., Delshammar T., Review of Urban Tree Inventory Methods Used to Collect Data at Single-Tree Level, Arboric. E Urban For, 40, pp. 96-111, (2014); Wagner F., Ferreira M., Sanchez A., Hirye M., Zortea M., Glorr E., ans Carlos Souza Filho O.P., Shimabukuro Y., Aragao L., Individual tree crown delineation in a highly diverse tropical forest using very high resolution satellite images, ISPRS J. Photogramm. Remote Sens, 145, pp. 362-377, (2018); Weinstein B.G., Marconi S., Bohlman S., Zare A., White E., Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks, Remote Sens, 11, (2019); dos Santos A.A., Junior J.M., Araujo M.S., Martini D.R.D., Tetila E.C., Siqueira H.L., Aoki C., Eltner A., Matsubara E.T., Pistori H., Et al., Assessment of CNN-Based Methods for Individual Tree Detection on Images Captured by RGB Cameras Attached to UAVs, Sensors, 19, (2019); Torres D.L., Feitosa R.Q., Happ P.N., Rosa L.E.C.L., Junior J.M., Martins J., Bressan P.O., Goncalves W.N., Liesenberg V., Applying Fully Convolutional Architectures for Semantic Segmentation of a Single Tree Species in Urban Environment on High Resolution UAV Optical Imagery, Sensors, 20, (2020); Osco L.P., dos Santos de Arruda M., Goncalves D.N., Dias A., Batistoti J., de Souza M., Gomes F.D.G., Ramos A.P.M., Jorge L.A.C., Liesenberg V., Et al., A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery, ISPRS J. Photogramm. Remote Sens, 174, pp. 1-17, (2021); Biffi L.J., Mitishita E., Liesenberg V., dos Santos A.A., Goncalves D.N., Estrabis N.V., de Andrade Silva J., Osco L.P., Ramos A.P.M., Centeno J.A.S., Et al., ATSS Deep Learning-Based Approach to Detect Apple Fruits, Remote Sens, 13, (2021); Gomes M., Silva J., Goncalves D., Zamboni P., Perez J., Batista E., Ramos A., Osco L., Matsubara E., Li J., Et al., Mapping Utility Poles in Aerial Orthoimages Using ATSS Deep Learning Method, Sensors, 20, (2020); Santos A., Junior J.M., de Andrade Silva J., Pereira R., Matos D., Menezes G., Higa L., Eltner A., Ramos A.P., Osco L., Et al., Storm-Drain and Manhole Detection Using the RetinaNet Method, Sensors, 20, (2020); Li K., Wan G., Cheng G., Meng L., Han J., Object detection in optical remote sensing images: A survey and a new benchmark, ISPRS J. Photogramm. Remote Sens, 159, pp. 296-307, (2020); Courtrai L., Pham M.T., Lefevre S., Small Object Detection in Remote Sensing Images Based on Super-Resolution with Auxiliary Generative Adversarial Networks, Remote Sens, 12, (2020); Lu X., Li Q., Li B., Yan J., MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection, (2020); Licheng J., Fan Z., Fang L., Shuyuan Y., Lingling L., Zhixi F., Rong Q., A Survey of Deep Learning-Based Object Detection, IEEE Access, 7, pp. 128837-128868, (2019); Zhang S., Chi C., Yao Y., Lei Z., Li S.Z., Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection, (2019); Chen X., Jiang K., Zhu Y., Wang X., Yun T., Individual Tree Crown Segmentation Directly from UAV-Borne LiDAR Data Using the PointNet of Deep Learning, Forests, 12, (2021); Miyoshi G.T., dos Santos Arruda M., Osco L.P., Junior J.M., Goncalves D.N., Imai N.N., Tommaselli A.M.G., Honkavaara E., Goncalves W.N., A Novel Deep Learning Method to Identify Single Tree Species in UAV-Based Hyperspectral Images, Remote Sens, 12, (2020); Ampatzidis Y., Partel V., Meyering B., Albrecht U., Citrus rootstock evaluation utilizing UAV-based remote sensing and artificial intelligence, Comput. Electron. Agric, 164, (2019); Ampatzidis Y., Partel V., UAV-Based High Throughput Phenotyping in Citrus Utilizing Multispectral Imaging and Artificial Intelligence, Remote Sens, 11, (2019); Hartling S., Sagan V., Sidike P., Maimaitijiang M., Carron J., Urban Tree Species Classification Using a WorldView-2/3 and LiDAR Data Fusion Approach and Deep Learning, Sensors, 19, (2019); Csillik O., Cherbini J., Johnson R., Lyons A., Kelly M., Identification of Citrus Trees from Unmanned Aerial Vehicle Imagery Using Convolutional Neural Networks, Drones, 2, (2018); Li W., Fu H., Yu L., Cracknell A., Deep Learning Based Oil Palm Tree Detection and Counting for High-Resolution Remote Sensing Images, Remote Sens, 9, (2017); Nezami S., Khoramshahi E., Nevalainen O., Polonen I., Honkavaara E., Tree Species Classification of Drone Hyperspectral and RGB Imagery with Deep Learning Convolutional Neural Networks, Remote Sens, 12, (2020); Plesoianu A.I., Stupariu M.S., Sandric I., Patru-Stupariu I., Dragut L., Individual Tree-Crown Detection and Species Classification in Very High-Resolution Remote Sensing Imagery Using a Deep Learning Ensemble Model, Remote Sens, 12, (2020); Culman M., Delalieux S., Tricht K.V., Individual Palm Tree Detection Using Deep Learning on RGB Imagery to Support Tree Inventory, Remote Sens, 12, (2020); Oh S., Chang A., Ashapure A., Jung J., Dube N., Maeda M., Gonzalez D., Landivar J., Plant Counting of Cotton from UAS Imagery Using Deep Learning-Based Object Detection Framework, Remote Sens, 12, (2020); Roslan Z., Long Z.A., Ismail R., Individual Tree Crown Detection using GAN and RetinaNet on Tropical Forest, Proceedings of the 2021 15th International Conference on Ubiquitous Information Management and Communication (IMCOM), pp. 1-7; Roslan Z., Awang Z., Husen M.N., Ismail R., Hamzah R., Deep Learning for Tree Crown Detection In Tropical Forest, Proceedings of the 2020 14th International Conference on Ubiquitous Information Management and Communication (IMCOM), pp. 1-7; Population Census, (2010); (2010); Chen K., Wang J., Pang J., Cao Y., Xiong Y., Li X., Sun S., Feng W., Liu Z., Xu J., Et al., MMDetection: Open MMLab Detection Toolbox and Benchmark, (2019); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, IEEE Trans. Pattern Anal. Mach. Intell, (2017); Lin T.Y., Goyal P., Girshick R., He K., Dollar P., Focal loss for dense object detection, Proceedings of the IEEE International Conference on Computer Vision, (2017); Micikevicius P., Narang S., Alben J., Diamos G., Elsen E., Garcia D., Ginsburg B., Houston M., Kuchaiev O., Venkatesh G., Mixed precision training, (2017); Zhu X., Hu H., Lin S., Dai J., Deformable ConvNets v2: More Deformable, Better Results, (2018); Redmon J., Farhadi A., YOLOv3: An Incremental Improvement, (2018); Qiao S., Wang H., Liu C., Shen W., Yuille A., Weight Standardization, (2019); Wang J., Chen K., Xu R., Liu Z., Loy C.C., Lin D., CARAFE: Content-Aware ReAssembly of FEatures, Proceedings of the The IEEE International Conference on Computer Vision (ICCV), (2019); Zhu C., He Y., Savvides M., Feature Selective Anchor-Free Module for Single-Shot Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 840-849; Ghiasi G., Lin T.Y., Le Q.V., Nas-fpn: Learning scalable feature pyramid architecture for object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7036-7045; Kong T., Sun F., Liu H., Jiang Y., Shi J., FoveaBox: Beyond Anchor-based Object Detector, (2019); Wu Y., Chen Y., Yuan L., Liu Z., Wang L., Li H., Fu Y., Rethinking Classification and Localization for Object Detection, (2019); Li B., Liu Y., Wang X., Gradient Harmonized Single-stage Detector, Proceedings of the AAAI Conference on Artificial Intelligence, (2019); Zhu X., Cheng D., Zhang Z., Lin S., Dai J., An Empirical Study of Spatial Attention Mechanisms in Deep Networks, (2019); Qiao S., Chen L.C., Yuille A., DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution, (2020); Zhang H., Wang Y., Dayoub F., Sunderhauf N., VarifocalNet: An IoU-aware Dense Object Detector, (2020); Wang J., Zhang W., Cao Y., Chen K., Pang J., Gong T., Shi J., Loy C.C., Lin D., Side-Aware Boundary Localization for More Precise Object Detection; ECCV 2020, 12349, (2020); Li X., Wang W., Wu L., Chen S., Hu X., Li J., Tang J., Yang J., Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection; Kim K., Lee H.S., Probabilistic Anchor Assignment with IoU Prediction for Object Detection; ECCV 2020, 12370, (2020); Zhang H., Chang H., Ma B., Wang N., Chen X., Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training, (2020); Wu J., Yang G., Yang H., Zhu Y., Li Z., Lei L., Zhao C., Extracting apple tree crown information from remote imagery using deep learning, Comput. Electron. Agric, 174, (2020); Lumnitz S., Devisscher T., Mayaud J., Radic V., Coops N., Griess V., Mapping trees along urban street networks with deep learning and street-level imagery, ISPRS J. Photogramm. Remote Sens, 175, pp. 144-157, (2021); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), (2017)","P. Zamboni; Faculty of Engineering, Architecture and Urbanism and Geography, Federal University of Mato Grosso do Sul, Campo Grande, 79070-900, Brazil; email: pedro.zamboni@ufms.br","","MDPI AG","","","","","","20724292","","","","English","Remote Sens.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85109397264"
